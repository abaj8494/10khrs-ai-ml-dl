
#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################
 
2025-01-27 18:35:07.139815: do_dummy_2d_data_aug: False 
2025-01-27 18:35:07.145179: Using splits from existing split file: /srv/scratch/z5362216/kits19/nnUNet_db/nnUNet_preprocessed/Dataset001_Kits19/splits_final.json 
2025-01-27 18:35:07.148715: The split file contains 5 splits. 
2025-01-27 18:35:07.151497: Desired fold for training: 3 
2025-01-27 18:35:07.153992: This split has 80 training and 20 validation cases. 
2025-01-27 18:35:09.476604: Using torch.compile... 

This is the configuration used by this training:
Configuration name: 3d_lowres
 {'data_identifier': 'nnUNetPlans_3d_lowres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 2, 'patch_size': [128, 128, 128], 'median_image_size_in_voxels': [200, 205, 205], 'spacing': [1.9849520718478983, 1.9849270710444444, 1.9849270710444444], 'normalization_schemes': ['CTNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': False, 'next_stage': '3d_cascade_fullres'} 
 
These are the global plan.json settings:
 {'dataset_name': 'Dataset001_Kits19', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [3.0, 0.7939453125, 0.7939453125], 'original_median_shape_after_transp': [104, 512, 512], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [2, 0, 1], 'transpose_backward': [1, 2, 0], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 2553.0, 'mean': 104.46720886230469, 'median': 104.0, 'min': -277.0, 'percentile_00_5': -73.0, 'percentile_99_5': 292.0, 'std': 74.68063354492188}}} 
 
2025-01-27 18:35:12.640973: unpacking dataset... 
2025-01-27 18:35:18.187026: unpacking done... 
2025-01-27 18:35:18.213282: 
printing the network instead:
 
2025-01-27 18:35:18.216018: OptimizedModule(
  (_orig_mod): PlainConvUNet(
    (encoder): PlainConvEncoder(
      (stages): Sequential(
        (0): Sequential(
          (0): StackedConvBlocks(
            (convs): Sequential(
              (0): ConvDropoutNormReLU(
                (conv): Conv3d(1, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(1, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (1): ConvDropoutNormReLU(
                (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
            )
          )
        )
        (1): Sequential(
          (0): StackedConvBlocks(
            (convs): Sequential(
              (0): ConvDropoutNormReLU(
                (conv): Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                  (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (1): ConvDropoutNormReLU(
                (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
            )
          )
        )
        (2): Sequential(
          (0): StackedConvBlocks(
            (convs): Sequential(
              (0): ConvDropoutNormReLU(
                (conv): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                  (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (1): ConvDropoutNormReLU(
                (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
            )
          )
        )
        (3): Sequential(
          (0): StackedConvBlocks(
            (convs): Sequential(
              (0): ConvDropoutNormReLU(
                (conv): Conv3d(128, 256, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                (norm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(128, 256, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                  (1): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (1): ConvDropoutNormReLU(
                (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
            )
          )
        )
        (4): Sequential(
          (0): StackedConvBlocks(
            (convs): Sequential(
              (0): ConvDropoutNormReLU(
                (conv): Conv3d(256, 320, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                (norm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(256, 320, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                  (1): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (1): ConvDropoutNormReLU(
                (conv): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
            )
          )
        )
        (5): Sequential(
          (0): StackedConvBlocks(
            (convs): Sequential(
              (0): ConvDropoutNormReLU(
                (conv): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                (norm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                  (1): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (1): ConvDropoutNormReLU(
                (conv): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
            )
          )
        )
      )
    )
    (decoder): UNetDecoder(
      (encoder): PlainConvEncoder(
        (stages): Sequential(
          (0): Sequential(
            (0): StackedConvBlocks(
              (convs): Sequential(
                (0): ConvDropoutNormReLU(
                  (conv): Conv3d(1, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                  (all_modules): Sequential(
                    (0): Conv3d(1, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                    (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                    (2): LeakyReLU(negative_slope=0.01, inplace=True)
                  )
                )
                (1): ConvDropoutNormReLU(
                  (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                  (all_modules): Sequential(
                    (0): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                    (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                    (2): LeakyReLU(negative_slope=0.01, inplace=True)
                  )
                )
              )
            )
          )
          (1): Sequential(
            (0): StackedConvBlocks(
              (convs): Sequential(
                (0): ConvDropoutNormReLU(
                  (conv): Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                  (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                  (all_modules): Sequential(
                    (0): Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                    (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                    (2): LeakyReLU(negative_slope=0.01, inplace=True)
                  )
                )
                (1): ConvDropoutNormReLU(
                  (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                  (all_modules): Sequential(
                    (0): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                    (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                    (2): LeakyReLU(negative_slope=0.01, inplace=True)
                  )
                )
              )
            )
          )
          (2): Sequential(
            (0): StackedConvBlocks(
              (convs): Sequential(
                (0): ConvDropoutNormReLU(
                  (conv): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                  (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                  (all_modules): Sequential(
                    (0): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                    (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                    (2): LeakyReLU(negative_slope=0.01, inplace=True)
                  )
                )
                (1): ConvDropoutNormReLU(
                  (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                  (all_modules): Sequential(
                    (0): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                    (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                    (2): LeakyReLU(negative_slope=0.01, inplace=True)
                  )
                )
              )
            )
          )
          (3): Sequential(
            (0): StackedConvBlocks(
              (convs): Sequential(
                (0): ConvDropoutNormReLU(
                  (conv): Conv3d(128, 256, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                  (norm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                  (all_modules): Sequential(
                    (0): Conv3d(128, 256, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                    (1): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                    (2): LeakyReLU(negative_slope=0.01, inplace=True)
                  )
                )
                (1): ConvDropoutNormReLU(
                  (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (norm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                  (all_modules): Sequential(
                    (0): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                    (1): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                    (2): LeakyReLU(negative_slope=0.01, inplace=True)
                  )
                )
              )
            )
          )
          (4): Sequential(
            (0): StackedConvBlocks(
              (convs): Sequential(
                (0): ConvDropoutNormReLU(
                  (conv): Conv3d(256, 320, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                  (norm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                  (all_modules): Sequential(
                    (0): Conv3d(256, 320, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                    (1): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                    (2): LeakyReLU(negative_slope=0.01, inplace=True)
                  )
                )
                (1): ConvDropoutNormReLU(
                  (conv): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (norm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                  (all_modules): Sequential(
                    (0): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                    (1): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                    (2): LeakyReLU(negative_slope=0.01, inplace=True)
                  )
                )
              )
            )
          )
          (5): Sequential(
            (0): StackedConvBlocks(
              (convs): Sequential(
                (0): ConvDropoutNormReLU(
                  (conv): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                  (norm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                  (all_modules): Sequential(
                    (0): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                    (1): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                    (2): LeakyReLU(negative_slope=0.01, inplace=True)
                  )
                )
                (1): ConvDropoutNormReLU(
                  (conv): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (norm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                  (all_modules): Sequential(
                    (0): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                    (1): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                    (2): LeakyReLU(negative_slope=0.01, inplace=True)
                  )
                )
              )
            )
          )
        )
      )
      (stages): ModuleList(
        (0): StackedConvBlocks(
          (convs): Sequential(
            (0): ConvDropoutNormReLU(
              (conv): Conv3d(640, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
              (norm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(640, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (1): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (1): ConvDropoutNormReLU(
              (conv): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
              (norm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (1): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
          )
        )
        (1): StackedConvBlocks(
          (convs): Sequential(
            (0): ConvDropoutNormReLU(
              (conv): Conv3d(512, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
              (norm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(512, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (1): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (1): ConvDropoutNormReLU(
              (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
              (norm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (1): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
          )
        )
        (2): StackedConvBlocks(
          (convs): Sequential(
            (0): ConvDropoutNormReLU(
              (conv): Conv3d(256, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
              (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(256, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (1): ConvDropoutNormReLU(
              (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
              (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
          )
        )
        (3): StackedConvBlocks(
          (convs): Sequential(
            (0): ConvDropoutNormReLU(
              (conv): Conv3d(128, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
              (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(128, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (1): ConvDropoutNormReLU(
              (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
              (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
          )
        )
        (4): StackedConvBlocks(
          (convs): Sequential(
            (0): ConvDropoutNormReLU(
              (conv): Conv3d(64, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
              (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(64, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (1): ConvDropoutNormReLU(
              (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
              (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
          )
        )
      )
      (transpconvs): ModuleList(
        (0): ConvTranspose3d(320, 320, kernel_size=(2, 2, 2), stride=(2, 2, 2))
        (1): ConvTranspose3d(320, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2))
        (2): ConvTranspose3d(256, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2))
        (3): ConvTranspose3d(128, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2))
        (4): ConvTranspose3d(64, 32, kernel_size=(2, 2, 2), stride=(2, 2, 2))
      )
      (seg_layers): ModuleList(
        (0): Conv3d(320, 3, kernel_size=(1, 1, 1), stride=(1, 1, 1))
        (1): Conv3d(256, 3, kernel_size=(1, 1, 1), stride=(1, 1, 1))
        (2): Conv3d(128, 3, kernel_size=(1, 1, 1), stride=(1, 1, 1))
        (3): Conv3d(64, 3, kernel_size=(1, 1, 1), stride=(1, 1, 1))
        (4): Conv3d(32, 3, kernel_size=(1, 1, 1), stride=(1, 1, 1))
      )
    )
  )
) 
2025-01-27 18:35:18.223541: 
 
2025-01-27 18:35:18.226281: Unable to plot network architecture: nnUNet_compile is enabled! 
2025-01-27 18:35:18.243400:  
2025-01-27 18:35:18.246258: Epoch 0 
2025-01-27 18:35:18.249487: Current learning rate: 0.01 
2025-01-27 18:36:30.947095: train_loss 0.0496 
2025-01-27 18:36:30.974616: val_loss -0.1113 
2025-01-27 18:36:30.977253: Pseudo dice [np.float32(0.1063), np.float32(0.0)] 
2025-01-27 18:36:30.980035: Epoch time: 72.7 s 
2025-01-27 18:36:30.982875: Yayy! New best EMA pseudo Dice: 0.05310000106692314 
2025-01-27 18:36:32.471074:  
2025-01-27 18:36:32.473658: Epoch 1 
2025-01-27 18:36:32.476346: Current learning rate: 0.00999 
2025-01-27 18:37:20.218462: train_loss -0.2656 
2025-01-27 18:37:20.223155: val_loss -0.2738 
2025-01-27 18:37:20.225978: Pseudo dice [np.float32(0.7428), np.float32(0.0)] 
2025-01-27 18:37:20.228563: Epoch time: 47.75 s 
2025-01-27 18:37:20.230977: Yayy! New best EMA pseudo Dice: 0.08500000089406967 
2025-01-27 18:37:21.871959:  
2025-01-27 18:37:21.875147: Epoch 2 
2025-01-27 18:37:21.877752: Current learning rate: 0.00998 
2025-01-27 18:38:09.882503: train_loss -0.3379 
2025-01-27 18:38:09.888976: val_loss -0.3802 
2025-01-27 18:38:09.891620: Pseudo dice [np.float32(0.8258), np.float32(0.1669)] 
2025-01-27 18:38:09.894117: Epoch time: 48.01 s 
2025-01-27 18:38:09.896615: Yayy! New best EMA pseudo Dice: 0.12610000371932983 
2025-01-27 18:38:11.588765:  
2025-01-27 18:38:11.592896: Epoch 3 
2025-01-27 18:38:11.595693: Current learning rate: 0.00997 
2025-01-27 18:38:59.554063: train_loss -0.4081 
2025-01-27 18:38:59.558725: val_loss -0.4253 
2025-01-27 18:38:59.561682: Pseudo dice [np.float32(0.8563), np.float32(0.2937)] 
2025-01-27 18:38:59.564640: Epoch time: 47.97 s 
2025-01-27 18:38:59.567380: Yayy! New best EMA pseudo Dice: 0.17100000381469727 
2025-01-27 18:39:01.229885:  
2025-01-27 18:39:01.232575: Epoch 4 
2025-01-27 18:39:01.235289: Current learning rate: 0.00996 
2025-01-27 18:39:49.421497: train_loss -0.4259 
2025-01-27 18:39:49.426746: val_loss -0.3891 
2025-01-27 18:39:49.429567: Pseudo dice [np.float32(0.81), np.float32(0.331)] 
2025-01-27 18:39:49.432099: Epoch time: 48.19 s 
2025-01-27 18:39:49.434325: Yayy! New best EMA pseudo Dice: 0.21089999377727509 
2025-01-27 18:39:51.156343:  
2025-01-27 18:39:51.159206: Epoch 5 
2025-01-27 18:39:51.162332: Current learning rate: 0.00995 
2025-01-27 18:40:39.109068: train_loss -0.4469 
2025-01-27 18:40:39.113048: val_loss -0.4392 
2025-01-27 18:40:39.115917: Pseudo dice [np.float32(0.8544), np.float32(0.4884)] 
2025-01-27 18:40:39.118652: Epoch time: 47.95 s 
2025-01-27 18:40:39.121188: Yayy! New best EMA pseudo Dice: 0.25699999928474426 
2025-01-27 18:40:40.819500:  
2025-01-27 18:40:40.822366: Epoch 6 
2025-01-27 18:40:40.825116: Current learning rate: 0.00995 
2025-01-27 18:41:28.440267: train_loss -0.4708 
2025-01-27 18:41:28.446005: val_loss -0.4703 
2025-01-27 18:41:28.448632: Pseudo dice [np.float32(0.8642), np.float32(0.4756)] 
2025-01-27 18:41:28.451090: Epoch time: 47.62 s 
2025-01-27 18:41:28.453512: Yayy! New best EMA pseudo Dice: 0.29829999804496765 
2025-01-27 18:41:30.114120:  
2025-01-27 18:41:30.116977: Epoch 7 
2025-01-27 18:41:30.119956: Current learning rate: 0.00994 
2025-01-27 18:42:17.973894: train_loss -0.5006 
2025-01-27 18:42:17.978557: val_loss -0.5299 
2025-01-27 18:42:17.981580: Pseudo dice [np.float32(0.8544), np.float32(0.7423)] 
2025-01-27 18:42:17.984477: Epoch time: 47.86 s 
2025-01-27 18:42:17.987079: Yayy! New best EMA pseudo Dice: 0.3483000099658966 
2025-01-27 18:42:19.729237:  
2025-01-27 18:42:19.732061: Epoch 8 
2025-01-27 18:42:19.734831: Current learning rate: 0.00993 
2025-01-27 18:43:07.618437: train_loss -0.5165 
2025-01-27 18:43:07.624135: val_loss -0.5036 
2025-01-27 18:43:07.627018: Pseudo dice [np.float32(0.8525), np.float32(0.6978)] 
2025-01-27 18:43:07.629452: Epoch time: 47.89 s 
2025-01-27 18:43:07.631622: Yayy! New best EMA pseudo Dice: 0.39100000262260437 
2025-01-27 18:43:09.620588:  
2025-01-27 18:43:09.623384: Epoch 9 
2025-01-27 18:43:09.626100: Current learning rate: 0.00992 
2025-01-27 18:43:57.554316: train_loss -0.5397 
2025-01-27 18:43:57.558613: val_loss -0.4785 
2025-01-27 18:43:57.561364: Pseudo dice [np.float32(0.8637), np.float32(0.4747)] 
2025-01-27 18:43:57.564009: Epoch time: 47.93 s 
2025-01-27 18:43:57.566752: Yayy! New best EMA pseudo Dice: 0.4187999963760376 
2025-01-27 18:43:59.228701:  
2025-01-27 18:43:59.232328: Epoch 10 
2025-01-27 18:43:59.235829: Current learning rate: 0.00991 
2025-01-27 18:44:47.333968: train_loss -0.5416 
2025-01-27 18:44:47.340459: val_loss -0.5532 
2025-01-27 18:44:47.343214: Pseudo dice [np.float32(0.8644), np.float32(0.7304)] 
2025-01-27 18:44:47.345979: Epoch time: 48.11 s 
2025-01-27 18:44:47.348848: Yayy! New best EMA pseudo Dice: 0.45669999718666077 
2025-01-27 18:44:49.080228:  
2025-01-27 18:44:49.083565: Epoch 11 
2025-01-27 18:44:49.086410: Current learning rate: 0.0099 
2025-01-27 18:45:37.035512: train_loss -0.5411 
2025-01-27 18:45:37.040371: val_loss -0.5383 
2025-01-27 18:45:37.043109: Pseudo dice [np.float32(0.8954), np.float32(0.6145)] 
2025-01-27 18:45:37.045998: Epoch time: 47.96 s 
2025-01-27 18:45:37.048587: Yayy! New best EMA pseudo Dice: 0.48649999499320984 
2025-01-27 18:45:38.684933:  
2025-01-27 18:45:38.688112: Epoch 12 
2025-01-27 18:45:38.691016: Current learning rate: 0.00989 
2025-01-27 18:46:27.087173: train_loss -0.5837 
2025-01-27 18:46:27.093235: val_loss -0.5719 
2025-01-27 18:46:27.096208: Pseudo dice [np.float32(0.8846), np.float32(0.7383)] 
2025-01-27 18:46:27.099300: Epoch time: 48.4 s 
2025-01-27 18:46:27.101903: Yayy! New best EMA pseudo Dice: 0.5189999938011169 
2025-01-27 18:46:28.729488:  
2025-01-27 18:46:28.732933: Epoch 13 
2025-01-27 18:46:28.735627: Current learning rate: 0.00988 
2025-01-27 18:47:16.972418: train_loss -0.6026 
2025-01-27 18:47:16.977634: val_loss -0.5721 
2025-01-27 18:47:16.980467: Pseudo dice [np.float32(0.9065), np.float32(0.6615)] 
2025-01-27 18:47:16.983072: Epoch time: 48.24 s 
2025-01-27 18:47:16.985920: Yayy! New best EMA pseudo Dice: 0.5454999804496765 
2025-01-27 18:47:18.650624:  
2025-01-27 18:47:18.653768: Epoch 14 
2025-01-27 18:47:18.656636: Current learning rate: 0.00987 
2025-01-27 18:48:06.833865: train_loss -0.5836 
2025-01-27 18:48:06.839517: val_loss -0.5721 
2025-01-27 18:48:06.842419: Pseudo dice [np.float32(0.8618), np.float32(0.7251)] 
2025-01-27 18:48:06.844974: Epoch time: 48.18 s 
2025-01-27 18:48:06.847483: Yayy! New best EMA pseudo Dice: 0.5702999830245972 
2025-01-27 18:48:08.456771:  
2025-01-27 18:48:08.459805: Epoch 15 
2025-01-27 18:48:08.462601: Current learning rate: 0.00986 
2025-01-27 18:48:56.679847: train_loss -0.6035 
2025-01-27 18:48:56.684639: val_loss -0.6146 
2025-01-27 18:48:56.687553: Pseudo dice [np.float32(0.9072), np.float32(0.7712)] 
2025-01-27 18:48:56.690737: Epoch time: 48.22 s 
2025-01-27 18:48:56.693501: Yayy! New best EMA pseudo Dice: 0.5971999764442444 
2025-01-27 18:48:58.354095:  
2025-01-27 18:48:58.357718: Epoch 16 
2025-01-27 18:48:58.360828: Current learning rate: 0.00986 
2025-01-27 18:49:47.777688: train_loss -0.6309 
2025-01-27 18:49:47.783793: val_loss -0.6225 
2025-01-27 18:49:47.786520: Pseudo dice [np.float32(0.9038), np.float32(0.8233)] 
2025-01-27 18:49:47.789146: Epoch time: 49.42 s 
2025-01-27 18:49:47.792055: Yayy! New best EMA pseudo Dice: 0.6237999796867371 
2025-01-27 18:49:49.531759:  
2025-01-27 18:49:49.535146: Epoch 17 
2025-01-27 18:49:49.538324: Current learning rate: 0.00985 
2025-01-27 18:50:38.043194: train_loss -0.6388 
2025-01-27 18:50:38.047804: val_loss -0.5862 
2025-01-27 18:50:38.050854: Pseudo dice [np.float32(0.894), np.float32(0.7497)] 
2025-01-27 18:50:38.053646: Epoch time: 48.51 s 
2025-01-27 18:50:38.056026: Yayy! New best EMA pseudo Dice: 0.6435999870300293 
2025-01-27 18:50:39.710759:  
2025-01-27 18:50:39.714095: Epoch 18 
2025-01-27 18:50:39.717134: Current learning rate: 0.00984 
2025-01-27 18:51:28.054516: train_loss -0.636 
2025-01-27 18:51:28.060630: val_loss -0.6142 
2025-01-27 18:51:28.063403: Pseudo dice [np.float32(0.9024), np.float32(0.7621)] 
2025-01-27 18:51:28.066010: Epoch time: 48.34 s 
2025-01-27 18:51:28.068615: Yayy! New best EMA pseudo Dice: 0.6625000238418579 
2025-01-27 18:51:29.674580:  
2025-01-27 18:51:29.677516: Epoch 19 
2025-01-27 18:51:29.680238: Current learning rate: 0.00983 
2025-01-27 18:52:18.223557: train_loss -0.6308 
2025-01-27 18:52:18.229451: val_loss -0.6237 
2025-01-27 18:52:18.232289: Pseudo dice [np.float32(0.9167), np.float32(0.801)] 
2025-01-27 18:52:18.235069: Epoch time: 48.55 s 
2025-01-27 18:52:18.237891: Yayy! New best EMA pseudo Dice: 0.6820999979972839 
2025-01-27 18:52:19.995192:  
2025-01-27 18:52:19.998493: Epoch 20 
2025-01-27 18:52:20.001687: Current learning rate: 0.00982 
2025-01-27 18:53:07.987229: train_loss -0.6562 
2025-01-27 18:53:07.993985: val_loss -0.6097 
2025-01-27 18:53:07.996848: Pseudo dice [np.float32(0.8981), np.float32(0.7698)] 
2025-01-27 18:53:07.999861: Epoch time: 47.99 s 
2025-01-27 18:53:08.002999: Yayy! New best EMA pseudo Dice: 0.6973000168800354 
2025-01-27 18:53:10.120052:  
2025-01-27 18:53:10.123000: Epoch 21 
2025-01-27 18:53:10.126026: Current learning rate: 0.00981 
2025-01-27 18:53:58.692334: train_loss -0.6416 
2025-01-27 18:53:58.697179: val_loss -0.6354 
2025-01-27 18:53:58.699859: Pseudo dice [np.float32(0.9047), np.float32(0.8248)] 
2025-01-27 18:53:58.702809: Epoch time: 48.57 s 
2025-01-27 18:53:58.705670: Yayy! New best EMA pseudo Dice: 0.7139999866485596 
2025-01-27 18:54:00.354092:  
2025-01-27 18:54:00.357260: Epoch 22 
2025-01-27 18:54:00.360609: Current learning rate: 0.0098 
2025-01-27 18:54:48.150118: train_loss -0.6703 
2025-01-27 18:54:48.155543: val_loss -0.592 
2025-01-27 18:54:48.158436: Pseudo dice [np.float32(0.9032), np.float32(0.8047)] 
2025-01-27 18:54:48.161009: Epoch time: 47.8 s 
2025-01-27 18:54:48.163521: Yayy! New best EMA pseudo Dice: 0.7279999852180481 
2025-01-27 18:54:49.887176:  
2025-01-27 18:54:49.889844: Epoch 23 
2025-01-27 18:54:49.892521: Current learning rate: 0.00979 
2025-01-27 18:55:37.887973: train_loss -0.6486 
2025-01-27 18:55:37.892843: val_loss -0.6618 
2025-01-27 18:55:37.895640: Pseudo dice [np.float32(0.9038), np.float32(0.8205)] 
2025-01-27 18:55:37.898371: Epoch time: 48.0 s 
2025-01-27 18:55:37.901289: Yayy! New best EMA pseudo Dice: 0.7415000200271606 
2025-01-27 18:55:39.609345:  
2025-01-27 18:55:39.612577: Epoch 24 
2025-01-27 18:55:39.615438: Current learning rate: 0.00978 
2025-01-27 18:56:27.487905: train_loss -0.6803 
2025-01-27 18:56:27.494005: val_loss -0.603 
2025-01-27 18:56:27.497054: Pseudo dice [np.float32(0.8917), np.float32(0.843)] 
2025-01-27 18:56:27.499729: Epoch time: 47.88 s 
2025-01-27 18:56:27.502514: Yayy! New best EMA pseudo Dice: 0.7540000081062317 
2025-01-27 18:56:29.181274:  
2025-01-27 18:56:29.184560: Epoch 25 
2025-01-27 18:56:29.187428: Current learning rate: 0.00977 
2025-01-27 18:57:17.257375: train_loss -0.7008 
2025-01-27 18:57:17.261833: val_loss -0.6531 
2025-01-27 18:57:17.264599: Pseudo dice [np.float32(0.9078), np.float32(0.7735)] 
2025-01-27 18:57:17.267297: Epoch time: 48.08 s 
2025-01-27 18:57:17.269722: Yayy! New best EMA pseudo Dice: 0.7627000212669373 
2025-01-27 18:57:18.952690:  
2025-01-27 18:57:18.956155: Epoch 26 
2025-01-27 18:57:18.958929: Current learning rate: 0.00977 
2025-01-27 18:58:07.086771: train_loss -0.675 
2025-01-27 18:58:07.094837: val_loss -0.6396 
2025-01-27 18:58:07.097577: Pseudo dice [np.float32(0.9189), np.float32(0.8617)] 
2025-01-27 18:58:07.100135: Epoch time: 48.13 s 
2025-01-27 18:58:07.102847: Yayy! New best EMA pseudo Dice: 0.7754999995231628 
2025-01-27 18:58:08.828158:  
2025-01-27 18:58:08.831684: Epoch 27 
2025-01-27 18:58:08.835105: Current learning rate: 0.00976 
2025-01-27 18:58:56.787791: train_loss -0.6764 
2025-01-27 18:58:56.792829: val_loss -0.5666 
2025-01-27 18:58:56.795848: Pseudo dice [np.float32(0.8989), np.float32(0.6954)] 
2025-01-27 18:58:56.798494: Epoch time: 47.96 s 
2025-01-27 18:58:56.801180: Yayy! New best EMA pseudo Dice: 0.7775999903678894 
2025-01-27 18:58:58.496840:  
2025-01-27 18:58:58.500002: Epoch 28 
2025-01-27 18:58:58.503142: Current learning rate: 0.00975 
2025-01-27 18:59:46.190654: train_loss -0.6794 
2025-01-27 18:59:46.196564: val_loss -0.6525 
2025-01-27 18:59:46.199300: Pseudo dice [np.float32(0.8906), np.float32(0.798)] 
2025-01-27 18:59:46.202378: Epoch time: 47.69 s 
2025-01-27 18:59:46.204998: Yayy! New best EMA pseudo Dice: 0.7843000292778015 
2025-01-27 18:59:47.880858:  
2025-01-27 18:59:47.884041: Epoch 29 
2025-01-27 18:59:47.887119: Current learning rate: 0.00974 
2025-01-27 19:00:36.079578: train_loss -0.6893 
2025-01-27 19:00:36.084715: val_loss -0.633 
2025-01-27 19:00:36.087430: Pseudo dice [np.float32(0.9221), np.float32(0.7981)] 
2025-01-27 19:00:36.090572: Epoch time: 48.2 s 
2025-01-27 19:00:36.093355: Yayy! New best EMA pseudo Dice: 0.7918999791145325 
2025-01-27 19:00:37.730437:  
2025-01-27 19:00:37.733556: Epoch 30 
2025-01-27 19:00:37.736686: Current learning rate: 0.00973 
2025-01-27 19:01:26.295620: train_loss -0.6859 
2025-01-27 19:01:26.301037: val_loss -0.6412 
2025-01-27 19:01:26.303891: Pseudo dice [np.float32(0.901), np.float32(0.893)] 
2025-01-27 19:01:26.306394: Epoch time: 48.57 s 
2025-01-27 19:01:26.308843: Yayy! New best EMA pseudo Dice: 0.8023999929428101 
2025-01-27 19:01:27.904348:  
2025-01-27 19:01:27.907676: Epoch 31 
2025-01-27 19:01:27.910580: Current learning rate: 0.00972 
2025-01-27 19:02:16.768322: train_loss -0.6968 
2025-01-27 19:02:16.774240: val_loss -0.6361 
2025-01-27 19:02:16.776771: Pseudo dice [np.float32(0.916), np.float32(0.8707)] 
2025-01-27 19:02:16.779233: Epoch time: 48.87 s 
2025-01-27 19:02:16.781976: Yayy! New best EMA pseudo Dice: 0.8115000128746033 
2025-01-27 19:02:18.368032:  
2025-01-27 19:02:18.370984: Epoch 32 
2025-01-27 19:02:18.374028: Current learning rate: 0.00971 
2025-01-27 19:03:06.898844: train_loss -0.7155 
2025-01-27 19:03:06.904378: val_loss -0.6179 
2025-01-27 19:03:06.907254: Pseudo dice [np.float32(0.898), np.float32(0.8318)] 
2025-01-27 19:03:06.910155: Epoch time: 48.53 s 
2025-01-27 19:03:06.912870: Yayy! New best EMA pseudo Dice: 0.8167999982833862 
2025-01-27 19:03:08.903991:  
2025-01-27 19:03:08.906726: Epoch 33 
2025-01-27 19:03:08.909649: Current learning rate: 0.0097 
2025-01-27 19:03:57.545709: train_loss -0.7068 
2025-01-27 19:03:57.549873: val_loss -0.631 
2025-01-27 19:03:57.552655: Pseudo dice [np.float32(0.906), np.float32(0.8349)] 
2025-01-27 19:03:57.555239: Epoch time: 48.64 s 
2025-01-27 19:03:57.557685: Yayy! New best EMA pseudo Dice: 0.8222000002861023 
2025-01-27 19:03:59.149828:  
2025-01-27 19:03:59.153545: Epoch 34 
2025-01-27 19:03:59.157213: Current learning rate: 0.00969 
2025-01-27 19:04:47.545842: train_loss -0.697 
2025-01-27 19:04:47.551429: val_loss -0.6642 
2025-01-27 19:04:47.554347: Pseudo dice [np.float32(0.9065), np.float32(0.8318)] 
2025-01-27 19:04:47.556937: Epoch time: 48.4 s 
2025-01-27 19:04:47.559433: Yayy! New best EMA pseudo Dice: 0.8269000053405762 
2025-01-27 19:04:49.161570:  
2025-01-27 19:04:49.164616: Epoch 35 
2025-01-27 19:04:49.168157: Current learning rate: 0.00968 
2025-01-27 19:05:37.627841: train_loss -0.7077 
2025-01-27 19:05:37.632877: val_loss -0.6853 
2025-01-27 19:05:37.635921: Pseudo dice [np.float32(0.9135), np.float32(0.8586)] 
2025-01-27 19:05:37.638687: Epoch time: 48.47 s 
2025-01-27 19:05:37.641105: Yayy! New best EMA pseudo Dice: 0.8327999711036682 
2025-01-27 19:05:39.255745:  
2025-01-27 19:05:39.259357: Epoch 36 
2025-01-27 19:05:39.262462: Current learning rate: 0.00968 
2025-01-27 19:06:27.828163: train_loss -0.7049 
2025-01-27 19:06:27.833869: val_loss -0.5776 
2025-01-27 19:06:27.836614: Pseudo dice [np.float32(0.873), np.float32(0.8187)] 
2025-01-27 19:06:27.839042: Epoch time: 48.57 s 
2025-01-27 19:06:27.841321: Yayy! New best EMA pseudo Dice: 0.8341000080108643 
2025-01-27 19:06:29.434711:  
2025-01-27 19:06:29.438011: Epoch 37 
2025-01-27 19:06:29.440835: Current learning rate: 0.00967 
2025-01-27 19:07:17.415399: train_loss -0.7178 
2025-01-27 19:07:17.421479: val_loss -0.6536 
2025-01-27 19:07:17.424596: Pseudo dice [np.float32(0.9131), np.float32(0.8755)] 
2025-01-27 19:07:17.427098: Epoch time: 47.98 s 
2025-01-27 19:07:17.429480: Yayy! New best EMA pseudo Dice: 0.8400999903678894 
2025-01-27 19:07:19.018811:  
2025-01-27 19:07:19.021775: Epoch 38 
2025-01-27 19:07:19.024870: Current learning rate: 0.00966 
2025-01-27 19:08:07.130473: train_loss -0.7091 
2025-01-27 19:08:07.135339: val_loss -0.6466 
2025-01-27 19:08:07.137968: Pseudo dice [np.float32(0.9286), np.float32(0.8593)] 
2025-01-27 19:08:07.140686: Epoch time: 48.11 s 
2025-01-27 19:08:07.143001: Yayy! New best EMA pseudo Dice: 0.8454999923706055 
2025-01-27 19:08:08.760593:  
2025-01-27 19:08:08.765810: Epoch 39 
2025-01-27 19:08:08.768798: Current learning rate: 0.00965 
2025-01-27 19:08:56.690737: train_loss -0.716 
2025-01-27 19:08:56.695812: val_loss -0.6387 
2025-01-27 19:08:56.698849: Pseudo dice [np.float32(0.9183), np.float32(0.8686)] 
2025-01-27 19:08:56.701955: Epoch time: 47.93 s 
2025-01-27 19:08:56.704923: Yayy! New best EMA pseudo Dice: 0.8503000140190125 
2025-01-27 19:08:58.355192:  
2025-01-27 19:08:58.358485: Epoch 40 
2025-01-27 19:08:58.361710: Current learning rate: 0.00964 
2025-01-27 19:09:46.366160: train_loss -0.7175 
2025-01-27 19:09:46.371830: val_loss -0.623 
2025-01-27 19:09:46.374821: Pseudo dice [np.float32(0.9033), np.float32(0.8324)] 
2025-01-27 19:09:46.377614: Epoch time: 48.01 s 
2025-01-27 19:09:46.380392: Yayy! New best EMA pseudo Dice: 0.8521000146865845 
2025-01-27 19:09:48.017685:  
2025-01-27 19:09:48.020667: Epoch 41 
2025-01-27 19:09:48.023690: Current learning rate: 0.00963 
2025-01-27 19:10:35.736127: train_loss -0.7142 
2025-01-27 19:10:35.740777: val_loss -0.673 
2025-01-27 19:10:35.743792: Pseudo dice [np.float32(0.9255), np.float32(0.8686)] 
2025-01-27 19:10:35.746612: Epoch time: 47.72 s 
2025-01-27 19:10:35.749311: Yayy! New best EMA pseudo Dice: 0.8565000295639038 
2025-01-27 19:10:37.338964:  
2025-01-27 19:10:37.342293: Epoch 42 
2025-01-27 19:10:37.345067: Current learning rate: 0.00962 
2025-01-27 19:11:25.928348: train_loss -0.7076 
2025-01-27 19:11:25.933745: val_loss -0.6213 
2025-01-27 19:11:25.936506: Pseudo dice [np.float32(0.9155), np.float32(0.8765)] 
2025-01-27 19:11:25.939122: Epoch time: 48.59 s 
2025-01-27 19:11:25.941564: Yayy! New best EMA pseudo Dice: 0.8604999780654907 
2025-01-27 19:11:27.555782:  
2025-01-27 19:11:27.559160: Epoch 43 
2025-01-27 19:11:27.561989: Current learning rate: 0.00961 
2025-01-27 19:12:15.823617: train_loss -0.7135 
2025-01-27 19:12:15.827896: val_loss -0.6605 
2025-01-27 19:12:15.830817: Pseudo dice [np.float32(0.9326), np.float32(0.8285)] 
2025-01-27 19:12:15.833692: Epoch time: 48.27 s 
2025-01-27 19:12:15.836215: Yayy! New best EMA pseudo Dice: 0.862500011920929 
2025-01-27 19:12:17.416768:  
2025-01-27 19:12:17.419953: Epoch 44 
2025-01-27 19:12:17.422959: Current learning rate: 0.0096 
2025-01-27 19:13:05.556395: train_loss -0.7282 
2025-01-27 19:13:05.561893: val_loss -0.6733 
2025-01-27 19:13:05.564330: Pseudo dice [np.float32(0.9266), np.float32(0.8734)] 
2025-01-27 19:13:05.566731: Epoch time: 48.14 s 
2025-01-27 19:13:05.569252: Yayy! New best EMA pseudo Dice: 0.8662999868392944 
2025-01-27 19:13:07.637627:  
2025-01-27 19:13:07.641079: Epoch 45 
2025-01-27 19:13:07.644062: Current learning rate: 0.00959 
2025-01-27 19:13:55.731062: train_loss -0.712 
2025-01-27 19:13:55.737772: val_loss -0.6774 
2025-01-27 19:13:55.740782: Pseudo dice [np.float32(0.9291), np.float32(0.9146)] 
2025-01-27 19:13:55.743628: Epoch time: 48.09 s 
2025-01-27 19:13:55.746258: Yayy! New best EMA pseudo Dice: 0.8718000054359436 
2025-01-27 19:13:57.445131:  
2025-01-27 19:13:57.448510: Epoch 46 
2025-01-27 19:13:57.451369: Current learning rate: 0.00959 
2025-01-27 19:14:45.433215: train_loss -0.7387 
2025-01-27 19:14:45.439618: val_loss -0.6613 
2025-01-27 19:14:45.442265: Pseudo dice [np.float32(0.9128), np.float32(0.8569)] 
2025-01-27 19:14:45.445069: Epoch time: 47.99 s 
2025-01-27 19:14:45.447867: Yayy! New best EMA pseudo Dice: 0.8730999827384949 
2025-01-27 19:14:47.165448:  
2025-01-27 19:14:47.168563: Epoch 47 
2025-01-27 19:14:47.171290: Current learning rate: 0.00958 
2025-01-27 19:15:34.960816: train_loss -0.7315 
2025-01-27 19:15:34.967062: val_loss -0.6641 
2025-01-27 19:15:34.970073: Pseudo dice [np.float32(0.9319), np.float32(0.8848)] 
2025-01-27 19:15:34.972685: Epoch time: 47.8 s 
2025-01-27 19:15:34.975271: Yayy! New best EMA pseudo Dice: 0.8766000270843506 
2025-01-27 19:15:36.562327:  
2025-01-27 19:15:36.566103: Epoch 48 
2025-01-27 19:15:36.569211: Current learning rate: 0.00957 
2025-01-27 19:16:24.777543: train_loss -0.7418 
2025-01-27 19:16:24.785682: val_loss -0.6772 
2025-01-27 19:16:24.788382: Pseudo dice [np.float32(0.9257), np.float32(0.9044)] 
2025-01-27 19:16:24.791008: Epoch time: 48.22 s 
2025-01-27 19:16:24.793438: Yayy! New best EMA pseudo Dice: 0.8805000185966492 
2025-01-27 19:16:26.379548:  
2025-01-27 19:16:26.382267: Epoch 49 
2025-01-27 19:16:26.384968: Current learning rate: 0.00956 
2025-01-27 19:17:14.595041: train_loss -0.7268 
2025-01-27 19:17:14.599735: val_loss -0.6598 
2025-01-27 19:17:14.602509: Pseudo dice [np.float32(0.9215), np.float32(0.8641)] 
2025-01-27 19:17:14.605238: Epoch time: 48.22 s 
2025-01-27 19:17:15.096490: Yayy! New best EMA pseudo Dice: 0.8816999793052673 
2025-01-27 19:17:16.721272:  
2025-01-27 19:17:16.724802: Epoch 50 
2025-01-27 19:17:16.727844: Current learning rate: 0.00955 
2025-01-27 19:18:05.391800: train_loss -0.7417 
2025-01-27 19:18:05.398252: val_loss -0.6969 
2025-01-27 19:18:05.401345: Pseudo dice [np.float32(0.9265), np.float32(0.9034)] 
2025-01-27 19:18:05.403994: Epoch time: 48.67 s 
2025-01-27 19:18:05.406478: Yayy! New best EMA pseudo Dice: 0.8849999904632568 
2025-01-27 19:18:06.969560:  
2025-01-27 19:18:06.972405: Epoch 51 
2025-01-27 19:18:06.975032: Current learning rate: 0.00954 
2025-01-27 19:18:55.304164: train_loss -0.7371 
2025-01-27 19:18:55.308662: val_loss -0.6776 
2025-01-27 19:18:55.311465: Pseudo dice [np.float32(0.9295), np.float32(0.8447)] 
2025-01-27 19:18:55.314208: Epoch time: 48.34 s 
2025-01-27 19:18:55.317443: Yayy! New best EMA pseudo Dice: 0.885200023651123 
2025-01-27 19:18:56.865863:  
2025-01-27 19:18:56.869133: Epoch 52 
2025-01-27 19:18:56.871881: Current learning rate: 0.00953 
2025-01-27 19:19:44.665983: train_loss -0.7354 
2025-01-27 19:19:44.671919: val_loss -0.677 
2025-01-27 19:19:44.674765: Pseudo dice [np.float32(0.9361), np.float32(0.8841)] 
2025-01-27 19:19:44.677414: Epoch time: 47.8 s 
2025-01-27 19:19:44.680074: Yayy! New best EMA pseudo Dice: 0.8877000212669373 
2025-01-27 19:19:46.230180:  
2025-01-27 19:19:46.233311: Epoch 53 
2025-01-27 19:19:46.236481: Current learning rate: 0.00952 
2025-01-27 19:20:34.538662: train_loss -0.7532 
2025-01-27 19:20:34.543295: val_loss -0.6836 
2025-01-27 19:20:34.546224: Pseudo dice [np.float32(0.9382), np.float32(0.9278)] 
2025-01-27 19:20:34.549297: Epoch time: 48.31 s 
2025-01-27 19:20:34.552295: Yayy! New best EMA pseudo Dice: 0.892300009727478 
2025-01-27 19:20:36.124256:  
2025-01-27 19:20:36.127343: Epoch 54 
2025-01-27 19:20:36.130196: Current learning rate: 0.00951 
2025-01-27 19:21:24.197286: train_loss -0.7433 
2025-01-27 19:21:24.205586: val_loss -0.689 
2025-01-27 19:21:24.208594: Pseudo dice [np.float32(0.9416), np.float32(0.9067)] 
2025-01-27 19:21:24.211415: Epoch time: 48.07 s 
2025-01-27 19:21:24.214067: Yayy! New best EMA pseudo Dice: 0.8953999876976013 
2025-01-27 19:21:25.801406:  
2025-01-27 19:21:25.804507: Epoch 55 
2025-01-27 19:21:25.807162: Current learning rate: 0.0095 
2025-01-27 19:22:13.701868: train_loss -0.7353 
2025-01-27 19:22:13.706057: val_loss -0.6975 
2025-01-27 19:22:13.708741: Pseudo dice [np.float32(0.9214), np.float32(0.8718)] 
2025-01-27 19:22:13.711169: Epoch time: 47.9 s 
2025-01-27 19:22:13.713622: Yayy! New best EMA pseudo Dice: 0.8956000208854675 
2025-01-27 19:22:15.426403:  
2025-01-27 19:22:15.429491: Epoch 56 
2025-01-27 19:22:15.432490: Current learning rate: 0.00949 
2025-01-27 19:23:03.801807: train_loss -0.7343 
2025-01-27 19:23:03.807812: val_loss -0.6294 
2025-01-27 19:23:03.810821: Pseudo dice [np.float32(0.9273), np.float32(0.841)] 
2025-01-27 19:23:03.813518: Epoch time: 48.38 s 
2025-01-27 19:23:05.201730:  
2025-01-27 19:23:05.204599: Epoch 57 
2025-01-27 19:23:05.207782: Current learning rate: 0.00949 
2025-01-27 19:23:54.241003: train_loss -0.7541 
2025-01-27 19:23:54.245928: val_loss -0.7354 
2025-01-27 19:23:54.248883: Pseudo dice [np.float32(0.938), np.float32(0.8783)] 
2025-01-27 19:23:54.251366: Epoch time: 49.04 s 
2025-01-27 19:23:54.253659: Yayy! New best EMA pseudo Dice: 0.895799994468689 
2025-01-27 19:23:55.847430:  
2025-01-27 19:23:55.850465: Epoch 58 
2025-01-27 19:23:55.853323: Current learning rate: 0.00948 
2025-01-27 19:24:44.546903: train_loss -0.7547 
2025-01-27 19:24:44.552709: val_loss -0.6838 
2025-01-27 19:24:44.555432: Pseudo dice [np.float32(0.9179), np.float32(0.8897)] 
2025-01-27 19:24:44.557883: Epoch time: 48.7 s 
2025-01-27 19:24:44.560817: Yayy! New best EMA pseudo Dice: 0.8966000080108643 
2025-01-27 19:24:46.145391:  
2025-01-27 19:24:46.148413: Epoch 59 
2025-01-27 19:24:46.151076: Current learning rate: 0.00947 
2025-01-27 19:25:34.072676: train_loss -0.7532 
2025-01-27 19:25:34.076806: val_loss -0.6712 
2025-01-27 19:25:34.079379: Pseudo dice [np.float32(0.9214), np.float32(0.8646)] 
2025-01-27 19:25:34.081918: Epoch time: 47.93 s 
2025-01-27 19:25:35.150481:  
2025-01-27 19:25:35.153604: Epoch 60 
2025-01-27 19:25:35.156395: Current learning rate: 0.00946 
2025-01-27 19:26:23.304701: train_loss -0.7536 
2025-01-27 19:26:23.311087: val_loss -0.681 
2025-01-27 19:26:23.313879: Pseudo dice [np.float32(0.9403), np.float32(0.889)] 
2025-01-27 19:26:23.316394: Epoch time: 48.16 s 
2025-01-27 19:26:23.318800: Yayy! New best EMA pseudo Dice: 0.8981000185012817 
2025-01-27 19:26:24.907011:  
2025-01-27 19:26:24.910038: Epoch 61 
2025-01-27 19:26:24.912949: Current learning rate: 0.00945 
2025-01-27 19:27:13.092705: train_loss -0.7525 
2025-01-27 19:27:13.098301: val_loss -0.6877 
2025-01-27 19:27:13.101130: Pseudo dice [np.float32(0.9319), np.float32(0.8972)] 
2025-01-27 19:27:13.103558: Epoch time: 48.19 s 
2025-01-27 19:27:13.105963: Yayy! New best EMA pseudo Dice: 0.8996999859809875 
2025-01-27 19:27:14.694336:  
2025-01-27 19:27:14.697234: Epoch 62 
2025-01-27 19:27:14.700085: Current learning rate: 0.00944 
2025-01-27 19:28:03.381776: train_loss -0.753 
2025-01-27 19:28:03.388022: val_loss -0.7434 
2025-01-27 19:28:03.390956: Pseudo dice [np.float32(0.9381), np.float32(0.9179)] 
2025-01-27 19:28:03.393769: Epoch time: 48.69 s 
2025-01-27 19:28:03.396627: Yayy! New best EMA pseudo Dice: 0.9025999903678894 
2025-01-27 19:28:05.024095:  
2025-01-27 19:28:05.027499: Epoch 63 
2025-01-27 19:28:05.030308: Current learning rate: 0.00943 
2025-01-27 19:28:54.010417: train_loss -0.7849 
2025-01-27 19:28:54.015296: val_loss -0.6168 
2025-01-27 19:28:54.018299: Pseudo dice [np.float32(0.9156), np.float32(0.7969)] 
2025-01-27 19:28:54.021068: Epoch time: 48.99 s 
2025-01-27 19:28:55.221528:  
2025-01-27 19:28:55.225549: Epoch 64 
2025-01-27 19:28:55.228394: Current learning rate: 0.00942 
2025-01-27 19:29:44.078244: train_loss -0.7543 
2025-01-27 19:29:44.083745: val_loss -0.6593 
2025-01-27 19:29:44.086491: Pseudo dice [np.float32(0.932), np.float32(0.8728)] 
2025-01-27 19:29:44.089026: Epoch time: 48.86 s 
2025-01-27 19:29:45.282932:  
2025-01-27 19:29:45.286003: Epoch 65 
2025-01-27 19:29:45.289193: Current learning rate: 0.00941 
2025-01-27 19:30:33.081206: train_loss -0.7473 
2025-01-27 19:30:33.085870: val_loss -0.6722 
2025-01-27 19:30:33.088506: Pseudo dice [np.float32(0.9417), np.float32(0.8818)] 
2025-01-27 19:30:33.091021: Epoch time: 47.8 s 
2025-01-27 19:30:34.220178:  
2025-01-27 19:30:34.223218: Epoch 66 
2025-01-27 19:30:34.226321: Current learning rate: 0.0094 
2025-01-27 19:31:22.066049: train_loss -0.7636 
2025-01-27 19:31:22.072905: val_loss -0.6341 
2025-01-27 19:31:22.075963: Pseudo dice [np.float32(0.9311), np.float32(0.8275)] 
2025-01-27 19:31:22.078986: Epoch time: 47.85 s 
2025-01-27 19:31:23.201382:  
2025-01-27 19:31:23.204848: Epoch 67 
2025-01-27 19:31:23.207600: Current learning rate: 0.00939 
2025-01-27 19:32:10.869306: train_loss -0.7799 
2025-01-27 19:32:10.874334: val_loss -0.7133 
2025-01-27 19:32:10.876941: Pseudo dice [np.float32(0.9358), np.float32(0.9051)] 
2025-01-27 19:32:10.879737: Epoch time: 47.67 s 
2025-01-27 19:32:12.015807:  
2025-01-27 19:32:12.018789: Epoch 68 
2025-01-27 19:32:12.021653: Current learning rate: 0.00939 
2025-01-27 19:33:01.101941: train_loss -0.7859 
2025-01-27 19:33:01.110508: val_loss -0.6731 
2025-01-27 19:33:01.113349: Pseudo dice [np.float32(0.9217), np.float32(0.8567)] 
2025-01-27 19:33:01.116095: Epoch time: 49.09 s 
2025-01-27 19:33:02.219081:  
2025-01-27 19:33:02.223345: Epoch 69 
2025-01-27 19:33:02.226549: Current learning rate: 0.00938 
2025-01-27 19:33:50.735795: train_loss -0.7579 
2025-01-27 19:33:50.742757: val_loss -0.6468 
2025-01-27 19:33:50.745693: Pseudo dice [np.float32(0.918), np.float32(0.8541)] 
2025-01-27 19:33:50.748642: Epoch time: 48.52 s 
2025-01-27 19:33:52.250340:  
2025-01-27 19:33:52.253697: Epoch 70 
2025-01-27 19:33:52.256549: Current learning rate: 0.00937 
2025-01-27 19:34:39.849351: train_loss -0.7782 
2025-01-27 19:34:39.855978: val_loss -0.6673 
2025-01-27 19:34:39.858853: Pseudo dice [np.float32(0.9402), np.float32(0.8798)] 
2025-01-27 19:34:39.861435: Epoch time: 47.6 s 
2025-01-27 19:34:41.006089:  
2025-01-27 19:34:41.009073: Epoch 71 
2025-01-27 19:34:41.011763: Current learning rate: 0.00936 
2025-01-27 19:35:29.111371: train_loss -0.7727 
2025-01-27 19:35:29.115894: val_loss -0.6713 
2025-01-27 19:35:29.118717: Pseudo dice [np.float32(0.9452), np.float32(0.8912)] 
2025-01-27 19:35:29.121251: Epoch time: 48.11 s 
2025-01-27 19:35:30.218958:  
2025-01-27 19:35:30.222332: Epoch 72 
2025-01-27 19:35:30.225088: Current learning rate: 0.00935 
2025-01-27 19:36:18.531433: train_loss -0.7718 
2025-01-27 19:36:18.536938: val_loss -0.6443 
2025-01-27 19:36:18.539676: Pseudo dice [np.float32(0.9398), np.float32(0.7875)] 
2025-01-27 19:36:18.542202: Epoch time: 48.31 s 
2025-01-27 19:36:19.729681:  
2025-01-27 19:36:19.732931: Epoch 73 
2025-01-27 19:36:19.735887: Current learning rate: 0.00934 
2025-01-27 19:37:07.710989: train_loss -0.7748 
2025-01-27 19:37:07.715857: val_loss -0.6699 
2025-01-27 19:37:07.718856: Pseudo dice [np.float32(0.9217), np.float32(0.8366)] 
2025-01-27 19:37:07.721613: Epoch time: 47.98 s 
2025-01-27 19:37:08.880510:  
2025-01-27 19:37:08.883829: Epoch 74 
2025-01-27 19:37:08.886864: Current learning rate: 0.00933 
2025-01-27 19:37:56.686738: train_loss -0.7742 
2025-01-27 19:37:56.692719: val_loss -0.6984 
2025-01-27 19:37:56.695679: Pseudo dice [np.float32(0.9317), np.float32(0.8423)] 
2025-01-27 19:37:56.698153: Epoch time: 47.81 s 
2025-01-27 19:37:57.840284:  
2025-01-27 19:37:57.843366: Epoch 75 
2025-01-27 19:37:57.846161: Current learning rate: 0.00932 
2025-01-27 19:38:45.805032: train_loss -0.7623 
2025-01-27 19:38:45.812587: val_loss -0.6953 
2025-01-27 19:38:45.815298: Pseudo dice [np.float32(0.9339), np.float32(0.9019)] 
2025-01-27 19:38:45.818310: Epoch time: 47.97 s 
2025-01-27 19:38:46.999445:  
2025-01-27 19:38:47.002309: Epoch 76 
2025-01-27 19:38:47.004836: Current learning rate: 0.00931 
2025-01-27 19:39:35.024459: train_loss -0.7741 
2025-01-27 19:39:35.030269: val_loss -0.7247 
2025-01-27 19:39:35.033111: Pseudo dice [np.float32(0.9125), np.float32(0.9039)] 
2025-01-27 19:39:35.035747: Epoch time: 48.03 s 
2025-01-27 19:39:36.175380:  
2025-01-27 19:39:36.178258: Epoch 77 
2025-01-27 19:39:36.180832: Current learning rate: 0.0093 
2025-01-27 19:40:23.985594: train_loss -0.7773 
2025-01-27 19:40:23.989872: val_loss -0.737 
2025-01-27 19:40:23.992664: Pseudo dice [np.float32(0.9245), np.float32(0.8481)] 
2025-01-27 19:40:23.995601: Epoch time: 47.81 s 
2025-01-27 19:40:25.147289:  
2025-01-27 19:40:25.150391: Epoch 78 
2025-01-27 19:40:25.153445: Current learning rate: 0.0093 
2025-01-27 19:41:13.464802: train_loss -0.7694 
2025-01-27 19:41:13.470656: val_loss -0.6714 
2025-01-27 19:41:13.473536: Pseudo dice [np.float32(0.9387), np.float32(0.8838)] 
2025-01-27 19:41:13.475937: Epoch time: 48.32 s 
2025-01-27 19:41:14.627229:  
2025-01-27 19:41:14.630514: Epoch 79 
2025-01-27 19:41:14.633506: Current learning rate: 0.00929 
2025-01-27 19:42:02.635850: train_loss -0.7698 
2025-01-27 19:42:02.640705: val_loss -0.6959 
2025-01-27 19:42:02.643639: Pseudo dice [np.float32(0.9325), np.float32(0.9109)] 
2025-01-27 19:42:02.646446: Epoch time: 48.01 s 
2025-01-27 19:42:03.854574:  
2025-01-27 19:42:03.857592: Epoch 80 
2025-01-27 19:42:03.860524: Current learning rate: 0.00928 
2025-01-27 19:42:51.423604: train_loss -0.764 
2025-01-27 19:42:51.428977: val_loss -0.6825 
2025-01-27 19:42:51.431608: Pseudo dice [np.float32(0.9331), np.float32(0.8847)] 
2025-01-27 19:42:51.434082: Epoch time: 47.57 s 
2025-01-27 19:42:52.634395:  
2025-01-27 19:42:52.637418: Epoch 81 
2025-01-27 19:42:52.640712: Current learning rate: 0.00927 
2025-01-27 19:43:40.598166: train_loss -0.789 
2025-01-27 19:43:40.603918: val_loss -0.7201 
2025-01-27 19:43:40.606466: Pseudo dice [np.float32(0.94), np.float32(0.875)] 
2025-01-27 19:43:40.609100: Epoch time: 47.96 s 
2025-01-27 19:43:42.170388:  
2025-01-27 19:43:42.173528: Epoch 82 
2025-01-27 19:43:42.176372: Current learning rate: 0.00926 
2025-01-27 19:44:30.089813: train_loss -0.7654 
2025-01-27 19:44:30.097955: val_loss -0.6471 
2025-01-27 19:44:30.100769: Pseudo dice [np.float32(0.9416), np.float32(0.8865)] 
2025-01-27 19:44:30.103551: Epoch time: 47.92 s 
2025-01-27 19:44:30.106230: Yayy! New best EMA pseudo Dice: 0.9031999707221985 
2025-01-27 19:44:31.761252:  
2025-01-27 19:44:31.764306: Epoch 83 
2025-01-27 19:44:31.767064: Current learning rate: 0.00925 
2025-01-27 19:45:19.738507: train_loss -0.7608 
2025-01-27 19:45:19.746398: val_loss -0.6833 
2025-01-27 19:45:19.749276: Pseudo dice [np.float32(0.9395), np.float32(0.8863)] 
2025-01-27 19:45:19.752109: Epoch time: 47.98 s 
2025-01-27 19:45:19.754861: Yayy! New best EMA pseudo Dice: 0.90420001745224 
2025-01-27 19:45:21.372534:  
2025-01-27 19:45:21.375472: Epoch 84 
2025-01-27 19:45:21.378031: Current learning rate: 0.00924 
2025-01-27 19:46:09.778487: train_loss -0.7715 
2025-01-27 19:46:09.784970: val_loss -0.6816 
2025-01-27 19:46:09.787908: Pseudo dice [np.float32(0.9403), np.float32(0.8807)] 
2025-01-27 19:46:09.790558: Epoch time: 48.41 s 
2025-01-27 19:46:09.793056: Yayy! New best EMA pseudo Dice: 0.9047999978065491 
2025-01-27 19:46:11.394828:  
2025-01-27 19:46:11.398260: Epoch 85 
2025-01-27 19:46:11.401147: Current learning rate: 0.00923 
2025-01-27 19:47:00.168437: train_loss -0.7788 
2025-01-27 19:47:00.174774: val_loss -0.7004 
2025-01-27 19:47:00.177640: Pseudo dice [np.float32(0.9425), np.float32(0.8498)] 
2025-01-27 19:47:00.180383: Epoch time: 48.77 s 
2025-01-27 19:47:01.327720:  
2025-01-27 19:47:01.331023: Epoch 86 
2025-01-27 19:47:01.334382: Current learning rate: 0.00922 
2025-01-27 19:47:50.338134: train_loss -0.7848 
2025-01-27 19:47:50.345390: val_loss -0.7438 
2025-01-27 19:47:50.348588: Pseudo dice [np.float32(0.942), np.float32(0.9114)] 
2025-01-27 19:47:50.351618: Epoch time: 49.01 s 
2025-01-27 19:47:50.354228: Yayy! New best EMA pseudo Dice: 0.9061999917030334 
2025-01-27 19:47:52.021913:  
2025-01-27 19:47:52.025837: Epoch 87 
2025-01-27 19:47:52.028678: Current learning rate: 0.00921 
2025-01-27 19:48:40.705252: train_loss -0.7925 
2025-01-27 19:48:40.710491: val_loss -0.7435 
2025-01-27 19:48:40.713531: Pseudo dice [np.float32(0.9466), np.float32(0.9343)] 
2025-01-27 19:48:40.716392: Epoch time: 48.68 s 
2025-01-27 19:48:40.719241: Yayy! New best EMA pseudo Dice: 0.909600019454956 
2025-01-27 19:48:42.328061:  
2025-01-27 19:48:42.331285: Epoch 88 
2025-01-27 19:48:42.334240: Current learning rate: 0.0092 
2025-01-27 19:49:30.441688: train_loss -0.7733 
2025-01-27 19:49:30.448970: val_loss -0.6656 
2025-01-27 19:49:30.451742: Pseudo dice [np.float32(0.9384), np.float32(0.878)] 
2025-01-27 19:49:30.454538: Epoch time: 48.11 s 
2025-01-27 19:49:31.516609:  
2025-01-27 19:49:31.520304: Epoch 89 
2025-01-27 19:49:31.523611: Current learning rate: 0.0092 
2025-01-27 19:50:19.939827: train_loss -0.7679 
2025-01-27 19:50:19.944348: val_loss -0.683 
2025-01-27 19:50:19.946899: Pseudo dice [np.float32(0.9281), np.float32(0.8014)] 
2025-01-27 19:50:19.949476: Epoch time: 48.42 s 
2025-01-27 19:50:20.992901:  
2025-01-27 19:50:20.996656: Epoch 90 
2025-01-27 19:50:20.999208: Current learning rate: 0.00919 
2025-01-27 19:51:09.258646: train_loss -0.757 
2025-01-27 19:51:09.264066: val_loss -0.7113 
2025-01-27 19:51:09.266645: Pseudo dice [np.float32(0.9378), np.float32(0.9072)] 
2025-01-27 19:51:09.269331: Epoch time: 48.27 s 
2025-01-27 19:51:10.318321:  
2025-01-27 19:51:10.321317: Epoch 91 
2025-01-27 19:51:10.324034: Current learning rate: 0.00918 
2025-01-27 19:51:58.884675: train_loss -0.7783 
2025-01-27 19:51:58.889336: val_loss -0.734 
2025-01-27 19:51:58.892372: Pseudo dice [np.float32(0.9409), np.float32(0.9115)] 
2025-01-27 19:51:58.895083: Epoch time: 48.57 s 
2025-01-27 19:51:59.925844:  
2025-01-27 19:51:59.929075: Epoch 92 
2025-01-27 19:51:59.932126: Current learning rate: 0.00917 
2025-01-27 19:52:48.077216: train_loss -0.7661 
2025-01-27 19:52:48.083118: val_loss -0.6877 
2025-01-27 19:52:48.086025: Pseudo dice [np.float32(0.9277), np.float32(0.855)] 
2025-01-27 19:52:48.088605: Epoch time: 48.15 s 
2025-01-27 19:52:49.242633:  
2025-01-27 19:52:49.245791: Epoch 93 
2025-01-27 19:52:49.248735: Current learning rate: 0.00916 
2025-01-27 19:53:37.641165: train_loss -0.7794 
2025-01-27 19:53:37.647884: val_loss -0.6514 
2025-01-27 19:53:37.650717: Pseudo dice [np.float32(0.933), np.float32(0.8305)] 
2025-01-27 19:53:37.653258: Epoch time: 48.4 s 
2025-01-27 19:53:39.076919:  
2025-01-27 19:53:39.080611: Epoch 94 
2025-01-27 19:53:39.083419: Current learning rate: 0.00915 
2025-01-27 19:54:26.840693: train_loss -0.7542 
2025-01-27 19:54:26.846424: val_loss -0.7081 
2025-01-27 19:54:26.849211: Pseudo dice [np.float32(0.9301), np.float32(0.9131)] 
2025-01-27 19:54:26.852145: Epoch time: 47.76 s 
2025-01-27 19:54:27.973563:  
2025-01-27 19:54:27.976001: Epoch 95 
2025-01-27 19:54:27.978560: Current learning rate: 0.00914 
2025-01-27 19:55:16.087401: train_loss -0.7606 
2025-01-27 19:55:16.093495: val_loss -0.6747 
2025-01-27 19:55:16.096106: Pseudo dice [np.float32(0.9244), np.float32(0.8649)] 
2025-01-27 19:55:16.098401: Epoch time: 48.11 s 
2025-01-27 19:55:17.180447:  
2025-01-27 19:55:17.183402: Epoch 96 
2025-01-27 19:55:17.186301: Current learning rate: 0.00913 
2025-01-27 19:56:05.087143: train_loss -0.7822 
2025-01-27 19:56:05.093684: val_loss -0.7035 
2025-01-27 19:56:05.096704: Pseudo dice [np.float32(0.9274), np.float32(0.9213)] 
2025-01-27 19:56:05.099319: Epoch time: 47.91 s 
2025-01-27 19:56:06.200020:  
2025-01-27 19:56:06.203186: Epoch 97 
2025-01-27 19:56:06.206449: Current learning rate: 0.00912 
2025-01-27 19:56:54.355617: train_loss -0.7587 
2025-01-27 19:56:54.359624: val_loss -0.7346 
2025-01-27 19:56:54.362191: Pseudo dice [np.float32(0.9405), np.float32(0.8438)] 
2025-01-27 19:56:54.364718: Epoch time: 48.16 s 
2025-01-27 19:56:55.468822:  
2025-01-27 19:56:55.471803: Epoch 98 
2025-01-27 19:56:55.474507: Current learning rate: 0.00911 
2025-01-27 19:57:43.419252: train_loss -0.7707 
2025-01-27 19:57:43.425521: val_loss -0.6831 
2025-01-27 19:57:43.428484: Pseudo dice [np.float32(0.9247), np.float32(0.9041)] 
2025-01-27 19:57:43.431087: Epoch time: 47.95 s 
2025-01-27 19:57:44.571073:  
2025-01-27 19:57:44.573914: Epoch 99 
2025-01-27 19:57:44.576863: Current learning rate: 0.0091 
2025-01-27 19:58:32.399572: train_loss -0.7793 
2025-01-27 19:58:32.403913: val_loss -0.6959 
2025-01-27 19:58:32.406669: Pseudo dice [np.float32(0.9402), np.float32(0.9029)] 
2025-01-27 19:58:32.409271: Epoch time: 47.83 s 
2025-01-27 19:58:34.259246:  
2025-01-27 19:58:34.262451: Epoch 100 
2025-01-27 19:58:34.265640: Current learning rate: 0.0091 
2025-01-27 19:59:22.036348: train_loss -0.784 
2025-01-27 19:59:22.042315: val_loss -0.6879 
2025-01-27 19:59:22.044929: Pseudo dice [np.float32(0.9394), np.float32(0.8748)] 
2025-01-27 19:59:22.047547: Epoch time: 47.78 s 
2025-01-27 19:59:23.123203:  
2025-01-27 19:59:23.127629: Epoch 101 
2025-01-27 19:59:23.130713: Current learning rate: 0.00909 
2025-01-27 20:00:11.606462: train_loss -0.7803 
2025-01-27 20:00:11.611474: val_loss -0.7243 
2025-01-27 20:00:11.614543: Pseudo dice [np.float32(0.9391), np.float32(0.892)] 
2025-01-27 20:00:11.617631: Epoch time: 48.48 s 
2025-01-27 20:00:12.724591:  
2025-01-27 20:00:12.728097: Epoch 102 
2025-01-27 20:00:12.730933: Current learning rate: 0.00908 
2025-01-27 20:01:00.434993: train_loss -0.7782 
2025-01-27 20:01:00.442452: val_loss -0.6973 
2025-01-27 20:01:00.445034: Pseudo dice [np.float32(0.912), np.float32(0.8852)] 
2025-01-27 20:01:00.447541: Epoch time: 47.71 s 
2025-01-27 20:01:01.555966:  
2025-01-27 20:01:01.560507: Epoch 103 
2025-01-27 20:01:01.563453: Current learning rate: 0.00907 
2025-01-27 20:01:49.732301: train_loss -0.7906 
2025-01-27 20:01:49.738719: val_loss -0.6908 
2025-01-27 20:01:49.741261: Pseudo dice [np.float32(0.9405), np.float32(0.8644)] 
2025-01-27 20:01:49.743781: Epoch time: 48.18 s 
2025-01-27 20:01:50.842575:  
2025-01-27 20:01:50.845587: Epoch 104 
2025-01-27 20:01:50.848497: Current learning rate: 0.00906 
2025-01-27 20:02:38.585419: train_loss -0.7679 
2025-01-27 20:02:38.592774: val_loss -0.6456 
2025-01-27 20:02:38.595441: Pseudo dice [np.float32(0.9333), np.float32(0.8915)] 
2025-01-27 20:02:38.598097: Epoch time: 47.74 s 
2025-01-27 20:02:39.736481:  
2025-01-27 20:02:39.739812: Epoch 105 
2025-01-27 20:02:39.742658: Current learning rate: 0.00905 
2025-01-27 20:03:28.095555: train_loss -0.7791 
2025-01-27 20:03:28.100193: val_loss -0.69 
2025-01-27 20:03:28.103230: Pseudo dice [np.float32(0.9315), np.float32(0.8381)] 
2025-01-27 20:03:28.105952: Epoch time: 48.36 s 
2025-01-27 20:03:29.242723:  
2025-01-27 20:03:29.245659: Epoch 106 
2025-01-27 20:03:29.248308: Current learning rate: 0.00904 
2025-01-27 20:04:17.276875: train_loss -0.7827 
2025-01-27 20:04:17.283150: val_loss -0.6772 
2025-01-27 20:04:17.285991: Pseudo dice [np.float32(0.9438), np.float32(0.8826)] 
2025-01-27 20:04:17.288194: Epoch time: 48.04 s 
2025-01-27 20:04:18.390684:  
2025-01-27 20:04:18.393671: Epoch 107 
2025-01-27 20:04:18.396121: Current learning rate: 0.00903 
2025-01-27 20:05:06.160506: train_loss -0.7979 
2025-01-27 20:05:06.165366: val_loss -0.6748 
2025-01-27 20:05:06.168381: Pseudo dice [np.float32(0.9469), np.float32(0.9229)] 
2025-01-27 20:05:06.171189: Epoch time: 47.77 s 
2025-01-27 20:05:07.585929:  
2025-01-27 20:05:07.588820: Epoch 108 
2025-01-27 20:05:07.591446: Current learning rate: 0.00902 
2025-01-27 20:05:55.568414: train_loss -0.7816 
2025-01-27 20:05:55.574203: val_loss -0.7328 
2025-01-27 20:05:55.576878: Pseudo dice [np.float32(0.9344), np.float32(0.8922)] 
2025-01-27 20:05:55.579463: Epoch time: 47.98 s 
2025-01-27 20:05:56.725651:  
2025-01-27 20:05:56.728740: Epoch 109 
2025-01-27 20:05:56.731308: Current learning rate: 0.00901 
2025-01-27 20:06:44.848615: train_loss -0.7847 
2025-01-27 20:06:44.853300: val_loss -0.7288 
2025-01-27 20:06:44.856484: Pseudo dice [np.float32(0.9428), np.float32(0.8762)] 
2025-01-27 20:06:44.859196: Epoch time: 48.12 s 
2025-01-27 20:06:45.963934:  
2025-01-27 20:06:45.967255: Epoch 110 
2025-01-27 20:06:45.970294: Current learning rate: 0.009 
2025-01-27 20:07:34.074706: train_loss -0.7682 
2025-01-27 20:07:34.080434: val_loss -0.6944 
2025-01-27 20:07:34.085359: Pseudo dice [np.float32(0.9306), np.float32(0.8672)] 
2025-01-27 20:07:34.088138: Epoch time: 48.11 s 
2025-01-27 20:07:35.194090:  
2025-01-27 20:07:35.197219: Epoch 111 
2025-01-27 20:07:35.200112: Current learning rate: 0.009 
2025-01-27 20:08:23.170816: train_loss -0.7556 
2025-01-27 20:08:23.175583: val_loss -0.7259 
2025-01-27 20:08:23.178568: Pseudo dice [np.float32(0.9318), np.float32(0.9163)] 
2025-01-27 20:08:23.181650: Epoch time: 47.98 s 
2025-01-27 20:08:23.184635: Yayy! New best EMA pseudo Dice: 0.9099000096321106 
2025-01-27 20:08:24.820017:  
2025-01-27 20:08:24.822695: Epoch 112 
2025-01-27 20:08:24.825874: Current learning rate: 0.00899 
2025-01-27 20:09:12.327687: train_loss -0.7634 
2025-01-27 20:09:12.333616: val_loss -0.6778 
2025-01-27 20:09:12.336725: Pseudo dice [np.float32(0.9448), np.float32(0.8797)] 
2025-01-27 20:09:12.339667: Epoch time: 47.51 s 
2025-01-27 20:09:12.342533: Yayy! New best EMA pseudo Dice: 0.9101999998092651 
2025-01-27 20:09:13.958581:  
2025-01-27 20:09:13.961615: Epoch 113 
2025-01-27 20:09:13.964210: Current learning rate: 0.00898 
2025-01-27 20:10:01.804684: train_loss -0.7798 
2025-01-27 20:10:01.809067: val_loss -0.711 
2025-01-27 20:10:01.811683: Pseudo dice [np.float32(0.9406), np.float32(0.9113)] 
2025-01-27 20:10:01.814445: Epoch time: 47.85 s 
2025-01-27 20:10:01.817439: Yayy! New best EMA pseudo Dice: 0.9117000102996826 
2025-01-27 20:10:03.438267:  
2025-01-27 20:10:03.441379: Epoch 114 
2025-01-27 20:10:03.444290: Current learning rate: 0.00897 
2025-01-27 20:10:51.796505: train_loss -0.7943 
2025-01-27 20:10:51.802845: val_loss -0.7013 
2025-01-27 20:10:51.805508: Pseudo dice [np.float32(0.9459), np.float32(0.8617)] 
2025-01-27 20:10:51.808344: Epoch time: 48.36 s 
2025-01-27 20:10:52.912282:  
2025-01-27 20:10:52.915424: Epoch 115 
2025-01-27 20:10:52.918346: Current learning rate: 0.00896 
2025-01-27 20:11:40.872259: train_loss -0.7732 
2025-01-27 20:11:40.877794: val_loss -0.7321 
2025-01-27 20:11:40.881562: Pseudo dice [np.float32(0.9293), np.float32(0.8628)] 
2025-01-27 20:11:40.884679: Epoch time: 47.96 s 
2025-01-27 20:11:42.007788:  
2025-01-27 20:11:42.010764: Epoch 116 
2025-01-27 20:11:42.013793: Current learning rate: 0.00895 
2025-01-27 20:12:29.777905: train_loss -0.7676 
2025-01-27 20:12:29.783959: val_loss -0.7266 
2025-01-27 20:12:29.786869: Pseudo dice [np.float32(0.9373), np.float32(0.8923)] 
2025-01-27 20:12:29.789341: Epoch time: 47.77 s 
2025-01-27 20:12:30.908502:  
2025-01-27 20:12:30.910916: Epoch 117 
2025-01-27 20:12:30.913415: Current learning rate: 0.00894 
2025-01-27 20:13:18.715542: train_loss -0.7798 
2025-01-27 20:13:18.719896: val_loss -0.7117 
2025-01-27 20:13:18.723097: Pseudo dice [np.float32(0.9349), np.float32(0.8525)] 
2025-01-27 20:13:18.725597: Epoch time: 47.81 s 
2025-01-27 20:13:19.849650:  
2025-01-27 20:13:19.852592: Epoch 118 
2025-01-27 20:13:19.855436: Current learning rate: 0.00893 
2025-01-27 20:14:07.785777: train_loss -0.7717 
2025-01-27 20:14:07.790947: val_loss -0.7115 
2025-01-27 20:14:07.793588: Pseudo dice [np.float32(0.9439), np.float32(0.9145)] 
2025-01-27 20:14:07.796081: Epoch time: 47.94 s 
2025-01-27 20:14:08.929016:  
2025-01-27 20:14:08.932478: Epoch 119 
2025-01-27 20:14:08.935849: Current learning rate: 0.00892 
2025-01-27 20:14:56.991922: train_loss -0.7782 
2025-01-27 20:14:56.996860: val_loss -0.7281 
2025-01-27 20:14:57.000081: Pseudo dice [np.float32(0.9397), np.float32(0.9122)] 
2025-01-27 20:14:57.003489: Epoch time: 48.07 s 
2025-01-27 20:14:57.006305: Yayy! New best EMA pseudo Dice: 0.9120000004768372 
2025-01-27 20:14:59.079783:  
2025-01-27 20:14:59.082964: Epoch 120 
2025-01-27 20:14:59.085732: Current learning rate: 0.00891 
2025-01-27 20:15:46.860152: train_loss -0.7958 
2025-01-27 20:15:46.866599: val_loss -0.7285 
2025-01-27 20:15:46.870232: Pseudo dice [np.float32(0.9474), np.float32(0.9263)] 
2025-01-27 20:15:46.874380: Epoch time: 47.78 s 
2025-01-27 20:15:46.877564: Yayy! New best EMA pseudo Dice: 0.9144999980926514 
2025-01-27 20:15:48.606968:  
2025-01-27 20:15:48.609736: Epoch 121 
2025-01-27 20:15:48.612102: Current learning rate: 0.0089 
2025-01-27 20:16:36.568219: train_loss -0.7761 
2025-01-27 20:16:36.572797: val_loss -0.6684 
2025-01-27 20:16:36.575720: Pseudo dice [np.float32(0.9448), np.float32(0.8634)] 
2025-01-27 20:16:36.578548: Epoch time: 47.96 s 
2025-01-27 20:16:37.739049:  
2025-01-27 20:16:37.742312: Epoch 122 
2025-01-27 20:16:37.744871: Current learning rate: 0.00889 
2025-01-27 20:17:25.528408: train_loss -0.7865 
2025-01-27 20:17:25.534501: val_loss -0.6958 
2025-01-27 20:17:25.537152: Pseudo dice [np.float32(0.9335), np.float32(0.9034)] 
2025-01-27 20:17:25.539767: Epoch time: 47.79 s 
2025-01-27 20:17:26.663782:  
2025-01-27 20:17:26.666454: Epoch 123 
2025-01-27 20:17:26.669401: Current learning rate: 0.00889 
2025-01-27 20:18:14.575781: train_loss -0.7762 
2025-01-27 20:18:14.579963: val_loss -0.7102 
2025-01-27 20:18:14.582595: Pseudo dice [np.float32(0.9309), np.float32(0.8719)] 
2025-01-27 20:18:14.585513: Epoch time: 47.91 s 
2025-01-27 20:18:15.706654:  
2025-01-27 20:18:15.709831: Epoch 124 
2025-01-27 20:18:15.712547: Current learning rate: 0.00888 
2025-01-27 20:19:03.469317: train_loss -0.7891 
2025-01-27 20:19:03.475678: val_loss -0.6759 
2025-01-27 20:19:03.478800: Pseudo dice [np.float32(0.9277), np.float32(0.8739)] 
2025-01-27 20:19:03.481458: Epoch time: 47.76 s 
2025-01-27 20:19:04.603931:  
2025-01-27 20:19:04.607246: Epoch 125 
2025-01-27 20:19:04.610409: Current learning rate: 0.00887 
2025-01-27 20:19:52.753160: train_loss -0.7665 
2025-01-27 20:19:52.758108: val_loss -0.699 
2025-01-27 20:19:52.760954: Pseudo dice [np.float32(0.9468), np.float32(0.9271)] 
2025-01-27 20:19:52.763585: Epoch time: 48.15 s 
2025-01-27 20:19:53.925497:  
2025-01-27 20:19:53.928353: Epoch 126 
2025-01-27 20:19:53.931165: Current learning rate: 0.00886 
2025-01-27 20:20:41.705812: train_loss -0.7929 
2025-01-27 20:20:41.711136: val_loss -0.65 
2025-01-27 20:20:41.713831: Pseudo dice [np.float32(0.8827), np.float32(0.8405)] 
2025-01-27 20:20:41.716573: Epoch time: 47.78 s 
2025-01-27 20:20:42.837831:  
2025-01-27 20:20:42.840742: Epoch 127 
2025-01-27 20:20:42.843543: Current learning rate: 0.00885 
2025-01-27 20:21:30.337049: train_loss -0.7812 
2025-01-27 20:21:30.341396: val_loss -0.7085 
2025-01-27 20:21:30.344099: Pseudo dice [np.float32(0.9345), np.float32(0.8493)] 
2025-01-27 20:21:30.346640: Epoch time: 47.5 s 
2025-01-27 20:21:31.470110:  
2025-01-27 20:21:31.472842: Epoch 128 
2025-01-27 20:21:31.475665: Current learning rate: 0.00884 
2025-01-27 20:22:19.042408: train_loss -0.7737 
2025-01-27 20:22:19.047814: val_loss -0.7091 
2025-01-27 20:22:19.051149: Pseudo dice [np.float32(0.9418), np.float32(0.9252)] 
2025-01-27 20:22:19.054027: Epoch time: 47.57 s 
2025-01-27 20:22:20.179926:  
2025-01-27 20:22:20.182953: Epoch 129 
2025-01-27 20:22:20.185666: Current learning rate: 0.00883 
2025-01-27 20:23:08.034676: train_loss -0.7808 
2025-01-27 20:23:08.038682: val_loss -0.7552 
2025-01-27 20:23:08.041156: Pseudo dice [np.float32(0.9427), np.float32(0.9221)] 
2025-01-27 20:23:08.043510: Epoch time: 47.86 s 
2025-01-27 20:23:09.171801:  
2025-01-27 20:23:09.174926: Epoch 130 
2025-01-27 20:23:09.178083: Current learning rate: 0.00882 
2025-01-27 20:23:57.169302: train_loss -0.7852 
2025-01-27 20:23:57.177039: val_loss -0.7315 
2025-01-27 20:23:57.179638: Pseudo dice [np.float32(0.9464), np.float32(0.9139)] 
2025-01-27 20:23:57.182370: Epoch time: 48.0 s 
2025-01-27 20:23:58.304079:  
2025-01-27 20:23:58.307164: Epoch 131 
2025-01-27 20:23:58.310085: Current learning rate: 0.00881 
2025-01-27 20:24:46.347672: train_loss -0.7876 
2025-01-27 20:24:46.351668: val_loss -0.7015 
2025-01-27 20:24:46.354342: Pseudo dice [np.float32(0.9391), np.float32(0.8271)] 
2025-01-27 20:24:46.356774: Epoch time: 48.04 s 
2025-01-27 20:24:47.478647:  
2025-01-27 20:24:47.481145: Epoch 132 
2025-01-27 20:24:47.483829: Current learning rate: 0.0088 
2025-01-27 20:25:35.503983: train_loss -0.7693 
2025-01-27 20:25:35.510172: val_loss -0.7089 
2025-01-27 20:25:35.513035: Pseudo dice [np.float32(0.9121), np.float32(0.849)] 
2025-01-27 20:25:35.515624: Epoch time: 48.03 s 
2025-01-27 20:25:36.939478:  
2025-01-27 20:25:36.943042: Epoch 133 
2025-01-27 20:25:36.945821: Current learning rate: 0.00879 
2025-01-27 20:26:25.011342: train_loss -0.7793 
2025-01-27 20:26:25.014942: val_loss -0.6867 
2025-01-27 20:26:25.017371: Pseudo dice [np.float32(0.9388), np.float32(0.8563)] 
2025-01-27 20:26:25.019906: Epoch time: 48.07 s 
2025-01-27 20:26:26.140304:  
2025-01-27 20:26:26.142665: Epoch 134 
2025-01-27 20:26:26.145020: Current learning rate: 0.00879 
2025-01-27 20:27:14.120120: train_loss -0.7944 
2025-01-27 20:27:14.126052: val_loss -0.7161 
2025-01-27 20:27:14.129171: Pseudo dice [np.float32(0.9408), np.float32(0.8733)] 
2025-01-27 20:27:14.132204: Epoch time: 47.98 s 
2025-01-27 20:27:15.272295:  
2025-01-27 20:27:15.275626: Epoch 135 
2025-01-27 20:27:15.278511: Current learning rate: 0.00878 
2025-01-27 20:28:03.471292: train_loss -0.7793 
2025-01-27 20:28:03.475967: val_loss -0.7315 
2025-01-27 20:28:03.479247: Pseudo dice [np.float32(0.9359), np.float32(0.8993)] 
2025-01-27 20:28:03.482025: Epoch time: 48.2 s 
2025-01-27 20:28:04.624247:  
2025-01-27 20:28:04.627192: Epoch 136 
2025-01-27 20:28:04.630297: Current learning rate: 0.00877 
2025-01-27 20:28:52.579398: train_loss -0.7952 
2025-01-27 20:28:52.585302: val_loss -0.7216 
2025-01-27 20:28:52.588273: Pseudo dice [np.float32(0.939), np.float32(0.8993)] 
2025-01-27 20:28:52.590976: Epoch time: 47.96 s 
2025-01-27 20:28:53.730070:  
2025-01-27 20:28:53.733287: Epoch 137 
2025-01-27 20:28:53.736239: Current learning rate: 0.00876 
2025-01-27 20:29:41.654377: train_loss -0.7945 
2025-01-27 20:29:41.658397: val_loss -0.7173 
2025-01-27 20:29:41.661162: Pseudo dice [np.float32(0.9363), np.float32(0.8843)] 
2025-01-27 20:29:41.663401: Epoch time: 47.93 s 
2025-01-27 20:29:42.800637:  
2025-01-27 20:29:42.803324: Epoch 138 
2025-01-27 20:29:42.805964: Current learning rate: 0.00875 
2025-01-27 20:30:30.607378: train_loss -0.8013 
2025-01-27 20:30:30.613270: val_loss -0.7428 
2025-01-27 20:30:30.616308: Pseudo dice [np.float32(0.9464), np.float32(0.9381)] 
2025-01-27 20:30:30.618985: Epoch time: 47.81 s 
2025-01-27 20:30:31.759564:  
2025-01-27 20:30:31.762180: Epoch 139 
2025-01-27 20:30:31.764857: Current learning rate: 0.00874 
2025-01-27 20:31:19.907667: train_loss -0.7991 
2025-01-27 20:31:19.912554: val_loss -0.6838 
2025-01-27 20:31:19.915569: Pseudo dice [np.float32(0.921), np.float32(0.8399)] 
2025-01-27 20:31:19.918126: Epoch time: 48.15 s 
2025-01-27 20:31:21.102792:  
2025-01-27 20:31:21.105977: Epoch 140 
2025-01-27 20:31:21.108647: Current learning rate: 0.00873 
2025-01-27 20:32:08.984843: train_loss -0.7861 
2025-01-27 20:32:08.990329: val_loss -0.727 
2025-01-27 20:32:08.993023: Pseudo dice [np.float32(0.9402), np.float32(0.8968)] 
2025-01-27 20:32:08.995870: Epoch time: 47.88 s 
2025-01-27 20:32:10.134647:  
2025-01-27 20:32:10.137572: Epoch 141 
2025-01-27 20:32:10.140508: Current learning rate: 0.00872 
2025-01-27 20:32:58.191498: train_loss -0.7807 
2025-01-27 20:32:58.197141: val_loss -0.7415 
2025-01-27 20:32:58.199632: Pseudo dice [np.float32(0.9428), np.float32(0.9068)] 
2025-01-27 20:32:58.202168: Epoch time: 48.06 s 
2025-01-27 20:32:59.389894:  
2025-01-27 20:32:59.393033: Epoch 142 
2025-01-27 20:32:59.396220: Current learning rate: 0.00871 
2025-01-27 20:33:47.245042: train_loss -0.7887 
2025-01-27 20:33:47.251226: val_loss -0.6991 
2025-01-27 20:33:47.254053: Pseudo dice [np.float32(0.9389), np.float32(0.8635)] 
2025-01-27 20:33:47.256780: Epoch time: 47.86 s 
2025-01-27 20:33:48.398946:  
2025-01-27 20:33:48.401706: Epoch 143 
2025-01-27 20:33:48.404497: Current learning rate: 0.0087 
2025-01-27 20:34:36.474800: train_loss -0.7793 
2025-01-27 20:34:36.478378: val_loss -0.6921 
2025-01-27 20:34:36.480883: Pseudo dice [np.float32(0.9448), np.float32(0.9017)] 
2025-01-27 20:34:36.483433: Epoch time: 48.08 s 
2025-01-27 20:34:37.619035:  
2025-01-27 20:34:37.621588: Epoch 144 
2025-01-27 20:34:37.624022: Current learning rate: 0.00869 
2025-01-27 20:35:26.017174: train_loss -0.7871 
2025-01-27 20:35:26.023090: val_loss -0.7132 
2025-01-27 20:35:26.025578: Pseudo dice [np.float32(0.9301), np.float32(0.8143)] 
2025-01-27 20:35:26.028031: Epoch time: 48.4 s 
2025-01-27 20:35:27.473301:  
2025-01-27 20:35:27.476272: Epoch 145 
2025-01-27 20:35:27.479156: Current learning rate: 0.00868 
2025-01-27 20:36:15.187716: train_loss -0.7571 
2025-01-27 20:36:15.192508: val_loss -0.6737 
2025-01-27 20:36:15.195757: Pseudo dice [np.float32(0.9422), np.float32(0.7773)] 
2025-01-27 20:36:15.198669: Epoch time: 47.72 s 
2025-01-27 20:36:16.338908:  
2025-01-27 20:36:16.341897: Epoch 146 
2025-01-27 20:36:16.344866: Current learning rate: 0.00868 
2025-01-27 20:37:04.468231: train_loss -0.7609 
2025-01-27 20:37:04.474139: val_loss -0.6629 
2025-01-27 20:37:04.476943: Pseudo dice [np.float32(0.9395), np.float32(0.8338)] 
2025-01-27 20:37:04.479498: Epoch time: 48.13 s 
2025-01-27 20:37:05.618779:  
2025-01-27 20:37:05.621665: Epoch 147 
2025-01-27 20:37:05.624290: Current learning rate: 0.00867 
2025-01-27 20:37:53.364564: train_loss -0.7632 
2025-01-27 20:37:53.371489: val_loss -0.7162 
2025-01-27 20:37:53.374565: Pseudo dice [np.float32(0.9305), np.float32(0.9069)] 
2025-01-27 20:37:53.377451: Epoch time: 47.75 s 
2025-01-27 20:37:54.560593:  
2025-01-27 20:37:54.563551: Epoch 148 
2025-01-27 20:37:54.566370: Current learning rate: 0.00866 
2025-01-27 20:38:42.092855: train_loss -0.7678 
2025-01-27 20:38:42.097433: val_loss -0.623 
2025-01-27 20:38:42.099761: Pseudo dice [np.float32(0.9302), np.float32(0.8396)] 
2025-01-27 20:38:42.102073: Epoch time: 47.53 s 
2025-01-27 20:38:43.250369:  
2025-01-27 20:38:43.253191: Epoch 149 
2025-01-27 20:38:43.255776: Current learning rate: 0.00865 
2025-01-27 20:39:31.007548: train_loss -0.7783 
2025-01-27 20:39:31.011685: val_loss -0.7249 
2025-01-27 20:39:31.014554: Pseudo dice [np.float32(0.9394), np.float32(0.8927)] 
2025-01-27 20:39:31.017285: Epoch time: 47.76 s 
2025-01-27 20:39:32.717179:  
2025-01-27 20:39:32.720027: Epoch 150 
2025-01-27 20:39:32.722924: Current learning rate: 0.00864 
2025-01-27 20:40:20.509126: train_loss -0.7896 
2025-01-27 20:40:20.514444: val_loss -0.6782 
2025-01-27 20:40:20.517123: Pseudo dice [np.float32(0.9381), np.float32(0.8845)] 
2025-01-27 20:40:20.519732: Epoch time: 47.79 s 
2025-01-27 20:40:21.663661:  
2025-01-27 20:40:21.667360: Epoch 151 
2025-01-27 20:40:21.670627: Current learning rate: 0.00863 
2025-01-27 20:41:09.542137: train_loss -0.7826 
2025-01-27 20:41:09.546479: val_loss -0.7578 
2025-01-27 20:41:09.549347: Pseudo dice [np.float32(0.9488), np.float32(0.9239)] 
2025-01-27 20:41:09.552070: Epoch time: 47.88 s 
2025-01-27 20:41:10.737079:  
2025-01-27 20:41:10.740173: Epoch 152 
2025-01-27 20:41:10.743781: Current learning rate: 0.00862 
2025-01-27 20:41:58.622999: train_loss -0.7926 
2025-01-27 20:41:58.628130: val_loss -0.7255 
2025-01-27 20:41:58.630602: Pseudo dice [np.float32(0.9455), np.float32(0.9094)] 
2025-01-27 20:41:58.632963: Epoch time: 47.89 s 
2025-01-27 20:41:59.776687:  
2025-01-27 20:41:59.779812: Epoch 153 
2025-01-27 20:41:59.782557: Current learning rate: 0.00861 
2025-01-27 20:42:47.415267: train_loss -0.796 
2025-01-27 20:42:47.418753: val_loss -0.7746 
2025-01-27 20:42:47.420870: Pseudo dice [np.float32(0.9439), np.float32(0.9176)] 
2025-01-27 20:42:47.423187: Epoch time: 47.64 s 
2025-01-27 20:42:48.578368:  
2025-01-27 20:42:48.580785: Epoch 154 
2025-01-27 20:42:48.583296: Current learning rate: 0.0086 
2025-01-27 20:43:36.331479: train_loss -0.7986 
2025-01-27 20:43:36.337600: val_loss -0.7569 
2025-01-27 20:43:36.340463: Pseudo dice [np.float32(0.9454), np.float32(0.8925)] 
2025-01-27 20:43:36.342971: Epoch time: 47.75 s 
2025-01-27 20:43:37.521931:  
2025-01-27 20:43:37.524503: Epoch 155 
2025-01-27 20:43:37.527232: Current learning rate: 0.00859 
2025-01-27 20:44:25.302733: train_loss -0.7934 
2025-01-27 20:44:25.308719: val_loss -0.7386 
2025-01-27 20:44:25.311698: Pseudo dice [np.float32(0.9443), np.float32(0.9111)] 
2025-01-27 20:44:25.314306: Epoch time: 47.78 s 
2025-01-27 20:44:26.466919:  
2025-01-27 20:44:26.470196: Epoch 156 
2025-01-27 20:44:26.473172: Current learning rate: 0.00858 
2025-01-27 20:45:14.428665: train_loss -0.7995 
2025-01-27 20:45:14.434603: val_loss -0.7031 
2025-01-27 20:45:14.437220: Pseudo dice [np.float32(0.9466), np.float32(0.8903)] 
2025-01-27 20:45:14.439826: Epoch time: 47.96 s 
2025-01-27 20:45:15.630365:  
2025-01-27 20:45:15.633610: Epoch 157 
2025-01-27 20:45:15.636678: Current learning rate: 0.00858 
2025-01-27 20:46:03.165657: train_loss -0.7849 
2025-01-27 20:46:03.170275: val_loss -0.7095 
2025-01-27 20:46:03.172950: Pseudo dice [np.float32(0.9296), np.float32(0.8977)] 
2025-01-27 20:46:03.175640: Epoch time: 47.54 s 
2025-01-27 20:46:04.644624:  
2025-01-27 20:46:04.647596: Epoch 158 
2025-01-27 20:46:04.650560: Current learning rate: 0.00857 
2025-01-27 20:46:52.706839: train_loss -0.78 
2025-01-27 20:46:52.713552: val_loss -0.6818 
2025-01-27 20:46:52.716389: Pseudo dice [np.float32(0.941), np.float32(0.8409)] 
2025-01-27 20:46:52.719262: Epoch time: 48.06 s 
2025-01-27 20:46:53.927709:  
2025-01-27 20:46:53.930627: Epoch 159 
2025-01-27 20:46:53.933434: Current learning rate: 0.00856 
2025-01-27 20:47:41.814726: train_loss -0.7996 
2025-01-27 20:47:41.819241: val_loss -0.7049 
2025-01-27 20:47:41.822435: Pseudo dice [np.float32(0.9528), np.float32(0.9048)] 
2025-01-27 20:47:41.825088: Epoch time: 47.89 s 
2025-01-27 20:47:42.985563:  
2025-01-27 20:47:42.988227: Epoch 160 
2025-01-27 20:47:42.991379: Current learning rate: 0.00855 
2025-01-27 20:48:30.769741: train_loss -0.7967 
2025-01-27 20:48:30.776355: val_loss -0.7761 
2025-01-27 20:48:30.779096: Pseudo dice [np.float32(0.9401), np.float32(0.9128)] 
2025-01-27 20:48:30.781752: Epoch time: 47.79 s 
2025-01-27 20:48:30.784509: Yayy! New best EMA pseudo Dice: 0.9146999716758728 
2025-01-27 20:48:32.477566:  
2025-01-27 20:48:32.480467: Epoch 161 
2025-01-27 20:48:32.483180: Current learning rate: 0.00854 
2025-01-27 20:49:20.572171: train_loss -0.7998 
2025-01-27 20:49:20.576909: val_loss -0.7458 
2025-01-27 20:49:20.579860: Pseudo dice [np.float32(0.9428), np.float32(0.8835)] 
2025-01-27 20:49:20.582653: Epoch time: 48.1 s 
2025-01-27 20:49:21.740844:  
2025-01-27 20:49:21.743735: Epoch 162 
2025-01-27 20:49:21.746699: Current learning rate: 0.00853 
2025-01-27 20:50:09.830049: train_loss -0.7982 
2025-01-27 20:50:09.836474: val_loss -0.7002 
2025-01-27 20:50:09.839348: Pseudo dice [np.float32(0.9463), np.float32(0.8923)] 
2025-01-27 20:50:09.842309: Epoch time: 48.09 s 
2025-01-27 20:50:09.847008: Yayy! New best EMA pseudo Dice: 0.9150000214576721 
2025-01-27 20:50:11.613739:  
2025-01-27 20:50:11.616850: Epoch 163 
2025-01-27 20:50:11.619676: Current learning rate: 0.00852 
2025-01-27 20:50:59.315383: train_loss -0.7902 
2025-01-27 20:50:59.319651: val_loss -0.688 
2025-01-27 20:50:59.322493: Pseudo dice [np.float32(0.9406), np.float32(0.9144)] 
2025-01-27 20:50:59.325261: Epoch time: 47.7 s 
2025-01-27 20:50:59.327618: Yayy! New best EMA pseudo Dice: 0.9162999987602234 
2025-01-27 20:51:01.093580:  
2025-01-27 20:51:01.096981: Epoch 164 
2025-01-27 20:51:01.099606: Current learning rate: 0.00851 
2025-01-27 20:51:49.133734: train_loss -0.8039 
2025-01-27 20:51:49.139068: val_loss -0.6925 
2025-01-27 20:51:49.141904: Pseudo dice [np.float32(0.9482), np.float32(0.8994)] 
2025-01-27 20:51:49.144543: Epoch time: 48.04 s 
2025-01-27 20:51:49.146899: Yayy! New best EMA pseudo Dice: 0.9169999957084656 
2025-01-27 20:51:50.793557:  
2025-01-27 20:51:50.796807: Epoch 165 
2025-01-27 20:51:50.800246: Current learning rate: 0.0085 
2025-01-27 20:52:39.382472: train_loss -0.806 
2025-01-27 20:52:39.386352: val_loss -0.715 
2025-01-27 20:52:39.388892: Pseudo dice [np.float32(0.9433), np.float32(0.9057)] 
2025-01-27 20:52:39.391207: Epoch time: 48.59 s 
2025-01-27 20:52:39.393585: Yayy! New best EMA pseudo Dice: 0.9178000092506409 
2025-01-27 20:52:41.099855:  
2025-01-27 20:52:41.102495: Epoch 166 
2025-01-27 20:52:41.104830: Current learning rate: 0.00849 
2025-01-27 20:53:29.460642: train_loss -0.8014 
2025-01-27 20:53:29.466168: val_loss -0.6906 
2025-01-27 20:53:29.469262: Pseudo dice [np.float32(0.9188), np.float32(0.9011)] 
2025-01-27 20:53:29.471723: Epoch time: 48.36 s 
2025-01-27 20:53:30.601608:  
2025-01-27 20:53:30.604700: Epoch 167 
2025-01-27 20:53:30.607917: Current learning rate: 0.00848 
2025-01-27 20:54:18.503918: train_loss -0.8079 
2025-01-27 20:54:18.507784: val_loss -0.7352 
2025-01-27 20:54:18.510280: Pseudo dice [np.float32(0.9463), np.float32(0.9244)] 
2025-01-27 20:54:18.512997: Epoch time: 47.9 s 
2025-01-27 20:54:18.515793: Yayy! New best EMA pseudo Dice: 0.9187999963760376 
2025-01-27 20:54:20.238507:  
2025-01-27 20:54:20.241226: Epoch 168 
2025-01-27 20:54:20.243848: Current learning rate: 0.00847 
2025-01-27 20:55:08.036904: train_loss -0.7888 
2025-01-27 20:55:08.042003: val_loss -0.7194 
2025-01-27 20:55:08.044548: Pseudo dice [np.float32(0.9361), np.float32(0.9219)] 
2025-01-27 20:55:08.046886: Epoch time: 47.8 s 
2025-01-27 20:55:08.049306: Yayy! New best EMA pseudo Dice: 0.9197999835014343 
2025-01-27 20:55:10.068848:  
2025-01-27 20:55:10.071973: Epoch 169 
2025-01-27 20:55:10.074600: Current learning rate: 0.00847 
2025-01-27 20:55:57.967891: train_loss -0.8024 
2025-01-27 20:55:57.972118: val_loss -0.6752 
2025-01-27 20:55:57.974687: Pseudo dice [np.float32(0.9384), np.float32(0.8778)] 
2025-01-27 20:55:57.977548: Epoch time: 47.9 s 
2025-01-27 20:55:59.123095:  
2025-01-27 20:55:59.126112: Epoch 170 
2025-01-27 20:55:59.129021: Current learning rate: 0.00846 
2025-01-27 20:56:47.081310: train_loss -0.7908 
2025-01-27 20:56:47.087416: val_loss -0.7259 
2025-01-27 20:56:47.090405: Pseudo dice [np.float32(0.9421), np.float32(0.8924)] 
2025-01-27 20:56:47.093165: Epoch time: 47.96 s 
2025-01-27 20:56:48.240989:  
2025-01-27 20:56:48.244087: Epoch 171 
2025-01-27 20:56:48.247140: Current learning rate: 0.00845 
2025-01-27 20:57:36.282264: train_loss -0.7678 
2025-01-27 20:57:36.286759: val_loss -0.7186 
2025-01-27 20:57:36.289628: Pseudo dice [np.float32(0.9281), np.float32(0.8704)] 
2025-01-27 20:57:36.292343: Epoch time: 48.04 s 
2025-01-27 20:57:37.441909:  
2025-01-27 20:57:37.445026: Epoch 172 
2025-01-27 20:57:37.448031: Current learning rate: 0.00844 
2025-01-27 20:58:25.910604: train_loss -0.8 
2025-01-27 20:58:25.918589: val_loss -0.7136 
2025-01-27 20:58:25.922064: Pseudo dice [np.float32(0.9477), np.float32(0.9068)] 
2025-01-27 20:58:25.925230: Epoch time: 48.47 s 
2025-01-27 20:58:27.073800:  
2025-01-27 20:58:27.076360: Epoch 173 
2025-01-27 20:58:27.078981: Current learning rate: 0.00843 
2025-01-27 20:59:15.174941: train_loss -0.8028 
2025-01-27 20:59:15.179392: val_loss -0.6736 
2025-01-27 20:59:15.181935: Pseudo dice [np.float32(0.945), np.float32(0.923)] 
2025-01-27 20:59:15.184944: Epoch time: 48.1 s 
2025-01-27 20:59:16.300600:  
2025-01-27 20:59:16.303768: Epoch 174 
2025-01-27 20:59:16.306800: Current learning rate: 0.00842 
2025-01-27 21:00:04.976255: train_loss -0.7943 
2025-01-27 21:00:04.982466: val_loss -0.7008 
2025-01-27 21:00:04.985212: Pseudo dice [np.float32(0.9481), np.float32(0.9063)] 
2025-01-27 21:00:04.987988: Epoch time: 48.68 s 
2025-01-27 21:00:04.990741: Yayy! New best EMA pseudo Dice: 0.9200999736785889 
2025-01-27 21:00:06.781611:  
2025-01-27 21:00:06.785136: Epoch 175 
2025-01-27 21:00:06.788497: Current learning rate: 0.00841 
2025-01-27 21:00:55.413507: train_loss -0.7821 
2025-01-27 21:00:55.417423: val_loss -0.7184 
2025-01-27 21:00:55.419841: Pseudo dice [np.float32(0.9336), np.float32(0.9117)] 
2025-01-27 21:00:55.422623: Epoch time: 48.63 s 
2025-01-27 21:00:55.425177: Yayy! New best EMA pseudo Dice: 0.9203000068664551 
2025-01-27 21:00:57.080717:  
2025-01-27 21:00:57.083999: Epoch 176 
2025-01-27 21:00:57.086883: Current learning rate: 0.0084 
2025-01-27 21:01:45.328964: train_loss -0.7964 
2025-01-27 21:01:45.335179: val_loss -0.7024 
2025-01-27 21:01:45.338264: Pseudo dice [np.float32(0.9471), np.float32(0.9006)] 
2025-01-27 21:01:45.340994: Epoch time: 48.25 s 
2025-01-27 21:01:45.343317: Yayy! New best EMA pseudo Dice: 0.9207000136375427 
2025-01-27 21:01:47.129200:  
2025-01-27 21:01:47.133029: Epoch 177 
2025-01-27 21:01:47.136175: Current learning rate: 0.00839 
2025-01-27 21:02:35.564959: train_loss -0.8048 
2025-01-27 21:02:35.569299: val_loss -0.7176 
2025-01-27 21:02:35.571533: Pseudo dice [np.float32(0.9546), np.float32(0.9307)] 
2025-01-27 21:02:35.573645: Epoch time: 48.44 s 
2025-01-27 21:02:35.575593: Yayy! New best EMA pseudo Dice: 0.9229000210762024 
2025-01-27 21:02:37.289018:  
2025-01-27 21:02:37.291590: Epoch 178 
2025-01-27 21:02:37.294198: Current learning rate: 0.00838 
2025-01-27 21:03:25.979102: train_loss -0.7942 
2025-01-27 21:03:25.986007: val_loss -0.6665 
2025-01-27 21:03:25.988578: Pseudo dice [np.float32(0.9299), np.float32(0.8424)] 
2025-01-27 21:03:25.991049: Epoch time: 48.69 s 
2025-01-27 21:03:27.092720:  
2025-01-27 21:03:27.095465: Epoch 179 
2025-01-27 21:03:27.098279: Current learning rate: 0.00837 
2025-01-27 21:04:15.823894: train_loss -0.7803 
2025-01-27 21:04:15.828224: val_loss -0.7176 
2025-01-27 21:04:15.830850: Pseudo dice [np.float32(0.9408), np.float32(0.902)] 
2025-01-27 21:04:15.833617: Epoch time: 48.73 s 
2025-01-27 21:04:17.030733:  
2025-01-27 21:04:17.034110: Epoch 180 
2025-01-27 21:04:17.036702: Current learning rate: 0.00836 
2025-01-27 21:05:05.332343: train_loss -0.8086 
2025-01-27 21:05:05.339013: val_loss -0.7403 
2025-01-27 21:05:05.341510: Pseudo dice [np.float32(0.9537), np.float32(0.9182)] 
2025-01-27 21:05:05.344026: Epoch time: 48.3 s 
2025-01-27 21:05:06.820611:  
2025-01-27 21:05:06.824005: Epoch 181 
2025-01-27 21:05:06.826985: Current learning rate: 0.00836 
2025-01-27 21:05:54.949605: train_loss -0.7922 
2025-01-27 21:05:54.954159: val_loss -0.7267 
2025-01-27 21:05:54.957362: Pseudo dice [np.float32(0.9379), np.float32(0.9176)] 
2025-01-27 21:05:54.960392: Epoch time: 48.13 s 
2025-01-27 21:05:56.101597:  
2025-01-27 21:05:56.104556: Epoch 182 
2025-01-27 21:05:56.107440: Current learning rate: 0.00835 
2025-01-27 21:06:44.015616: train_loss -0.7792 
2025-01-27 21:06:44.021744: val_loss -0.7163 
2025-01-27 21:06:44.024725: Pseudo dice [np.float32(0.9513), np.float32(0.9216)] 
2025-01-27 21:06:44.027713: Epoch time: 47.91 s 
2025-01-27 21:06:44.030200: Yayy! New best EMA pseudo Dice: 0.9232000112533569 
2025-01-27 21:06:45.719610:  
2025-01-27 21:06:45.722612: Epoch 183 
2025-01-27 21:06:45.725844: Current learning rate: 0.00834 
2025-01-27 21:07:33.481190: train_loss -0.8033 
2025-01-27 21:07:33.485554: val_loss -0.7584 
2025-01-27 21:07:33.488484: Pseudo dice [np.float32(0.9499), np.float32(0.9282)] 
2025-01-27 21:07:33.491147: Epoch time: 47.76 s 
2025-01-27 21:07:33.493994: Yayy! New best EMA pseudo Dice: 0.9247999787330627 
2025-01-27 21:07:35.282382:  
2025-01-27 21:07:35.285762: Epoch 184 
2025-01-27 21:07:35.288928: Current learning rate: 0.00833 
2025-01-27 21:08:23.114078: train_loss -0.8083 
2025-01-27 21:08:23.119403: val_loss -0.729 
2025-01-27 21:08:23.122119: Pseudo dice [np.float32(0.9341), np.float32(0.9094)] 
2025-01-27 21:08:23.124787: Epoch time: 47.83 s 
2025-01-27 21:08:24.275853:  
2025-01-27 21:08:24.279014: Epoch 185 
2025-01-27 21:08:24.282071: Current learning rate: 0.00832 
2025-01-27 21:09:11.790151: train_loss -0.7995 
2025-01-27 21:09:11.795660: val_loss -0.6872 
2025-01-27 21:09:11.798642: Pseudo dice [np.float32(0.9437), np.float32(0.8889)] 
2025-01-27 21:09:11.801637: Epoch time: 47.52 s 
2025-01-27 21:09:12.947564:  
2025-01-27 21:09:12.950235: Epoch 186 
2025-01-27 21:09:12.953036: Current learning rate: 0.00831 
2025-01-27 21:10:01.001776: train_loss -0.807 
2025-01-27 21:10:01.007602: val_loss -0.7336 
2025-01-27 21:10:01.010357: Pseudo dice [np.float32(0.9408), np.float32(0.9025)] 
2025-01-27 21:10:01.012972: Epoch time: 48.06 s 
2025-01-27 21:10:02.211116:  
2025-01-27 21:10:02.214863: Epoch 187 
2025-01-27 21:10:02.218026: Current learning rate: 0.0083 
2025-01-27 21:10:50.018773: train_loss -0.7955 
2025-01-27 21:10:50.022249: val_loss -0.7178 
2025-01-27 21:10:50.024493: Pseudo dice [np.float32(0.9449), np.float32(0.9139)] 
2025-01-27 21:10:50.026573: Epoch time: 47.81 s 
2025-01-27 21:10:51.174671:  
2025-01-27 21:10:51.177075: Epoch 188 
2025-01-27 21:10:51.179696: Current learning rate: 0.00829 
2025-01-27 21:11:39.030432: train_loss -0.8001 
2025-01-27 21:11:39.035657: val_loss -0.7348 
2025-01-27 21:11:39.038377: Pseudo dice [np.float32(0.9496), np.float32(0.9283)] 
2025-01-27 21:11:39.040992: Epoch time: 47.86 s 
2025-01-27 21:11:39.043704: Yayy! New best EMA pseudo Dice: 0.925599992275238 
2025-01-27 21:11:40.760695:  
2025-01-27 21:11:40.763931: Epoch 189 
2025-01-27 21:11:40.767264: Current learning rate: 0.00828 
2025-01-27 21:12:28.646058: train_loss -0.7924 
2025-01-27 21:12:28.650464: val_loss -0.7412 
2025-01-27 21:12:28.653209: Pseudo dice [np.float32(0.9506), np.float32(0.9263)] 
2025-01-27 21:12:28.655876: Epoch time: 47.89 s 
2025-01-27 21:12:28.658683: Yayy! New best EMA pseudo Dice: 0.926800012588501 
2025-01-27 21:12:30.436975:  
2025-01-27 21:12:30.439640: Epoch 190 
2025-01-27 21:12:30.442079: Current learning rate: 0.00827 
2025-01-27 21:13:18.401496: train_loss -0.782 
2025-01-27 21:13:18.407515: val_loss -0.6808 
2025-01-27 21:13:18.410507: Pseudo dice [np.float32(0.9507), np.float32(0.9169)] 
2025-01-27 21:13:18.413063: Epoch time: 47.97 s 
2025-01-27 21:13:18.415555: Yayy! New best EMA pseudo Dice: 0.9275000095367432 
2025-01-27 21:13:20.094994:  
2025-01-27 21:13:20.098175: Epoch 191 
2025-01-27 21:13:20.100960: Current learning rate: 0.00826 
2025-01-27 21:14:08.214858: train_loss -0.782 
2025-01-27 21:14:08.219696: val_loss -0.6982 
2025-01-27 21:14:08.222492: Pseudo dice [np.float32(0.9424), np.float32(0.8857)] 
2025-01-27 21:14:08.225157: Epoch time: 48.12 s 
2025-01-27 21:14:09.391235:  
2025-01-27 21:14:09.394600: Epoch 192 
2025-01-27 21:14:09.397688: Current learning rate: 0.00825 
2025-01-27 21:14:57.380516: train_loss -0.7685 
2025-01-27 21:14:57.386441: val_loss -0.7104 
2025-01-27 21:14:57.389101: Pseudo dice [np.float32(0.9305), np.float32(0.8756)] 
2025-01-27 21:14:57.391611: Epoch time: 47.99 s 
2025-01-27 21:14:58.949919:  
2025-01-27 21:14:58.954446: Epoch 193 
2025-01-27 21:14:58.957473: Current learning rate: 0.00824 
2025-01-27 21:15:47.589860: train_loss -0.7775 
2025-01-27 21:15:47.594556: val_loss -0.7115 
2025-01-27 21:15:47.597542: Pseudo dice [np.float32(0.914), np.float32(0.8488)] 
2025-01-27 21:15:47.600132: Epoch time: 48.64 s 
2025-01-27 21:15:48.782808:  
2025-01-27 21:15:48.786057: Epoch 194 
2025-01-27 21:15:48.789249: Current learning rate: 0.00824 
2025-01-27 21:16:37.458751: train_loss -0.7616 
2025-01-27 21:16:37.465907: val_loss -0.6624 
2025-01-27 21:16:37.469175: Pseudo dice [np.float32(0.9235), np.float32(0.8398)] 
2025-01-27 21:16:37.472415: Epoch time: 48.68 s 
2025-01-27 21:16:38.584962:  
2025-01-27 21:16:38.588360: Epoch 195 
2025-01-27 21:16:38.591458: Current learning rate: 0.00823 
2025-01-27 21:17:27.103061: train_loss -0.7612 
2025-01-27 21:17:27.108672: val_loss -0.6759 
2025-01-27 21:17:27.111620: Pseudo dice [np.float32(0.9393), np.float32(0.9103)] 
2025-01-27 21:17:27.114408: Epoch time: 48.52 s 
2025-01-27 21:17:28.297610:  
2025-01-27 21:17:28.300885: Epoch 196 
2025-01-27 21:17:28.303721: Current learning rate: 0.00822 
2025-01-27 21:18:16.070867: train_loss -0.7825 
2025-01-27 21:18:16.077361: val_loss -0.6844 
2025-01-27 21:18:16.079904: Pseudo dice [np.float32(0.934), np.float32(0.8637)] 
2025-01-27 21:18:16.082612: Epoch time: 47.77 s 
2025-01-27 21:18:17.249582:  
2025-01-27 21:18:17.252286: Epoch 197 
2025-01-27 21:18:17.254802: Current learning rate: 0.00821 
2025-01-27 21:19:05.983332: train_loss -0.7827 
2025-01-27 21:19:05.988461: val_loss -0.725 
2025-01-27 21:19:05.991483: Pseudo dice [np.float32(0.9495), np.float32(0.9236)] 
2025-01-27 21:19:05.994666: Epoch time: 48.73 s 
2025-01-27 21:19:07.203262:  
2025-01-27 21:19:07.206557: Epoch 198 
2025-01-27 21:19:07.209195: Current learning rate: 0.0082 
2025-01-27 21:19:55.116750: train_loss -0.7979 
2025-01-27 21:19:55.122251: val_loss -0.7493 
2025-01-27 21:19:55.124922: Pseudo dice [np.float32(0.9457), np.float32(0.9266)] 
2025-01-27 21:19:55.127681: Epoch time: 47.91 s 
2025-01-27 21:19:56.290127:  
2025-01-27 21:19:56.293026: Epoch 199 
2025-01-27 21:19:56.295967: Current learning rate: 0.00819 
2025-01-27 21:20:44.162620: train_loss -0.7946 
2025-01-27 21:20:44.166415: val_loss -0.7205 
2025-01-27 21:20:44.169142: Pseudo dice [np.float32(0.9479), np.float32(0.8817)] 
2025-01-27 21:20:44.171878: Epoch time: 47.87 s 
2025-01-27 21:20:45.912707:  
2025-01-27 21:20:45.915897: Epoch 200 
2025-01-27 21:20:45.919101: Current learning rate: 0.00818 
2025-01-27 21:21:34.120002: train_loss -0.7892 
2025-01-27 21:21:34.125793: val_loss -0.7264 
2025-01-27 21:21:34.128469: Pseudo dice [np.float32(0.95), np.float32(0.9295)] 
2025-01-27 21:21:34.130801: Epoch time: 48.21 s 
2025-01-27 21:21:35.287401:  
2025-01-27 21:21:35.290144: Epoch 201 
2025-01-27 21:21:35.292780: Current learning rate: 0.00817 
2025-01-27 21:22:23.757553: train_loss -0.8085 
2025-01-27 21:22:23.761209: val_loss -0.7355 
2025-01-27 21:22:23.763804: Pseudo dice [np.float32(0.9428), np.float32(0.9197)] 
2025-01-27 21:22:23.766261: Epoch time: 48.47 s 
2025-01-27 21:22:24.924497:  
2025-01-27 21:22:24.927938: Epoch 202 
2025-01-27 21:22:24.931054: Current learning rate: 0.00816 
2025-01-27 21:23:12.855186: train_loss -0.7743 
2025-01-27 21:23:12.861563: val_loss -0.6939 
2025-01-27 21:23:12.864808: Pseudo dice [np.float32(0.9363), np.float32(0.8102)] 
2025-01-27 21:23:12.867803: Epoch time: 47.93 s 
2025-01-27 21:23:14.074595:  
2025-01-27 21:23:14.077555: Epoch 203 
2025-01-27 21:23:14.080439: Current learning rate: 0.00815 
2025-01-27 21:24:02.044068: train_loss -0.8002 
2025-01-27 21:24:02.047723: val_loss -0.7134 
2025-01-27 21:24:02.050258: Pseudo dice [np.float32(0.9333), np.float32(0.8842)] 
2025-01-27 21:24:02.052787: Epoch time: 47.97 s 
2025-01-27 21:24:03.175255:  
2025-01-27 21:24:03.177654: Epoch 204 
2025-01-27 21:24:03.180031: Current learning rate: 0.00814 
2025-01-27 21:24:51.194959: train_loss -0.7815 
2025-01-27 21:24:51.201236: val_loss -0.7187 
2025-01-27 21:24:51.204038: Pseudo dice [np.float32(0.9333), np.float32(0.8982)] 
2025-01-27 21:24:51.206602: Epoch time: 48.02 s 
2025-01-27 21:24:52.697723:  
2025-01-27 21:24:52.701042: Epoch 205 
2025-01-27 21:24:52.703899: Current learning rate: 0.00813 
2025-01-27 21:25:40.523963: train_loss -0.7827 
2025-01-27 21:25:40.528945: val_loss -0.7283 
2025-01-27 21:25:40.531588: Pseudo dice [np.float32(0.9357), np.float32(0.8807)] 
2025-01-27 21:25:40.534265: Epoch time: 47.83 s 
2025-01-27 21:25:41.638453:  
2025-01-27 21:25:41.641513: Epoch 206 
2025-01-27 21:25:41.644262: Current learning rate: 0.00813 
2025-01-27 21:26:29.520361: train_loss -0.7768 
2025-01-27 21:26:29.526435: val_loss -0.7295 
2025-01-27 21:26:29.528627: Pseudo dice [np.float32(0.9331), np.float32(0.8988)] 
2025-01-27 21:26:29.531143: Epoch time: 47.88 s 
2025-01-27 21:26:30.603099:  
2025-01-27 21:26:30.605952: Epoch 207 
2025-01-27 21:26:30.608798: Current learning rate: 0.00812 
2025-01-27 21:27:19.087779: train_loss -0.776 
2025-01-27 21:27:19.092304: val_loss -0.7018 
2025-01-27 21:27:19.095335: Pseudo dice [np.float32(0.9401), np.float32(0.9277)] 
2025-01-27 21:27:19.098258: Epoch time: 48.49 s 
2025-01-27 21:27:20.152007:  
2025-01-27 21:27:20.157028: Epoch 208 
2025-01-27 21:27:20.160448: Current learning rate: 0.00811 
2025-01-27 21:28:08.777174: train_loss -0.7996 
2025-01-27 21:28:08.782002: val_loss -0.7026 
2025-01-27 21:28:08.784215: Pseudo dice [np.float32(0.9439), np.float32(0.8863)] 
2025-01-27 21:28:08.786557: Epoch time: 48.63 s 
2025-01-27 21:28:09.836473:  
2025-01-27 21:28:09.838930: Epoch 209 
2025-01-27 21:28:09.841486: Current learning rate: 0.0081 
2025-01-27 21:28:57.740932: train_loss -0.7961 
2025-01-27 21:28:57.744870: val_loss -0.7389 
2025-01-27 21:28:57.747518: Pseudo dice [np.float32(0.9504), np.float32(0.9253)] 
2025-01-27 21:28:57.750124: Epoch time: 47.91 s 
2025-01-27 21:28:58.796012:  
2025-01-27 21:28:58.799028: Epoch 210 
2025-01-27 21:28:58.801869: Current learning rate: 0.00809 
2025-01-27 21:29:46.554103: train_loss -0.7951 
2025-01-27 21:29:46.560283: val_loss -0.7133 
2025-01-27 21:29:46.563263: Pseudo dice [np.float32(0.9284), np.float32(0.9018)] 
2025-01-27 21:29:46.565852: Epoch time: 47.76 s 
2025-01-27 21:29:47.624792:  
2025-01-27 21:29:47.627765: Epoch 211 
2025-01-27 21:29:47.630335: Current learning rate: 0.00808 
2025-01-27 21:30:35.962673: train_loss -0.8099 
2025-01-27 21:30:35.967044: val_loss -0.7434 
2025-01-27 21:30:35.969749: Pseudo dice [np.float32(0.9441), np.float32(0.928)] 
2025-01-27 21:30:35.972419: Epoch time: 48.34 s 
2025-01-27 21:30:37.027789:  
2025-01-27 21:30:37.030574: Epoch 212 
2025-01-27 21:30:37.033900: Current learning rate: 0.00807 
2025-01-27 21:31:25.370157: train_loss -0.7911 
2025-01-27 21:31:25.376198: val_loss -0.7234 
2025-01-27 21:31:25.379343: Pseudo dice [np.float32(0.9396), np.float32(0.9169)] 
2025-01-27 21:31:25.381951: Epoch time: 48.34 s 
2025-01-27 21:31:26.498931:  
2025-01-27 21:31:26.501870: Epoch 213 
2025-01-27 21:31:26.504714: Current learning rate: 0.00806 
2025-01-27 21:32:14.655255: train_loss -0.7895 
2025-01-27 21:32:14.660569: val_loss -0.6528 
2025-01-27 21:32:14.663484: Pseudo dice [np.float32(0.9459), np.float32(0.8981)] 
2025-01-27 21:32:14.666281: Epoch time: 48.16 s 
2025-01-27 21:32:15.799485:  
2025-01-27 21:32:15.802606: Epoch 214 
2025-01-27 21:32:15.805573: Current learning rate: 0.00805 
2025-01-27 21:33:03.713718: train_loss -0.7845 
2025-01-27 21:33:03.719426: val_loss -0.7228 
2025-01-27 21:33:03.722052: Pseudo dice [np.float32(0.944), np.float32(0.9245)] 
2025-01-27 21:33:03.724536: Epoch time: 47.92 s 
2025-01-27 21:33:04.836473:  
2025-01-27 21:33:04.839748: Epoch 215 
2025-01-27 21:33:04.842664: Current learning rate: 0.00804 
2025-01-27 21:33:52.698448: train_loss -0.8189 
2025-01-27 21:33:52.701875: val_loss -0.7418 
2025-01-27 21:33:52.704263: Pseudo dice [np.float32(0.9502), np.float32(0.9296)] 
2025-01-27 21:33:52.706672: Epoch time: 47.86 s 
2025-01-27 21:33:53.808889:  
2025-01-27 21:33:53.811700: Epoch 216 
2025-01-27 21:33:53.814599: Current learning rate: 0.00803 
2025-01-27 21:34:41.321392: train_loss -0.8012 
2025-01-27 21:34:41.327214: val_loss -0.7644 
2025-01-27 21:34:41.330166: Pseudo dice [np.float32(0.9468), np.float32(0.9212)] 
2025-01-27 21:34:41.332818: Epoch time: 47.51 s 
2025-01-27 21:34:42.439571:  
2025-01-27 21:34:42.443044: Epoch 217 
2025-01-27 21:34:42.446007: Current learning rate: 0.00802 
2025-01-27 21:35:30.185214: train_loss -0.8166 
2025-01-27 21:35:30.189851: val_loss -0.7727 
2025-01-27 21:35:30.192620: Pseudo dice [np.float32(0.9557), np.float32(0.9378)] 
2025-01-27 21:35:30.195448: Epoch time: 47.75 s 
2025-01-27 21:35:31.613407:  
2025-01-27 21:35:31.616630: Epoch 218 
2025-01-27 21:35:31.619341: Current learning rate: 0.00801 
2025-01-27 21:36:20.018570: train_loss -0.791 
2025-01-27 21:36:20.024343: val_loss -0.7244 
2025-01-27 21:36:20.027051: Pseudo dice [np.float32(0.9317), np.float32(0.8811)] 
2025-01-27 21:36:20.029660: Epoch time: 48.41 s 
2025-01-27 21:36:21.131933:  
2025-01-27 21:36:21.135943: Epoch 219 
2025-01-27 21:36:21.138582: Current learning rate: 0.00801 
2025-01-27 21:37:09.000001: train_loss -0.7701 
2025-01-27 21:37:09.005135: val_loss -0.6799 
2025-01-27 21:37:09.008072: Pseudo dice [np.float32(0.9283), np.float32(0.8897)] 
2025-01-27 21:37:09.011064: Epoch time: 47.87 s 
2025-01-27 21:37:10.155685:  
2025-01-27 21:37:10.158674: Epoch 220 
2025-01-27 21:37:10.161454: Current learning rate: 0.008 
2025-01-27 21:37:57.663437: train_loss -0.7913 
2025-01-27 21:37:57.669634: val_loss -0.719 
2025-01-27 21:37:57.672480: Pseudo dice [np.float32(0.9334), np.float32(0.9034)] 
2025-01-27 21:37:57.675367: Epoch time: 47.51 s 
2025-01-27 21:37:58.782613:  
2025-01-27 21:37:58.785046: Epoch 221 
2025-01-27 21:37:58.787860: Current learning rate: 0.00799 
2025-01-27 21:38:46.849681: train_loss -0.7949 
2025-01-27 21:38:46.853712: val_loss -0.7391 
2025-01-27 21:38:46.856248: Pseudo dice [np.float32(0.945), np.float32(0.913)] 
2025-01-27 21:38:46.858790: Epoch time: 48.07 s 
2025-01-27 21:38:47.981483:  
2025-01-27 21:38:47.984843: Epoch 222 
2025-01-27 21:38:47.988559: Current learning rate: 0.00798 
2025-01-27 21:39:35.838426: train_loss -0.8165 
2025-01-27 21:39:35.845261: val_loss -0.7315 
2025-01-27 21:39:35.848275: Pseudo dice [np.float32(0.9318), np.float32(0.8916)] 
2025-01-27 21:39:35.851280: Epoch time: 47.86 s 
2025-01-27 21:39:36.998167:  
2025-01-27 21:39:37.001189: Epoch 223 
2025-01-27 21:39:37.004140: Current learning rate: 0.00797 
2025-01-27 21:40:24.863319: train_loss -0.7764 
2025-01-27 21:40:24.867820: val_loss -0.6907 
2025-01-27 21:40:24.870473: Pseudo dice [np.float32(0.9281), np.float32(0.8294)] 
2025-01-27 21:40:24.873149: Epoch time: 47.87 s 
2025-01-27 21:40:26.016133:  
2025-01-27 21:40:26.019093: Epoch 224 
2025-01-27 21:40:26.022079: Current learning rate: 0.00796 
2025-01-27 21:41:13.829703: train_loss -0.8085 
2025-01-27 21:41:13.835699: val_loss -0.7305 
2025-01-27 21:41:13.838572: Pseudo dice [np.float32(0.9505), np.float32(0.9239)] 
2025-01-27 21:41:13.841543: Epoch time: 47.81 s 
2025-01-27 21:41:14.950983:  
2025-01-27 21:41:14.954189: Epoch 225 
2025-01-27 21:41:14.956967: Current learning rate: 0.00795 
2025-01-27 21:42:02.483042: train_loss -0.7978 
2025-01-27 21:42:02.486979: val_loss -0.7607 
2025-01-27 21:42:02.489773: Pseudo dice [np.float32(0.9447), np.float32(0.8955)] 
2025-01-27 21:42:02.492413: Epoch time: 47.53 s 
2025-01-27 21:42:03.576202:  
2025-01-27 21:42:03.579599: Epoch 226 
2025-01-27 21:42:03.582479: Current learning rate: 0.00794 
2025-01-27 21:42:51.505173: train_loss -0.7977 
2025-01-27 21:42:51.510440: val_loss -0.697 
2025-01-27 21:42:51.512960: Pseudo dice [np.float32(0.9357), np.float32(0.8912)] 
2025-01-27 21:42:51.515321: Epoch time: 47.93 s 
2025-01-27 21:42:52.646956:  
2025-01-27 21:42:52.649746: Epoch 227 
2025-01-27 21:42:52.652148: Current learning rate: 0.00793 
2025-01-27 21:43:40.617793: train_loss -0.7963 
2025-01-27 21:43:40.622058: val_loss -0.7138 
2025-01-27 21:43:40.624822: Pseudo dice [np.float32(0.9535), np.float32(0.9407)] 
2025-01-27 21:43:40.627256: Epoch time: 47.97 s 
2025-01-27 21:43:41.712267:  
2025-01-27 21:43:41.715297: Epoch 228 
2025-01-27 21:43:41.718121: Current learning rate: 0.00792 
2025-01-27 21:44:29.634406: train_loss -0.794 
2025-01-27 21:44:29.639823: val_loss -0.6913 
2025-01-27 21:44:29.642603: Pseudo dice [np.float32(0.9404), np.float32(0.9195)] 
2025-01-27 21:44:29.644955: Epoch time: 47.92 s 
2025-01-27 21:44:30.735917:  
2025-01-27 21:44:30.739121: Epoch 229 
2025-01-27 21:44:30.742131: Current learning rate: 0.00791 
2025-01-27 21:45:19.183647: train_loss -0.7878 
2025-01-27 21:45:19.187344: val_loss -0.7399 
2025-01-27 21:45:19.189799: Pseudo dice [np.float32(0.9467), np.float32(0.878)] 
2025-01-27 21:45:19.192051: Epoch time: 48.45 s 
2025-01-27 21:45:20.290098:  
2025-01-27 21:45:20.292903: Epoch 230 
2025-01-27 21:45:20.295367: Current learning rate: 0.0079 
2025-01-27 21:46:08.741895: train_loss -0.7948 
2025-01-27 21:46:08.747228: val_loss -0.7317 
2025-01-27 21:46:08.749688: Pseudo dice [np.float32(0.9488), np.float32(0.8942)] 
2025-01-27 21:46:08.752251: Epoch time: 48.45 s 
2025-01-27 21:46:10.144135:  
2025-01-27 21:46:10.146616: Epoch 231 
2025-01-27 21:46:10.149278: Current learning rate: 0.00789 
2025-01-27 21:46:58.141541: train_loss -0.7907 
2025-01-27 21:46:58.146672: val_loss -0.6861 
2025-01-27 21:46:58.150025: Pseudo dice [np.float32(0.9448), np.float32(0.8933)] 
2025-01-27 21:46:58.152983: Epoch time: 48.0 s 
2025-01-27 21:46:59.247558:  
2025-01-27 21:46:59.250829: Epoch 232 
2025-01-27 21:46:59.253673: Current learning rate: 0.00789 
2025-01-27 21:47:47.177693: train_loss -0.8066 
2025-01-27 21:47:47.183359: val_loss -0.7258 
2025-01-27 21:47:47.186108: Pseudo dice [np.float32(0.9376), np.float32(0.8921)] 
2025-01-27 21:47:47.188935: Epoch time: 47.93 s 
2025-01-27 21:47:48.288571:  
2025-01-27 21:47:48.292099: Epoch 233 
2025-01-27 21:47:48.295315: Current learning rate: 0.00788 
2025-01-27 21:48:35.999310: train_loss -0.7954 
2025-01-27 21:48:36.004350: val_loss -0.7211 
2025-01-27 21:48:36.007053: Pseudo dice [np.float32(0.9346), np.float32(0.8937)] 
2025-01-27 21:48:36.009767: Epoch time: 47.71 s 
2025-01-27 21:48:37.136330:  
2025-01-27 21:48:37.139591: Epoch 234 
2025-01-27 21:48:37.142262: Current learning rate: 0.00787 
2025-01-27 21:49:25.015553: train_loss -0.8115 
2025-01-27 21:49:25.020777: val_loss -0.7509 
2025-01-27 21:49:25.023388: Pseudo dice [np.float32(0.9433), np.float32(0.9239)] 
2025-01-27 21:49:25.025878: Epoch time: 47.88 s 
2025-01-27 21:49:26.150637:  
2025-01-27 21:49:26.153688: Epoch 235 
2025-01-27 21:49:26.156515: Current learning rate: 0.00786 
2025-01-27 21:50:14.180154: train_loss -0.8206 
2025-01-27 21:50:14.184416: val_loss -0.7699 
2025-01-27 21:50:14.186922: Pseudo dice [np.float32(0.9557), np.float32(0.9145)] 
2025-01-27 21:50:14.189312: Epoch time: 48.03 s 
2025-01-27 21:50:15.277857:  
2025-01-27 21:50:15.280578: Epoch 236 
2025-01-27 21:50:15.283143: Current learning rate: 0.00785 
2025-01-27 21:51:02.926308: train_loss -0.8134 
2025-01-27 21:51:02.934234: val_loss -0.7533 
2025-01-27 21:51:02.937319: Pseudo dice [np.float32(0.9514), np.float32(0.9192)] 
2025-01-27 21:51:02.940106: Epoch time: 47.65 s 
2025-01-27 21:51:04.066189:  
2025-01-27 21:51:04.068928: Epoch 237 
2025-01-27 21:51:04.071959: Current learning rate: 0.00784 
2025-01-27 21:51:52.541413: train_loss -0.8077 
2025-01-27 21:51:52.545964: val_loss -0.7526 
2025-01-27 21:51:52.548938: Pseudo dice [np.float32(0.9478), np.float32(0.9337)] 
2025-01-27 21:51:52.551519: Epoch time: 48.48 s 
2025-01-27 21:51:53.642627:  
2025-01-27 21:51:53.645608: Epoch 238 
2025-01-27 21:51:53.648322: Current learning rate: 0.00783 
2025-01-27 21:52:41.466545: train_loss -0.8235 
2025-01-27 21:52:41.472589: val_loss -0.7309 
2025-01-27 21:52:41.475295: Pseudo dice [np.float32(0.9499), np.float32(0.9171)] 
2025-01-27 21:52:41.478244: Epoch time: 47.82 s 
2025-01-27 21:52:42.568236:  
2025-01-27 21:52:42.571418: Epoch 239 
2025-01-27 21:52:42.574383: Current learning rate: 0.00782 
2025-01-27 21:53:30.455676: train_loss -0.8007 
2025-01-27 21:53:30.460299: val_loss -0.7114 
2025-01-27 21:53:30.463428: Pseudo dice [np.float32(0.9315), np.float32(0.8769)] 
2025-01-27 21:53:30.465975: Epoch time: 47.89 s 
2025-01-27 21:53:31.568875:  
2025-01-27 21:53:31.572069: Epoch 240 
2025-01-27 21:53:31.574737: Current learning rate: 0.00781 
2025-01-27 21:54:19.568620: train_loss -0.7982 
2025-01-27 21:54:19.574617: val_loss -0.7542 
2025-01-27 21:54:19.577693: Pseudo dice [np.float32(0.9454), np.float32(0.9373)] 
2025-01-27 21:54:19.580680: Epoch time: 48.0 s 
2025-01-27 21:54:20.686828:  
2025-01-27 21:54:20.689131: Epoch 241 
2025-01-27 21:54:20.691551: Current learning rate: 0.0078 
2025-01-27 21:55:08.994938: train_loss -0.8112 
2025-01-27 21:55:09.000949: val_loss -0.7491 
2025-01-27 21:55:09.003937: Pseudo dice [np.float32(0.9445), np.float32(0.9186)] 
2025-01-27 21:55:09.006373: Epoch time: 48.31 s 
2025-01-27 21:55:10.150895:  
2025-01-27 21:55:10.154432: Epoch 242 
2025-01-27 21:55:10.157684: Current learning rate: 0.00779 
2025-01-27 21:55:58.528106: train_loss -0.8028 
2025-01-27 21:55:58.534116: val_loss -0.7466 
2025-01-27 21:55:58.537192: Pseudo dice [np.float32(0.9448), np.float32(0.9115)] 
2025-01-27 21:55:58.540102: Epoch time: 48.38 s 
2025-01-27 21:55:59.673475:  
2025-01-27 21:55:59.676497: Epoch 243 
2025-01-27 21:55:59.679457: Current learning rate: 0.00778 
2025-01-27 21:56:47.484854: train_loss -0.7984 
2025-01-27 21:56:47.489318: val_loss -0.7044 
2025-01-27 21:56:47.492031: Pseudo dice [np.float32(0.9453), np.float32(0.9125)] 
2025-01-27 21:56:47.494699: Epoch time: 47.81 s 
2025-01-27 21:56:48.640787:  
2025-01-27 21:56:48.644224: Epoch 244 
2025-01-27 21:56:48.647069: Current learning rate: 0.00777 
2025-01-27 21:57:36.616155: train_loss -0.8145 
2025-01-27 21:57:36.622675: val_loss -0.724 
2025-01-27 21:57:36.625347: Pseudo dice [np.float32(0.9522), np.float32(0.9332)] 
2025-01-27 21:57:36.627986: Epoch time: 47.98 s 
2025-01-27 21:57:36.630535: Yayy! New best EMA pseudo Dice: 0.9284999966621399 
2025-01-27 21:57:38.589517:  
2025-01-27 21:57:38.592600: Epoch 245 
2025-01-27 21:57:38.595450: Current learning rate: 0.00777 
2025-01-27 21:58:26.256565: train_loss -0.8172 
2025-01-27 21:58:26.261805: val_loss -0.7628 
2025-01-27 21:58:26.265062: Pseudo dice [np.float32(0.9585), np.float32(0.9215)] 
2025-01-27 21:58:26.268055: Epoch time: 47.67 s 
2025-01-27 21:58:26.270847: Yayy! New best EMA pseudo Dice: 0.9297000169754028 
2025-01-27 21:58:27.918691:  
2025-01-27 21:58:27.921755: Epoch 246 
2025-01-27 21:58:27.924283: Current learning rate: 0.00776 
2025-01-27 21:59:15.642800: train_loss -0.8087 
2025-01-27 21:59:15.647975: val_loss -0.7398 
2025-01-27 21:59:15.650422: Pseudo dice [np.float32(0.9488), np.float32(0.9371)] 
2025-01-27 21:59:15.652905: Epoch time: 47.73 s 
2025-01-27 21:59:15.655183: Yayy! New best EMA pseudo Dice: 0.9309999942779541 
2025-01-27 21:59:17.295827:  
2025-01-27 21:59:17.298981: Epoch 247 
2025-01-27 21:59:17.302047: Current learning rate: 0.00775 
2025-01-27 22:00:05.583532: train_loss -0.7969 
2025-01-27 22:00:05.587963: val_loss -0.7181 
2025-01-27 22:00:05.590798: Pseudo dice [np.float32(0.9482), np.float32(0.8944)] 
2025-01-27 22:00:05.593657: Epoch time: 48.29 s 
2025-01-27 22:00:06.659117:  
2025-01-27 22:00:06.663165: Epoch 248 
2025-01-27 22:00:06.665713: Current learning rate: 0.00774 
2025-01-27 22:00:54.442812: train_loss -0.7975 
2025-01-27 22:00:54.449423: val_loss -0.7137 
2025-01-27 22:00:54.452211: Pseudo dice [np.float32(0.931), np.float32(0.891)] 
2025-01-27 22:00:54.454921: Epoch time: 47.78 s 
2025-01-27 22:00:55.539194:  
2025-01-27 22:00:55.541862: Epoch 249 
2025-01-27 22:00:55.544883: Current learning rate: 0.00773 
2025-01-27 22:01:43.738622: train_loss -0.7996 
2025-01-27 22:01:43.743101: val_loss -0.7166 
2025-01-27 22:01:43.745412: Pseudo dice [np.float32(0.9385), np.float32(0.9063)] 
2025-01-27 22:01:43.747744: Epoch time: 48.2 s 
2025-01-27 22:01:45.356116:  
2025-01-27 22:01:45.358609: Epoch 250 
2025-01-27 22:01:45.360856: Current learning rate: 0.00772 
2025-01-27 22:02:33.208024: train_loss -0.8003 
2025-01-27 22:02:33.213378: val_loss -0.7569 
2025-01-27 22:02:33.216149: Pseudo dice [np.float32(0.9501), np.float32(0.9287)] 
2025-01-27 22:02:33.218643: Epoch time: 47.85 s 
2025-01-27 22:02:34.271952:  
2025-01-27 22:02:34.274863: Epoch 251 
2025-01-27 22:02:34.277443: Current learning rate: 0.00771 
2025-01-27 22:03:22.771885: train_loss -0.8127 
2025-01-27 22:03:22.776167: val_loss -0.7441 
2025-01-27 22:03:22.779154: Pseudo dice [np.float32(0.9257), np.float32(0.8993)] 
2025-01-27 22:03:22.781989: Epoch time: 48.5 s 
2025-01-27 22:03:23.835461:  
2025-01-27 22:03:23.838633: Epoch 252 
2025-01-27 22:03:23.841384: Current learning rate: 0.0077 
2025-01-27 22:04:12.392001: train_loss -0.7924 
2025-01-27 22:04:12.397793: val_loss -0.7512 
2025-01-27 22:04:12.400414: Pseudo dice [np.float32(0.9493), np.float32(0.9192)] 
2025-01-27 22:04:12.403032: Epoch time: 48.56 s 
2025-01-27 22:04:13.457265:  
2025-01-27 22:04:13.460508: Epoch 253 
2025-01-27 22:04:13.463435: Current learning rate: 0.00769 
2025-01-27 22:05:01.931440: train_loss -0.8145 
2025-01-27 22:05:01.936036: val_loss -0.7012 
2025-01-27 22:05:01.939054: Pseudo dice [np.float32(0.9443), np.float32(0.8991)] 
2025-01-27 22:05:01.941955: Epoch time: 48.48 s 
2025-01-27 22:05:03.067060:  
2025-01-27 22:05:03.071014: Epoch 254 
2025-01-27 22:05:03.073920: Current learning rate: 0.00768 
2025-01-27 22:05:50.973010: train_loss -0.7916 
2025-01-27 22:05:50.980090: val_loss -0.7042 
2025-01-27 22:05:50.982996: Pseudo dice [np.float32(0.9351), np.float32(0.9156)] 
2025-01-27 22:05:50.985789: Epoch time: 47.91 s 
2025-01-27 22:05:52.115866:  
2025-01-27 22:05:52.119157: Epoch 255 
2025-01-27 22:05:52.122471: Current learning rate: 0.00767 
2025-01-27 22:06:39.906735: train_loss -0.7924 
2025-01-27 22:06:39.911304: val_loss -0.7315 
2025-01-27 22:06:39.914308: Pseudo dice [np.float32(0.9431), np.float32(0.9194)] 
2025-01-27 22:06:39.917207: Epoch time: 47.79 s 
2025-01-27 22:06:41.019753:  
2025-01-27 22:06:41.022246: Epoch 256 
2025-01-27 22:06:41.024767: Current learning rate: 0.00766 
2025-01-27 22:07:28.828457: train_loss -0.8038 
2025-01-27 22:07:28.836290: val_loss -0.7326 
2025-01-27 22:07:28.838844: Pseudo dice [np.float32(0.9514), np.float32(0.9093)] 
2025-01-27 22:07:28.841543: Epoch time: 47.81 s 
2025-01-27 22:07:29.954946:  
2025-01-27 22:07:29.958115: Epoch 257 
2025-01-27 22:07:29.961018: Current learning rate: 0.00765 
2025-01-27 22:08:17.702079: train_loss -0.8138 
2025-01-27 22:08:17.706080: val_loss -0.7191 
2025-01-27 22:08:17.708919: Pseudo dice [np.float32(0.9342), np.float32(0.9053)] 
2025-01-27 22:08:17.711543: Epoch time: 47.75 s 
2025-01-27 22:08:19.129421:  
2025-01-27 22:08:19.133609: Epoch 258 
2025-01-27 22:08:19.136557: Current learning rate: 0.00764 
2025-01-27 22:09:07.128052: train_loss -0.8032 
2025-01-27 22:09:07.134274: val_loss -0.7827 
2025-01-27 22:09:07.137413: Pseudo dice [np.float32(0.9562), np.float32(0.9098)] 
2025-01-27 22:09:07.140224: Epoch time: 48.0 s 
2025-01-27 22:09:08.253035:  
2025-01-27 22:09:08.256219: Epoch 259 
2025-01-27 22:09:08.259028: Current learning rate: 0.00764 
2025-01-27 22:09:55.990248: train_loss -0.8036 
2025-01-27 22:09:55.994660: val_loss -0.7445 
2025-01-27 22:09:55.997348: Pseudo dice [np.float32(0.9459), np.float32(0.9161)] 
2025-01-27 22:09:55.999902: Epoch time: 47.74 s 
2025-01-27 22:09:57.111610:  
2025-01-27 22:09:57.114502: Epoch 260 
2025-01-27 22:09:57.117219: Current learning rate: 0.00763 
2025-01-27 22:10:44.833616: train_loss -0.8021 
2025-01-27 22:10:44.839477: val_loss -0.703 
2025-01-27 22:10:44.842488: Pseudo dice [np.float32(0.9441), np.float32(0.8769)] 
2025-01-27 22:10:44.845575: Epoch time: 47.72 s 
2025-01-27 22:10:45.955705:  
2025-01-27 22:10:45.958191: Epoch 261 
2025-01-27 22:10:45.960740: Current learning rate: 0.00762 
2025-01-27 22:11:33.735803: train_loss -0.8032 
2025-01-27 22:11:33.739861: val_loss -0.7425 
2025-01-27 22:11:33.743390: Pseudo dice [np.float32(0.9525), np.float32(0.9142)] 
2025-01-27 22:11:33.746008: Epoch time: 47.78 s 
2025-01-27 22:11:34.855965:  
2025-01-27 22:11:34.859235: Epoch 262 
2025-01-27 22:11:34.862042: Current learning rate: 0.00761 
2025-01-27 22:12:22.639384: train_loss -0.7856 
2025-01-27 22:12:22.645498: val_loss -0.7338 
2025-01-27 22:12:22.648657: Pseudo dice [np.float32(0.9467), np.float32(0.9128)] 
2025-01-27 22:12:22.651832: Epoch time: 47.78 s 
2025-01-27 22:12:23.798785:  
2025-01-27 22:12:23.801764: Epoch 263 
2025-01-27 22:12:23.804731: Current learning rate: 0.0076 
2025-01-27 22:13:11.689962: train_loss -0.8144 
2025-01-27 22:13:11.693932: val_loss -0.7352 
2025-01-27 22:13:11.696742: Pseudo dice [np.float32(0.9462), np.float32(0.9115)] 
2025-01-27 22:13:11.699183: Epoch time: 47.89 s 
2025-01-27 22:13:12.810822:  
2025-01-27 22:13:12.814018: Epoch 264 
2025-01-27 22:13:12.816992: Current learning rate: 0.00759 
2025-01-27 22:14:00.540208: train_loss -0.8147 
2025-01-27 22:14:00.546395: val_loss -0.7108 
2025-01-27 22:14:00.549098: Pseudo dice [np.float32(0.9419), np.float32(0.9229)] 
2025-01-27 22:14:00.551785: Epoch time: 47.73 s 
2025-01-27 22:14:01.661103:  
2025-01-27 22:14:01.664286: Epoch 265 
2025-01-27 22:14:01.667629: Current learning rate: 0.00758 
2025-01-27 22:14:49.675494: train_loss -0.8047 
2025-01-27 22:14:49.681508: val_loss -0.7046 
2025-01-27 22:14:49.684307: Pseudo dice [np.float32(0.9414), np.float32(0.8807)] 
2025-01-27 22:14:49.687093: Epoch time: 48.02 s 
2025-01-27 22:14:50.798566:  
2025-01-27 22:14:50.801614: Epoch 266 
2025-01-27 22:14:50.804784: Current learning rate: 0.00757 
2025-01-27 22:15:38.580119: train_loss -0.7987 
2025-01-27 22:15:38.585420: val_loss -0.687 
2025-01-27 22:15:38.588146: Pseudo dice [np.float32(0.9557), np.float32(0.8676)] 
2025-01-27 22:15:38.590819: Epoch time: 47.78 s 
2025-01-27 22:15:39.699602:  
2025-01-27 22:15:39.702348: Epoch 267 
2025-01-27 22:15:39.705064: Current learning rate: 0.00756 
2025-01-27 22:16:27.709983: train_loss -0.7941 
2025-01-27 22:16:27.713731: val_loss -0.7457 
2025-01-27 22:16:27.716213: Pseudo dice [np.float32(0.9458), np.float32(0.8876)] 
2025-01-27 22:16:27.718615: Epoch time: 48.01 s 
2025-01-27 22:16:28.825507:  
2025-01-27 22:16:28.828253: Epoch 268 
2025-01-27 22:16:28.831072: Current learning rate: 0.00755 
2025-01-27 22:17:17.099414: train_loss -0.7969 
2025-01-27 22:17:17.105384: val_loss -0.7565 
2025-01-27 22:17:17.107987: Pseudo dice [np.float32(0.9529), np.float32(0.9045)] 
2025-01-27 22:17:17.110961: Epoch time: 48.27 s 
2025-01-27 22:17:18.267449:  
2025-01-27 22:17:18.270884: Epoch 269 
2025-01-27 22:17:18.273578: Current learning rate: 0.00754 
2025-01-27 22:18:06.281676: train_loss -0.7991 
2025-01-27 22:18:06.285245: val_loss -0.7189 
2025-01-27 22:18:06.287883: Pseudo dice [np.float32(0.9434), np.float32(0.8907)] 
2025-01-27 22:18:06.290515: Epoch time: 48.02 s 
2025-01-27 22:18:07.750883:  
2025-01-27 22:18:07.753632: Epoch 270 
2025-01-27 22:18:07.756109: Current learning rate: 0.00753 
2025-01-27 22:18:55.761314: train_loss -0.7968 
2025-01-27 22:18:55.767287: val_loss -0.7482 
2025-01-27 22:18:55.769770: Pseudo dice [np.float32(0.9475), np.float32(0.9015)] 
2025-01-27 22:18:55.772270: Epoch time: 48.01 s 
2025-01-27 22:18:56.882880:  
2025-01-27 22:18:56.885933: Epoch 271 
2025-01-27 22:18:56.889169: Current learning rate: 0.00752 
2025-01-27 22:19:45.448020: train_loss -0.8104 
2025-01-27 22:19:45.452885: val_loss -0.7176 
2025-01-27 22:19:45.455724: Pseudo dice [np.float32(0.9412), np.float32(0.9169)] 
2025-01-27 22:19:45.458349: Epoch time: 48.57 s 
2025-01-27 22:19:46.533302:  
2025-01-27 22:19:46.536213: Epoch 272 
2025-01-27 22:19:46.539061: Current learning rate: 0.00751 
2025-01-27 22:20:34.927226: train_loss -0.8007 
2025-01-27 22:20:34.932384: val_loss -0.6144 
2025-01-27 22:20:34.934675: Pseudo dice [np.float32(0.9224), np.float32(0.8004)] 
2025-01-27 22:20:34.937050: Epoch time: 48.39 s 
2025-01-27 22:20:36.072454:  
2025-01-27 22:20:36.074865: Epoch 273 
2025-01-27 22:20:36.077053: Current learning rate: 0.00751 
2025-01-27 22:21:23.925247: train_loss -0.797 
2025-01-27 22:21:23.930129: val_loss -0.7812 
2025-01-27 22:21:23.933045: Pseudo dice [np.float32(0.9348), np.float32(0.8695)] 
2025-01-27 22:21:23.935927: Epoch time: 47.85 s 
2025-01-27 22:21:25.046086:  
2025-01-27 22:21:25.049322: Epoch 274 
2025-01-27 22:21:25.052088: Current learning rate: 0.0075 
2025-01-27 22:22:12.812077: train_loss -0.7999 
2025-01-27 22:22:12.817850: val_loss -0.7059 
2025-01-27 22:22:12.820679: Pseudo dice [np.float32(0.9376), np.float32(0.9004)] 
2025-01-27 22:22:12.823631: Epoch time: 47.77 s 
2025-01-27 22:22:13.934406:  
2025-01-27 22:22:13.936887: Epoch 275 
2025-01-27 22:22:13.939401: Current learning rate: 0.00749 
2025-01-27 22:23:01.635814: train_loss -0.8068 
2025-01-27 22:23:01.640425: val_loss -0.7084 
2025-01-27 22:23:01.643111: Pseudo dice [np.float32(0.9204), np.float32(0.8677)] 
2025-01-27 22:23:01.646198: Epoch time: 47.7 s 
2025-01-27 22:23:02.763449:  
2025-01-27 22:23:02.766835: Epoch 276 
2025-01-27 22:23:02.770022: Current learning rate: 0.00748 
2025-01-27 22:23:51.148848: train_loss -0.7907 
2025-01-27 22:23:51.155348: val_loss -0.7378 
2025-01-27 22:23:51.158780: Pseudo dice [np.float32(0.9379), np.float32(0.9088)] 
2025-01-27 22:23:51.161732: Epoch time: 48.39 s 
2025-01-27 22:23:52.309940:  
2025-01-27 22:23:52.312877: Epoch 277 
2025-01-27 22:23:52.315886: Current learning rate: 0.00747 
2025-01-27 22:24:40.104818: train_loss -0.7885 
2025-01-27 22:24:40.109751: val_loss -0.6998 
2025-01-27 22:24:40.112722: Pseudo dice [np.float32(0.9426), np.float32(0.9195)] 
2025-01-27 22:24:40.115711: Epoch time: 47.8 s 
2025-01-27 22:24:41.223560:  
2025-01-27 22:24:41.226873: Epoch 278 
2025-01-27 22:24:41.229637: Current learning rate: 0.00746 
2025-01-27 22:25:29.109421: train_loss -0.8098 
2025-01-27 22:25:29.114908: val_loss -0.7292 
2025-01-27 22:25:29.117457: Pseudo dice [np.float32(0.9483), np.float32(0.9156)] 
2025-01-27 22:25:29.119866: Epoch time: 47.89 s 
2025-01-27 22:25:30.228277:  
2025-01-27 22:25:30.231054: Epoch 279 
2025-01-27 22:25:30.234086: Current learning rate: 0.00745 
2025-01-27 22:26:18.218560: train_loss -0.8061 
2025-01-27 22:26:18.223267: val_loss -0.7276 
2025-01-27 22:26:18.226346: Pseudo dice [np.float32(0.9372), np.float32(0.91)] 
2025-01-27 22:26:18.229306: Epoch time: 47.99 s 
2025-01-27 22:26:19.340210:  
2025-01-27 22:26:19.343327: Epoch 280 
2025-01-27 22:26:19.346014: Current learning rate: 0.00744 
2025-01-27 22:27:07.494106: train_loss -0.8139 
2025-01-27 22:27:07.499094: val_loss -0.7201 
2025-01-27 22:27:07.501544: Pseudo dice [np.float32(0.9471), np.float32(0.9057)] 
2025-01-27 22:27:07.503768: Epoch time: 48.15 s 
2025-01-27 22:27:08.611855:  
2025-01-27 22:27:08.614387: Epoch 281 
2025-01-27 22:27:08.617073: Current learning rate: 0.00743 
2025-01-27 22:27:56.489682: train_loss -0.8074 
2025-01-27 22:27:56.494763: val_loss -0.7545 
2025-01-27 22:27:56.497835: Pseudo dice [np.float32(0.9467), np.float32(0.8988)] 
2025-01-27 22:27:56.500495: Epoch time: 47.88 s 
2025-01-27 22:27:57.606861:  
2025-01-27 22:27:57.609627: Epoch 282 
2025-01-27 22:27:57.612586: Current learning rate: 0.00742 
2025-01-27 22:28:45.828909: train_loss -0.8158 
2025-01-27 22:28:45.834889: val_loss -0.6992 
2025-01-27 22:28:45.837968: Pseudo dice [np.float32(0.9316), np.float32(0.876)] 
2025-01-27 22:28:45.840693: Epoch time: 48.22 s 
2025-01-27 22:28:47.289901:  
2025-01-27 22:28:47.292895: Epoch 283 
2025-01-27 22:28:47.295747: Current learning rate: 0.00741 
2025-01-27 22:29:35.164762: train_loss -0.8116 
2025-01-27 22:29:35.169845: val_loss -0.7154 
2025-01-27 22:29:35.172996: Pseudo dice [np.float32(0.943), np.float32(0.9076)] 
2025-01-27 22:29:35.175659: Epoch time: 47.88 s 
2025-01-27 22:29:36.284384:  
2025-01-27 22:29:36.287677: Epoch 284 
2025-01-27 22:29:36.290703: Current learning rate: 0.0074 
2025-01-27 22:30:24.395655: train_loss -0.8178 
2025-01-27 22:30:24.402369: val_loss -0.7394 
2025-01-27 22:30:24.405159: Pseudo dice [np.float32(0.9514), np.float32(0.9164)] 
2025-01-27 22:30:24.407680: Epoch time: 48.11 s 
2025-01-27 22:30:25.595885:  
2025-01-27 22:30:25.599851: Epoch 285 
2025-01-27 22:30:25.603120: Current learning rate: 0.00739 
2025-01-27 22:31:13.721095: train_loss -0.8157 
2025-01-27 22:31:13.725161: val_loss -0.6998 
2025-01-27 22:31:13.728685: Pseudo dice [np.float32(0.9495), np.float32(0.8762)] 
2025-01-27 22:31:13.731162: Epoch time: 48.13 s 
2025-01-27 22:31:14.814020:  
2025-01-27 22:31:14.817094: Epoch 286 
2025-01-27 22:31:14.820035: Current learning rate: 0.00738 
2025-01-27 22:32:03.617488: train_loss -0.813 
2025-01-27 22:32:03.623809: val_loss -0.7116 
2025-01-27 22:32:03.626353: Pseudo dice [np.float32(0.9536), np.float32(0.9157)] 
2025-01-27 22:32:03.628973: Epoch time: 48.8 s 
2025-01-27 22:32:04.831334:  
2025-01-27 22:32:04.833859: Epoch 287 
2025-01-27 22:32:04.836378: Current learning rate: 0.00738 
2025-01-27 22:32:52.842100: train_loss -0.792 
2025-01-27 22:32:52.846694: val_loss -0.7129 
2025-01-27 22:32:52.849541: Pseudo dice [np.float32(0.9419), np.float32(0.91)] 
2025-01-27 22:32:52.852406: Epoch time: 48.01 s 
2025-01-27 22:32:53.936825:  
2025-01-27 22:32:53.939797: Epoch 288 
2025-01-27 22:32:53.942635: Current learning rate: 0.00737 
2025-01-27 22:33:42.152909: train_loss -0.8097 
2025-01-27 22:33:42.159684: val_loss -0.7188 
2025-01-27 22:33:42.162349: Pseudo dice [np.float32(0.9376), np.float32(0.9146)] 
2025-01-27 22:33:42.165287: Epoch time: 48.22 s 
2025-01-27 22:33:43.267683:  
2025-01-27 22:33:43.270922: Epoch 289 
2025-01-27 22:33:43.273774: Current learning rate: 0.00736 
2025-01-27 22:34:31.466958: train_loss -0.8067 
2025-01-27 22:34:31.471585: val_loss -0.7636 
2025-01-27 22:34:31.474587: Pseudo dice [np.float32(0.9508), np.float32(0.926)] 
2025-01-27 22:34:31.477329: Epoch time: 48.2 s 
2025-01-27 22:34:32.558084:  
2025-01-27 22:34:32.561300: Epoch 290 
2025-01-27 22:34:32.564076: Current learning rate: 0.00735 
2025-01-27 22:35:20.527534: train_loss -0.8146 
2025-01-27 22:35:20.534043: val_loss -0.7355 
2025-01-27 22:35:20.536944: Pseudo dice [np.float32(0.9451), np.float32(0.9309)] 
2025-01-27 22:35:20.539867: Epoch time: 47.97 s 
2025-01-27 22:35:21.618378:  
2025-01-27 22:35:21.621370: Epoch 291 
2025-01-27 22:35:21.624450: Current learning rate: 0.00734 
2025-01-27 22:36:09.817895: train_loss -0.7965 
2025-01-27 22:36:09.823618: val_loss -0.7669 
2025-01-27 22:36:09.826127: Pseudo dice [np.float32(0.9446), np.float32(0.9305)] 
2025-01-27 22:36:09.828492: Epoch time: 48.2 s 
2025-01-27 22:36:10.903035:  
2025-01-27 22:36:10.906065: Epoch 292 
2025-01-27 22:36:10.909336: Current learning rate: 0.00733 
2025-01-27 22:36:59.088668: train_loss -0.8087 
2025-01-27 22:36:59.094946: val_loss -0.7407 
2025-01-27 22:36:59.097816: Pseudo dice [np.float32(0.9481), np.float32(0.9138)] 
2025-01-27 22:36:59.101107: Epoch time: 48.19 s 
2025-01-27 22:37:00.176155:  
2025-01-27 22:37:00.179024: Epoch 293 
2025-01-27 22:37:00.181911: Current learning rate: 0.00732 
2025-01-27 22:37:48.246405: train_loss -0.8103 
2025-01-27 22:37:48.250950: val_loss -0.7533 
2025-01-27 22:37:48.253911: Pseudo dice [np.float32(0.9487), np.float32(0.9287)] 
2025-01-27 22:37:48.256822: Epoch time: 48.07 s 
2025-01-27 22:37:49.410312:  
2025-01-27 22:37:49.413155: Epoch 294 
2025-01-27 22:37:49.415780: Current learning rate: 0.00731 
2025-01-27 22:38:37.867469: train_loss -0.7849 
2025-01-27 22:38:37.872556: val_loss -0.7328 
2025-01-27 22:38:37.874968: Pseudo dice [np.float32(0.9443), np.float32(0.8933)] 
2025-01-27 22:38:37.877433: Epoch time: 48.46 s 
2025-01-27 22:38:39.052105:  
2025-01-27 22:38:39.055029: Epoch 295 
2025-01-27 22:38:39.057871: Current learning rate: 0.0073 
2025-01-27 22:39:26.686575: train_loss -0.8059 
2025-01-27 22:39:26.691214: val_loss -0.7231 
2025-01-27 22:39:26.693926: Pseudo dice [np.float32(0.9479), np.float32(0.9044)] 
2025-01-27 22:39:26.696751: Epoch time: 47.64 s 
2025-01-27 22:39:28.133152:  
2025-01-27 22:39:28.136433: Epoch 296 
2025-01-27 22:39:28.139386: Current learning rate: 0.00729 
2025-01-27 22:40:16.022698: train_loss -0.8129 
2025-01-27 22:40:16.029388: val_loss -0.7548 
2025-01-27 22:40:16.032216: Pseudo dice [np.float32(0.9463), np.float32(0.9005)] 
2025-01-27 22:40:16.035036: Epoch time: 47.89 s 
2025-01-27 22:40:17.162251:  
2025-01-27 22:40:17.165160: Epoch 297 
2025-01-27 22:40:17.168156: Current learning rate: 0.00728 
2025-01-27 22:41:05.099718: train_loss -0.8034 
2025-01-27 22:41:05.103634: val_loss -0.7193 
2025-01-27 22:41:05.106187: Pseudo dice [np.float32(0.9491), np.float32(0.8917)] 
2025-01-27 22:41:05.108768: Epoch time: 47.94 s 
2025-01-27 22:41:06.241002:  
2025-01-27 22:41:06.243469: Epoch 298 
2025-01-27 22:41:06.246098: Current learning rate: 0.00727 
2025-01-27 22:41:54.002949: train_loss -0.7876 
2025-01-27 22:41:54.009234: val_loss -0.7382 
2025-01-27 22:41:54.012405: Pseudo dice [np.float32(0.9432), np.float32(0.8567)] 
2025-01-27 22:41:54.015033: Epoch time: 47.76 s 
2025-01-27 22:41:55.182095:  
2025-01-27 22:41:55.185170: Epoch 299 
2025-01-27 22:41:55.188199: Current learning rate: 0.00726 
2025-01-27 22:42:43.198853: train_loss -0.7929 
2025-01-27 22:42:43.205179: val_loss -0.7381 
2025-01-27 22:42:43.208098: Pseudo dice [np.float32(0.952), np.float32(0.9392)] 
2025-01-27 22:42:43.210977: Epoch time: 48.02 s 
2025-01-27 22:42:44.881202:  
2025-01-27 22:42:44.884471: Epoch 300 
2025-01-27 22:42:44.887493: Current learning rate: 0.00725 
2025-01-27 22:43:33.044071: train_loss -0.8069 
2025-01-27 22:43:33.049984: val_loss -0.6984 
2025-01-27 22:43:33.052872: Pseudo dice [np.float32(0.9529), np.float32(0.9011)] 
2025-01-27 22:43:33.055866: Epoch time: 48.16 s 
2025-01-27 22:43:34.184357:  
2025-01-27 22:43:34.187781: Epoch 301 
2025-01-27 22:43:34.190323: Current learning rate: 0.00724 
2025-01-27 22:44:22.177310: train_loss -0.8066 
2025-01-27 22:44:22.182330: val_loss -0.7509 
2025-01-27 22:44:22.185531: Pseudo dice [np.float32(0.9516), np.float32(0.908)] 
2025-01-27 22:44:22.188559: Epoch time: 47.99 s 
2025-01-27 22:44:23.314793:  
2025-01-27 22:44:23.317950: Epoch 302 
2025-01-27 22:44:23.320602: Current learning rate: 0.00724 
2025-01-27 22:45:11.282399: train_loss -0.811 
2025-01-27 22:45:11.288332: val_loss -0.7316 
2025-01-27 22:45:11.291287: Pseudo dice [np.float32(0.9375), np.float32(0.8866)] 
2025-01-27 22:45:11.294100: Epoch time: 47.97 s 
2025-01-27 22:45:12.420033:  
2025-01-27 22:45:12.423092: Epoch 303 
2025-01-27 22:45:12.426400: Current learning rate: 0.00723 
2025-01-27 22:46:00.152740: train_loss -0.7834 
2025-01-27 22:46:00.156947: val_loss -0.7498 
2025-01-27 22:46:00.159470: Pseudo dice [np.float32(0.9466), np.float32(0.9307)] 
2025-01-27 22:46:00.161948: Epoch time: 47.73 s 
2025-01-27 22:46:01.327752:  
2025-01-27 22:46:01.331005: Epoch 304 
2025-01-27 22:46:01.333736: Current learning rate: 0.00722 
2025-01-27 22:46:49.627371: train_loss -0.7866 
2025-01-27 22:46:49.632798: val_loss -0.721 
2025-01-27 22:46:49.635478: Pseudo dice [np.float32(0.9384), np.float32(0.9116)] 
2025-01-27 22:46:49.637977: Epoch time: 48.3 s 
2025-01-27 22:46:50.803113:  
2025-01-27 22:46:50.805714: Epoch 305 
2025-01-27 22:46:50.808140: Current learning rate: 0.00721 
2025-01-27 22:47:38.641696: train_loss -0.7916 
2025-01-27 22:47:38.646715: val_loss -0.6998 
2025-01-27 22:47:38.649316: Pseudo dice [np.float32(0.9377), np.float32(0.901)] 
2025-01-27 22:47:38.651801: Epoch time: 47.84 s 
2025-01-27 22:47:39.773394:  
2025-01-27 22:47:39.776680: Epoch 306 
2025-01-27 22:47:39.779456: Current learning rate: 0.0072 
2025-01-27 22:48:27.300802: train_loss -0.8002 
2025-01-27 22:48:27.306586: val_loss -0.748 
2025-01-27 22:48:27.309211: Pseudo dice [np.float32(0.9501), np.float32(0.9072)] 
2025-01-27 22:48:27.311580: Epoch time: 47.53 s 
2025-01-27 22:48:28.476091:  
2025-01-27 22:48:28.479572: Epoch 307 
2025-01-27 22:48:28.482520: Current learning rate: 0.00719 
2025-01-27 22:49:16.498051: train_loss -0.7848 
2025-01-27 22:49:16.502999: val_loss -0.7185 
2025-01-27 22:49:16.505633: Pseudo dice [np.float32(0.9531), np.float32(0.8484)] 
2025-01-27 22:49:16.508207: Epoch time: 48.02 s 
2025-01-27 22:49:17.632298:  
2025-01-27 22:49:17.635522: Epoch 308 
2025-01-27 22:49:17.638472: Current learning rate: 0.00718 
2025-01-27 22:50:05.750093: train_loss -0.7998 
2025-01-27 22:50:05.756058: val_loss -0.7374 
2025-01-27 22:50:05.758835: Pseudo dice [np.float32(0.9381), np.float32(0.9216)] 
2025-01-27 22:50:05.761269: Epoch time: 48.12 s 
2025-01-27 22:50:07.252070:  
2025-01-27 22:50:07.255762: Epoch 309 
2025-01-27 22:50:07.258766: Current learning rate: 0.00717 
2025-01-27 22:50:55.086604: train_loss -0.8011 
2025-01-27 22:50:55.092321: val_loss -0.7242 
2025-01-27 22:50:55.095090: Pseudo dice [np.float32(0.9497), np.float32(0.8882)] 
2025-01-27 22:50:55.097859: Epoch time: 47.84 s 
2025-01-27 22:50:56.224822:  
2025-01-27 22:50:56.228133: Epoch 310 
2025-01-27 22:50:56.231317: Current learning rate: 0.00716 
2025-01-27 22:51:44.080939: train_loss -0.8023 
2025-01-27 22:51:44.086979: val_loss -0.7015 
2025-01-27 22:51:44.090036: Pseudo dice [np.float32(0.9482), np.float32(0.8995)] 
2025-01-27 22:51:44.093194: Epoch time: 47.86 s 
2025-01-27 22:51:45.215860:  
2025-01-27 22:51:45.219436: Epoch 311 
2025-01-27 22:51:45.222304: Current learning rate: 0.00715 
2025-01-27 22:52:33.399580: train_loss -0.7958 
2025-01-27 22:52:33.409233: val_loss -0.7269 
2025-01-27 22:52:33.411965: Pseudo dice [np.float32(0.9467), np.float32(0.8931)] 
2025-01-27 22:52:33.414714: Epoch time: 48.18 s 
2025-01-27 22:52:34.539738:  
2025-01-27 22:52:34.542818: Epoch 312 
2025-01-27 22:52:34.545816: Current learning rate: 0.00714 
2025-01-27 22:53:23.083825: train_loss -0.7882 
2025-01-27 22:53:23.090474: val_loss -0.7186 
2025-01-27 22:53:23.093449: Pseudo dice [np.float32(0.9427), np.float32(0.8253)] 
2025-01-27 22:53:23.096094: Epoch time: 48.54 s 
2025-01-27 22:53:24.234670:  
2025-01-27 22:53:24.238118: Epoch 313 
2025-01-27 22:53:24.241168: Current learning rate: 0.00713 
2025-01-27 22:54:12.058505: train_loss -0.8087 
2025-01-27 22:54:12.062744: val_loss -0.7275 
2025-01-27 22:54:12.065389: Pseudo dice [np.float32(0.9406), np.float32(0.8985)] 
2025-01-27 22:54:12.068024: Epoch time: 47.82 s 
2025-01-27 22:54:13.199162:  
2025-01-27 22:54:13.202682: Epoch 314 
2025-01-27 22:54:13.205812: Current learning rate: 0.00712 
2025-01-27 22:55:01.196747: train_loss -0.8133 
2025-01-27 22:55:01.202461: val_loss -0.719 
2025-01-27 22:55:01.205371: Pseudo dice [np.float32(0.9486), np.float32(0.9231)] 
2025-01-27 22:55:01.207773: Epoch time: 48.0 s 
2025-01-27 22:55:02.345906:  
2025-01-27 22:55:02.349487: Epoch 315 
2025-01-27 22:55:02.352582: Current learning rate: 0.00711 
2025-01-27 22:55:50.442204: train_loss -0.8091 
2025-01-27 22:55:50.446718: val_loss -0.7512 
2025-01-27 22:55:50.449645: Pseudo dice [np.float32(0.9381), np.float32(0.9073)] 
2025-01-27 22:55:50.452397: Epoch time: 48.1 s 
2025-01-27 22:55:51.583139:  
2025-01-27 22:55:51.585539: Epoch 316 
2025-01-27 22:55:51.587602: Current learning rate: 0.0071 
2025-01-27 22:56:39.380135: train_loss -0.8086 
2025-01-27 22:56:39.386023: val_loss -0.7236 
2025-01-27 22:56:39.388758: Pseudo dice [np.float32(0.9498), np.float32(0.9181)] 
2025-01-27 22:56:39.391379: Epoch time: 47.8 s 
2025-01-27 22:56:40.522594:  
2025-01-27 22:56:40.525867: Epoch 317 
2025-01-27 22:56:40.528811: Current learning rate: 0.0071 
2025-01-27 22:57:28.570143: train_loss -0.8212 
2025-01-27 22:57:28.574904: val_loss -0.7686 
2025-01-27 22:57:28.577808: Pseudo dice [np.float32(0.9461), np.float32(0.9295)] 
2025-01-27 22:57:28.580600: Epoch time: 48.05 s 
2025-01-27 22:57:29.708396:  
2025-01-27 22:57:29.711582: Epoch 318 
2025-01-27 22:57:29.714586: Current learning rate: 0.00709 
2025-01-27 22:58:17.663627: train_loss -0.8044 
2025-01-27 22:58:17.670972: val_loss -0.6902 
2025-01-27 22:58:17.673576: Pseudo dice [np.float32(0.9447), np.float32(0.8356)] 
2025-01-27 22:58:17.676455: Epoch time: 47.96 s 
2025-01-27 22:58:18.809563:  
2025-01-27 22:58:18.812976: Epoch 319 
2025-01-27 22:58:18.815948: Current learning rate: 0.00708 
2025-01-27 22:59:07.074868: train_loss -0.8104 
2025-01-27 22:59:07.079237: val_loss -0.7067 
2025-01-27 22:59:07.082121: Pseudo dice [np.float32(0.9489), np.float32(0.8771)] 
2025-01-27 22:59:07.084873: Epoch time: 48.27 s 
2025-01-27 22:59:08.223744:  
2025-01-27 22:59:08.227030: Epoch 320 
2025-01-27 22:59:08.229962: Current learning rate: 0.00707 
2025-01-27 22:59:56.083620: train_loss -0.8005 
2025-01-27 22:59:56.089602: val_loss -0.7178 
2025-01-27 22:59:56.092565: Pseudo dice [np.float32(0.9439), np.float32(0.8685)] 
2025-01-27 22:59:56.095223: Epoch time: 47.86 s 
2025-01-27 22:59:57.528630:  
2025-01-27 22:59:57.532154: Epoch 321 
2025-01-27 22:59:57.535081: Current learning rate: 0.00706 
2025-01-27 23:00:45.635500: train_loss -0.8197 
2025-01-27 23:00:45.639691: val_loss -0.748 
2025-01-27 23:00:45.642314: Pseudo dice [np.float32(0.9559), np.float32(0.9416)] 
2025-01-27 23:00:45.645048: Epoch time: 48.11 s 
2025-01-27 23:00:46.778599:  
2025-01-27 23:00:46.781832: Epoch 322 
2025-01-27 23:00:46.784973: Current learning rate: 0.00705 
2025-01-27 23:01:35.267378: train_loss -0.8085 
2025-01-27 23:01:35.272394: val_loss -0.7266 
2025-01-27 23:01:35.274748: Pseudo dice [np.float32(0.948), np.float32(0.9191)] 
2025-01-27 23:01:35.277055: Epoch time: 48.49 s 
2025-01-27 23:01:36.379130:  
2025-01-27 23:01:36.381956: Epoch 323 
2025-01-27 23:01:36.384282: Current learning rate: 0.00704 
2025-01-27 23:02:25.710916: train_loss -0.8133 
2025-01-27 23:02:25.715341: val_loss -0.7762 
2025-01-27 23:02:25.718698: Pseudo dice [np.float32(0.9335), np.float32(0.8947)] 
2025-01-27 23:02:25.721303: Epoch time: 49.33 s 
2025-01-27 23:02:26.857765:  
2025-01-27 23:02:26.861037: Epoch 324 
2025-01-27 23:02:26.864264: Current learning rate: 0.00703 
2025-01-27 23:03:14.805073: train_loss -0.7899 
2025-01-27 23:03:14.810460: val_loss -0.707 
2025-01-27 23:03:14.813011: Pseudo dice [np.float32(0.9207), np.float32(0.8822)] 
2025-01-27 23:03:14.815668: Epoch time: 47.95 s 
2025-01-27 23:03:15.993100:  
2025-01-27 23:03:15.996118: Epoch 325 
2025-01-27 23:03:15.999062: Current learning rate: 0.00702 
2025-01-27 23:04:03.629176: train_loss -0.7975 
2025-01-27 23:04:03.633697: val_loss -0.713 
2025-01-27 23:04:03.636745: Pseudo dice [np.float32(0.9235), np.float32(0.8765)] 
2025-01-27 23:04:03.639519: Epoch time: 47.64 s 
2025-01-27 23:04:04.780897:  
2025-01-27 23:04:04.784018: Epoch 326 
2025-01-27 23:04:04.786893: Current learning rate: 0.00701 
2025-01-27 23:04:52.763403: train_loss -0.8145 
2025-01-27 23:04:52.770752: val_loss -0.7005 
2025-01-27 23:04:52.773932: Pseudo dice [np.float32(0.9572), np.float32(0.8976)] 
2025-01-27 23:04:52.776595: Epoch time: 47.98 s 
2025-01-27 23:04:53.905968:  
2025-01-27 23:04:53.908792: Epoch 327 
2025-01-27 23:04:53.911206: Current learning rate: 0.007 
2025-01-27 23:05:41.757039: train_loss -0.8121 
2025-01-27 23:05:41.761613: val_loss -0.7343 
2025-01-27 23:05:41.764408: Pseudo dice [np.float32(0.9561), np.float32(0.7646)] 
2025-01-27 23:05:41.766902: Epoch time: 47.85 s 
2025-01-27 23:05:42.904119:  
2025-01-27 23:05:42.907562: Epoch 328 
2025-01-27 23:05:42.910654: Current learning rate: 0.00699 
2025-01-27 23:06:31.121077: train_loss -0.8123 
2025-01-27 23:06:31.127581: val_loss -0.7051 
2025-01-27 23:06:31.130677: Pseudo dice [np.float32(0.9454), np.float32(0.881)] 
2025-01-27 23:06:31.133537: Epoch time: 48.22 s 
2025-01-27 23:06:32.306555:  
2025-01-27 23:06:32.309601: Epoch 329 
2025-01-27 23:06:32.312681: Current learning rate: 0.00698 
2025-01-27 23:07:20.095756: train_loss -0.7991 
2025-01-27 23:07:20.100864: val_loss -0.6954 
2025-01-27 23:07:20.104200: Pseudo dice [np.float32(0.9396), np.float32(0.8737)] 
2025-01-27 23:07:20.107102: Epoch time: 47.79 s 
2025-01-27 23:07:21.277004:  
2025-01-27 23:07:21.280151: Epoch 330 
2025-01-27 23:07:21.283031: Current learning rate: 0.00697 
2025-01-27 23:08:09.412586: train_loss -0.8026 
2025-01-27 23:08:09.417948: val_loss -0.7252 
2025-01-27 23:08:09.420386: Pseudo dice [np.float32(0.9235), np.float32(0.8523)] 
2025-01-27 23:08:09.422754: Epoch time: 48.14 s 
2025-01-27 23:08:10.546951:  
2025-01-27 23:08:10.549356: Epoch 331 
2025-01-27 23:08:10.551891: Current learning rate: 0.00696 
2025-01-27 23:08:58.202423: train_loss -0.8024 
2025-01-27 23:08:58.207501: val_loss -0.7071 
2025-01-27 23:08:58.210980: Pseudo dice [np.float32(0.9358), np.float32(0.8877)] 
2025-01-27 23:08:58.214158: Epoch time: 47.66 s 
2025-01-27 23:08:59.344121:  
2025-01-27 23:08:59.347503: Epoch 332 
2025-01-27 23:08:59.350389: Current learning rate: 0.00696 
2025-01-27 23:09:47.338805: train_loss -0.8034 
2025-01-27 23:09:47.343729: val_loss -0.7005 
2025-01-27 23:09:47.346244: Pseudo dice [np.float32(0.9512), np.float32(0.8907)] 
2025-01-27 23:09:47.348639: Epoch time: 48.0 s 
2025-01-27 23:09:48.775220:  
2025-01-27 23:09:48.777650: Epoch 333 
2025-01-27 23:09:48.780101: Current learning rate: 0.00695 
2025-01-27 23:10:36.578179: train_loss -0.8126 
2025-01-27 23:10:36.582723: val_loss -0.7012 
2025-01-27 23:10:36.585727: Pseudo dice [np.float32(0.9311), np.float32(0.8912)] 
2025-01-27 23:10:36.588501: Epoch time: 47.8 s 
2025-01-27 23:10:37.717105:  
2025-01-27 23:10:37.720147: Epoch 334 
2025-01-27 23:10:37.723483: Current learning rate: 0.00694 
2025-01-27 23:11:25.617781: train_loss -0.8151 
2025-01-27 23:11:25.623969: val_loss -0.7517 
2025-01-27 23:11:25.626969: Pseudo dice [np.float32(0.943), np.float32(0.9326)] 
2025-01-27 23:11:25.629666: Epoch time: 47.9 s 
2025-01-27 23:11:26.781976:  
2025-01-27 23:11:26.785319: Epoch 335 
2025-01-27 23:11:26.788268: Current learning rate: 0.00693 
2025-01-27 23:12:14.827326: train_loss -0.8196 
2025-01-27 23:12:14.832215: val_loss -0.7389 
2025-01-27 23:12:14.835173: Pseudo dice [np.float32(0.9533), np.float32(0.9428)] 
2025-01-27 23:12:14.837971: Epoch time: 48.05 s 
2025-01-27 23:12:16.029679:  
2025-01-27 23:12:16.032620: Epoch 336 
2025-01-27 23:12:16.035373: Current learning rate: 0.00692 
2025-01-27 23:13:04.151106: train_loss -0.8131 
2025-01-27 23:13:04.156987: val_loss -0.712 
2025-01-27 23:13:04.159471: Pseudo dice [np.float32(0.937), np.float32(0.9045)] 
2025-01-27 23:13:04.162482: Epoch time: 48.12 s 
2025-01-27 23:13:05.343016:  
2025-01-27 23:13:05.345962: Epoch 337 
2025-01-27 23:13:05.348789: Current learning rate: 0.00691 
2025-01-27 23:13:53.556666: train_loss -0.8073 
2025-01-27 23:13:53.561388: val_loss -0.7185 
2025-01-27 23:13:53.564457: Pseudo dice [np.float32(0.9329), np.float32(0.8713)] 
2025-01-27 23:13:53.567075: Epoch time: 48.21 s 
2025-01-27 23:13:54.713001:  
2025-01-27 23:13:54.715961: Epoch 338 
2025-01-27 23:13:54.718641: Current learning rate: 0.0069 
2025-01-27 23:14:42.876302: train_loss -0.8066 
2025-01-27 23:14:42.881998: val_loss -0.734 
2025-01-27 23:14:42.884796: Pseudo dice [np.float32(0.9532), np.float32(0.9247)] 
2025-01-27 23:14:42.887299: Epoch time: 48.16 s 
2025-01-27 23:14:44.083706:  
2025-01-27 23:14:44.086960: Epoch 339 
2025-01-27 23:14:44.089702: Current learning rate: 0.00689 
2025-01-27 23:15:31.977790: train_loss -0.814 
2025-01-27 23:15:31.982095: val_loss -0.7444 
2025-01-27 23:15:31.985067: Pseudo dice [np.float32(0.9504), np.float32(0.9201)] 
2025-01-27 23:15:31.987835: Epoch time: 47.89 s 
2025-01-27 23:15:33.180960:  
2025-01-27 23:15:33.184004: Epoch 340 
2025-01-27 23:15:33.186944: Current learning rate: 0.00688 
2025-01-27 23:16:21.450938: train_loss -0.8075 
2025-01-27 23:16:21.457290: val_loss -0.756 
2025-01-27 23:16:21.460170: Pseudo dice [np.float32(0.9465), np.float32(0.9173)] 
2025-01-27 23:16:21.462720: Epoch time: 48.27 s 
2025-01-27 23:16:22.612354:  
2025-01-27 23:16:22.615629: Epoch 341 
2025-01-27 23:16:22.618285: Current learning rate: 0.00687 
2025-01-27 23:17:10.667746: train_loss -0.8022 
2025-01-27 23:17:10.672155: val_loss -0.75 
2025-01-27 23:17:10.675088: Pseudo dice [np.float32(0.9509), np.float32(0.9258)] 
2025-01-27 23:17:10.678042: Epoch time: 48.06 s 
2025-01-27 23:17:11.869229:  
2025-01-27 23:17:11.872454: Epoch 342 
2025-01-27 23:17:11.875538: Current learning rate: 0.00686 
2025-01-27 23:17:59.637573: train_loss -0.81 
2025-01-27 23:17:59.643402: val_loss -0.6679 
2025-01-27 23:17:59.646043: Pseudo dice [np.float32(0.9347), np.float32(0.8884)] 
2025-01-27 23:17:59.648492: Epoch time: 47.77 s 
2025-01-27 23:18:00.788427:  
2025-01-27 23:18:00.791504: Epoch 343 
2025-01-27 23:18:00.794723: Current learning rate: 0.00685 
2025-01-27 23:18:48.559536: train_loss -0.7981 
2025-01-27 23:18:48.564147: val_loss -0.7224 
2025-01-27 23:18:48.567293: Pseudo dice [np.float32(0.9474), np.float32(0.9194)] 
2025-01-27 23:18:48.570026: Epoch time: 47.77 s 
2025-01-27 23:18:49.745023:  
2025-01-27 23:18:49.748350: Epoch 344 
2025-01-27 23:18:49.751369: Current learning rate: 0.00684 
2025-01-27 23:19:37.699905: train_loss -0.797 
2025-01-27 23:19:37.705196: val_loss -0.6952 
2025-01-27 23:19:37.707614: Pseudo dice [np.float32(0.915), np.float32(0.8643)] 
2025-01-27 23:19:37.709784: Epoch time: 47.96 s 
2025-01-27 23:19:38.852297:  
2025-01-27 23:19:38.854915: Epoch 345 
2025-01-27 23:19:38.857679: Current learning rate: 0.00683 
2025-01-27 23:20:26.545511: train_loss -0.8029 
2025-01-27 23:20:26.550257: val_loss -0.7311 
2025-01-27 23:20:26.552979: Pseudo dice [np.float32(0.9323), np.float32(0.8715)] 
2025-01-27 23:20:26.555439: Epoch time: 47.69 s 
2025-01-27 23:20:28.017676:  
2025-01-27 23:20:28.020817: Epoch 346 
2025-01-27 23:20:28.023476: Current learning rate: 0.00682 
2025-01-27 23:21:16.588926: train_loss -0.8173 
2025-01-27 23:21:16.594944: val_loss -0.707 
2025-01-27 23:21:16.597815: Pseudo dice [np.float32(0.9284), np.float32(0.9025)] 
2025-01-27 23:21:16.600675: Epoch time: 48.57 s 
2025-01-27 23:21:17.734050:  
2025-01-27 23:21:17.737203: Epoch 347 
2025-01-27 23:21:17.739876: Current learning rate: 0.00681 
2025-01-27 23:22:05.959099: train_loss -0.8156 
2025-01-27 23:22:05.963667: val_loss -0.7103 
2025-01-27 23:22:05.966660: Pseudo dice [np.float32(0.945), np.float32(0.8935)] 
2025-01-27 23:22:05.970104: Epoch time: 48.23 s 
2025-01-27 23:22:07.124886:  
2025-01-27 23:22:07.127948: Epoch 348 
2025-01-27 23:22:07.130504: Current learning rate: 0.0068 
2025-01-27 23:22:54.861336: train_loss -0.8122 
2025-01-27 23:22:54.867919: val_loss -0.6764 
2025-01-27 23:22:54.870683: Pseudo dice [np.float32(0.9259), np.float32(0.8144)] 
2025-01-27 23:22:54.873826: Epoch time: 47.74 s 
2025-01-27 23:22:56.087351:  
2025-01-27 23:22:56.090498: Epoch 349 
2025-01-27 23:22:56.093479: Current learning rate: 0.0068 
2025-01-27 23:23:43.976589: train_loss -0.8004 
2025-01-27 23:23:43.981346: val_loss -0.7437 
2025-01-27 23:23:43.984453: Pseudo dice [np.float32(0.9346), np.float32(0.9071)] 
2025-01-27 23:23:43.987219: Epoch time: 47.89 s 
2025-01-27 23:23:45.653821:  
2025-01-27 23:23:45.657204: Epoch 350 
2025-01-27 23:23:45.660767: Current learning rate: 0.00679 
2025-01-27 23:24:33.871382: train_loss -0.7997 
2025-01-27 23:24:33.876186: val_loss -0.6718 
2025-01-27 23:24:33.878498: Pseudo dice [np.float32(0.9485), np.float32(0.8688)] 
2025-01-27 23:24:33.880783: Epoch time: 48.22 s 
2025-01-27 23:24:35.031156:  
2025-01-27 23:24:35.033946: Epoch 351 
2025-01-27 23:24:35.036685: Current learning rate: 0.00678 
2025-01-27 23:25:23.300595: train_loss -0.7907 
2025-01-27 23:25:23.305134: val_loss -0.6909 
2025-01-27 23:25:23.307995: Pseudo dice [np.float32(0.9296), np.float32(0.9069)] 
2025-01-27 23:25:23.310380: Epoch time: 48.27 s 
2025-01-27 23:25:24.424206:  
2025-01-27 23:25:24.428905: Epoch 352 
2025-01-27 23:25:24.432051: Current learning rate: 0.00677 
2025-01-27 23:26:12.697855: train_loss -0.8092 
2025-01-27 23:26:12.703867: val_loss -0.7147 
2025-01-27 23:26:12.706531: Pseudo dice [np.float32(0.9479), np.float32(0.8873)] 
2025-01-27 23:26:12.709184: Epoch time: 48.27 s 
2025-01-27 23:26:13.831249:  
2025-01-27 23:26:13.833949: Epoch 353 
2025-01-27 23:26:13.836660: Current learning rate: 0.00676 
2025-01-27 23:27:02.434695: train_loss -0.7932 
2025-01-27 23:27:02.439095: val_loss -0.6866 
2025-01-27 23:27:02.441645: Pseudo dice [np.float32(0.9053), np.float32(0.8762)] 
2025-01-27 23:27:02.444072: Epoch time: 48.6 s 
2025-01-27 23:27:03.546207:  
2025-01-27 23:27:03.549672: Epoch 354 
2025-01-27 23:27:03.552854: Current learning rate: 0.00675 
2025-01-27 23:27:52.194933: train_loss -0.7982 
2025-01-27 23:27:52.200123: val_loss -0.7114 
2025-01-27 23:27:52.202915: Pseudo dice [np.float32(0.9481), np.float32(0.8889)] 
2025-01-27 23:27:52.205625: Epoch time: 48.65 s 
2025-01-27 23:27:53.299188:  
2025-01-27 23:27:53.302202: Epoch 355 
2025-01-27 23:27:53.305114: Current learning rate: 0.00674 
2025-01-27 23:28:41.682081: train_loss -0.8152 
2025-01-27 23:28:41.686161: val_loss -0.7162 
2025-01-27 23:28:41.688786: Pseudo dice [np.float32(0.9407), np.float32(0.8727)] 
2025-01-27 23:28:41.691673: Epoch time: 48.38 s 
2025-01-27 23:28:42.901980:  
2025-01-27 23:28:42.905460: Epoch 356 
2025-01-27 23:28:42.909136: Current learning rate: 0.00673 
2025-01-27 23:29:31.180504: train_loss -0.7939 
2025-01-27 23:29:31.187080: val_loss -0.7616 
2025-01-27 23:29:31.189763: Pseudo dice [np.float32(0.9526), np.float32(0.9026)] 
2025-01-27 23:29:31.192511: Epoch time: 48.28 s 
2025-01-27 23:29:32.365875:  
2025-01-27 23:29:32.368936: Epoch 357 
2025-01-27 23:29:32.371952: Current learning rate: 0.00672 
2025-01-27 23:30:21.186385: train_loss -0.8057 
2025-01-27 23:30:21.190868: val_loss -0.6941 
2025-01-27 23:30:21.193807: Pseudo dice [np.float32(0.9391), np.float32(0.9037)] 
2025-01-27 23:30:21.196532: Epoch time: 48.82 s 
2025-01-27 23:30:22.716077:  
2025-01-27 23:30:22.720087: Epoch 358 
2025-01-27 23:30:22.723212: Current learning rate: 0.00671 
2025-01-27 23:31:11.006474: train_loss -0.8062 
2025-01-27 23:31:11.013312: val_loss -0.7484 
2025-01-27 23:31:11.016268: Pseudo dice [np.float32(0.9475), np.float32(0.9214)] 
2025-01-27 23:31:11.018831: Epoch time: 48.29 s 
2025-01-27 23:31:12.136198:  
2025-01-27 23:31:12.139143: Epoch 359 
2025-01-27 23:31:12.141809: Current learning rate: 0.0067 
2025-01-27 23:32:00.546288: train_loss -0.7844 
2025-01-27 23:32:00.550711: val_loss -0.726 
2025-01-27 23:32:00.553783: Pseudo dice [np.float32(0.9338), np.float32(0.8993)] 
2025-01-27 23:32:00.556226: Epoch time: 48.41 s 
2025-01-27 23:32:01.651538:  
2025-01-27 23:32:01.654871: Epoch 360 
2025-01-27 23:32:01.661068: Current learning rate: 0.00669 
2025-01-27 23:32:50.192508: train_loss -0.7919 
2025-01-27 23:32:50.197793: val_loss -0.6965 
2025-01-27 23:32:50.200256: Pseudo dice [np.float32(0.9459), np.float32(0.8594)] 
2025-01-27 23:32:50.202539: Epoch time: 48.54 s 
2025-01-27 23:32:51.298686:  
2025-01-27 23:32:51.301153: Epoch 361 
2025-01-27 23:32:51.303576: Current learning rate: 0.00668 
2025-01-27 23:33:39.519162: train_loss -0.782 
2025-01-27 23:33:39.523814: val_loss -0.7081 
2025-01-27 23:33:39.526665: Pseudo dice [np.float32(0.8978), np.float32(0.8845)] 
2025-01-27 23:33:39.529176: Epoch time: 48.22 s 
2025-01-27 23:33:40.623478:  
2025-01-27 23:33:40.626477: Epoch 362 
2025-01-27 23:33:40.629319: Current learning rate: 0.00667 
2025-01-27 23:34:28.458307: train_loss -0.7873 
2025-01-27 23:34:28.464094: val_loss -0.6906 
2025-01-27 23:34:28.466800: Pseudo dice [np.float32(0.9459), np.float32(0.9245)] 
2025-01-27 23:34:28.469426: Epoch time: 47.84 s 
2025-01-27 23:34:29.570343:  
2025-01-27 23:34:29.573284: Epoch 363 
2025-01-27 23:34:29.576179: Current learning rate: 0.00666 
2025-01-27 23:35:17.931168: train_loss -0.806 
2025-01-27 23:35:17.936040: val_loss -0.7136 
2025-01-27 23:35:17.938935: Pseudo dice [np.float32(0.952), np.float32(0.9291)] 
2025-01-27 23:35:17.941522: Epoch time: 48.36 s 
2025-01-27 23:35:19.104126:  
2025-01-27 23:35:19.107261: Epoch 364 
2025-01-27 23:35:19.110518: Current learning rate: 0.00665 
2025-01-27 23:36:07.212958: train_loss -0.8151 
2025-01-27 23:36:07.218362: val_loss -0.698 
2025-01-27 23:36:07.220841: Pseudo dice [np.float32(0.952), np.float32(0.7917)] 
2025-01-27 23:36:07.223367: Epoch time: 48.11 s 
2025-01-27 23:36:08.334018:  
2025-01-27 23:36:08.337430: Epoch 365 
2025-01-27 23:36:08.340629: Current learning rate: 0.00665 
2025-01-27 23:36:57.000417: train_loss -0.8183 
2025-01-27 23:36:57.006274: val_loss -0.7088 
2025-01-27 23:36:57.009282: Pseudo dice [np.float32(0.9586), np.float32(0.9239)] 
2025-01-27 23:36:57.012077: Epoch time: 48.67 s 
2025-01-27 23:36:58.235664:  
2025-01-27 23:36:58.238902: Epoch 366 
2025-01-27 23:36:58.241811: Current learning rate: 0.00664 
2025-01-27 23:37:46.274369: train_loss -0.8135 
2025-01-27 23:37:46.281074: val_loss -0.7229 
2025-01-27 23:37:46.283935: Pseudo dice [np.float32(0.9442), np.float32(0.8291)] 
2025-01-27 23:37:46.286926: Epoch time: 48.04 s 
2025-01-27 23:37:47.745922:  
2025-01-27 23:37:47.748869: Epoch 367 
2025-01-27 23:37:47.751609: Current learning rate: 0.00663 
2025-01-27 23:38:35.804343: train_loss -0.8206 
2025-01-27 23:38:35.809275: val_loss -0.7454 
2025-01-27 23:38:35.811898: Pseudo dice [np.float32(0.9437), np.float32(0.8926)] 
2025-01-27 23:38:35.814443: Epoch time: 48.06 s 
2025-01-27 23:38:36.965808:  
2025-01-27 23:38:36.968990: Epoch 368 
2025-01-27 23:38:36.972050: Current learning rate: 0.00662 
2025-01-27 23:39:24.862048: train_loss -0.8037 
2025-01-27 23:39:24.867268: val_loss -0.7607 
2025-01-27 23:39:24.869670: Pseudo dice [np.float32(0.9414), np.float32(0.8935)] 
2025-01-27 23:39:24.872005: Epoch time: 47.9 s 
2025-01-27 23:39:26.020268:  
2025-01-27 23:39:26.022580: Epoch 369 
2025-01-27 23:39:26.024839: Current learning rate: 0.00661 
2025-01-27 23:40:14.089848: train_loss -0.8078 
2025-01-27 23:40:14.094416: val_loss -0.7007 
2025-01-27 23:40:14.097439: Pseudo dice [np.float32(0.9565), np.float32(0.87)] 
2025-01-27 23:40:14.100586: Epoch time: 48.07 s 
2025-01-27 23:40:15.653619:  
2025-01-27 23:40:15.657166: Epoch 370 
2025-01-27 23:40:15.660812: Current learning rate: 0.0066 
2025-01-27 23:41:03.528432: train_loss -0.8154 
2025-01-27 23:41:03.534535: val_loss -0.7683 
2025-01-27 23:41:03.537797: Pseudo dice [np.float32(0.9457), np.float32(0.9214)] 
2025-01-27 23:41:03.540439: Epoch time: 47.88 s 
2025-01-27 23:41:04.698207:  
2025-01-27 23:41:04.701376: Epoch 371 
2025-01-27 23:41:04.704179: Current learning rate: 0.00659 
2025-01-27 23:41:52.613643: train_loss -0.8145 
2025-01-27 23:41:52.617640: val_loss -0.7184 
2025-01-27 23:41:52.620482: Pseudo dice [np.float32(0.9478), np.float32(0.9236)] 
2025-01-27 23:41:52.623093: Epoch time: 47.92 s 
2025-01-27 23:41:53.779073:  
2025-01-27 23:41:53.782243: Epoch 372 
2025-01-27 23:41:53.785044: Current learning rate: 0.00658 
2025-01-27 23:42:41.790122: train_loss -0.8109 
2025-01-27 23:42:41.797228: val_loss -0.7478 
2025-01-27 23:42:41.800211: Pseudo dice [np.float32(0.9527), np.float32(0.918)] 
2025-01-27 23:42:41.802690: Epoch time: 48.01 s 
2025-01-27 23:42:42.930741:  
2025-01-27 23:42:42.934384: Epoch 373 
2025-01-27 23:42:42.937379: Current learning rate: 0.00657 
2025-01-27 23:43:31.852898: train_loss -0.8168 
2025-01-27 23:43:31.858008: val_loss -0.7536 
2025-01-27 23:43:31.860600: Pseudo dice [np.float32(0.949), np.float32(0.8714)] 
2025-01-27 23:43:31.863024: Epoch time: 48.92 s 
2025-01-27 23:43:32.987596:  
2025-01-27 23:43:32.990118: Epoch 374 
2025-01-27 23:43:32.992584: Current learning rate: 0.00656 
2025-01-27 23:44:20.760876: train_loss -0.8068 
2025-01-27 23:44:20.766818: val_loss -0.7159 
2025-01-27 23:44:20.769921: Pseudo dice [np.float32(0.9246), np.float32(0.8758)] 
2025-01-27 23:44:20.772641: Epoch time: 47.77 s 
2025-01-27 23:44:21.873175:  
2025-01-27 23:44:21.876563: Epoch 375 
2025-01-27 23:44:21.879606: Current learning rate: 0.00655 
2025-01-27 23:45:09.933067: train_loss -0.8131 
2025-01-27 23:45:09.937677: val_loss -0.7787 
2025-01-27 23:45:09.940703: Pseudo dice [np.float32(0.9449), np.float32(0.9252)] 
2025-01-27 23:45:09.943757: Epoch time: 48.06 s 
2025-01-27 23:45:11.106982:  
2025-01-27 23:45:11.110314: Epoch 376 
2025-01-27 23:45:11.113250: Current learning rate: 0.00654 
2025-01-27 23:45:59.292180: train_loss -0.815 
2025-01-27 23:45:59.298650: val_loss -0.7392 
2025-01-27 23:45:59.301338: Pseudo dice [np.float32(0.9468), np.float32(0.9273)] 
2025-01-27 23:45:59.303872: Epoch time: 48.19 s 
2025-01-27 23:46:00.519155:  
2025-01-27 23:46:00.522155: Epoch 377 
2025-01-27 23:46:00.524927: Current learning rate: 0.00653 
2025-01-27 23:46:48.360436: train_loss -0.807 
2025-01-27 23:46:48.364535: val_loss -0.6717 
2025-01-27 23:46:48.367242: Pseudo dice [np.float32(0.9375), np.float32(0.813)] 
2025-01-27 23:46:48.370135: Epoch time: 47.84 s 
2025-01-27 23:46:49.523450:  
2025-01-27 23:46:49.526614: Epoch 378 
2025-01-27 23:46:49.529474: Current learning rate: 0.00652 
2025-01-27 23:47:38.065523: train_loss -0.8128 
2025-01-27 23:47:38.071350: val_loss -0.6972 
2025-01-27 23:47:38.074080: Pseudo dice [np.float32(0.9249), np.float32(0.849)] 
2025-01-27 23:47:38.076563: Epoch time: 48.54 s 
2025-01-27 23:47:39.230766:  
2025-01-27 23:47:39.234157: Epoch 379 
2025-01-27 23:47:39.237404: Current learning rate: 0.00651 
2025-01-27 23:48:26.981684: train_loss -0.801 
2025-01-27 23:48:26.987725: val_loss -0.7666 
2025-01-27 23:48:26.990449: Pseudo dice [np.float32(0.9405), np.float32(0.9052)] 
2025-01-27 23:48:26.993299: Epoch time: 47.75 s 
2025-01-27 23:48:28.143626:  
2025-01-27 23:48:28.146866: Epoch 380 
2025-01-27 23:48:28.149649: Current learning rate: 0.0065 
2025-01-27 23:49:16.083625: train_loss -0.8126 
2025-01-27 23:49:16.089697: val_loss -0.7095 
2025-01-27 23:49:16.092749: Pseudo dice [np.float32(0.9361), np.float32(0.9174)] 
2025-01-27 23:49:16.095326: Epoch time: 47.94 s 
2025-01-27 23:49:17.245980:  
2025-01-27 23:49:17.248976: Epoch 381 
2025-01-27 23:49:17.251909: Current learning rate: 0.00649 
2025-01-27 23:50:04.983066: train_loss -0.8068 
2025-01-27 23:50:04.988079: val_loss -0.7467 
2025-01-27 23:50:04.990994: Pseudo dice [np.float32(0.9489), np.float32(0.9295)] 
2025-01-27 23:50:04.993571: Epoch time: 47.74 s 
2025-01-27 23:50:06.566016:  
2025-01-27 23:50:06.569059: Epoch 382 
2025-01-27 23:50:06.571881: Current learning rate: 0.00648 
2025-01-27 23:50:54.584257: train_loss -0.8327 
2025-01-27 23:50:54.590136: val_loss -0.6915 
2025-01-27 23:50:54.592846: Pseudo dice [np.float32(0.929), np.float32(0.7993)] 
2025-01-27 23:50:54.595308: Epoch time: 48.02 s 
2025-01-27 23:50:55.730200:  
2025-01-27 23:50:55.734002: Epoch 383 
2025-01-27 23:50:55.736886: Current learning rate: 0.00648 
2025-01-27 23:51:44.853459: train_loss -0.7936 
2025-01-27 23:51:44.857095: val_loss -0.7815 
2025-01-27 23:51:44.859350: Pseudo dice [np.float32(0.9446), np.float32(0.8714)] 
2025-01-27 23:51:44.861651: Epoch time: 49.13 s 
2025-01-27 23:51:45.996535:  
2025-01-27 23:51:45.999008: Epoch 384 
2025-01-27 23:51:46.001555: Current learning rate: 0.00647 
2025-01-27 23:52:34.053922: train_loss -0.7823 
2025-01-27 23:52:34.059825: val_loss -0.7465 
2025-01-27 23:52:34.062839: Pseudo dice [np.float32(0.9411), np.float32(0.8686)] 
2025-01-27 23:52:34.065474: Epoch time: 48.06 s 
2025-01-27 23:52:35.187877:  
2025-01-27 23:52:35.191224: Epoch 385 
2025-01-27 23:52:35.193891: Current learning rate: 0.00646 
2025-01-27 23:53:23.343592: train_loss -0.8047 
2025-01-27 23:53:23.347306: val_loss -0.7189 
2025-01-27 23:53:23.349583: Pseudo dice [np.float32(0.9116), np.float32(0.9046)] 
2025-01-27 23:53:23.351775: Epoch time: 48.16 s 
2025-01-27 23:53:24.533355:  
2025-01-27 23:53:24.538500: Epoch 386 
2025-01-27 23:53:24.541141: Current learning rate: 0.00645 
2025-01-27 23:54:12.574499: train_loss -0.8217 
2025-01-27 23:54:12.581139: val_loss -0.7327 
2025-01-27 23:54:12.583910: Pseudo dice [np.float32(0.947), np.float32(0.8762)] 
2025-01-27 23:54:12.586397: Epoch time: 48.04 s 
2025-01-27 23:54:13.780718:  
2025-01-27 23:54:13.783507: Epoch 387 
2025-01-27 23:54:13.786384: Current learning rate: 0.00644 
2025-01-27 23:55:01.603286: train_loss -0.7972 
2025-01-27 23:55:01.607773: val_loss -0.7455 
2025-01-27 23:55:01.610750: Pseudo dice [np.float32(0.9351), np.float32(0.8736)] 
2025-01-27 23:55:01.613403: Epoch time: 47.82 s 
2025-01-27 23:55:02.779452:  
2025-01-27 23:55:02.782769: Epoch 388 
2025-01-27 23:55:02.785664: Current learning rate: 0.00643 
2025-01-27 23:55:50.796584: train_loss -0.805 
2025-01-27 23:55:50.802165: val_loss -0.7012 
2025-01-27 23:55:50.804824: Pseudo dice [np.float32(0.9502), np.float32(0.9335)] 
2025-01-27 23:55:50.807449: Epoch time: 48.02 s 
2025-01-27 23:55:51.978101:  
2025-01-27 23:55:51.981155: Epoch 389 
2025-01-27 23:55:51.984045: Current learning rate: 0.00642 
2025-01-27 23:56:39.791155: train_loss -0.8285 
2025-01-27 23:56:39.796094: val_loss -0.6845 
2025-01-27 23:56:39.799321: Pseudo dice [np.float32(0.9505), np.float32(0.7769)] 
2025-01-27 23:56:39.801975: Epoch time: 47.81 s 
2025-01-27 23:56:40.967159:  
2025-01-27 23:56:40.970636: Epoch 390 
2025-01-27 23:56:40.973662: Current learning rate: 0.00641 
2025-01-27 23:57:28.773159: train_loss -0.8072 
2025-01-27 23:57:28.778118: val_loss -0.7482 
2025-01-27 23:57:28.780478: Pseudo dice [np.float32(0.937), np.float32(0.8862)] 
2025-01-27 23:57:28.782795: Epoch time: 47.81 s 
2025-01-27 23:57:29.949819:  
2025-01-27 23:57:29.952428: Epoch 391 
2025-01-27 23:57:29.954884: Current learning rate: 0.0064 
2025-01-27 23:58:17.762270: train_loss -0.8232 
2025-01-27 23:58:17.766811: val_loss -0.7457 
2025-01-27 23:58:17.769651: Pseudo dice [np.float32(0.9524), np.float32(0.9291)] 
2025-01-27 23:58:17.772703: Epoch time: 47.81 s 
2025-01-27 23:58:18.979346:  
2025-01-27 23:58:18.982545: Epoch 392 
2025-01-27 23:58:18.985520: Current learning rate: 0.00639 
2025-01-27 23:59:07.125165: train_loss -0.8082 
2025-01-27 23:59:07.131710: val_loss -0.7559 
2025-01-27 23:59:07.134709: Pseudo dice [np.float32(0.9441), np.float32(0.902)] 
2025-01-27 23:59:07.137565: Epoch time: 48.15 s 
2025-01-27 23:59:08.260281:  
2025-01-27 23:59:08.263305: Epoch 393 
2025-01-27 23:59:08.266093: Current learning rate: 0.00638 
2025-01-27 23:59:56.302867: train_loss -0.8079 
2025-01-27 23:59:56.307321: val_loss -0.7138 
2025-01-27 23:59:56.310163: Pseudo dice [np.float32(0.9404), np.float32(0.8649)] 
2025-01-27 23:59:56.313081: Epoch time: 48.04 s 
2025-01-27 23:59:57.793806:  
2025-01-27 23:59:57.796655: Epoch 394 
2025-01-27 23:59:57.799584: Current learning rate: 0.00637 
2025-01-28 00:00:45.715626: train_loss -0.7956 
2025-01-28 00:00:45.721921: val_loss -0.7047 
2025-01-28 00:00:45.724862: Pseudo dice [np.float32(0.9223), np.float32(0.8583)] 
2025-01-28 00:00:45.727200: Epoch time: 47.92 s 
2025-01-28 00:00:46.894408:  
2025-01-28 00:00:46.898410: Epoch 395 
2025-01-28 00:00:46.901162: Current learning rate: 0.00636 
2025-01-28 00:01:35.001674: train_loss -0.8067 
2025-01-28 00:01:35.007163: val_loss -0.709 
2025-01-28 00:01:35.009720: Pseudo dice [np.float32(0.9409), np.float32(0.9274)] 
2025-01-28 00:01:35.012086: Epoch time: 48.11 s 
2025-01-28 00:01:36.208930:  
2025-01-28 00:01:36.211918: Epoch 396 
2025-01-28 00:01:36.214745: Current learning rate: 0.00635 
2025-01-28 00:02:24.165662: train_loss -0.8049 
2025-01-28 00:02:24.171550: val_loss -0.7456 
2025-01-28 00:02:24.174522: Pseudo dice [np.float32(0.9461), np.float32(0.8395)] 
2025-01-28 00:02:24.177336: Epoch time: 47.96 s 
2025-01-28 00:02:25.342406:  
2025-01-28 00:02:25.346984: Epoch 397 
2025-01-28 00:02:25.349529: Current learning rate: 0.00634 
2025-01-28 00:03:14.320298: train_loss -0.8144 
2025-01-28 00:03:14.325518: val_loss -0.7465 
2025-01-28 00:03:14.328008: Pseudo dice [np.float32(0.9526), np.float32(0.9047)] 
2025-01-28 00:03:14.330751: Epoch time: 48.98 s 
2025-01-28 00:03:15.484609:  
2025-01-28 00:03:15.487447: Epoch 398 
2025-01-28 00:03:15.490214: Current learning rate: 0.00633 
2025-01-28 00:04:04.021589: train_loss -0.7715 
2025-01-28 00:04:04.027262: val_loss -0.742 
2025-01-28 00:04:04.030097: Pseudo dice [np.float32(0.9369), np.float32(0.8978)] 
2025-01-28 00:04:04.032840: Epoch time: 48.54 s 
2025-01-28 00:04:05.258419:  
2025-01-28 00:04:05.261702: Epoch 399 
2025-01-28 00:04:05.265003: Current learning rate: 0.00632 
2025-01-28 00:04:53.293276: train_loss -0.7916 
2025-01-28 00:04:53.298979: val_loss -0.758 
2025-01-28 00:04:53.301858: Pseudo dice [np.float32(0.9545), np.float32(0.8945)] 
2025-01-28 00:04:53.304832: Epoch time: 48.04 s 
2025-01-28 00:04:55.075216:  
2025-01-28 00:04:55.078358: Epoch 400 
2025-01-28 00:04:55.081037: Current learning rate: 0.00631 
2025-01-28 00:05:42.879374: train_loss -0.7959 
2025-01-28 00:05:42.885212: val_loss -0.7215 
2025-01-28 00:05:42.888273: Pseudo dice [np.float32(0.9283), np.float32(0.9206)] 
2025-01-28 00:05:42.891054: Epoch time: 47.81 s 
2025-01-28 00:05:44.056195:  
2025-01-28 00:05:44.059468: Epoch 401 
2025-01-28 00:05:44.062352: Current learning rate: 0.0063 
2025-01-28 00:06:32.077979: train_loss -0.8163 
2025-01-28 00:06:32.082026: val_loss -0.7459 
2025-01-28 00:06:32.084518: Pseudo dice [np.float32(0.9436), np.float32(0.9008)] 
2025-01-28 00:06:32.087116: Epoch time: 48.02 s 
2025-01-28 00:06:33.262893:  
2025-01-28 00:06:33.266351: Epoch 402 
2025-01-28 00:06:33.269510: Current learning rate: 0.0063 
2025-01-28 00:07:21.450752: train_loss -0.8081 
2025-01-28 00:07:21.456172: val_loss -0.7264 
2025-01-28 00:07:21.458760: Pseudo dice [np.float32(0.9511), np.float32(0.9407)] 
2025-01-28 00:07:21.461140: Epoch time: 48.19 s 
2025-01-28 00:07:22.627512:  
2025-01-28 00:07:22.630220: Epoch 403 
2025-01-28 00:07:22.632790: Current learning rate: 0.00629 
2025-01-28 00:08:10.242049: train_loss -0.8243 
2025-01-28 00:08:10.246391: val_loss -0.7486 
2025-01-28 00:08:10.249593: Pseudo dice [np.float32(0.9514), np.float32(0.9363)] 
2025-01-28 00:08:10.252512: Epoch time: 47.62 s 
2025-01-28 00:08:11.463353:  
2025-01-28 00:08:11.466852: Epoch 404 
2025-01-28 00:08:11.469866: Current learning rate: 0.00628 
2025-01-28 00:08:59.290910: train_loss -0.8121 
2025-01-28 00:08:59.296932: val_loss -0.7799 
2025-01-28 00:08:59.300014: Pseudo dice [np.float32(0.9518), np.float32(0.928)] 
2025-01-28 00:08:59.302791: Epoch time: 47.83 s 
2025-01-28 00:09:00.465976:  
2025-01-28 00:09:00.468829: Epoch 405 
2025-01-28 00:09:00.471482: Current learning rate: 0.00627 
2025-01-28 00:09:48.194322: train_loss -0.8155 
2025-01-28 00:09:48.200163: val_loss -0.7446 
2025-01-28 00:09:48.202891: Pseudo dice [np.float32(0.9528), np.float32(0.9063)] 
2025-01-28 00:09:48.205711: Epoch time: 47.73 s 
2025-01-28 00:09:49.674019:  
2025-01-28 00:09:49.677015: Epoch 406 
2025-01-28 00:09:49.680110: Current learning rate: 0.00626 
2025-01-28 00:10:37.630944: train_loss -0.7994 
2025-01-28 00:10:37.637800: val_loss -0.7461 
2025-01-28 00:10:37.640271: Pseudo dice [np.float32(0.9515), np.float32(0.9272)] 
2025-01-28 00:10:37.642742: Epoch time: 47.96 s 
2025-01-28 00:10:38.812826:  
2025-01-28 00:10:38.815882: Epoch 407 
2025-01-28 00:10:38.818446: Current learning rate: 0.00625 
2025-01-28 00:11:26.980037: train_loss -0.8115 
2025-01-28 00:11:26.984485: val_loss -0.7306 
2025-01-28 00:11:26.987357: Pseudo dice [np.float32(0.9451), np.float32(0.9036)] 
2025-01-28 00:11:26.990367: Epoch time: 48.17 s 
2025-01-28 00:11:28.170378:  
2025-01-28 00:11:28.173375: Epoch 408 
2025-01-28 00:11:28.175837: Current learning rate: 0.00624 
2025-01-28 00:12:16.775850: train_loss -0.8155 
2025-01-28 00:12:16.781502: val_loss -0.7031 
2025-01-28 00:12:16.784249: Pseudo dice [np.float32(0.9432), np.float32(0.8832)] 
2025-01-28 00:12:16.786721: Epoch time: 48.61 s 
2025-01-28 00:12:17.960932:  
2025-01-28 00:12:17.963749: Epoch 409 
2025-01-28 00:12:17.966464: Current learning rate: 0.00623 
2025-01-28 00:13:06.254096: train_loss -0.8207 
2025-01-28 00:13:06.260597: val_loss -0.7684 
2025-01-28 00:13:06.263627: Pseudo dice [np.float32(0.9554), np.float32(0.9127)] 
2025-01-28 00:13:06.266404: Epoch time: 48.29 s 
2025-01-28 00:13:07.448579:  
2025-01-28 00:13:07.451739: Epoch 410 
2025-01-28 00:13:07.454372: Current learning rate: 0.00622 
2025-01-28 00:13:55.463173: train_loss -0.8139 
2025-01-28 00:13:55.468592: val_loss -0.749 
2025-01-28 00:13:55.471086: Pseudo dice [np.float32(0.9505), np.float32(0.8836)] 
2025-01-28 00:13:55.473630: Epoch time: 48.02 s 
2025-01-28 00:13:56.548394:  
2025-01-28 00:13:56.551301: Epoch 411 
2025-01-28 00:13:56.554027: Current learning rate: 0.00621 
2025-01-28 00:14:45.508999: train_loss -0.8143 
2025-01-28 00:14:45.512746: val_loss -0.6579 
2025-01-28 00:14:45.515353: Pseudo dice [np.float32(0.9479), np.float32(0.9111)] 
2025-01-28 00:14:45.517588: Epoch time: 48.96 s 
2025-01-28 00:14:46.665920:  
2025-01-28 00:14:46.668561: Epoch 412 
2025-01-28 00:14:46.670994: Current learning rate: 0.0062 
2025-01-28 00:15:34.878925: train_loss -0.8185 
2025-01-28 00:15:34.885014: val_loss -0.7307 
2025-01-28 00:15:34.887643: Pseudo dice [np.float32(0.9357), np.float32(0.8958)] 
2025-01-28 00:15:34.890169: Epoch time: 48.21 s 
2025-01-28 00:15:35.974145:  
2025-01-28 00:15:35.977614: Epoch 413 
2025-01-28 00:15:35.980508: Current learning rate: 0.00619 
2025-01-28 00:16:23.998438: train_loss -0.8231 
2025-01-28 00:16:24.002580: val_loss -0.7418 
2025-01-28 00:16:24.005667: Pseudo dice [np.float32(0.9517), np.float32(0.9156)] 
2025-01-28 00:16:24.008536: Epoch time: 48.03 s 
2025-01-28 00:16:25.060698:  
2025-01-28 00:16:25.064008: Epoch 414 
2025-01-28 00:16:25.066421: Current learning rate: 0.00618 
2025-01-28 00:17:13.856890: train_loss -0.8125 
2025-01-28 00:17:13.862362: val_loss -0.7622 
2025-01-28 00:17:13.864763: Pseudo dice [np.float32(0.9361), np.float32(0.9117)] 
2025-01-28 00:17:13.867420: Epoch time: 48.8 s 
2025-01-28 00:17:14.932238:  
2025-01-28 00:17:14.935680: Epoch 415 
2025-01-28 00:17:14.938391: Current learning rate: 0.00617 
2025-01-28 00:18:03.566071: train_loss -0.8051 
2025-01-28 00:18:03.570319: val_loss -0.712 
2025-01-28 00:18:03.573075: Pseudo dice [np.float32(0.9563), np.float32(0.9129)] 
2025-01-28 00:18:03.576922: Epoch time: 48.63 s 
2025-01-28 00:18:04.718351:  
2025-01-28 00:18:04.722009: Epoch 416 
2025-01-28 00:18:04.725153: Current learning rate: 0.00616 
2025-01-28 00:18:53.100358: train_loss -0.8183 
2025-01-28 00:18:53.107316: val_loss -0.7394 
2025-01-28 00:18:53.110428: Pseudo dice [np.float32(0.9454), np.float32(0.9003)] 
2025-01-28 00:18:53.113403: Epoch time: 48.38 s 
2025-01-28 00:18:54.173254:  
2025-01-28 00:18:54.175926: Epoch 417 
2025-01-28 00:18:54.178980: Current learning rate: 0.00615 
2025-01-28 00:19:42.285062: train_loss -0.8087 
2025-01-28 00:19:42.289705: val_loss -0.7879 
2025-01-28 00:19:42.292542: Pseudo dice [np.float32(0.9532), np.float32(0.9429)] 
2025-01-28 00:19:42.295233: Epoch time: 48.11 s 
2025-01-28 00:19:43.741914:  
2025-01-28 00:19:43.744808: Epoch 418 
2025-01-28 00:19:43.747550: Current learning rate: 0.00614 
2025-01-28 00:20:31.669421: train_loss -0.8226 
2025-01-28 00:20:31.673941: val_loss -0.7378 
2025-01-28 00:20:31.676367: Pseudo dice [np.float32(0.9545), np.float32(0.9082)] 
2025-01-28 00:20:31.678653: Epoch time: 47.93 s 
2025-01-28 00:20:32.773384:  
2025-01-28 00:20:32.775951: Epoch 419 
2025-01-28 00:20:32.778305: Current learning rate: 0.00613 
2025-01-28 00:21:20.852318: train_loss -0.8056 
2025-01-28 00:21:20.856308: val_loss -0.7012 
2025-01-28 00:21:20.858688: Pseudo dice [np.float32(0.9453), np.float32(0.8657)] 
2025-01-28 00:21:20.861167: Epoch time: 48.08 s 
2025-01-28 00:21:21.923552:  
2025-01-28 00:21:21.926708: Epoch 420 
2025-01-28 00:21:21.929406: Current learning rate: 0.00612 
2025-01-28 00:22:10.149371: train_loss -0.8106 
2025-01-28 00:22:10.154041: val_loss -0.7169 
2025-01-28 00:22:10.156407: Pseudo dice [np.float32(0.9304), np.float32(0.9208)] 
2025-01-28 00:22:10.158805: Epoch time: 48.23 s 
2025-01-28 00:22:11.280227:  
2025-01-28 00:22:11.283923: Epoch 421 
2025-01-28 00:22:11.286913: Current learning rate: 0.00612 
2025-01-28 00:22:59.378540: train_loss -0.8274 
2025-01-28 00:22:59.383190: val_loss -0.6995 
2025-01-28 00:22:59.386349: Pseudo dice [np.float32(0.9405), np.float32(0.9138)] 
2025-01-28 00:22:59.390054: Epoch time: 48.1 s 
2025-01-28 00:23:00.559294:  
2025-01-28 00:23:00.562458: Epoch 422 
2025-01-28 00:23:00.565292: Current learning rate: 0.00611 
2025-01-28 00:23:49.440451: train_loss -0.8342 
2025-01-28 00:23:49.446480: val_loss -0.738 
2025-01-28 00:23:49.449592: Pseudo dice [np.float32(0.9365), np.float32(0.9242)] 
2025-01-28 00:23:49.452432: Epoch time: 48.88 s 
2025-01-28 00:23:50.528322:  
2025-01-28 00:23:50.531787: Epoch 423 
2025-01-28 00:23:50.534495: Current learning rate: 0.0061 
2025-01-28 00:24:38.478845: train_loss -0.8009 
2025-01-28 00:24:38.484381: val_loss -0.6676 
2025-01-28 00:24:38.487768: Pseudo dice [np.float32(0.9362), np.float32(0.7763)] 
2025-01-28 00:24:38.490911: Epoch time: 47.95 s 
2025-01-28 00:24:39.564076:  
2025-01-28 00:24:39.566991: Epoch 424 
2025-01-28 00:24:39.569911: Current learning rate: 0.00609 
2025-01-28 00:25:27.876708: train_loss -0.7945 
2025-01-28 00:25:27.884137: val_loss -0.71 
2025-01-28 00:25:27.886926: Pseudo dice [np.float32(0.9452), np.float32(0.9055)] 
2025-01-28 00:25:27.889731: Epoch time: 48.31 s 
2025-01-28 00:25:28.950989:  
2025-01-28 00:25:28.953919: Epoch 425 
2025-01-28 00:25:28.956581: Current learning rate: 0.00608 
2025-01-28 00:26:17.348311: train_loss -0.8155 
2025-01-28 00:26:17.352917: val_loss -0.7548 
2025-01-28 00:26:17.355654: Pseudo dice [np.float32(0.9487), np.float32(0.9193)] 
2025-01-28 00:26:17.358407: Epoch time: 48.4 s 
2025-01-28 00:26:18.539072:  
2025-01-28 00:26:18.542255: Epoch 426 
2025-01-28 00:26:18.545072: Current learning rate: 0.00607 
2025-01-28 00:27:07.427837: train_loss -0.7883 
2025-01-28 00:27:07.433439: val_loss -0.6833 
2025-01-28 00:27:07.436050: Pseudo dice [np.float32(0.9313), np.float32(0.8781)] 
2025-01-28 00:27:07.438410: Epoch time: 48.89 s 
2025-01-28 00:27:08.499694:  
2025-01-28 00:27:08.502430: Epoch 427 
2025-01-28 00:27:08.505107: Current learning rate: 0.00606 
2025-01-28 00:27:57.099016: train_loss -0.8088 
2025-01-28 00:27:57.103655: val_loss -0.7513 
2025-01-28 00:27:57.106430: Pseudo dice [np.float32(0.9502), np.float32(0.9268)] 
2025-01-28 00:27:57.109340: Epoch time: 48.6 s 
2025-01-28 00:27:58.283976:  
2025-01-28 00:27:58.287228: Epoch 428 
2025-01-28 00:27:58.290235: Current learning rate: 0.00605 
2025-01-28 00:28:46.876318: train_loss -0.8003 
2025-01-28 00:28:46.882523: val_loss -0.6665 
2025-01-28 00:28:46.885676: Pseudo dice [np.float32(0.9255), np.float32(0.8933)] 
2025-01-28 00:28:46.888588: Epoch time: 48.59 s 
2025-01-28 00:28:47.963494:  
2025-01-28 00:28:47.966808: Epoch 429 
2025-01-28 00:28:47.970100: Current learning rate: 0.00604 
2025-01-28 00:29:36.575058: train_loss -0.8138 
2025-01-28 00:29:36.579239: val_loss -0.7467 
2025-01-28 00:29:36.581847: Pseudo dice [np.float32(0.9473), np.float32(0.9103)] 
2025-01-28 00:29:36.584257: Epoch time: 48.61 s 
2025-01-28 00:29:37.669508:  
2025-01-28 00:29:37.672156: Epoch 430 
2025-01-28 00:29:37.674796: Current learning rate: 0.00603 
2025-01-28 00:30:26.021322: train_loss -0.8055 
2025-01-28 00:30:26.029212: val_loss -0.74 
2025-01-28 00:30:26.031948: Pseudo dice [np.float32(0.9281), np.float32(0.9143)] 
2025-01-28 00:30:26.034799: Epoch time: 48.35 s 
2025-01-28 00:30:27.571476:  
2025-01-28 00:30:27.575302: Epoch 431 
2025-01-28 00:30:27.578250: Current learning rate: 0.00602 
2025-01-28 00:31:16.158640: train_loss -0.8197 
2025-01-28 00:31:16.163514: val_loss -0.7354 
2025-01-28 00:31:16.166753: Pseudo dice [np.float32(0.9538), np.float32(0.9203)] 
2025-01-28 00:31:16.169779: Epoch time: 48.59 s 
2025-01-28 00:31:17.311226:  
2025-01-28 00:31:17.314807: Epoch 432 
2025-01-28 00:31:17.317500: Current learning rate: 0.00601 
2025-01-28 00:32:05.290231: train_loss -0.8109 
2025-01-28 00:32:05.296108: val_loss -0.7446 
2025-01-28 00:32:05.298974: Pseudo dice [np.float32(0.9418), np.float32(0.9152)] 
2025-01-28 00:32:05.301655: Epoch time: 47.98 s 
2025-01-28 00:32:06.412328:  
2025-01-28 00:32:06.415567: Epoch 433 
2025-01-28 00:32:06.418631: Current learning rate: 0.006 
2025-01-28 00:32:54.224140: train_loss -0.8038 
2025-01-28 00:32:54.228358: val_loss -0.7392 
2025-01-28 00:32:54.230736: Pseudo dice [np.float32(0.9458), np.float32(0.891)] 
2025-01-28 00:32:54.233332: Epoch time: 47.81 s 
2025-01-28 00:32:55.349553:  
2025-01-28 00:32:55.352533: Epoch 434 
2025-01-28 00:32:55.355353: Current learning rate: 0.00599 
2025-01-28 00:33:43.218932: train_loss -0.8188 
2025-01-28 00:33:43.224732: val_loss -0.7438 
2025-01-28 00:33:43.227614: Pseudo dice [np.float32(0.9531), np.float32(0.9334)] 
2025-01-28 00:33:43.230017: Epoch time: 47.87 s 
2025-01-28 00:33:44.340273:  
2025-01-28 00:33:44.343682: Epoch 435 
2025-01-28 00:33:44.346689: Current learning rate: 0.00598 
2025-01-28 00:34:32.419976: train_loss -0.825 
2025-01-28 00:34:32.423957: val_loss -0.7424 
2025-01-28 00:34:32.426617: Pseudo dice [np.float32(0.9535), np.float32(0.9449)] 
2025-01-28 00:34:32.429303: Epoch time: 48.08 s 
2025-01-28 00:34:33.538768:  
2025-01-28 00:34:33.541745: Epoch 436 
2025-01-28 00:34:33.545137: Current learning rate: 0.00597 
2025-01-28 00:35:21.569081: train_loss -0.8058 
2025-01-28 00:35:21.574542: val_loss -0.7246 
2025-01-28 00:35:21.577394: Pseudo dice [np.float32(0.9465), np.float32(0.9256)] 
2025-01-28 00:35:21.580002: Epoch time: 48.03 s 
2025-01-28 00:35:22.741972:  
2025-01-28 00:35:22.747464: Epoch 437 
2025-01-28 00:35:22.750950: Current learning rate: 0.00596 
2025-01-28 00:36:11.178605: train_loss -0.8092 
2025-01-28 00:36:11.183318: val_loss -0.7534 
2025-01-28 00:36:11.186327: Pseudo dice [np.float32(0.9484), np.float32(0.9201)] 
2025-01-28 00:36:11.189191: Epoch time: 48.44 s 
2025-01-28 00:36:12.258318:  
2025-01-28 00:36:12.262135: Epoch 438 
2025-01-28 00:36:12.265241: Current learning rate: 0.00595 
2025-01-28 00:37:00.409066: train_loss -0.8178 
2025-01-28 00:37:00.415783: val_loss -0.7574 
2025-01-28 00:37:00.418859: Pseudo dice [np.float32(0.9498), np.float32(0.942)] 
2025-01-28 00:37:00.421651: Epoch time: 48.15 s 
2025-01-28 00:37:01.534801:  
2025-01-28 00:37:01.538276: Epoch 439 
2025-01-28 00:37:01.541450: Current learning rate: 0.00594 
2025-01-28 00:37:49.852345: train_loss -0.8251 
2025-01-28 00:37:49.857495: val_loss -0.7662 
2025-01-28 00:37:49.860649: Pseudo dice [np.float32(0.9446), np.float32(0.9266)] 
2025-01-28 00:37:49.863379: Epoch time: 48.32 s 
2025-01-28 00:37:49.865833: Yayy! New best EMA pseudo Dice: 0.9309999942779541 
2025-01-28 00:37:51.547816:  
2025-01-28 00:37:51.551002: Epoch 440 
2025-01-28 00:37:51.554522: Current learning rate: 0.00593 
2025-01-28 00:38:39.484477: train_loss -0.8078 
2025-01-28 00:38:39.491048: val_loss -0.7232 
2025-01-28 00:38:39.493885: Pseudo dice [np.float32(0.9472), np.float32(0.9071)] 
2025-01-28 00:38:39.496694: Epoch time: 47.94 s 
2025-01-28 00:38:40.644245:  
2025-01-28 00:38:40.648355: Epoch 441 
2025-01-28 00:38:40.651540: Current learning rate: 0.00592 
2025-01-28 00:39:28.649792: train_loss -0.7912 
2025-01-28 00:39:28.653415: val_loss -0.7428 
2025-01-28 00:39:28.655879: Pseudo dice [np.float32(0.9413), np.float32(0.8687)] 
2025-01-28 00:39:28.658273: Epoch time: 48.01 s 
2025-01-28 00:39:29.763733:  
2025-01-28 00:39:29.766231: Epoch 442 
2025-01-28 00:39:29.769312: Current learning rate: 0.00592 
2025-01-28 00:40:17.696510: train_loss -0.8137 
2025-01-28 00:40:17.701960: val_loss -0.7643 
2025-01-28 00:40:17.704482: Pseudo dice [np.float32(0.9427), np.float32(0.895)] 
2025-01-28 00:40:17.707035: Epoch time: 47.93 s 
2025-01-28 00:40:18.812118:  
2025-01-28 00:40:18.814511: Epoch 443 
2025-01-28 00:40:18.816794: Current learning rate: 0.00591 
2025-01-28 00:41:06.800792: train_loss -0.7916 
2025-01-28 00:41:06.805586: val_loss -0.7186 
2025-01-28 00:41:06.808504: Pseudo dice [np.float32(0.9231), np.float32(0.9081)] 
2025-01-28 00:41:06.811263: Epoch time: 47.99 s 
2025-01-28 00:41:08.225392:  
2025-01-28 00:41:08.228432: Epoch 444 
2025-01-28 00:41:08.231755: Current learning rate: 0.0059 
2025-01-28 00:41:56.524101: train_loss -0.7884 
2025-01-28 00:41:56.530633: val_loss -0.7388 
2025-01-28 00:41:56.533414: Pseudo dice [np.float32(0.9351), np.float32(0.9279)] 
2025-01-28 00:41:56.535974: Epoch time: 48.3 s 
2025-01-28 00:41:57.634684:  
2025-01-28 00:41:57.638099: Epoch 445 
2025-01-28 00:41:57.640975: Current learning rate: 0.00589 
2025-01-28 00:42:45.465615: train_loss -0.8036 
2025-01-28 00:42:45.470231: val_loss -0.7383 
2025-01-28 00:42:45.472939: Pseudo dice [np.float32(0.9416), np.float32(0.8976)] 
2025-01-28 00:42:45.475450: Epoch time: 47.83 s 
2025-01-28 00:42:46.573376:  
2025-01-28 00:42:46.576414: Epoch 446 
2025-01-28 00:42:46.579649: Current learning rate: 0.00588 
2025-01-28 00:43:34.509930: train_loss -0.8155 
2025-01-28 00:43:34.516485: val_loss -0.7324 
2025-01-28 00:43:34.519575: Pseudo dice [np.float32(0.951), np.float32(0.8405)] 
2025-01-28 00:43:34.522394: Epoch time: 47.94 s 
2025-01-28 00:43:35.663885:  
2025-01-28 00:43:35.667274: Epoch 447 
2025-01-28 00:43:35.670199: Current learning rate: 0.00587 
2025-01-28 00:44:23.783583: train_loss -0.813 
2025-01-28 00:44:23.790385: val_loss -0.7049 
2025-01-28 00:44:23.793570: Pseudo dice [np.float32(0.9461), np.float32(0.8809)] 
2025-01-28 00:44:23.796512: Epoch time: 48.12 s 
2025-01-28 00:44:24.934636:  
2025-01-28 00:44:24.937402: Epoch 448 
2025-01-28 00:44:24.939835: Current learning rate: 0.00586 
2025-01-28 00:45:12.493978: train_loss -0.8113 
2025-01-28 00:45:12.500192: val_loss -0.7271 
2025-01-28 00:45:12.502799: Pseudo dice [np.float32(0.9501), np.float32(0.9168)] 
2025-01-28 00:45:12.505388: Epoch time: 47.56 s 
2025-01-28 00:45:13.604622:  
2025-01-28 00:45:13.607393: Epoch 449 
2025-01-28 00:45:13.610240: Current learning rate: 0.00585 
2025-01-28 00:46:01.557354: train_loss -0.7897 
2025-01-28 00:46:01.562303: val_loss -0.7719 
2025-01-28 00:46:01.565430: Pseudo dice [np.float32(0.9411), np.float32(0.9345)] 
2025-01-28 00:46:01.568472: Epoch time: 47.95 s 
2025-01-28 00:46:03.224410:  
2025-01-28 00:46:03.227687: Epoch 450 
2025-01-28 00:46:03.230477: Current learning rate: 0.00584 
2025-01-28 00:46:51.238770: train_loss -0.806 
2025-01-28 00:46:51.244471: val_loss -0.7619 
2025-01-28 00:46:51.247211: Pseudo dice [np.float32(0.9352), np.float32(0.9006)] 
2025-01-28 00:46:51.249852: Epoch time: 48.02 s 
2025-01-28 00:46:52.351944:  
2025-01-28 00:46:52.355220: Epoch 451 
2025-01-28 00:46:52.358057: Current learning rate: 0.00583 
2025-01-28 00:47:40.672991: train_loss -0.8094 
2025-01-28 00:47:40.677595: val_loss -0.7644 
2025-01-28 00:47:40.680619: Pseudo dice [np.float32(0.9519), np.float32(0.8926)] 
2025-01-28 00:47:40.683715: Epoch time: 48.32 s 
2025-01-28 00:47:41.779752:  
2025-01-28 00:47:41.782900: Epoch 452 
2025-01-28 00:47:41.786445: Current learning rate: 0.00582 
2025-01-28 00:48:29.958697: train_loss -0.8052 
2025-01-28 00:48:29.964009: val_loss -0.7248 
2025-01-28 00:48:29.966332: Pseudo dice [np.float32(0.9308), np.float32(0.9131)] 
2025-01-28 00:48:29.968806: Epoch time: 48.18 s 
2025-01-28 00:48:31.066058:  
2025-01-28 00:48:31.069454: Epoch 453 
2025-01-28 00:48:31.072664: Current learning rate: 0.00581 
2025-01-28 00:49:19.009893: train_loss -0.8078 
2025-01-28 00:49:19.014500: val_loss -0.7474 
2025-01-28 00:49:19.017227: Pseudo dice [np.float32(0.9451), np.float32(0.8694)] 
2025-01-28 00:49:19.020380: Epoch time: 47.94 s 
2025-01-28 00:49:20.156277:  
2025-01-28 00:49:20.159373: Epoch 454 
2025-01-28 00:49:20.162496: Current learning rate: 0.0058 
2025-01-28 00:50:08.099563: train_loss -0.8068 
2025-01-28 00:50:08.105130: val_loss -0.7164 
2025-01-28 00:50:08.107966: Pseudo dice [np.float32(0.9402), np.float32(0.852)] 
2025-01-28 00:50:08.110585: Epoch time: 47.94 s 
2025-01-28 00:50:09.242878:  
2025-01-28 00:50:09.245641: Epoch 455 
2025-01-28 00:50:09.248553: Current learning rate: 0.00579 
2025-01-28 00:50:57.584370: train_loss -0.8122 
2025-01-28 00:50:57.589421: val_loss -0.7584 
2025-01-28 00:50:57.592129: Pseudo dice [np.float32(0.9471), np.float32(0.9215)] 
2025-01-28 00:50:57.594716: Epoch time: 48.34 s 
2025-01-28 00:50:58.669611:  
2025-01-28 00:50:58.672791: Epoch 456 
2025-01-28 00:50:58.675907: Current learning rate: 0.00578 
2025-01-28 00:51:46.972324: train_loss -0.8189 
2025-01-28 00:51:46.978915: val_loss -0.7301 
2025-01-28 00:51:46.982027: Pseudo dice [np.float32(0.9493), np.float32(0.919)] 
2025-01-28 00:51:46.984759: Epoch time: 48.3 s 
2025-01-28 00:51:48.032283:  
2025-01-28 00:51:48.035490: Epoch 457 
2025-01-28 00:51:48.038292: Current learning rate: 0.00577 
2025-01-28 00:52:36.269534: train_loss -0.8008 
2025-01-28 00:52:36.273126: val_loss -0.7521 
2025-01-28 00:52:36.275546: Pseudo dice [np.float32(0.9525), np.float32(0.918)] 
2025-01-28 00:52:36.277622: Epoch time: 48.24 s 
2025-01-28 00:52:37.323502:  
2025-01-28 00:52:37.325742: Epoch 458 
2025-01-28 00:52:37.327996: Current learning rate: 0.00576 
2025-01-28 00:53:25.464591: train_loss -0.8108 
2025-01-28 00:53:25.472594: val_loss -0.7549 
2025-01-28 00:53:25.476200: Pseudo dice [np.float32(0.9473), np.float32(0.9215)] 
2025-01-28 00:53:25.479301: Epoch time: 48.14 s 
2025-01-28 00:53:26.528186:  
2025-01-28 00:53:26.532123: Epoch 459 
2025-01-28 00:53:26.534942: Current learning rate: 0.00575 
2025-01-28 00:54:14.310752: train_loss -0.8162 
2025-01-28 00:54:14.314155: val_loss -0.7616 
2025-01-28 00:54:14.316549: Pseudo dice [np.float32(0.9535), np.float32(0.9317)] 
2025-01-28 00:54:14.318576: Epoch time: 47.78 s 
2025-01-28 00:54:15.363033:  
2025-01-28 00:54:15.365983: Epoch 460 
2025-01-28 00:54:15.368642: Current learning rate: 0.00574 
2025-01-28 00:55:03.387410: train_loss -0.8157 
2025-01-28 00:55:03.393921: val_loss -0.7276 
2025-01-28 00:55:03.396668: Pseudo dice [np.float32(0.9526), np.float32(0.8879)] 
2025-01-28 00:55:03.399551: Epoch time: 48.03 s 
2025-01-28 00:55:04.450321:  
2025-01-28 00:55:04.453608: Epoch 461 
2025-01-28 00:55:04.456486: Current learning rate: 0.00573 
2025-01-28 00:55:52.548974: train_loss -0.808 
2025-01-28 00:55:52.553210: val_loss -0.7107 
2025-01-28 00:55:52.556413: Pseudo dice [np.float32(0.9402), np.float32(0.873)] 
2025-01-28 00:55:52.559269: Epoch time: 48.1 s 
2025-01-28 00:55:53.604737:  
2025-01-28 00:55:53.607912: Epoch 462 
2025-01-28 00:55:53.610723: Current learning rate: 0.00572 
2025-01-28 00:56:41.955188: train_loss -0.8144 
2025-01-28 00:56:41.960040: val_loss -0.7302 
2025-01-28 00:56:41.962325: Pseudo dice [np.float32(0.9371), np.float32(0.8569)] 
2025-01-28 00:56:41.964764: Epoch time: 48.35 s 
2025-01-28 00:56:43.026346:  
2025-01-28 00:56:43.029326: Epoch 463 
2025-01-28 00:56:43.032150: Current learning rate: 0.00571 
2025-01-28 00:57:31.090203: train_loss -0.7837 
2025-01-28 00:57:31.094624: val_loss -0.7346 
2025-01-28 00:57:31.097368: Pseudo dice [np.float32(0.9473), np.float32(0.9142)] 
2025-01-28 00:57:31.099735: Epoch time: 48.06 s 
2025-01-28 00:57:32.151076:  
2025-01-28 00:57:32.154547: Epoch 464 
2025-01-28 00:57:32.157377: Current learning rate: 0.0057 
2025-01-28 00:58:20.193829: train_loss -0.7845 
2025-01-28 00:58:20.199053: val_loss -0.7499 
2025-01-28 00:58:20.201574: Pseudo dice [np.float32(0.9373), np.float32(0.8904)] 
2025-01-28 00:58:20.203892: Epoch time: 48.04 s 
2025-01-28 00:58:21.249322:  
2025-01-28 00:58:21.253323: Epoch 465 
2025-01-28 00:58:21.255887: Current learning rate: 0.0057 
2025-01-28 00:59:10.006365: train_loss -0.7566 
2025-01-28 00:59:10.011449: val_loss -0.7005 
2025-01-28 00:59:10.014383: Pseudo dice [np.float32(0.946), np.float32(0.9122)] 
2025-01-28 00:59:10.016948: Epoch time: 48.76 s 
2025-01-28 00:59:11.135885:  
2025-01-28 00:59:11.139918: Epoch 466 
2025-01-28 00:59:11.143570: Current learning rate: 0.00569 
2025-01-28 00:59:59.252696: train_loss -0.8099 
2025-01-28 00:59:59.258688: val_loss -0.7535 
2025-01-28 00:59:59.261509: Pseudo dice [np.float32(0.9471), np.float32(0.9378)] 
2025-01-28 00:59:59.264539: Epoch time: 48.12 s 
2025-01-28 01:00:00.383406:  
2025-01-28 01:00:00.386336: Epoch 467 
2025-01-28 01:00:00.389453: Current learning rate: 0.00568 
2025-01-28 01:00:48.279052: train_loss -0.8154 
2025-01-28 01:00:48.283827: val_loss -0.7561 
2025-01-28 01:00:48.286786: Pseudo dice [np.float32(0.9456), np.float32(0.9331)] 
2025-01-28 01:00:48.289383: Epoch time: 47.9 s 
2025-01-28 01:00:49.389749:  
2025-01-28 01:00:49.393223: Epoch 468 
2025-01-28 01:00:49.396190: Current learning rate: 0.00567 
2025-01-28 01:01:37.325940: train_loss -0.8004 
2025-01-28 01:01:37.331386: val_loss -0.6919 
2025-01-28 01:01:37.334223: Pseudo dice [np.float32(0.9379), np.float32(0.803)] 
2025-01-28 01:01:37.336650: Epoch time: 47.94 s 
2025-01-28 01:01:38.433058:  
2025-01-28 01:01:38.436026: Epoch 469 
2025-01-28 01:01:38.438776: Current learning rate: 0.00566 
2025-01-28 01:02:26.557431: train_loss -0.8107 
2025-01-28 01:02:26.561738: val_loss -0.7609 
2025-01-28 01:02:26.565342: Pseudo dice [np.float32(0.9525), np.float32(0.8881)] 
2025-01-28 01:02:26.568397: Epoch time: 48.13 s 
2025-01-28 01:02:27.671135:  
2025-01-28 01:02:27.674252: Epoch 470 
2025-01-28 01:02:27.677171: Current learning rate: 0.00565 
2025-01-28 01:03:16.396544: train_loss -0.8001 
2025-01-28 01:03:16.403579: val_loss -0.7366 
2025-01-28 01:03:16.405888: Pseudo dice [np.float32(0.9442), np.float32(0.8957)] 
2025-01-28 01:03:16.408195: Epoch time: 48.73 s 
2025-01-28 01:03:17.476955:  
2025-01-28 01:03:17.480072: Epoch 471 
2025-01-28 01:03:17.482879: Current learning rate: 0.00564 
2025-01-28 01:04:06.054819: train_loss -0.7999 
2025-01-28 01:04:06.060080: val_loss -0.6891 
2025-01-28 01:04:06.062609: Pseudo dice [np.float32(0.9459), np.float32(0.9302)] 
2025-01-28 01:04:06.065278: Epoch time: 48.58 s 
2025-01-28 01:04:07.134680:  
2025-01-28 01:04:07.138121: Epoch 472 
2025-01-28 01:04:07.141401: Current learning rate: 0.00563 
2025-01-28 01:04:55.386875: train_loss -0.7998 
2025-01-28 01:04:55.392549: val_loss -0.7352 
2025-01-28 01:04:55.395142: Pseudo dice [np.float32(0.9403), np.float32(0.9084)] 
2025-01-28 01:04:55.397712: Epoch time: 48.25 s 
2025-01-28 01:04:56.440826:  
2025-01-28 01:04:56.443192: Epoch 473 
2025-01-28 01:04:56.445833: Current learning rate: 0.00562 
2025-01-28 01:05:44.719755: train_loss -0.7963 
2025-01-28 01:05:44.723833: val_loss -0.7598 
2025-01-28 01:05:44.726849: Pseudo dice [np.float32(0.9385), np.float32(0.9179)] 
2025-01-28 01:05:44.729311: Epoch time: 48.28 s 
2025-01-28 01:05:45.780768:  
2025-01-28 01:05:45.783475: Epoch 474 
2025-01-28 01:05:45.786054: Current learning rate: 0.00561 
2025-01-28 01:06:33.866397: train_loss -0.8252 
2025-01-28 01:06:33.872014: val_loss -0.6987 
2025-01-28 01:06:33.874847: Pseudo dice [np.float32(0.938), np.float32(0.9086)] 
2025-01-28 01:06:33.877385: Epoch time: 48.09 s 
2025-01-28 01:06:34.935189:  
2025-01-28 01:06:34.939107: Epoch 475 
2025-01-28 01:06:34.943999: Current learning rate: 0.0056 
2025-01-28 01:07:23.190990: train_loss -0.8215 
2025-01-28 01:07:23.195374: val_loss -0.755 
2025-01-28 01:07:23.198309: Pseudo dice [np.float32(0.961), np.float32(0.9084)] 
2025-01-28 01:07:23.204998: Epoch time: 48.26 s 
2025-01-28 01:07:24.256747:  
2025-01-28 01:07:24.260076: Epoch 476 
2025-01-28 01:07:24.262921: Current learning rate: 0.00559 
2025-01-28 01:08:12.891616: train_loss -0.8155 
2025-01-28 01:08:12.897175: val_loss -0.733 
2025-01-28 01:08:12.899769: Pseudo dice [np.float32(0.9531), np.float32(0.8797)] 
2025-01-28 01:08:12.902309: Epoch time: 48.64 s 
2025-01-28 01:08:13.953547:  
2025-01-28 01:08:13.956360: Epoch 477 
2025-01-28 01:08:13.959219: Current learning rate: 0.00558 
2025-01-28 01:09:01.986550: train_loss -0.8027 
2025-01-28 01:09:01.991685: val_loss -0.7509 
2025-01-28 01:09:01.994467: Pseudo dice [np.float32(0.944), np.float32(0.908)] 
2025-01-28 01:09:01.997022: Epoch time: 48.03 s 
2025-01-28 01:09:03.057424:  
2025-01-28 01:09:03.060864: Epoch 478 
2025-01-28 01:09:03.063906: Current learning rate: 0.00557 
2025-01-28 01:09:50.946563: train_loss -0.8233 
2025-01-28 01:09:50.952641: val_loss -0.7472 
2025-01-28 01:09:50.955498: Pseudo dice [np.float32(0.9391), np.float32(0.9052)] 
2025-01-28 01:09:50.957983: Epoch time: 47.89 s 
2025-01-28 01:09:52.022699:  
2025-01-28 01:09:52.026938: Epoch 479 
2025-01-28 01:09:52.029777: Current learning rate: 0.00556 
2025-01-28 01:10:40.016009: train_loss -0.8014 
2025-01-28 01:10:40.020323: val_loss -0.7003 
2025-01-28 01:10:40.023193: Pseudo dice [np.float32(0.922), np.float32(0.8739)] 
2025-01-28 01:10:40.025961: Epoch time: 48.0 s 
2025-01-28 01:10:41.086879:  
2025-01-28 01:10:41.089925: Epoch 480 
2025-01-28 01:10:41.092731: Current learning rate: 0.00555 
2025-01-28 01:11:29.293837: train_loss -0.8105 
2025-01-28 01:11:29.298995: val_loss -0.7549 
2025-01-28 01:11:29.301746: Pseudo dice [np.float32(0.9472), np.float32(0.8907)] 
2025-01-28 01:11:29.304181: Epoch time: 48.21 s 
2025-01-28 01:11:30.369642:  
2025-01-28 01:11:30.372854: Epoch 481 
2025-01-28 01:11:30.375897: Current learning rate: 0.00554 
2025-01-28 01:12:19.235790: train_loss -0.8146 
2025-01-28 01:12:19.240699: val_loss -0.7061 
2025-01-28 01:12:19.243265: Pseudo dice [np.float32(0.9455), np.float32(0.9035)] 
2025-01-28 01:12:19.245759: Epoch time: 48.87 s 
2025-01-28 01:12:20.307758:  
2025-01-28 01:12:20.311031: Epoch 482 
2025-01-28 01:12:20.314009: Current learning rate: 0.00553 
2025-01-28 01:13:09.318193: train_loss -0.7904 
2025-01-28 01:13:09.323710: val_loss -0.72 
2025-01-28 01:13:09.326428: Pseudo dice [np.float32(0.9474), np.float32(0.8238)] 
2025-01-28 01:13:09.328619: Epoch time: 49.01 s 
2025-01-28 01:13:10.771495:  
2025-01-28 01:13:10.774198: Epoch 483 
2025-01-28 01:13:10.776923: Current learning rate: 0.00552 
2025-01-28 01:13:59.734380: train_loss -0.8111 
2025-01-28 01:13:59.738645: val_loss -0.7481 
2025-01-28 01:13:59.741627: Pseudo dice [np.float32(0.9439), np.float32(0.8702)] 
2025-01-28 01:13:59.744226: Epoch time: 48.96 s 
2025-01-28 01:14:00.816593:  
2025-01-28 01:14:00.819550: Epoch 484 
2025-01-28 01:14:00.822848: Current learning rate: 0.00551 
2025-01-28 01:14:49.027913: train_loss -0.8222 
2025-01-28 01:14:49.033501: val_loss -0.7436 
2025-01-28 01:14:49.036618: Pseudo dice [np.float32(0.9533), np.float32(0.9154)] 
2025-01-28 01:14:49.039276: Epoch time: 48.21 s 
2025-01-28 01:14:50.112192:  
2025-01-28 01:14:50.115574: Epoch 485 
2025-01-28 01:14:50.118549: Current learning rate: 0.0055 
2025-01-28 01:15:38.881696: train_loss -0.8196 
2025-01-28 01:15:38.885716: val_loss -0.7703 
2025-01-28 01:15:38.888411: Pseudo dice [np.float32(0.9575), np.float32(0.9243)] 
2025-01-28 01:15:38.891009: Epoch time: 48.77 s 
2025-01-28 01:15:39.959617:  
2025-01-28 01:15:39.962646: Epoch 486 
2025-01-28 01:15:39.965441: Current learning rate: 0.00549 
2025-01-28 01:16:28.088128: train_loss -0.8231 
2025-01-28 01:16:28.094110: val_loss -0.7588 
2025-01-28 01:16:28.096989: Pseudo dice [np.float32(0.9488), np.float32(0.9231)] 
2025-01-28 01:16:28.099806: Epoch time: 48.13 s 
2025-01-28 01:16:29.160930:  
2025-01-28 01:16:29.164211: Epoch 487 
2025-01-28 01:16:29.167088: Current learning rate: 0.00548 
2025-01-28 01:17:17.353015: train_loss -0.827 
2025-01-28 01:17:17.356965: val_loss -0.7478 
2025-01-28 01:17:17.359337: Pseudo dice [np.float32(0.9432), np.float32(0.912)] 
2025-01-28 01:17:17.361810: Epoch time: 48.19 s 
2025-01-28 01:17:18.438460:  
2025-01-28 01:17:18.441167: Epoch 488 
2025-01-28 01:17:18.443739: Current learning rate: 0.00547 
2025-01-28 01:18:06.571394: train_loss -0.818 
2025-01-28 01:18:06.576916: val_loss -0.7105 
2025-01-28 01:18:06.579489: Pseudo dice [np.float32(0.9538), np.float32(0.9171)] 
2025-01-28 01:18:06.582370: Epoch time: 48.13 s 
2025-01-28 01:18:07.651020:  
2025-01-28 01:18:07.654246: Epoch 489 
2025-01-28 01:18:07.657210: Current learning rate: 0.00546 
2025-01-28 01:18:56.219806: train_loss -0.8004 
2025-01-28 01:18:56.224547: val_loss -0.7809 
2025-01-28 01:18:56.227135: Pseudo dice [np.float32(0.9537), np.float32(0.9264)] 
2025-01-28 01:18:56.229622: Epoch time: 48.57 s 
2025-01-28 01:18:57.295243:  
2025-01-28 01:18:57.298131: Epoch 490 
2025-01-28 01:18:57.300793: Current learning rate: 0.00546 
2025-01-28 01:19:46.334391: train_loss -0.7971 
2025-01-28 01:19:46.339420: val_loss -0.7581 
2025-01-28 01:19:46.342136: Pseudo dice [np.float32(0.9378), np.float32(0.8795)] 
2025-01-28 01:19:46.344460: Epoch time: 49.04 s 
2025-01-28 01:19:47.415365:  
2025-01-28 01:19:47.418465: Epoch 491 
2025-01-28 01:19:47.421460: Current learning rate: 0.00545 
2025-01-28 01:20:35.631080: train_loss -0.8054 
2025-01-28 01:20:35.635398: val_loss -0.7239 
2025-01-28 01:20:35.637913: Pseudo dice [np.float32(0.9485), np.float32(0.9348)] 
2025-01-28 01:20:35.640635: Epoch time: 48.22 s 
2025-01-28 01:20:36.702288:  
2025-01-28 01:20:36.705544: Epoch 492 
2025-01-28 01:20:36.708607: Current learning rate: 0.00544 
2025-01-28 01:21:24.750045: train_loss -0.8273 
2025-01-28 01:21:24.756601: val_loss -0.7421 
2025-01-28 01:21:24.759631: Pseudo dice [np.float32(0.953), np.float32(0.9148)] 
2025-01-28 01:21:24.762586: Epoch time: 48.05 s 
2025-01-28 01:21:25.823846:  
2025-01-28 01:21:25.826490: Epoch 493 
2025-01-28 01:21:25.829374: Current learning rate: 0.00543 
2025-01-28 01:22:14.260245: train_loss -0.8277 
2025-01-28 01:22:14.265346: val_loss -0.7744 
2025-01-28 01:22:14.268484: Pseudo dice [np.float32(0.9557), np.float32(0.9479)] 
2025-01-28 01:22:14.271065: Epoch time: 48.44 s 
2025-01-28 01:22:15.333856:  
2025-01-28 01:22:15.336721: Epoch 494 
2025-01-28 01:22:15.339465: Current learning rate: 0.00542 
2025-01-28 01:23:04.339232: train_loss -0.8267 
2025-01-28 01:23:04.345831: val_loss -0.7242 
2025-01-28 01:23:04.348654: Pseudo dice [np.float32(0.9417), np.float32(0.9131)] 
2025-01-28 01:23:04.351496: Epoch time: 49.01 s 
2025-01-28 01:23:05.415030:  
2025-01-28 01:23:05.418212: Epoch 495 
2025-01-28 01:23:05.421626: Current learning rate: 0.00541 
2025-01-28 01:23:53.343457: train_loss -0.8242 
2025-01-28 01:23:53.349866: val_loss -0.7279 
2025-01-28 01:23:53.353127: Pseudo dice [np.float32(0.9521), np.float32(0.8672)] 
2025-01-28 01:23:53.355757: Epoch time: 47.93 s 
2025-01-28 01:23:54.780481:  
2025-01-28 01:23:54.783248: Epoch 496 
2025-01-28 01:23:54.785769: Current learning rate: 0.0054 
2025-01-28 01:24:43.269536: train_loss -0.8237 
2025-01-28 01:24:43.275226: val_loss -0.7843 
2025-01-28 01:24:43.277940: Pseudo dice [np.float32(0.9514), np.float32(0.9001)] 
2025-01-28 01:24:43.280625: Epoch time: 48.49 s 
2025-01-28 01:24:44.370636:  
2025-01-28 01:24:44.373842: Epoch 497 
2025-01-28 01:24:44.377043: Current learning rate: 0.00539 
2025-01-28 01:25:32.029591: train_loss -0.8158 
2025-01-28 01:25:32.034236: val_loss -0.7629 
2025-01-28 01:25:32.037042: Pseudo dice [np.float32(0.951), np.float32(0.8995)] 
2025-01-28 01:25:32.039658: Epoch time: 47.66 s 
2025-01-28 01:25:33.175140:  
2025-01-28 01:25:33.178162: Epoch 498 
2025-01-28 01:25:33.181034: Current learning rate: 0.00538 
2025-01-28 01:26:20.821129: train_loss -0.8073 
2025-01-28 01:26:20.828622: val_loss -0.78 
2025-01-28 01:26:20.831673: Pseudo dice [np.float32(0.9471), np.float32(0.9164)] 
2025-01-28 01:26:20.834257: Epoch time: 47.65 s 
2025-01-28 01:26:21.976414:  
2025-01-28 01:26:21.979517: Epoch 499 
2025-01-28 01:26:21.982043: Current learning rate: 0.00537 
2025-01-28 01:27:09.593941: train_loss -0.8329 
2025-01-28 01:27:09.598155: val_loss -0.7509 
2025-01-28 01:27:09.600908: Pseudo dice [np.float32(0.9534), np.float32(0.9139)] 
2025-01-28 01:27:09.603798: Epoch time: 47.62 s 
2025-01-28 01:27:11.245095:  
2025-01-28 01:27:11.248375: Epoch 500 
2025-01-28 01:27:11.251152: Current learning rate: 0.00536 
2025-01-28 01:27:59.285747: train_loss -0.8183 
2025-01-28 01:27:59.292619: val_loss -0.7245 
2025-01-28 01:27:59.295656: Pseudo dice [np.float32(0.947), np.float32(0.9193)] 
2025-01-28 01:27:59.298408: Epoch time: 48.04 s 
2025-01-28 01:28:00.419950:  
2025-01-28 01:28:00.423158: Epoch 501 
2025-01-28 01:28:00.426101: Current learning rate: 0.00535 
2025-01-28 01:28:48.238292: train_loss -0.825 
2025-01-28 01:28:48.242708: val_loss -0.7533 
2025-01-28 01:28:48.245425: Pseudo dice [np.float32(0.9453), np.float32(0.9064)] 
2025-01-28 01:28:48.247703: Epoch time: 47.82 s 
2025-01-28 01:28:49.366588:  
2025-01-28 01:28:49.370195: Epoch 502 
2025-01-28 01:28:49.373302: Current learning rate: 0.00534 
2025-01-28 01:29:37.264094: train_loss -0.843 
2025-01-28 01:29:37.268984: val_loss -0.7157 
2025-01-28 01:29:37.271206: Pseudo dice [np.float32(0.9398), np.float32(0.916)] 
2025-01-28 01:29:37.273462: Epoch time: 47.9 s 
2025-01-28 01:29:38.424905:  
2025-01-28 01:29:38.427384: Epoch 503 
2025-01-28 01:29:38.429839: Current learning rate: 0.00533 
2025-01-28 01:30:26.454346: train_loss -0.794 
2025-01-28 01:30:26.458878: val_loss -0.7348 
2025-01-28 01:30:26.461874: Pseudo dice [np.float32(0.9525), np.float32(0.9122)] 
2025-01-28 01:30:26.464643: Epoch time: 48.03 s 
2025-01-28 01:30:27.576713:  
2025-01-28 01:30:27.579819: Epoch 504 
2025-01-28 01:30:27.582828: Current learning rate: 0.00532 
2025-01-28 01:31:15.532455: train_loss -0.8287 
2025-01-28 01:31:15.537845: val_loss -0.7479 
2025-01-28 01:31:15.540344: Pseudo dice [np.float32(0.9461), np.float32(0.9253)] 
2025-01-28 01:31:15.542601: Epoch time: 47.96 s 
2025-01-28 01:31:16.657219:  
2025-01-28 01:31:16.660150: Epoch 505 
2025-01-28 01:31:16.662750: Current learning rate: 0.00531 
2025-01-28 01:32:04.884297: train_loss -0.8253 
2025-01-28 01:32:04.888723: val_loss -0.7609 
2025-01-28 01:32:04.891531: Pseudo dice [np.float32(0.9561), np.float32(0.945)] 
2025-01-28 01:32:04.893962: Epoch time: 48.23 s 
2025-01-28 01:32:04.896691: Yayy! New best EMA pseudo Dice: 0.9312999844551086 
2025-01-28 01:32:06.569972:  
2025-01-28 01:32:06.573014: Epoch 506 
2025-01-28 01:32:06.576042: Current learning rate: 0.0053 
2025-01-28 01:32:55.010354: train_loss -0.8207 
2025-01-28 01:32:55.016268: val_loss -0.6912 
2025-01-28 01:32:55.019285: Pseudo dice [np.float32(0.934), np.float32(0.8953)] 
2025-01-28 01:32:55.021840: Epoch time: 48.44 s 
2025-01-28 01:32:56.110537:  
2025-01-28 01:32:56.114239: Epoch 507 
2025-01-28 01:32:56.117213: Current learning rate: 0.00529 
2025-01-28 01:33:44.948320: train_loss -0.8147 
2025-01-28 01:33:44.953110: val_loss -0.7287 
2025-01-28 01:33:44.956178: Pseudo dice [np.float32(0.9451), np.float32(0.9163)] 
2025-01-28 01:33:44.958929: Epoch time: 48.84 s 
2025-01-28 01:33:46.080969:  
2025-01-28 01:33:46.084090: Epoch 508 
2025-01-28 01:33:46.087258: Current learning rate: 0.00528 
2025-01-28 01:34:34.700883: train_loss -0.7973 
2025-01-28 01:34:34.707952: val_loss -0.7163 
2025-01-28 01:34:34.710888: Pseudo dice [np.float32(0.9407), np.float32(0.8769)] 
2025-01-28 01:34:34.713974: Epoch time: 48.62 s 
2025-01-28 01:34:36.141062:  
2025-01-28 01:34:36.144390: Epoch 509 
2025-01-28 01:34:36.147136: Current learning rate: 0.00527 
2025-01-28 01:35:24.232631: train_loss -0.8103 
2025-01-28 01:35:24.237599: val_loss -0.6826 
2025-01-28 01:35:24.240313: Pseudo dice [np.float32(0.9084), np.float32(0.9016)] 
2025-01-28 01:35:24.242840: Epoch time: 48.09 s 
2025-01-28 01:35:25.308016:  
2025-01-28 01:35:25.310895: Epoch 510 
2025-01-28 01:35:25.313513: Current learning rate: 0.00526 
2025-01-28 01:36:13.517957: train_loss -0.7961 
2025-01-28 01:36:13.522775: val_loss -0.7009 
2025-01-28 01:36:13.524997: Pseudo dice [np.float32(0.9441), np.float32(0.8951)] 
2025-01-28 01:36:13.527084: Epoch time: 48.21 s 
2025-01-28 01:36:14.587583:  
2025-01-28 01:36:14.589799: Epoch 511 
2025-01-28 01:36:14.591976: Current learning rate: 0.00525 
2025-01-28 01:37:03.252315: train_loss -0.8032 
2025-01-28 01:37:03.256549: val_loss -0.7283 
2025-01-28 01:37:03.259582: Pseudo dice [np.float32(0.9391), np.float32(0.8954)] 
2025-01-28 01:37:03.262307: Epoch time: 48.67 s 
2025-01-28 01:37:04.336179:  
2025-01-28 01:37:04.339433: Epoch 512 
2025-01-28 01:37:04.342615: Current learning rate: 0.00524 
2025-01-28 01:37:53.128997: train_loss -0.8228 
2025-01-28 01:37:53.133699: val_loss -0.7533 
2025-01-28 01:37:53.135897: Pseudo dice [np.float32(0.947), np.float32(0.9281)] 
2025-01-28 01:37:53.138056: Epoch time: 48.8 s 
2025-01-28 01:37:54.199643:  
2025-01-28 01:37:54.202054: Epoch 513 
2025-01-28 01:37:54.204313: Current learning rate: 0.00523 
2025-01-28 01:38:42.641876: train_loss -0.8091 
2025-01-28 01:38:42.646867: val_loss -0.7329 
2025-01-28 01:38:42.650038: Pseudo dice [np.float32(0.9439), np.float32(0.9023)] 
2025-01-28 01:38:42.652844: Epoch time: 48.44 s 
2025-01-28 01:38:43.790523:  
2025-01-28 01:38:43.793736: Epoch 514 
2025-01-28 01:38:43.796892: Current learning rate: 0.00522 
2025-01-28 01:39:31.521169: train_loss -0.8083 
2025-01-28 01:39:31.528857: val_loss -0.7757 
2025-01-28 01:39:31.531721: Pseudo dice [np.float32(0.9506), np.float32(0.9316)] 
2025-01-28 01:39:31.534428: Epoch time: 47.73 s 
2025-01-28 01:39:32.680865:  
2025-01-28 01:39:32.684198: Epoch 515 
2025-01-28 01:39:32.687001: Current learning rate: 0.00521 
2025-01-28 01:40:20.383080: train_loss -0.8265 
2025-01-28 01:40:20.386651: val_loss -0.718 
2025-01-28 01:40:20.388859: Pseudo dice [np.float32(0.9393), np.float32(0.921)] 
2025-01-28 01:40:20.391281: Epoch time: 47.7 s 
2025-01-28 01:40:21.514782:  
2025-01-28 01:40:21.517537: Epoch 516 
2025-01-28 01:40:21.520197: Current learning rate: 0.0052 
2025-01-28 01:41:09.633293: train_loss -0.8127 
2025-01-28 01:41:09.639425: val_loss -0.7429 
2025-01-28 01:41:09.642283: Pseudo dice [np.float32(0.9328), np.float32(0.9381)] 
2025-01-28 01:41:09.644994: Epoch time: 48.12 s 
2025-01-28 01:41:10.768185:  
2025-01-28 01:41:10.771536: Epoch 517 
2025-01-28 01:41:10.774709: Current learning rate: 0.00519 
2025-01-28 01:41:58.955728: train_loss -0.8121 
2025-01-28 01:41:58.960581: val_loss -0.7537 
2025-01-28 01:41:58.963380: Pseudo dice [np.float32(0.9547), np.float32(0.9328)] 
2025-01-28 01:41:58.966208: Epoch time: 48.19 s 
2025-01-28 01:42:00.087801:  
2025-01-28 01:42:00.090904: Epoch 518 
2025-01-28 01:42:00.093888: Current learning rate: 0.00518 
2025-01-28 01:42:47.916378: train_loss -0.8167 
2025-01-28 01:42:47.922215: val_loss -0.7142 
2025-01-28 01:42:47.925226: Pseudo dice [np.float32(0.9468), np.float32(0.8277)] 
2025-01-28 01:42:47.927835: Epoch time: 47.83 s 
2025-01-28 01:42:49.049030:  
2025-01-28 01:42:49.052371: Epoch 519 
2025-01-28 01:42:49.055350: Current learning rate: 0.00518 
2025-01-28 01:43:36.795012: train_loss -0.8098 
2025-01-28 01:43:36.799437: val_loss -0.7336 
2025-01-28 01:43:36.802176: Pseudo dice [np.float32(0.9488), np.float32(0.8986)] 
2025-01-28 01:43:36.805077: Epoch time: 47.75 s 
2025-01-28 01:43:37.931839:  
2025-01-28 01:43:37.934939: Epoch 520 
2025-01-28 01:43:37.937715: Current learning rate: 0.00517 
2025-01-28 01:44:25.586823: train_loss -0.821 
2025-01-28 01:44:25.592636: val_loss -0.721 
2025-01-28 01:44:25.595726: Pseudo dice [np.float32(0.954), np.float32(0.9123)] 
2025-01-28 01:44:25.598734: Epoch time: 47.66 s 
2025-01-28 01:44:26.723268:  
2025-01-28 01:44:26.726503: Epoch 521 
2025-01-28 01:44:26.729419: Current learning rate: 0.00516 
2025-01-28 01:45:14.573403: train_loss -0.8224 
2025-01-28 01:45:14.579300: val_loss -0.6662 
2025-01-28 01:45:14.582396: Pseudo dice [np.float32(0.9371), np.float32(0.8123)] 
2025-01-28 01:45:14.585150: Epoch time: 47.85 s 
2025-01-28 01:45:16.061142:  
2025-01-28 01:45:16.064311: Epoch 522 
2025-01-28 01:45:16.067081: Current learning rate: 0.00515 
2025-01-28 01:46:03.786829: train_loss -0.8027 
2025-01-28 01:46:03.792912: val_loss -0.7523 
2025-01-28 01:46:03.795813: Pseudo dice [np.float32(0.9468), np.float32(0.9139)] 
2025-01-28 01:46:03.798606: Epoch time: 47.73 s 
2025-01-28 01:46:04.918489:  
2025-01-28 01:46:04.921612: Epoch 523 
2025-01-28 01:46:04.924840: Current learning rate: 0.00514 
2025-01-28 01:46:52.581700: train_loss -0.8156 
2025-01-28 01:46:52.586311: val_loss -0.7321 
2025-01-28 01:46:52.589204: Pseudo dice [np.float32(0.9481), np.float32(0.9429)] 
2025-01-28 01:46:52.592190: Epoch time: 47.66 s 
2025-01-28 01:46:53.713146:  
2025-01-28 01:46:53.716530: Epoch 524 
2025-01-28 01:46:53.719222: Current learning rate: 0.00513 
2025-01-28 01:47:41.533025: train_loss -0.8296 
2025-01-28 01:47:41.539673: val_loss -0.7195 
2025-01-28 01:47:41.542036: Pseudo dice [np.float32(0.9602), np.float32(0.9414)] 
2025-01-28 01:47:41.544715: Epoch time: 47.82 s 
2025-01-28 01:47:42.666800:  
2025-01-28 01:47:42.670152: Epoch 525 
2025-01-28 01:47:42.673306: Current learning rate: 0.00512 
2025-01-28 01:48:30.768704: train_loss -0.8232 
2025-01-28 01:48:30.772697: val_loss -0.7493 
2025-01-28 01:48:30.775165: Pseudo dice [np.float32(0.9556), np.float32(0.9136)] 
2025-01-28 01:48:30.777876: Epoch time: 48.1 s 
2025-01-28 01:48:31.940477:  
2025-01-28 01:48:31.943200: Epoch 526 
2025-01-28 01:48:31.945925: Current learning rate: 0.00511 
2025-01-28 01:49:19.692568: train_loss -0.8277 
2025-01-28 01:49:19.697461: val_loss -0.7589 
2025-01-28 01:49:19.700152: Pseudo dice [np.float32(0.9482), np.float32(0.9167)] 
2025-01-28 01:49:19.702400: Epoch time: 47.75 s 
2025-01-28 01:49:20.826750:  
2025-01-28 01:49:20.829581: Epoch 527 
2025-01-28 01:49:20.832454: Current learning rate: 0.0051 
2025-01-28 01:50:08.927192: train_loss -0.8218 
2025-01-28 01:50:08.930900: val_loss -0.713 
2025-01-28 01:50:08.933030: Pseudo dice [np.float32(0.9504), np.float32(0.8265)] 
2025-01-28 01:50:08.935574: Epoch time: 48.1 s 
2025-01-28 01:50:10.054820:  
2025-01-28 01:50:10.057487: Epoch 528 
2025-01-28 01:50:10.059953: Current learning rate: 0.00509 
2025-01-28 01:50:57.702133: train_loss -0.8211 
2025-01-28 01:50:57.710202: val_loss -0.7304 
2025-01-28 01:50:57.713043: Pseudo dice [np.float32(0.9533), np.float32(0.9138)] 
2025-01-28 01:50:57.715940: Epoch time: 47.65 s 
2025-01-28 01:50:58.840699:  
2025-01-28 01:50:58.843998: Epoch 529 
2025-01-28 01:50:58.846832: Current learning rate: 0.00508 
2025-01-28 01:51:46.909416: train_loss -0.8158 
2025-01-28 01:51:46.913640: val_loss -0.7737 
2025-01-28 01:51:46.916733: Pseudo dice [np.float32(0.9371), np.float32(0.9315)] 
2025-01-28 01:51:46.919441: Epoch time: 48.07 s 
2025-01-28 01:51:48.014190:  
2025-01-28 01:51:48.018507: Epoch 530 
2025-01-28 01:51:48.021560: Current learning rate: 0.00507 
2025-01-28 01:52:36.131841: train_loss -0.8086 
2025-01-28 01:52:36.138464: val_loss -0.7254 
2025-01-28 01:52:36.141603: Pseudo dice [np.float32(0.9551), np.float32(0.9255)] 
2025-01-28 01:52:36.144473: Epoch time: 48.12 s 
2025-01-28 01:52:37.235769:  
2025-01-28 01:52:37.238773: Epoch 531 
2025-01-28 01:52:37.241635: Current learning rate: 0.00506 
2025-01-28 01:53:25.398340: train_loss -0.8321 
2025-01-28 01:53:25.403975: val_loss -0.7217 
2025-01-28 01:53:25.406767: Pseudo dice [np.float32(0.9481), np.float32(0.914)] 
2025-01-28 01:53:25.409229: Epoch time: 48.16 s 
2025-01-28 01:53:26.479038:  
2025-01-28 01:53:26.482178: Epoch 532 
2025-01-28 01:53:26.485414: Current learning rate: 0.00505 
2025-01-28 01:54:15.410153: train_loss -0.8051 
2025-01-28 01:54:15.416332: val_loss -0.7569 
2025-01-28 01:54:15.419073: Pseudo dice [np.float32(0.9479), np.float32(0.9117)] 
2025-01-28 01:54:15.421718: Epoch time: 48.93 s 
2025-01-28 01:54:16.492783:  
2025-01-28 01:54:16.496087: Epoch 533 
2025-01-28 01:54:16.498801: Current learning rate: 0.00504 
2025-01-28 01:55:05.260207: train_loss -0.8138 
2025-01-28 01:55:05.264358: val_loss -0.7028 
2025-01-28 01:55:05.267222: Pseudo dice [np.float32(0.95), np.float32(0.9152)] 
2025-01-28 01:55:05.269779: Epoch time: 48.77 s 
2025-01-28 01:55:06.685909:  
2025-01-28 01:55:06.689023: Epoch 534 
2025-01-28 01:55:06.691729: Current learning rate: 0.00503 
2025-01-28 01:55:55.152503: train_loss -0.8189 
2025-01-28 01:55:55.157979: val_loss -0.7624 
2025-01-28 01:55:55.160887: Pseudo dice [np.float32(0.9497), np.float32(0.9328)] 
2025-01-28 01:55:55.163482: Epoch time: 48.47 s 
2025-01-28 01:55:56.232121:  
2025-01-28 01:55:56.236382: Epoch 535 
2025-01-28 01:55:56.239628: Current learning rate: 0.00502 
2025-01-28 01:56:44.418295: train_loss -0.8048 
2025-01-28 01:56:44.422233: val_loss -0.7457 
2025-01-28 01:56:44.424847: Pseudo dice [np.float32(0.9483), np.float32(0.8974)] 
2025-01-28 01:56:44.427395: Epoch time: 48.19 s 
2025-01-28 01:56:45.506188:  
2025-01-28 01:56:45.508893: Epoch 536 
2025-01-28 01:56:45.511521: Current learning rate: 0.00501 
2025-01-28 01:57:33.719395: train_loss -0.8241 
2025-01-28 01:57:33.724738: val_loss -0.7305 
2025-01-28 01:57:33.727956: Pseudo dice [np.float32(0.9493), np.float32(0.9213)] 
2025-01-28 01:57:33.730664: Epoch time: 48.21 s 
2025-01-28 01:57:34.920815:  
2025-01-28 01:57:34.924917: Epoch 537 
2025-01-28 01:57:34.928304: Current learning rate: 0.005 
2025-01-28 01:58:23.345301: train_loss -0.8084 
2025-01-28 01:58:23.351310: val_loss -0.7841 
2025-01-28 01:58:23.354004: Pseudo dice [np.float32(0.9469), np.float32(0.9114)] 
2025-01-28 01:58:23.356767: Epoch time: 48.43 s 
2025-01-28 01:58:24.429888:  
2025-01-28 01:58:24.433024: Epoch 538 
2025-01-28 01:58:24.435921: Current learning rate: 0.00499 
2025-01-28 01:59:12.521689: train_loss -0.8256 
2025-01-28 01:59:12.528436: val_loss -0.7238 
2025-01-28 01:59:12.531327: Pseudo dice [np.float32(0.9426), np.float32(0.8878)] 
2025-01-28 01:59:12.533962: Epoch time: 48.09 s 
2025-01-28 01:59:13.608929:  
2025-01-28 01:59:13.612068: Epoch 539 
2025-01-28 01:59:13.614922: Current learning rate: 0.00498 
2025-01-28 02:00:01.946257: train_loss -0.8197 
2025-01-28 02:00:01.953367: val_loss -0.7534 
2025-01-28 02:00:01.956537: Pseudo dice [np.float32(0.9521), np.float32(0.9291)] 
2025-01-28 02:00:01.959358: Epoch time: 48.34 s 
2025-01-28 02:00:03.034428:  
2025-01-28 02:00:03.038598: Epoch 540 
2025-01-28 02:00:03.041538: Current learning rate: 0.00497 
2025-01-28 02:00:51.593097: train_loss -0.8039 
2025-01-28 02:00:51.599399: val_loss -0.7072 
2025-01-28 02:00:51.602399: Pseudo dice [np.float32(0.9403), np.float32(0.9271)] 
2025-01-28 02:00:51.605265: Epoch time: 48.56 s 
2025-01-28 02:00:52.674168:  
2025-01-28 02:00:52.677096: Epoch 541 
2025-01-28 02:00:52.679910: Current learning rate: 0.00496 
2025-01-28 02:01:41.031585: train_loss -0.8144 
2025-01-28 02:01:41.035635: val_loss -0.767 
2025-01-28 02:01:41.038408: Pseudo dice [np.float32(0.9532), np.float32(0.9227)] 
2025-01-28 02:01:41.041257: Epoch time: 48.36 s 
2025-01-28 02:01:42.112638:  
2025-01-28 02:01:42.115835: Epoch 542 
2025-01-28 02:01:42.118607: Current learning rate: 0.00495 
2025-01-28 02:02:30.525540: train_loss -0.8321 
2025-01-28 02:02:30.531061: val_loss -0.7432 
2025-01-28 02:02:30.533939: Pseudo dice [np.float32(0.9567), np.float32(0.918)] 
2025-01-28 02:02:30.536434: Epoch time: 48.41 s 
2025-01-28 02:02:31.607001:  
2025-01-28 02:02:31.610015: Epoch 543 
2025-01-28 02:02:31.612975: Current learning rate: 0.00494 
2025-01-28 02:03:19.591088: train_loss -0.7915 
2025-01-28 02:03:19.595133: val_loss -0.6904 
2025-01-28 02:03:19.597982: Pseudo dice [np.float32(0.9233), np.float32(0.9252)] 
2025-01-28 02:03:19.600891: Epoch time: 47.98 s 
2025-01-28 02:03:20.669185:  
2025-01-28 02:03:20.672237: Epoch 544 
2025-01-28 02:03:20.674778: Current learning rate: 0.00493 
2025-01-28 02:04:08.709396: train_loss -0.8063 
2025-01-28 02:04:08.717950: val_loss -0.7316 
2025-01-28 02:04:08.720837: Pseudo dice [np.float32(0.9403), np.float32(0.9223)] 
2025-01-28 02:04:08.723545: Epoch time: 48.04 s 
2025-01-28 02:04:09.795875:  
2025-01-28 02:04:09.799296: Epoch 545 
2025-01-28 02:04:09.802243: Current learning rate: 0.00492 
2025-01-28 02:04:57.707741: train_loss -0.8163 
2025-01-28 02:04:57.711156: val_loss -0.6991 
2025-01-28 02:04:57.713406: Pseudo dice [np.float32(0.9367), np.float32(0.887)] 
2025-01-28 02:04:57.715682: Epoch time: 47.91 s 
2025-01-28 02:04:58.782060:  
2025-01-28 02:04:58.784726: Epoch 546 
2025-01-28 02:04:58.787287: Current learning rate: 0.00491 
2025-01-28 02:05:46.832930: train_loss -0.8145 
2025-01-28 02:05:46.838888: val_loss -0.718 
2025-01-28 02:05:46.841654: Pseudo dice [np.float32(0.9413), np.float32(0.8908)] 
2025-01-28 02:05:46.844142: Epoch time: 48.05 s 
2025-01-28 02:05:48.293543:  
2025-01-28 02:05:48.296627: Epoch 547 
2025-01-28 02:05:48.299193: Current learning rate: 0.0049 
2025-01-28 02:06:36.238278: train_loss -0.8088 
2025-01-28 02:06:36.241993: val_loss -0.728 
2025-01-28 02:06:36.244330: Pseudo dice [np.float32(0.9477), np.float32(0.9163)] 
2025-01-28 02:06:36.246409: Epoch time: 47.95 s 
2025-01-28 02:06:37.312256:  
2025-01-28 02:06:37.314770: Epoch 548 
2025-01-28 02:06:37.317006: Current learning rate: 0.00489 
2025-01-28 02:07:25.473257: train_loss -0.8175 
2025-01-28 02:07:25.479633: val_loss -0.7346 
2025-01-28 02:07:25.482686: Pseudo dice [np.float32(0.9526), np.float32(0.9098)] 
2025-01-28 02:07:25.485588: Epoch time: 48.16 s 
2025-01-28 02:07:26.555603:  
2025-01-28 02:07:26.558698: Epoch 549 
2025-01-28 02:07:26.561641: Current learning rate: 0.00488 
2025-01-28 02:08:14.589794: train_loss -0.8122 
2025-01-28 02:08:14.593718: val_loss -0.7578 
2025-01-28 02:08:14.596156: Pseudo dice [np.float32(0.9526), np.float32(0.9087)] 
2025-01-28 02:08:14.598395: Epoch time: 48.04 s 
2025-01-28 02:08:16.223289:  
2025-01-28 02:08:16.225740: Epoch 550 
2025-01-28 02:08:16.228429: Current learning rate: 0.00487 
2025-01-28 02:09:04.622742: train_loss -0.8106 
2025-01-28 02:09:04.629548: val_loss -0.7493 
2025-01-28 02:09:04.632723: Pseudo dice [np.float32(0.9424), np.float32(0.8623)] 
2025-01-28 02:09:04.635355: Epoch time: 48.4 s 
2025-01-28 02:09:05.785779:  
2025-01-28 02:09:05.789144: Epoch 551 
2025-01-28 02:09:05.792079: Current learning rate: 0.00486 
2025-01-28 02:09:53.493227: train_loss -0.8067 
2025-01-28 02:09:53.498820: val_loss -0.6814 
2025-01-28 02:09:53.501449: Pseudo dice [np.float32(0.9345), np.float32(0.9003)] 
2025-01-28 02:09:53.504144: Epoch time: 47.71 s 
2025-01-28 02:09:54.629776:  
2025-01-28 02:09:54.633059: Epoch 552 
2025-01-28 02:09:54.635896: Current learning rate: 0.00485 
2025-01-28 02:10:42.830684: train_loss -0.8199 
2025-01-28 02:10:42.837550: val_loss -0.751 
2025-01-28 02:10:42.840520: Pseudo dice [np.float32(0.9534), np.float32(0.9013)] 
2025-01-28 02:10:42.843522: Epoch time: 48.2 s 
2025-01-28 02:10:44.006279:  
2025-01-28 02:10:44.009179: Epoch 553 
2025-01-28 02:10:44.012433: Current learning rate: 0.00484 
2025-01-28 02:11:31.986004: train_loss -0.8106 
2025-01-28 02:11:31.990716: val_loss -0.7306 
2025-01-28 02:11:31.993638: Pseudo dice [np.float32(0.9447), np.float32(0.896)] 
2025-01-28 02:11:31.996207: Epoch time: 47.98 s 
2025-01-28 02:11:33.116786:  
2025-01-28 02:11:33.119560: Epoch 554 
2025-01-28 02:11:33.122359: Current learning rate: 0.00484 
2025-01-28 02:12:20.972355: train_loss -0.8206 
2025-01-28 02:12:20.979004: val_loss -0.7522 
2025-01-28 02:12:20.981635: Pseudo dice [np.float32(0.9536), np.float32(0.8282)] 
2025-01-28 02:12:20.984469: Epoch time: 47.86 s 
2025-01-28 02:12:22.148578:  
2025-01-28 02:12:22.151620: Epoch 555 
2025-01-28 02:12:22.154445: Current learning rate: 0.00483 
2025-01-28 02:13:10.462437: train_loss -0.8224 
2025-01-28 02:13:10.466672: val_loss -0.7551 
2025-01-28 02:13:10.469291: Pseudo dice [np.float32(0.953), np.float32(0.8511)] 
2025-01-28 02:13:10.471585: Epoch time: 48.31 s 
2025-01-28 02:13:11.593375:  
2025-01-28 02:13:11.596348: Epoch 556 
2025-01-28 02:13:11.599016: Current learning rate: 0.00482 
2025-01-28 02:13:59.258299: train_loss -0.8032 
2025-01-28 02:13:59.264646: val_loss -0.7332 
2025-01-28 02:13:59.267570: Pseudo dice [np.float32(0.951), np.float32(0.9194)] 
2025-01-28 02:13:59.270428: Epoch time: 47.67 s 
2025-01-28 02:14:00.390244:  
2025-01-28 02:14:00.393553: Epoch 557 
2025-01-28 02:14:00.396618: Current learning rate: 0.00481 
2025-01-28 02:14:48.451981: train_loss -0.826 
2025-01-28 02:14:48.456203: val_loss -0.6891 
2025-01-28 02:14:48.458813: Pseudo dice [np.float32(0.9434), np.float32(0.9147)] 
2025-01-28 02:14:48.461256: Epoch time: 48.06 s 
2025-01-28 02:14:49.541156:  
2025-01-28 02:14:49.544715: Epoch 558 
2025-01-28 02:14:49.547287: Current learning rate: 0.0048 
2025-01-28 02:15:37.785960: train_loss -0.8327 
2025-01-28 02:15:37.792375: val_loss -0.7419 
2025-01-28 02:15:37.795408: Pseudo dice [np.float32(0.9513), np.float32(0.9423)] 
2025-01-28 02:15:37.798441: Epoch time: 48.25 s 
2025-01-28 02:15:38.918801:  
2025-01-28 02:15:38.921490: Epoch 559 
2025-01-28 02:15:38.923990: Current learning rate: 0.00479 
2025-01-28 02:16:26.847703: train_loss -0.8119 
2025-01-28 02:16:26.852707: val_loss -0.7527 
2025-01-28 02:16:26.855745: Pseudo dice [np.float32(0.9503), np.float32(0.9347)] 
2025-01-28 02:16:26.858395: Epoch time: 47.93 s 
2025-01-28 02:16:28.392756:  
2025-01-28 02:16:28.395965: Epoch 560 
2025-01-28 02:16:28.398947: Current learning rate: 0.00478 
2025-01-28 02:17:16.057861: train_loss -0.8289 
2025-01-28 02:17:16.064059: val_loss -0.7393 
2025-01-28 02:17:16.066823: Pseudo dice [np.float32(0.9595), np.float32(0.9341)] 
2025-01-28 02:17:16.069234: Epoch time: 47.67 s 
2025-01-28 02:17:17.228771:  
2025-01-28 02:17:17.231584: Epoch 561 
2025-01-28 02:17:17.234113: Current learning rate: 0.00477 
2025-01-28 02:18:05.318647: train_loss -0.8167 
2025-01-28 02:18:05.323349: val_loss -0.7181 
2025-01-28 02:18:05.326158: Pseudo dice [np.float32(0.953), np.float32(0.9234)] 
2025-01-28 02:18:05.328958: Epoch time: 48.09 s 
2025-01-28 02:18:06.491778:  
2025-01-28 02:18:06.495116: Epoch 562 
2025-01-28 02:18:06.498314: Current learning rate: 0.00476 
2025-01-28 02:18:54.248769: train_loss -0.8091 
2025-01-28 02:18:54.253596: val_loss -0.729 
2025-01-28 02:18:54.255909: Pseudo dice [np.float32(0.951), np.float32(0.9053)] 
2025-01-28 02:18:54.258316: Epoch time: 47.76 s 
2025-01-28 02:18:55.379495:  
2025-01-28 02:18:55.382598: Epoch 563 
2025-01-28 02:18:55.385556: Current learning rate: 0.00475 
2025-01-28 02:19:43.474053: train_loss -0.814 
2025-01-28 02:19:43.478895: val_loss -0.7212 
2025-01-28 02:19:43.481876: Pseudo dice [np.float32(0.9553), np.float32(0.7918)] 
2025-01-28 02:19:43.484503: Epoch time: 48.1 s 
2025-01-28 02:19:44.600832:  
2025-01-28 02:19:44.604501: Epoch 564 
2025-01-28 02:19:44.607621: Current learning rate: 0.00474 
2025-01-28 02:20:32.601658: train_loss -0.8182 
2025-01-28 02:20:32.607765: val_loss -0.7315 
2025-01-28 02:20:32.610569: Pseudo dice [np.float32(0.9496), np.float32(0.8831)] 
2025-01-28 02:20:32.613231: Epoch time: 48.0 s 
2025-01-28 02:20:33.773929:  
2025-01-28 02:20:33.777302: Epoch 565 
2025-01-28 02:20:33.780627: Current learning rate: 0.00473 
2025-01-28 02:21:21.423257: train_loss -0.8169 
2025-01-28 02:21:21.427219: val_loss -0.6941 
2025-01-28 02:21:21.429885: Pseudo dice [np.float32(0.9465), np.float32(0.852)] 
2025-01-28 02:21:21.432351: Epoch time: 47.65 s 
2025-01-28 02:21:22.553576:  
2025-01-28 02:21:22.556014: Epoch 566 
2025-01-28 02:21:22.558549: Current learning rate: 0.00472 
2025-01-28 02:22:10.544042: train_loss -0.8379 
2025-01-28 02:22:10.549685: val_loss -0.7472 
2025-01-28 02:22:10.552365: Pseudo dice [np.float32(0.9325), np.float32(0.8749)] 
2025-01-28 02:22:10.555076: Epoch time: 47.99 s 
2025-01-28 02:22:11.695415:  
2025-01-28 02:22:11.698862: Epoch 567 
2025-01-28 02:22:11.702230: Current learning rate: 0.00471 
2025-01-28 02:22:59.683535: train_loss -0.8292 
2025-01-28 02:22:59.687679: val_loss -0.7334 
2025-01-28 02:22:59.690514: Pseudo dice [np.float32(0.9503), np.float32(0.9179)] 
2025-01-28 02:22:59.693192: Epoch time: 47.99 s 
2025-01-28 02:23:00.851705:  
2025-01-28 02:23:00.854447: Epoch 568 
2025-01-28 02:23:00.856992: Current learning rate: 0.0047 
2025-01-28 02:23:48.576129: train_loss -0.8259 
2025-01-28 02:23:48.582149: val_loss -0.7115 
2025-01-28 02:23:48.584970: Pseudo dice [np.float32(0.9526), np.float32(0.9042)] 
2025-01-28 02:23:48.587605: Epoch time: 47.73 s 
2025-01-28 02:23:49.751173:  
2025-01-28 02:23:49.754207: Epoch 569 
2025-01-28 02:23:49.756870: Current learning rate: 0.00469 
2025-01-28 02:24:37.948862: train_loss -0.8259 
2025-01-28 02:24:37.953888: val_loss -0.7423 
2025-01-28 02:24:37.956964: Pseudo dice [np.float32(0.9593), np.float32(0.9304)] 
2025-01-28 02:24:37.959737: Epoch time: 48.2 s 
2025-01-28 02:24:39.079494:  
2025-01-28 02:24:39.082572: Epoch 570 
2025-01-28 02:24:39.085701: Current learning rate: 0.00468 
2025-01-28 02:25:26.899058: train_loss -0.8333 
2025-01-28 02:25:26.905304: val_loss -0.7354 
2025-01-28 02:25:26.908416: Pseudo dice [np.float32(0.9485), np.float32(0.907)] 
2025-01-28 02:25:26.911144: Epoch time: 47.82 s 
2025-01-28 02:25:28.031899:  
2025-01-28 02:25:28.034770: Epoch 571 
2025-01-28 02:25:28.037500: Current learning rate: 0.00467 
2025-01-28 02:26:15.772491: train_loss -0.8067 
2025-01-28 02:26:15.776757: val_loss -0.7421 
2025-01-28 02:26:15.779347: Pseudo dice [np.float32(0.942), np.float32(0.9326)] 
2025-01-28 02:26:15.781908: Epoch time: 47.74 s 
2025-01-28 02:26:16.901852:  
2025-01-28 02:26:16.904664: Epoch 572 
2025-01-28 02:26:16.907142: Current learning rate: 0.00466 
2025-01-28 02:27:04.775524: train_loss -0.8318 
2025-01-28 02:27:04.780996: val_loss -0.7771 
2025-01-28 02:27:04.783329: Pseudo dice [np.float32(0.9492), np.float32(0.9081)] 
2025-01-28 02:27:04.785768: Epoch time: 47.87 s 
2025-01-28 02:27:06.234436:  
2025-01-28 02:27:06.236994: Epoch 573 
2025-01-28 02:27:06.239218: Current learning rate: 0.00465 
2025-01-28 02:27:54.586907: train_loss -0.8432 
2025-01-28 02:27:54.593683: val_loss -0.7453 
2025-01-28 02:27:54.596538: Pseudo dice [np.float32(0.9424), np.float32(0.9159)] 
2025-01-28 02:27:54.599217: Epoch time: 48.35 s 
2025-01-28 02:27:55.735645:  
2025-01-28 02:27:55.739275: Epoch 574 
2025-01-28 02:27:55.742327: Current learning rate: 0.00464 
2025-01-28 02:28:44.543157: train_loss -0.8287 
2025-01-28 02:28:44.550059: val_loss -0.75 
2025-01-28 02:28:44.553116: Pseudo dice [np.float32(0.9418), np.float32(0.8715)] 
2025-01-28 02:28:44.555976: Epoch time: 48.81 s 
2025-01-28 02:28:45.674516:  
2025-01-28 02:28:45.677988: Epoch 575 
2025-01-28 02:28:45.680726: Current learning rate: 0.00463 
2025-01-28 02:29:34.375842: train_loss -0.8301 
2025-01-28 02:29:34.379658: val_loss -0.7456 
2025-01-28 02:29:34.381966: Pseudo dice [np.float32(0.9466), np.float32(0.9144)] 
2025-01-28 02:29:34.384208: Epoch time: 48.7 s 
2025-01-28 02:29:35.577187:  
2025-01-28 02:29:35.580315: Epoch 576 
2025-01-28 02:29:35.582769: Current learning rate: 0.00462 
2025-01-28 02:30:24.776460: train_loss -0.8265 
2025-01-28 02:30:24.784460: val_loss -0.7308 
2025-01-28 02:30:24.787450: Pseudo dice [np.float32(0.9409), np.float32(0.9118)] 
2025-01-28 02:30:24.790438: Epoch time: 49.2 s 
2025-01-28 02:30:26.006769:  
2025-01-28 02:30:26.009724: Epoch 577 
2025-01-28 02:30:26.012564: Current learning rate: 0.00461 
2025-01-28 02:31:14.177202: train_loss -0.836 
2025-01-28 02:31:14.183361: val_loss -0.7701 
2025-01-28 02:31:14.185946: Pseudo dice [np.float32(0.951), np.float32(0.9388)] 
2025-01-28 02:31:14.188537: Epoch time: 48.17 s 
2025-01-28 02:31:15.332730:  
2025-01-28 02:31:15.336324: Epoch 578 
2025-01-28 02:31:15.339061: Current learning rate: 0.0046 
2025-01-28 02:32:03.016807: train_loss -0.833 
2025-01-28 02:32:03.022775: val_loss -0.7637 
2025-01-28 02:32:03.025662: Pseudo dice [np.float32(0.9534), np.float32(0.9278)] 
2025-01-28 02:32:03.028457: Epoch time: 47.68 s 
2025-01-28 02:32:04.164879:  
2025-01-28 02:32:04.168277: Epoch 579 
2025-01-28 02:32:04.171246: Current learning rate: 0.00459 
2025-01-28 02:32:52.146119: train_loss -0.8282 
2025-01-28 02:32:52.151379: val_loss -0.7314 
2025-01-28 02:32:52.154676: Pseudo dice [np.float32(0.9554), np.float32(0.9007)] 
2025-01-28 02:32:52.157432: Epoch time: 47.98 s 
2025-01-28 02:32:53.334786:  
2025-01-28 02:32:53.337859: Epoch 580 
2025-01-28 02:32:53.340776: Current learning rate: 0.00458 
2025-01-28 02:33:41.377911: train_loss -0.8201 
2025-01-28 02:33:41.382969: val_loss -0.7375 
2025-01-28 02:33:41.385045: Pseudo dice [np.float32(0.9559), np.float32(0.9139)] 
2025-01-28 02:33:41.387103: Epoch time: 48.04 s 
2025-01-28 02:33:42.521334:  
2025-01-28 02:33:42.523725: Epoch 581 
2025-01-28 02:33:42.526079: Current learning rate: 0.00457 
2025-01-28 02:34:30.718417: train_loss -0.8195 
2025-01-28 02:34:30.725337: val_loss -0.7502 
2025-01-28 02:34:30.728461: Pseudo dice [np.float32(0.9502), np.float32(0.9352)] 
2025-01-28 02:34:30.731440: Epoch time: 48.2 s 
2025-01-28 02:34:31.866063:  
2025-01-28 02:34:31.869024: Epoch 582 
2025-01-28 02:34:31.872031: Current learning rate: 0.00456 
2025-01-28 02:35:19.757797: train_loss -0.8115 
2025-01-28 02:35:19.764493: val_loss -0.7168 
2025-01-28 02:35:19.767494: Pseudo dice [np.float32(0.9557), np.float32(0.9375)] 
2025-01-28 02:35:19.770490: Epoch time: 47.89 s 
2025-01-28 02:35:19.773606: Yayy! New best EMA pseudo Dice: 0.9319000244140625 
2025-01-28 02:35:21.469022:  
2025-01-28 02:35:21.471821: Epoch 583 
2025-01-28 02:35:21.474605: Current learning rate: 0.00455 
2025-01-28 02:36:09.092558: train_loss -0.845 
2025-01-28 02:36:09.097598: val_loss -0.7117 
2025-01-28 02:36:09.100471: Pseudo dice [np.float32(0.9483), np.float32(0.9127)] 
2025-01-28 02:36:09.103045: Epoch time: 47.62 s 
2025-01-28 02:36:10.237292:  
2025-01-28 02:36:10.240100: Epoch 584 
2025-01-28 02:36:10.243226: Current learning rate: 0.00454 
2025-01-28 02:36:58.194435: train_loss -0.8313 
2025-01-28 02:36:58.200905: val_loss -0.7394 
2025-01-28 02:36:58.203845: Pseudo dice [np.float32(0.9522), np.float32(0.941)] 
2025-01-28 02:36:58.206551: Epoch time: 47.96 s 
2025-01-28 02:36:58.209412: Yayy! New best EMA pseudo Dice: 0.9332000017166138 
2025-01-28 02:37:00.248390:  
2025-01-28 02:37:00.251664: Epoch 585 
2025-01-28 02:37:00.254509: Current learning rate: 0.00453 
2025-01-28 02:37:48.077652: train_loss -0.8296 
2025-01-28 02:37:48.081258: val_loss -0.7673 
2025-01-28 02:37:48.083493: Pseudo dice [np.float32(0.9476), np.float32(0.9083)] 
2025-01-28 02:37:48.085946: Epoch time: 47.83 s 
2025-01-28 02:37:49.221680:  
2025-01-28 02:37:49.224184: Epoch 586 
2025-01-28 02:37:49.226358: Current learning rate: 0.00452 
2025-01-28 02:38:36.873953: train_loss -0.8319 
2025-01-28 02:38:36.880060: val_loss -0.7757 
2025-01-28 02:38:36.882881: Pseudo dice [np.float32(0.9541), np.float32(0.9197)] 
2025-01-28 02:38:36.885778: Epoch time: 47.65 s 
2025-01-28 02:38:38.069806:  
2025-01-28 02:38:38.073707: Epoch 587 
2025-01-28 02:38:38.076724: Current learning rate: 0.00451 
2025-01-28 02:39:26.597680: train_loss -0.8257 
2025-01-28 02:39:26.602483: val_loss -0.7686 
2025-01-28 02:39:26.605415: Pseudo dice [np.float32(0.9568), np.float32(0.9302)] 
2025-01-28 02:39:26.608166: Epoch time: 48.53 s 
2025-01-28 02:39:26.610955: Yayy! New best EMA pseudo Dice: 0.9341999888420105 
2025-01-28 02:39:28.378765:  
2025-01-28 02:39:28.382522: Epoch 588 
2025-01-28 02:39:28.385499: Current learning rate: 0.0045 
2025-01-28 02:40:16.337943: train_loss -0.8274 
2025-01-28 02:40:16.343452: val_loss -0.7631 
2025-01-28 02:40:16.345815: Pseudo dice [np.float32(0.9538), np.float32(0.9452)] 
2025-01-28 02:40:16.348133: Epoch time: 47.96 s 
2025-01-28 02:40:16.350741: Yayy! New best EMA pseudo Dice: 0.935699999332428 
2025-01-28 02:40:18.030118:  
2025-01-28 02:40:18.033081: Epoch 589 
2025-01-28 02:40:18.035744: Current learning rate: 0.00449 
2025-01-28 02:41:06.151955: train_loss -0.8331 
2025-01-28 02:41:06.155929: val_loss -0.7922 
2025-01-28 02:41:06.158412: Pseudo dice [np.float32(0.9542), np.float32(0.9168)] 
2025-01-28 02:41:06.161134: Epoch time: 48.12 s 
2025-01-28 02:41:07.300544:  
2025-01-28 02:41:07.303616: Epoch 590 
2025-01-28 02:41:07.306616: Current learning rate: 0.00448 
2025-01-28 02:41:55.270765: train_loss -0.8245 
2025-01-28 02:41:55.276434: val_loss -0.7484 
2025-01-28 02:41:55.278862: Pseudo dice [np.float32(0.9429), np.float32(0.8989)] 
2025-01-28 02:41:55.281392: Epoch time: 47.97 s 
2025-01-28 02:41:56.432487:  
2025-01-28 02:41:56.434721: Epoch 591 
2025-01-28 02:41:56.437111: Current learning rate: 0.00447 
2025-01-28 02:42:44.373510: train_loss -0.8389 
2025-01-28 02:42:44.377437: val_loss -0.742 
2025-01-28 02:42:44.380005: Pseudo dice [np.float32(0.9364), np.float32(0.9008)] 
2025-01-28 02:42:44.382314: Epoch time: 47.94 s 
2025-01-28 02:42:45.561173:  
2025-01-28 02:42:45.563936: Epoch 592 
2025-01-28 02:42:45.566337: Current learning rate: 0.00446 
2025-01-28 02:43:33.807999: train_loss -0.817 
2025-01-28 02:43:33.813145: val_loss -0.7141 
2025-01-28 02:43:33.815677: Pseudo dice [np.float32(0.9196), np.float32(0.9224)] 
2025-01-28 02:43:33.818240: Epoch time: 48.25 s 
2025-01-28 02:43:34.955653:  
2025-01-28 02:43:34.960039: Epoch 593 
2025-01-28 02:43:34.962570: Current learning rate: 0.00445 
2025-01-28 02:44:22.741924: train_loss -0.8041 
2025-01-28 02:44:22.746724: val_loss -0.7402 
2025-01-28 02:44:22.749633: Pseudo dice [np.float32(0.9509), np.float32(0.8908)] 
2025-01-28 02:44:22.752259: Epoch time: 47.79 s 
2025-01-28 02:44:23.893316:  
2025-01-28 02:44:23.896880: Epoch 594 
2025-01-28 02:44:23.900087: Current learning rate: 0.00444 
2025-01-28 02:45:12.423209: train_loss -0.8195 
2025-01-28 02:45:12.430160: val_loss -0.7327 
2025-01-28 02:45:12.432860: Pseudo dice [np.float32(0.9352), np.float32(0.9084)] 
2025-01-28 02:45:12.435421: Epoch time: 48.53 s 
2025-01-28 02:45:13.535133:  
2025-01-28 02:45:13.537980: Epoch 595 
2025-01-28 02:45:13.540822: Current learning rate: 0.00443 
2025-01-28 02:46:02.507815: train_loss -0.8202 
2025-01-28 02:46:02.512286: val_loss -0.7013 
2025-01-28 02:46:02.515122: Pseudo dice [np.float32(0.9459), np.float32(0.8874)] 
2025-01-28 02:46:02.517667: Epoch time: 48.97 s 
2025-01-28 02:46:03.657961:  
2025-01-28 02:46:03.660644: Epoch 596 
2025-01-28 02:46:03.663633: Current learning rate: 0.00442 
2025-01-28 02:46:51.811218: train_loss -0.8245 
2025-01-28 02:46:51.816764: val_loss -0.7501 
2025-01-28 02:46:51.819379: Pseudo dice [np.float32(0.9348), np.float32(0.921)] 
2025-01-28 02:46:51.821831: Epoch time: 48.15 s 
2025-01-28 02:46:53.001558:  
2025-01-28 02:46:53.004271: Epoch 597 
2025-01-28 02:46:53.006828: Current learning rate: 0.00441 
2025-01-28 02:47:41.230256: train_loss -0.8149 
2025-01-28 02:47:41.234954: val_loss -0.7732 
2025-01-28 02:47:41.237396: Pseudo dice [np.float32(0.9574), np.float32(0.939)] 
2025-01-28 02:47:41.239966: Epoch time: 48.23 s 
2025-01-28 02:47:42.855215:  
2025-01-28 02:47:42.857712: Epoch 598 
2025-01-28 02:47:42.860600: Current learning rate: 0.0044 
2025-01-28 02:48:31.033759: train_loss -0.8182 
2025-01-28 02:48:31.039366: val_loss -0.6547 
2025-01-28 02:48:31.041759: Pseudo dice [np.float32(0.9457), np.float32(0.8827)] 
2025-01-28 02:48:31.043992: Epoch time: 48.18 s 
2025-01-28 02:48:32.157057:  
2025-01-28 02:48:32.159465: Epoch 599 
2025-01-28 02:48:32.161919: Current learning rate: 0.00439 
2025-01-28 02:49:20.805031: train_loss -0.8124 
2025-01-28 02:49:20.809117: val_loss -0.7338 
2025-01-28 02:49:20.811873: Pseudo dice [np.float32(0.9328), np.float32(0.8899)] 
2025-01-28 02:49:20.814635: Epoch time: 48.65 s 
2025-01-28 02:49:22.543110:  
2025-01-28 02:49:22.546501: Epoch 600 
2025-01-28 02:49:22.549808: Current learning rate: 0.00438 
2025-01-28 02:50:11.061924: train_loss -0.8286 
2025-01-28 02:50:11.068478: val_loss -0.7083 
2025-01-28 02:50:11.071587: Pseudo dice [np.float32(0.9413), np.float32(0.8474)] 
2025-01-28 02:50:11.074739: Epoch time: 48.52 s 
2025-01-28 02:50:12.178346:  
2025-01-28 02:50:12.181454: Epoch 601 
2025-01-28 02:50:12.184256: Current learning rate: 0.00437 
2025-01-28 02:51:01.182983: train_loss -0.8115 
2025-01-28 02:51:01.187483: val_loss -0.7737 
2025-01-28 02:51:01.190037: Pseudo dice [np.float32(0.9543), np.float32(0.9256)] 
2025-01-28 02:51:01.192683: Epoch time: 49.01 s 
2025-01-28 02:51:02.279138:  
2025-01-28 02:51:02.281628: Epoch 602 
2025-01-28 02:51:02.283957: Current learning rate: 0.00436 
2025-01-28 02:51:50.378798: train_loss -0.8287 
2025-01-28 02:51:50.385062: val_loss -0.7566 
2025-01-28 02:51:50.388415: Pseudo dice [np.float32(0.9479), np.float32(0.9114)] 
2025-01-28 02:51:50.391357: Epoch time: 48.1 s 
2025-01-28 02:51:51.564042:  
2025-01-28 02:51:51.567210: Epoch 603 
2025-01-28 02:51:51.570132: Current learning rate: 0.00435 
2025-01-28 02:52:39.465707: train_loss -0.822 
2025-01-28 02:52:39.470944: val_loss -0.6838 
2025-01-28 02:52:39.480515: Pseudo dice [np.float32(0.9497), np.float32(0.8505)] 
2025-01-28 02:52:39.483421: Epoch time: 47.9 s 
2025-01-28 02:52:40.627483:  
2025-01-28 02:52:40.629918: Epoch 604 
2025-01-28 02:52:40.632195: Current learning rate: 0.00434 
2025-01-28 02:53:28.807244: train_loss -0.8235 
2025-01-28 02:53:28.897571: val_loss -0.7787 
2025-01-28 02:53:28.900092: Pseudo dice [np.float32(0.9484), np.float32(0.9077)] 
2025-01-28 02:53:28.902570: Epoch time: 48.18 s 
2025-01-28 02:53:30.083331:  
2025-01-28 02:53:30.086173: Epoch 605 
2025-01-28 02:53:30.089066: Current learning rate: 0.00433 
2025-01-28 02:54:18.157864: train_loss -0.8456 
2025-01-28 02:54:18.161691: val_loss -0.7161 
2025-01-28 02:54:18.164448: Pseudo dice [np.float32(0.953), np.float32(0.93)] 
2025-01-28 02:54:18.166939: Epoch time: 48.08 s 
2025-01-28 02:54:19.340169:  
2025-01-28 02:54:19.342983: Epoch 606 
2025-01-28 02:54:19.345848: Current learning rate: 0.00432 
2025-01-28 02:55:07.470282: train_loss -0.8272 
2025-01-28 02:55:07.475573: val_loss -0.7646 
2025-01-28 02:55:07.478417: Pseudo dice [np.float32(0.9554), np.float32(0.9145)] 
2025-01-28 02:55:07.480963: Epoch time: 48.13 s 
2025-01-28 02:55:08.612414:  
2025-01-28 02:55:08.615427: Epoch 607 
2025-01-28 02:55:08.618151: Current learning rate: 0.00431 
2025-01-28 02:55:56.614414: train_loss -0.8276 
2025-01-28 02:55:56.618204: val_loss -0.7287 
2025-01-28 02:55:56.620840: Pseudo dice [np.float32(0.957), np.float32(0.9254)] 
2025-01-28 02:55:56.623249: Epoch time: 48.0 s 
2025-01-28 02:55:57.754417:  
2025-01-28 02:55:57.757112: Epoch 608 
2025-01-28 02:55:57.760221: Current learning rate: 0.0043 
2025-01-28 02:56:45.382428: train_loss -0.8266 
2025-01-28 02:56:45.388320: val_loss -0.7382 
2025-01-28 02:56:45.391311: Pseudo dice [np.float32(0.9528), np.float32(0.9126)] 
2025-01-28 02:56:45.394124: Epoch time: 47.63 s 
2025-01-28 02:56:46.556183:  
2025-01-28 02:56:46.559188: Epoch 609 
2025-01-28 02:56:46.562027: Current learning rate: 0.00429 
2025-01-28 02:57:34.931239: train_loss -0.8356 
2025-01-28 02:57:34.935225: val_loss -0.744 
2025-01-28 02:57:34.937791: Pseudo dice [np.float32(0.9576), np.float32(0.9197)] 
2025-01-28 02:57:34.940573: Epoch time: 48.38 s 
2025-01-28 02:57:36.402420:  
2025-01-28 02:57:36.405761: Epoch 610 
2025-01-28 02:57:36.408836: Current learning rate: 0.00429 
2025-01-28 02:58:24.911799: train_loss -0.8365 
2025-01-28 02:58:24.917414: val_loss -0.773 
2025-01-28 02:58:24.920055: Pseudo dice [np.float32(0.9478), np.float32(0.8858)] 
2025-01-28 02:58:24.922602: Epoch time: 48.51 s 
2025-01-28 02:58:26.056388:  
2025-01-28 02:58:26.059453: Epoch 611 
2025-01-28 02:58:26.062225: Current learning rate: 0.00428 
2025-01-28 02:59:14.234457: train_loss -0.815 
2025-01-28 02:59:14.238436: val_loss -0.7392 
2025-01-28 02:59:14.240944: Pseudo dice [np.float32(0.9567), np.float32(0.9027)] 
2025-01-28 02:59:14.243543: Epoch time: 48.18 s 
2025-01-28 02:59:15.423346:  
2025-01-28 02:59:15.426630: Epoch 612 
2025-01-28 02:59:15.429900: Current learning rate: 0.00427 
2025-01-28 03:00:03.308675: train_loss -0.82 
2025-01-28 03:00:03.314749: val_loss -0.7675 
2025-01-28 03:00:03.318219: Pseudo dice [np.float32(0.9513), np.float32(0.8743)] 
2025-01-28 03:00:03.321232: Epoch time: 47.89 s 
2025-01-28 03:00:04.461758:  
2025-01-28 03:00:04.464868: Epoch 613 
2025-01-28 03:00:04.467880: Current learning rate: 0.00426 
2025-01-28 03:00:52.408832: train_loss -0.8366 
2025-01-28 03:00:52.412637: val_loss -0.7255 
2025-01-28 03:00:52.415188: Pseudo dice [np.float32(0.9376), np.float32(0.9021)] 
2025-01-28 03:00:52.417750: Epoch time: 47.95 s 
2025-01-28 03:00:53.557796:  
2025-01-28 03:00:53.560384: Epoch 614 
2025-01-28 03:00:53.562814: Current learning rate: 0.00425 
2025-01-28 03:01:41.720480: train_loss -0.8321 
2025-01-28 03:01:41.725564: val_loss -0.7225 
2025-01-28 03:01:41.728131: Pseudo dice [np.float32(0.9445), np.float32(0.8834)] 
2025-01-28 03:01:41.730605: Epoch time: 48.16 s 
2025-01-28 03:01:42.869609:  
2025-01-28 03:01:42.872817: Epoch 615 
2025-01-28 03:01:42.875421: Current learning rate: 0.00424 
2025-01-28 03:02:30.755556: train_loss -0.8324 
2025-01-28 03:02:30.759252: val_loss -0.773 
2025-01-28 03:02:30.761964: Pseudo dice [np.float32(0.9505), np.float32(0.9314)] 
2025-01-28 03:02:30.764512: Epoch time: 47.89 s 
2025-01-28 03:02:31.899156:  
2025-01-28 03:02:31.902148: Epoch 616 
2025-01-28 03:02:31.905037: Current learning rate: 0.00423 
2025-01-28 03:03:19.835227: train_loss -0.8251 
2025-01-28 03:03:19.872658: val_loss -0.7817 
2025-01-28 03:03:19.875239: Pseudo dice [np.float32(0.9459), np.float32(0.9357)] 
2025-01-28 03:03:19.877748: Epoch time: 47.94 s 
2025-01-28 03:03:21.013424:  
2025-01-28 03:03:21.016132: Epoch 617 
2025-01-28 03:03:21.018833: Current learning rate: 0.00422 
2025-01-28 03:04:09.283368: train_loss -0.8263 
2025-01-28 03:04:09.287086: val_loss -0.7672 
2025-01-28 03:04:09.289602: Pseudo dice [np.float32(0.9545), np.float32(0.9167)] 
2025-01-28 03:04:09.292193: Epoch time: 48.27 s 
2025-01-28 03:04:10.406237:  
2025-01-28 03:04:10.409959: Epoch 618 
2025-01-28 03:04:10.412704: Current learning rate: 0.00421 
2025-01-28 03:04:58.766078: train_loss -0.836 
2025-01-28 03:04:58.771654: val_loss -0.7528 
2025-01-28 03:04:58.774064: Pseudo dice [np.float32(0.9595), np.float32(0.9507)] 
2025-01-28 03:04:58.776504: Epoch time: 48.36 s 
2025-01-28 03:04:59.924839:  
2025-01-28 03:04:59.927547: Epoch 619 
2025-01-28 03:04:59.930248: Current learning rate: 0.0042 
2025-01-28 03:05:48.017736: train_loss -0.8463 
2025-01-28 03:05:48.022437: val_loss -0.7452 
2025-01-28 03:05:48.024938: Pseudo dice [np.float32(0.9528), np.float32(0.9351)] 
2025-01-28 03:05:48.027577: Epoch time: 48.09 s 
2025-01-28 03:05:49.170510:  
2025-01-28 03:05:49.173218: Epoch 620 
2025-01-28 03:05:49.175714: Current learning rate: 0.00419 
2025-01-28 03:06:36.751767: train_loss -0.8114 
2025-01-28 03:06:36.757587: val_loss -0.7374 
2025-01-28 03:06:36.760458: Pseudo dice [np.float32(0.9505), np.float32(0.9295)] 
2025-01-28 03:06:36.763035: Epoch time: 47.58 s 
2025-01-28 03:06:37.944022:  
2025-01-28 03:06:37.946818: Epoch 621 
2025-01-28 03:06:37.949533: Current learning rate: 0.00418 
2025-01-28 03:07:26.193939: train_loss -0.8227 
2025-01-28 03:07:26.198624: val_loss -0.7347 
2025-01-28 03:07:26.201632: Pseudo dice [np.float32(0.9452), np.float32(0.8972)] 
2025-01-28 03:07:26.204589: Epoch time: 48.25 s 
2025-01-28 03:07:27.381879:  
2025-01-28 03:07:27.385315: Epoch 622 
2025-01-28 03:07:27.388274: Current learning rate: 0.00417 
2025-01-28 03:08:15.435803: train_loss -0.8137 
2025-01-28 03:08:15.441185: val_loss -0.7516 
2025-01-28 03:08:15.443879: Pseudo dice [np.float32(0.9364), np.float32(0.9224)] 
2025-01-28 03:08:15.446676: Epoch time: 48.05 s 
2025-01-28 03:08:16.947968:  
2025-01-28 03:08:16.951026: Epoch 623 
2025-01-28 03:08:16.953670: Current learning rate: 0.00416 
2025-01-28 03:09:04.677886: train_loss -0.8178 
2025-01-28 03:09:04.682669: val_loss -0.7987 
2025-01-28 03:09:04.685849: Pseudo dice [np.float32(0.9502), np.float32(0.9284)] 
2025-01-28 03:09:04.688744: Epoch time: 47.73 s 
2025-01-28 03:09:05.830562:  
2025-01-28 03:09:05.833709: Epoch 624 
2025-01-28 03:09:05.836420: Current learning rate: 0.00415 
2025-01-28 03:09:53.960124: train_loss -0.8206 
2025-01-28 03:09:53.965620: val_loss -0.76 
2025-01-28 03:09:53.967968: Pseudo dice [np.float32(0.9581), np.float32(0.9352)] 
2025-01-28 03:09:53.970707: Epoch time: 48.13 s 
2025-01-28 03:09:55.151314:  
2025-01-28 03:09:55.153965: Epoch 625 
2025-01-28 03:09:55.156317: Current learning rate: 0.00414 
2025-01-28 03:10:43.187957: train_loss -0.8102 
2025-01-28 03:10:43.191887: val_loss -0.6958 
2025-01-28 03:10:43.194528: Pseudo dice [np.float32(0.9541), np.float32(0.9305)] 
2025-01-28 03:10:43.196968: Epoch time: 48.04 s 
2025-01-28 03:10:44.336233:  
2025-01-28 03:10:44.339165: Epoch 626 
2025-01-28 03:10:44.341987: Current learning rate: 0.00413 
2025-01-28 03:11:32.342553: train_loss -0.8148 
2025-01-28 03:11:32.347795: val_loss -0.7546 
2025-01-28 03:11:32.350061: Pseudo dice [np.float32(0.9539), np.float32(0.9331)] 
2025-01-28 03:11:32.352678: Epoch time: 48.01 s 
2025-01-28 03:11:33.495346:  
2025-01-28 03:11:33.497847: Epoch 627 
2025-01-28 03:11:33.500051: Current learning rate: 0.00412 
2025-01-28 03:12:21.515118: train_loss -0.8341 
2025-01-28 03:12:21.519265: val_loss -0.7276 
2025-01-28 03:12:21.521765: Pseudo dice [np.float32(0.9557), np.float32(0.8998)] 
2025-01-28 03:12:21.524455: Epoch time: 48.02 s 
2025-01-28 03:12:22.665057:  
2025-01-28 03:12:22.667976: Epoch 628 
2025-01-28 03:12:22.670555: Current learning rate: 0.00411 
2025-01-28 03:13:10.654596: train_loss -0.8172 
2025-01-28 03:13:10.659576: val_loss -0.7439 
2025-01-28 03:13:10.662032: Pseudo dice [np.float32(0.9482), np.float32(0.945)] 
2025-01-28 03:13:10.664719: Epoch time: 47.99 s 
2025-01-28 03:13:10.667410: Yayy! New best EMA pseudo Dice: 0.9361000061035156 
2025-01-28 03:13:12.394578:  
2025-01-28 03:13:12.397488: Epoch 629 
2025-01-28 03:13:12.400766: Current learning rate: 0.0041 
2025-01-28 03:14:00.299402: train_loss -0.8097 
2025-01-28 03:14:00.303555: val_loss -0.699 
2025-01-28 03:14:00.306244: Pseudo dice [np.float32(0.9467), np.float32(0.8253)] 
2025-01-28 03:14:00.308861: Epoch time: 47.91 s 
2025-01-28 03:14:01.451963:  
2025-01-28 03:14:01.454855: Epoch 630 
2025-01-28 03:14:01.457806: Current learning rate: 0.00409 
2025-01-28 03:14:49.268659: train_loss -0.8062 
2025-01-28 03:14:49.274939: val_loss -0.7205 
2025-01-28 03:14:49.277652: Pseudo dice [np.float32(0.951), np.float32(0.8969)] 
2025-01-28 03:14:49.280263: Epoch time: 47.82 s 
2025-01-28 03:14:50.462730:  
2025-01-28 03:14:50.465884: Epoch 631 
2025-01-28 03:14:50.468505: Current learning rate: 0.00408 
2025-01-28 03:15:38.312694: train_loss -0.8067 
2025-01-28 03:15:38.317325: val_loss -0.6764 
2025-01-28 03:15:38.319974: Pseudo dice [np.float32(0.9451), np.float32(0.9062)] 
2025-01-28 03:15:38.322637: Epoch time: 47.85 s 
2025-01-28 03:15:39.500001:  
2025-01-28 03:15:39.502645: Epoch 632 
2025-01-28 03:15:39.505110: Current learning rate: 0.00407 
2025-01-28 03:16:27.325828: train_loss -0.8193 
2025-01-28 03:16:27.332063: val_loss -0.7248 
2025-01-28 03:16:27.334902: Pseudo dice [np.float32(0.9316), np.float32(0.9031)] 
2025-01-28 03:16:27.337513: Epoch time: 47.83 s 
2025-01-28 03:16:28.519897:  
2025-01-28 03:16:28.523308: Epoch 633 
2025-01-28 03:16:28.526061: Current learning rate: 0.00406 
2025-01-28 03:17:16.661212: train_loss -0.8313 
2025-01-28 03:17:16.665406: val_loss -0.7747 
2025-01-28 03:17:16.668031: Pseudo dice [np.float32(0.9509), np.float32(0.9496)] 
2025-01-28 03:17:16.670893: Epoch time: 48.14 s 
2025-01-28 03:17:17.813514:  
2025-01-28 03:17:17.816683: Epoch 634 
2025-01-28 03:17:17.819524: Current learning rate: 0.00405 
2025-01-28 03:18:05.845248: train_loss -0.8175 
2025-01-28 03:18:05.850218: val_loss -0.7594 
2025-01-28 03:18:05.852525: Pseudo dice [np.float32(0.9459), np.float32(0.9363)] 
2025-01-28 03:18:05.855014: Epoch time: 48.03 s 
2025-01-28 03:18:07.331573:  
2025-01-28 03:18:07.334097: Epoch 635 
2025-01-28 03:18:07.336774: Current learning rate: 0.00404 
2025-01-28 03:18:55.283467: train_loss -0.8216 
2025-01-28 03:18:55.288128: val_loss -0.7436 
2025-01-28 03:18:55.290645: Pseudo dice [np.float32(0.9266), np.float32(0.9212)] 
2025-01-28 03:18:55.293314: Epoch time: 47.95 s 
2025-01-28 03:18:56.435285:  
2025-01-28 03:18:56.438818: Epoch 636 
2025-01-28 03:18:56.442063: Current learning rate: 0.00403 
2025-01-28 03:19:44.361169: train_loss -0.8115 
2025-01-28 03:19:44.367126: val_loss -0.7166 
2025-01-28 03:19:44.369991: Pseudo dice [np.float32(0.9463), np.float32(0.8959)] 
2025-01-28 03:19:44.372621: Epoch time: 47.93 s 
2025-01-28 03:19:45.516216:  
2025-01-28 03:19:45.519387: Epoch 637 
2025-01-28 03:19:45.522224: Current learning rate: 0.00402 
2025-01-28 03:20:33.364561: train_loss -0.8099 
2025-01-28 03:20:33.368931: val_loss -0.712 
2025-01-28 03:20:33.371608: Pseudo dice [np.float32(0.9352), np.float32(0.8043)] 
2025-01-28 03:20:33.374112: Epoch time: 47.85 s 
2025-01-28 03:20:34.557583:  
2025-01-28 03:20:34.560993: Epoch 638 
2025-01-28 03:20:34.563777: Current learning rate: 0.00401 
2025-01-28 03:21:22.431724: train_loss -0.8134 
2025-01-28 03:21:22.437317: val_loss -0.7181 
2025-01-28 03:21:22.439671: Pseudo dice [np.float32(0.9515), np.float32(0.885)] 
2025-01-28 03:21:22.441947: Epoch time: 47.88 s 
2025-01-28 03:21:23.623112:  
2025-01-28 03:21:23.625906: Epoch 639 
2025-01-28 03:21:23.628382: Current learning rate: 0.004 
2025-01-28 03:22:11.559902: train_loss -0.8196 
2025-01-28 03:22:11.564059: val_loss -0.7535 
2025-01-28 03:22:11.566388: Pseudo dice [np.float32(0.9523), np.float32(0.9118)] 
2025-01-28 03:22:11.568950: Epoch time: 47.94 s 
2025-01-28 03:22:12.709142:  
2025-01-28 03:22:12.711792: Epoch 640 
2025-01-28 03:22:12.714457: Current learning rate: 0.00399 
2025-01-28 03:23:00.278510: train_loss -0.8256 
2025-01-28 03:23:00.284541: val_loss -0.7055 
2025-01-28 03:23:00.287095: Pseudo dice [np.float32(0.9497), np.float32(0.9315)] 
2025-01-28 03:23:00.289653: Epoch time: 47.57 s 
2025-01-28 03:23:01.434571:  
2025-01-28 03:23:01.437983: Epoch 641 
2025-01-28 03:23:01.440994: Current learning rate: 0.00398 
2025-01-28 03:23:49.186689: train_loss -0.8309 
2025-01-28 03:23:49.191029: val_loss -0.736 
2025-01-28 03:23:49.193887: Pseudo dice [np.float32(0.9367), np.float32(0.9198)] 
2025-01-28 03:23:49.196678: Epoch time: 47.75 s 
2025-01-28 03:23:50.341471:  
2025-01-28 03:23:50.344606: Epoch 642 
2025-01-28 03:23:50.347659: Current learning rate: 0.00397 
2025-01-28 03:24:38.012653: train_loss -0.8162 
2025-01-28 03:24:38.018688: val_loss -0.7133 
2025-01-28 03:24:38.021778: Pseudo dice [np.float32(0.94), np.float32(0.9192)] 
2025-01-28 03:24:38.024801: Epoch time: 47.67 s 
2025-01-28 03:24:39.200991:  
2025-01-28 03:24:39.203757: Epoch 643 
2025-01-28 03:24:39.206297: Current learning rate: 0.00396 
2025-01-28 03:25:26.943802: train_loss -0.835 
2025-01-28 03:25:26.947435: val_loss -0.758 
2025-01-28 03:25:26.949945: Pseudo dice [np.float32(0.9497), np.float32(0.8699)] 
2025-01-28 03:25:26.951967: Epoch time: 47.74 s 
2025-01-28 03:25:28.087088:  
2025-01-28 03:25:28.089444: Epoch 644 
2025-01-28 03:25:28.091725: Current learning rate: 0.00395 
2025-01-28 03:26:15.710489: train_loss -0.819 
2025-01-28 03:26:15.716851: val_loss -0.7212 
2025-01-28 03:26:15.719736: Pseudo dice [np.float32(0.9414), np.float32(0.938)] 
2025-01-28 03:26:15.722422: Epoch time: 47.62 s 
2025-01-28 03:26:16.860891:  
2025-01-28 03:26:16.863691: Epoch 645 
2025-01-28 03:26:16.866202: Current learning rate: 0.00394 
2025-01-28 03:27:04.866277: train_loss -0.8164 
2025-01-28 03:27:04.870775: val_loss -0.7338 
2025-01-28 03:27:04.873690: Pseudo dice [np.float32(0.9384), np.float32(0.8938)] 
2025-01-28 03:27:04.876367: Epoch time: 48.01 s 
2025-01-28 03:27:06.014716:  
2025-01-28 03:27:06.017389: Epoch 646 
2025-01-28 03:27:06.019933: Current learning rate: 0.00393 
2025-01-28 03:27:53.750269: train_loss -0.8324 
2025-01-28 03:27:53.756168: val_loss -0.7485 
2025-01-28 03:27:53.758812: Pseudo dice [np.float32(0.9499), np.float32(0.9232)] 
2025-01-28 03:27:53.761521: Epoch time: 47.74 s 
2025-01-28 03:27:54.900109:  
2025-01-28 03:27:54.902991: Epoch 647 
2025-01-28 03:27:54.905867: Current learning rate: 0.00392 
2025-01-28 03:28:42.929799: train_loss -0.8357 
2025-01-28 03:28:42.936546: val_loss -0.7418 
2025-01-28 03:28:42.939723: Pseudo dice [np.float32(0.9493), np.float32(0.8937)] 
2025-01-28 03:28:42.942586: Epoch time: 48.03 s 
2025-01-28 03:28:44.499181:  
2025-01-28 03:28:44.502446: Epoch 648 
2025-01-28 03:28:44.505493: Current learning rate: 0.00391 
2025-01-28 03:29:33.090755: train_loss -0.8218 
2025-01-28 03:29:33.096167: val_loss -0.7275 
2025-01-28 03:29:33.098453: Pseudo dice [np.float32(0.9545), np.float32(0.9263)] 
2025-01-28 03:29:33.100830: Epoch time: 48.59 s 
2025-01-28 03:29:34.196367:  
2025-01-28 03:29:34.199763: Epoch 649 
2025-01-28 03:29:34.202178: Current learning rate: 0.0039 
2025-01-28 03:30:22.658384: train_loss -0.8354 
2025-01-28 03:30:22.662727: val_loss -0.7995 
2025-01-28 03:30:22.665627: Pseudo dice [np.float32(0.9565), np.float32(0.9309)] 
2025-01-28 03:30:22.668253: Epoch time: 48.46 s 
2025-01-28 03:30:24.464620:  
2025-01-28 03:30:24.467507: Epoch 650 
2025-01-28 03:30:24.470060: Current learning rate: 0.00389 
2025-01-28 03:31:12.729166: train_loss -0.8243 
2025-01-28 03:31:12.736252: val_loss -0.7469 
2025-01-28 03:31:12.739229: Pseudo dice [np.float32(0.9549), np.float32(0.9231)] 
2025-01-28 03:31:12.742141: Epoch time: 48.27 s 
2025-01-28 03:31:13.901120:  
2025-01-28 03:31:13.904125: Epoch 651 
2025-01-28 03:31:13.907341: Current learning rate: 0.00388 
2025-01-28 03:32:01.550383: train_loss -0.8237 
2025-01-28 03:32:01.554616: val_loss -0.7831 
2025-01-28 03:32:01.557038: Pseudo dice [np.float32(0.9438), np.float32(0.9368)] 
2025-01-28 03:32:01.559607: Epoch time: 47.65 s 
2025-01-28 03:32:02.713632:  
2025-01-28 03:32:02.715969: Epoch 652 
2025-01-28 03:32:02.718298: Current learning rate: 0.00387 
2025-01-28 03:32:50.432599: train_loss -0.8232 
2025-01-28 03:32:50.437943: val_loss -0.6767 
2025-01-28 03:32:50.440459: Pseudo dice [np.float32(0.9546), np.float32(0.7872)] 
2025-01-28 03:32:50.442930: Epoch time: 47.72 s 
2025-01-28 03:32:51.627066:  
2025-01-28 03:32:51.630918: Epoch 653 
2025-01-28 03:32:51.633887: Current learning rate: 0.00386 
2025-01-28 03:33:39.314786: train_loss -0.8342 
2025-01-28 03:33:39.319725: val_loss -0.7004 
2025-01-28 03:33:39.322954: Pseudo dice [np.float32(0.9498), np.float32(0.9427)] 
2025-01-28 03:33:39.325827: Epoch time: 47.69 s 
2025-01-28 03:33:40.467731:  
2025-01-28 03:33:40.470754: Epoch 654 
2025-01-28 03:33:40.473743: Current learning rate: 0.00385 
2025-01-28 03:34:28.151981: train_loss -0.8257 
2025-01-28 03:34:28.158400: val_loss -0.725 
2025-01-28 03:34:28.161556: Pseudo dice [np.float32(0.9576), np.float32(0.9258)] 
2025-01-28 03:34:28.164465: Epoch time: 47.69 s 
2025-01-28 03:34:29.307674:  
2025-01-28 03:34:29.310357: Epoch 655 
2025-01-28 03:34:29.312842: Current learning rate: 0.00384 
2025-01-28 03:35:17.263973: train_loss -0.8283 
2025-01-28 03:35:17.268738: val_loss -0.7562 
2025-01-28 03:35:17.271569: Pseudo dice [np.float32(0.9535), np.float32(0.935)] 
2025-01-28 03:35:17.274189: Epoch time: 47.96 s 
2025-01-28 03:35:18.414382:  
2025-01-28 03:35:18.418044: Epoch 656 
2025-01-28 03:35:18.421027: Current learning rate: 0.00383 
2025-01-28 03:36:06.880474: train_loss -0.8143 
2025-01-28 03:36:06.885918: val_loss -0.7438 
2025-01-28 03:36:06.888488: Pseudo dice [np.float32(0.9583), np.float32(0.9296)] 
2025-01-28 03:36:06.890993: Epoch time: 48.47 s 
2025-01-28 03:36:08.034280:  
2025-01-28 03:36:08.037393: Epoch 657 
2025-01-28 03:36:08.040379: Current learning rate: 0.00382 
2025-01-28 03:36:55.673404: train_loss -0.8341 
2025-01-28 03:36:55.677744: val_loss -0.7335 
2025-01-28 03:36:55.680952: Pseudo dice [np.float32(0.9514), np.float32(0.9066)] 
2025-01-28 03:36:55.683792: Epoch time: 47.64 s 
2025-01-28 03:36:56.825129:  
2025-01-28 03:36:56.828334: Epoch 658 
2025-01-28 03:36:56.831552: Current learning rate: 0.00381 
2025-01-28 03:37:44.329642: train_loss -0.8341 
2025-01-28 03:37:44.335924: val_loss -0.7787 
2025-01-28 03:37:44.338578: Pseudo dice [np.float32(0.9519), np.float32(0.9439)] 
2025-01-28 03:37:44.341378: Epoch time: 47.51 s 
2025-01-28 03:37:45.481144:  
2025-01-28 03:37:45.484347: Epoch 659 
2025-01-28 03:37:45.487161: Current learning rate: 0.0038 
2025-01-28 03:38:33.546875: train_loss -0.8426 
2025-01-28 03:38:33.551518: val_loss -0.7748 
2025-01-28 03:38:33.554542: Pseudo dice [np.float32(0.9528), np.float32(0.9354)] 
2025-01-28 03:38:33.556939: Epoch time: 48.07 s 
2025-01-28 03:38:34.672031:  
2025-01-28 03:38:34.676019: Epoch 660 
2025-01-28 03:38:34.678792: Current learning rate: 0.00379 
2025-01-28 03:39:23.276938: train_loss -0.8262 
2025-01-28 03:39:23.281898: val_loss -0.7443 
2025-01-28 03:39:23.284190: Pseudo dice [np.float32(0.9592), np.float32(0.9267)] 
2025-01-28 03:39:23.286294: Epoch time: 48.61 s 
2025-01-28 03:39:24.388078:  
2025-01-28 03:39:24.390703: Epoch 661 
2025-01-28 03:39:24.393144: Current learning rate: 0.00378 
2025-01-28 03:40:12.831260: train_loss -0.8259 
2025-01-28 03:40:12.835538: val_loss -0.7773 
2025-01-28 03:40:12.838502: Pseudo dice [np.float32(0.9546), np.float32(0.9279)] 
2025-01-28 03:40:12.841183: Epoch time: 48.44 s 
2025-01-28 03:40:13.930396:  
2025-01-28 03:40:13.933938: Epoch 662 
2025-01-28 03:40:13.937116: Current learning rate: 0.00377 
2025-01-28 03:41:02.367919: train_loss -0.8502 
2025-01-28 03:41:02.373936: val_loss -0.7067 
2025-01-28 03:41:02.377116: Pseudo dice [np.float32(0.9446), np.float32(0.8898)] 
2025-01-28 03:41:02.379690: Epoch time: 48.44 s 
2025-01-28 03:41:03.471802:  
2025-01-28 03:41:03.474633: Epoch 663 
2025-01-28 03:41:03.477510: Current learning rate: 0.00376 
