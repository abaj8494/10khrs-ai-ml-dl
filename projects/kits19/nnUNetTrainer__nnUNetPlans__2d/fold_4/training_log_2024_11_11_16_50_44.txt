
#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################
 
2024-11-11 16:50:44.167920: do_dummy_2d_data_aug: False 
2024-11-11 16:50:44.182177: Using splits from existing split file: /srv/scratch/z5362216/kits19/nnUNet_db/nnUNet_preprocessed/Dataset001_Kits19/splits_final.json 
2024-11-11 16:50:44.184892: The split file contains 5 splits. 
2024-11-11 16:50:44.187310: Desired fold for training: 4 
2024-11-11 16:50:44.189741: This split has 80 training and 20 validation cases. 
2024-11-11 16:50:49.618193: Using torch.compile... 

This is the configuration used by this training:
Configuration name: 2d
 {'data_identifier': 'nnUNetPlans_2d', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 12, 'patch_size': [512, 512], 'median_image_size_in_voxels': [512.0, 512.0], 'spacing': [0.7939453125, 0.7939453125], 'normalization_schemes': ['CTNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 8, 'features_per_stage': [32, 64, 128, 256, 512, 512, 512, 512], 'conv_op': 'torch.nn.modules.conv.Conv2d', 'kernel_sizes': [[3, 3], [3, 3], [3, 3], [3, 3], [3, 3], [3, 3], [3, 3], [3, 3]], 'strides': [[1, 1], [2, 2], [2, 2], [2, 2], [2, 2], [2, 2], [2, 2], [2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm2d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True} 
 
These are the global plan.json settings:
 {'dataset_name': 'Dataset001_Kits19', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [3.0, 0.7939453125, 0.7939453125], 'original_median_shape_after_transp': [104, 512, 512], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [2, 0, 1], 'transpose_backward': [1, 2, 0], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 2553.0, 'mean': 104.46720886230469, 'median': 104.0, 'min': -277.0, 'percentile_00_5': -73.0, 'percentile_99_5': 292.0, 'std': 74.68063354492188}}} 
 
2024-11-11 16:50:53.315620: unpacking dataset... 
2024-11-11 16:50:59.157275: unpacking done... 
2024-11-11 16:50:59.220968: Unable to plot network architecture: nnUNet_compile is enabled! 
2024-11-11 16:50:59.235507:  
2024-11-11 16:50:59.237957: Epoch 0 
2024-11-11 16:50:59.240598: Current learning rate: 0.01 
2024-11-11 16:52:27.115230: train_loss 0.0488 
2024-11-11 16:52:27.120769: val_loss -0.0974 
2024-11-11 16:52:27.123095: Pseudo dice [np.float32(0.0), np.float32(0.0)] 
2024-11-11 16:52:27.126613: Epoch time: 87.88 s 
2024-11-11 16:52:27.129037: Yayy! New best EMA pseudo Dice: 0.0 
2024-11-11 16:52:29.990627:  
2024-11-11 16:52:29.993038: Epoch 1 
2024-11-11 16:52:29.995600: Current learning rate: 0.00999 
2024-11-11 16:53:09.427341: train_loss -0.3058 
2024-11-11 16:53:09.430533: val_loss -0.4461 
2024-11-11 16:53:09.432908: Pseudo dice [np.float32(0.8312), np.float32(0.0)] 
2024-11-11 16:53:09.435304: Epoch time: 39.44 s 
2024-11-11 16:53:09.437833: Yayy! New best EMA pseudo Dice: 0.041600000113248825 
2024-11-11 16:53:11.337034:  
2024-11-11 16:53:11.339272: Epoch 2 
2024-11-11 16:53:11.341768: Current learning rate: 0.00998 
2024-11-11 16:53:50.838853: train_loss -0.5076 
2024-11-11 16:53:50.844892: val_loss -0.4213 
2024-11-11 16:53:50.847327: Pseudo dice [np.float32(0.8342), np.float32(0.1265)] 
2024-11-11 16:53:50.849603: Epoch time: 39.5 s 
2024-11-11 16:53:50.851908: Yayy! New best EMA pseudo Dice: 0.08540000021457672 
2024-11-11 16:53:53.147165:  
2024-11-11 16:53:53.150080: Epoch 3 
2024-11-11 16:53:53.152641: Current learning rate: 0.00997 
2024-11-11 16:54:32.695547: train_loss -0.5644 
2024-11-11 16:54:32.699364: val_loss -0.6389 
2024-11-11 16:54:32.701865: Pseudo dice [np.float32(0.8978), np.float32(0.5465)] 
2024-11-11 16:54:32.704265: Epoch time: 39.55 s 
2024-11-11 16:54:32.706632: Yayy! New best EMA pseudo Dice: 0.14910000562667847 
2024-11-11 16:54:34.609619:  
2024-11-11 16:54:34.612034: Epoch 4 
2024-11-11 16:54:34.614331: Current learning rate: 0.00996 
2024-11-11 16:55:14.154629: train_loss -0.6433 
2024-11-11 16:55:14.160008: val_loss -0.6616 
2024-11-11 16:55:14.162241: Pseudo dice [np.float32(0.9161), np.float32(0.579)] 
2024-11-11 16:55:14.164650: Epoch time: 39.55 s 
2024-11-11 16:55:14.166857: Yayy! New best EMA pseudo Dice: 0.20890000462532043 
2024-11-11 16:55:16.184407:  
2024-11-11 16:55:16.186973: Epoch 5 
2024-11-11 16:55:16.189706: Current learning rate: 0.00995 
2024-11-11 16:55:55.730775: train_loss -0.6594 
2024-11-11 16:55:55.735087: val_loss -0.6834 
2024-11-11 16:55:55.737510: Pseudo dice [np.float32(0.9069), np.float32(0.6263)] 
2024-11-11 16:55:55.740007: Epoch time: 39.55 s 
2024-11-11 16:55:55.742497: Yayy! New best EMA pseudo Dice: 0.2646999955177307 
2024-11-11 16:55:57.673068:  
2024-11-11 16:55:57.675508: Epoch 6 
2024-11-11 16:55:57.677628: Current learning rate: 0.00995 
2024-11-11 16:56:37.236256: train_loss -0.6735 
2024-11-11 16:56:37.241082: val_loss -0.6698 
2024-11-11 16:56:37.243404: Pseudo dice [np.float32(0.9085), np.float32(0.6146)] 
2024-11-11 16:56:37.245709: Epoch time: 39.56 s 
2024-11-11 16:56:37.247741: Yayy! New best EMA pseudo Dice: 0.31439998745918274 
2024-11-11 16:56:39.157609:  
2024-11-11 16:56:39.160511: Epoch 7 
2024-11-11 16:56:39.163248: Current learning rate: 0.00994 
2024-11-11 16:57:18.720788: train_loss -0.6324 
2024-11-11 16:57:18.724375: val_loss -0.6364 
2024-11-11 16:57:18.727128: Pseudo dice [np.float32(0.8907), np.float32(0.562)] 
2024-11-11 16:57:18.729708: Epoch time: 39.56 s 
2024-11-11 16:57:18.732199: Yayy! New best EMA pseudo Dice: 0.3555999994277954 
2024-11-11 16:57:20.704553:  
2024-11-11 16:57:20.706852: Epoch 8 
2024-11-11 16:57:20.709337: Current learning rate: 0.00993 
2024-11-11 16:58:00.268923: train_loss -0.6731 
2024-11-11 16:58:00.273981: val_loss -0.6725 
2024-11-11 16:58:00.276510: Pseudo dice [np.float32(0.9089), np.float32(0.5842)] 
2024-11-11 16:58:00.278611: Epoch time: 39.57 s 
2024-11-11 16:58:00.281000: Yayy! New best EMA pseudo Dice: 0.39469999074935913 
2024-11-11 16:58:02.268363:  
2024-11-11 16:58:02.270792: Epoch 9 
2024-11-11 16:58:02.273356: Current learning rate: 0.00992 
2024-11-11 16:58:41.822079: train_loss -0.7038 
2024-11-11 16:58:41.825797: val_loss -0.7094 
2024-11-11 16:58:41.828643: Pseudo dice [np.float32(0.917), np.float32(0.6481)] 
2024-11-11 16:58:41.831154: Epoch time: 39.55 s 
2024-11-11 16:58:41.833604: Yayy! New best EMA pseudo Dice: 0.4334999918937683 
2024-11-11 16:58:43.696882:  
2024-11-11 16:58:43.699529: Epoch 10 
2024-11-11 16:58:43.701896: Current learning rate: 0.00991 
2024-11-11 16:59:23.285098: train_loss -0.7371 
2024-11-11 16:59:23.292893: val_loss -0.705 
2024-11-11 16:59:23.295123: Pseudo dice [np.float32(0.9274), np.float32(0.679)] 
2024-11-11 16:59:23.297695: Epoch time: 39.59 s 
2024-11-11 16:59:23.299846: Yayy! New best EMA pseudo Dice: 0.47040000557899475 
2024-11-11 16:59:25.165572:  
2024-11-11 16:59:25.168387: Epoch 11 
2024-11-11 16:59:25.171125: Current learning rate: 0.0099 
2024-11-11 17:00:04.750683: train_loss -0.7361 
2024-11-11 17:00:04.754369: val_loss -0.6878 
2024-11-11 17:00:04.756811: Pseudo dice [np.float32(0.9152), np.float32(0.6372)] 
2024-11-11 17:00:04.758998: Epoch time: 39.59 s 
2024-11-11 17:00:04.761582: Yayy! New best EMA pseudo Dice: 0.5009999871253967 
2024-11-11 17:00:06.668988:  
2024-11-11 17:00:06.671396: Epoch 12 
2024-11-11 17:00:06.673894: Current learning rate: 0.00989 
2024-11-11 17:00:46.258279: train_loss -0.7464 
2024-11-11 17:00:46.263108: val_loss -0.7291 
2024-11-11 17:00:46.265470: Pseudo dice [np.float32(0.9276), np.float32(0.6622)] 
2024-11-11 17:00:46.267906: Epoch time: 39.59 s 
2024-11-11 17:00:46.270133: Yayy! New best EMA pseudo Dice: 0.5303999781608582 
2024-11-11 17:00:48.183172:  
2024-11-11 17:00:48.185701: Epoch 13 
2024-11-11 17:00:48.188047: Current learning rate: 0.00988 
2024-11-11 17:01:27.760867: train_loss -0.7456 
2024-11-11 17:01:27.764293: val_loss -0.7401 
2024-11-11 17:01:27.766483: Pseudo dice [np.float32(0.9294), np.float32(0.6819)] 
2024-11-11 17:01:27.768795: Epoch time: 39.58 s 
2024-11-11 17:01:27.770964: Yayy! New best EMA pseudo Dice: 0.5579000115394592 
2024-11-11 17:01:30.027530:  
2024-11-11 17:01:30.030114: Epoch 14 
2024-11-11 17:01:30.032552: Current learning rate: 0.00987 
2024-11-11 17:02:09.540494: train_loss -0.7568 
2024-11-11 17:02:09.545165: val_loss -0.6997 
2024-11-11 17:02:09.547441: Pseudo dice [np.float32(0.9215), np.float32(0.6947)] 
2024-11-11 17:02:09.549668: Epoch time: 39.51 s 
2024-11-11 17:02:09.551841: Yayy! New best EMA pseudo Dice: 0.5830000042915344 
2024-11-11 17:02:11.492115:  
2024-11-11 17:02:11.494491: Epoch 15 
2024-11-11 17:02:11.496954: Current learning rate: 0.00986 
2024-11-11 17:02:51.016331: train_loss -0.7571 
2024-11-11 17:02:51.020703: val_loss -0.7029 
2024-11-11 17:02:51.023475: Pseudo dice [np.float32(0.933), np.float32(0.6605)] 
2024-11-11 17:02:51.026004: Epoch time: 39.53 s 
2024-11-11 17:02:51.028593: Yayy! New best EMA pseudo Dice: 0.6043000221252441 
2024-11-11 17:02:52.978397:  
2024-11-11 17:02:52.981056: Epoch 16 
2024-11-11 17:02:52.983569: Current learning rate: 0.00986 
2024-11-11 17:03:32.543474: train_loss -0.7539 
2024-11-11 17:03:32.548753: val_loss -0.6921 
2024-11-11 17:03:32.551210: Pseudo dice [np.float32(0.924), np.float32(0.6637)] 
2024-11-11 17:03:32.553885: Epoch time: 39.57 s 
2024-11-11 17:03:32.556453: Yayy! New best EMA pseudo Dice: 0.6233000159263611 
2024-11-11 17:03:34.536988:  
2024-11-11 17:03:34.539611: Epoch 17 
2024-11-11 17:03:34.542270: Current learning rate: 0.00985 
2024-11-11 17:04:14.103276: train_loss -0.7666 
2024-11-11 17:04:14.107189: val_loss -0.7502 
2024-11-11 17:04:14.109483: Pseudo dice [np.float32(0.9469), np.float32(0.6984)] 
2024-11-11 17:04:14.111739: Epoch time: 39.57 s 
2024-11-11 17:04:14.114119: Yayy! New best EMA pseudo Dice: 0.6431999802589417 
2024-11-11 17:04:16.076485:  
2024-11-11 17:04:16.079340: Epoch 18 
2024-11-11 17:04:16.083808: Current learning rate: 0.00984 
2024-11-11 17:04:55.617791: train_loss -0.777 
2024-11-11 17:04:55.622561: val_loss -0.7235 
2024-11-11 17:04:55.625033: Pseudo dice [np.float32(0.9333), np.float32(0.6886)] 
2024-11-11 17:04:55.627226: Epoch time: 39.54 s 
2024-11-11 17:04:55.630209: Yayy! New best EMA pseudo Dice: 0.6600000262260437 
2024-11-11 17:04:57.580468:  
2024-11-11 17:04:57.583246: Epoch 19 
2024-11-11 17:04:57.585588: Current learning rate: 0.00983 
2024-11-11 17:05:37.122638: train_loss -0.775 
2024-11-11 17:05:37.126135: val_loss -0.7114 
2024-11-11 17:05:37.128514: Pseudo dice [np.float32(0.9267), np.float32(0.6392)] 
2024-11-11 17:05:37.131011: Epoch time: 39.54 s 
2024-11-11 17:05:37.133124: Yayy! New best EMA pseudo Dice: 0.6722999811172485 
2024-11-11 17:05:39.077348:  
2024-11-11 17:05:39.079786: Epoch 20 
2024-11-11 17:05:39.082381: Current learning rate: 0.00982 
