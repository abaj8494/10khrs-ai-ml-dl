
#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################
 
2024-11-11 15:57:02.078873: Using torch.compile... 
2024-11-11 15:57:12.312741: do_dummy_2d_data_aug: False 
2024-11-11 15:57:12.360716: Using splits from existing split file: /srv/scratch/z5362216/kits19/nnUNet_db/nnUNet_preprocessed/Dataset001_Kits19/splits_final.json 
2024-11-11 15:57:12.403738: The split file contains 5 splits. 
2024-11-11 15:57:12.406073: Desired fold for training: 3 
2024-11-11 15:57:12.408347: This split has 80 training and 20 validation cases. 

This is the configuration used by this training:
Configuration name: 2d
 {'data_identifier': 'nnUNetPlans_2d', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 12, 'patch_size': [512, 512], 'median_image_size_in_voxels': [512.0, 512.0], 'spacing': [0.7939453125, 0.7939453125], 'normalization_schemes': ['CTNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 8, 'features_per_stage': [32, 64, 128, 256, 512, 512, 512, 512], 'conv_op': 'torch.nn.modules.conv.Conv2d', 'kernel_sizes': [[3, 3], [3, 3], [3, 3], [3, 3], [3, 3], [3, 3], [3, 3], [3, 3]], 'strides': [[1, 1], [2, 2], [2, 2], [2, 2], [2, 2], [2, 2], [2, 2], [2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm2d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True} 
 
These are the global plan.json settings:
 {'dataset_name': 'Dataset001_Kits19', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [3.0, 0.7939453125, 0.7939453125], 'original_median_shape_after_transp': [104, 512, 512], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [2, 0, 1], 'transpose_backward': [1, 2, 0], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 2553.0, 'mean': 104.46720886230469, 'median': 104.0, 'min': -277.0, 'percentile_00_5': -73.0, 'percentile_99_5': 292.0, 'std': 74.68063354492188}}} 
 
2024-11-11 15:57:22.609007: unpacking dataset... 
2024-11-11 15:57:32.320121: unpacking done... 
2024-11-11 15:57:32.394817: Unable to plot network architecture: nnUNet_compile is enabled! 
2024-11-11 15:57:32.474728:  
2024-11-11 15:57:32.477063: Epoch 950 
2024-11-11 15:57:32.479442: Current learning rate: 0.00067 
2024-11-11 15:59:57.518828: train_loss -0.9516 
2024-11-11 15:59:57.530029: val_loss -0.8795 
2024-11-11 15:59:57.555139: Pseudo dice [np.float32(0.9419), np.float32(0.9124)] 
2024-11-11 15:59:57.561809: Epoch time: 145.05 s 
2024-11-11 15:59:59.894040:  
2024-11-11 15:59:59.896817: Epoch 951 
2024-11-11 15:59:59.899838: Current learning rate: 0.00066 
2024-11-11 16:00:39.268072: train_loss -0.9571 
2024-11-11 16:00:39.272105: val_loss -0.8702 
2024-11-11 16:00:39.274539: Pseudo dice [np.float32(0.9296), np.float32(0.9064)] 
2024-11-11 16:00:39.276754: Epoch time: 39.38 s 
2024-11-11 16:00:40.914904:  
2024-11-11 16:00:40.917429: Epoch 952 
2024-11-11 16:00:40.919986: Current learning rate: 0.00065 
2024-11-11 16:01:20.530446: train_loss -0.9538 
2024-11-11 16:01:20.560930: val_loss -0.8717 
2024-11-11 16:01:20.563483: Pseudo dice [np.float32(0.9355), np.float32(0.8953)] 
2024-11-11 16:01:20.567983: Epoch time: 39.62 s 
2024-11-11 16:01:21.742013:  
2024-11-11 16:01:21.744767: Epoch 953 
2024-11-11 16:01:21.747309: Current learning rate: 0.00064 
2024-11-11 16:02:01.469933: train_loss -0.9531 
2024-11-11 16:02:01.475961: val_loss -0.8772 
2024-11-11 16:02:01.492159: Pseudo dice [np.float32(0.9403), np.float32(0.9116)] 
2024-11-11 16:02:01.494678: Epoch time: 39.73 s 
2024-11-11 16:02:02.700888:  
2024-11-11 16:02:02.703504: Epoch 954 
2024-11-11 16:02:02.705962: Current learning rate: 0.00063 
2024-11-11 16:02:42.216026: train_loss -0.9574 
2024-11-11 16:02:42.221231: val_loss -0.8679 
2024-11-11 16:02:42.223517: Pseudo dice [np.float32(0.9396), np.float32(0.8986)] 
2024-11-11 16:02:42.225855: Epoch time: 39.52 s 
2024-11-11 16:02:43.418020:  
2024-11-11 16:02:43.420855: Epoch 955 
2024-11-11 16:02:43.423249: Current learning rate: 0.00061 
2024-11-11 16:03:22.919665: train_loss -0.9567 
2024-11-11 16:03:22.924129: val_loss -0.8936 
2024-11-11 16:03:22.927075: Pseudo dice [np.float32(0.9462), np.float32(0.9092)] 
2024-11-11 16:03:22.929985: Epoch time: 39.5 s 
2024-11-11 16:03:24.116627:  
2024-11-11 16:03:24.119331: Epoch 956 
2024-11-11 16:03:24.121686: Current learning rate: 0.0006 
2024-11-11 16:04:03.615373: train_loss -0.9541 
2024-11-11 16:04:03.620725: val_loss -0.8768 
2024-11-11 16:04:03.623243: Pseudo dice [np.float32(0.9479), np.float32(0.8998)] 
2024-11-11 16:04:03.625861: Epoch time: 39.5 s 
2024-11-11 16:04:04.809150:  
2024-11-11 16:04:04.811805: Epoch 957 
2024-11-11 16:04:04.814391: Current learning rate: 0.00059 
2024-11-11 16:04:44.326152: train_loss -0.9533 
2024-11-11 16:04:44.329407: val_loss -0.8814 
2024-11-11 16:04:44.332380: Pseudo dice [np.float32(0.9378), np.float32(0.9109)] 
2024-11-11 16:04:44.334734: Epoch time: 39.52 s 
2024-11-11 16:04:45.512953:  
2024-11-11 16:04:45.515856: Epoch 958 
2024-11-11 16:04:45.518355: Current learning rate: 0.00058 
2024-11-11 16:05:25.052498: train_loss -0.957 
2024-11-11 16:05:25.058047: val_loss -0.8772 
2024-11-11 16:05:25.060719: Pseudo dice [np.float32(0.9451), np.float32(0.8818)] 
2024-11-11 16:05:25.063343: Epoch time: 39.54 s 
2024-11-11 16:05:26.242863:  
2024-11-11 16:05:26.245838: Epoch 959 
2024-11-11 16:05:26.248869: Current learning rate: 0.00056 
2024-11-11 16:06:05.785977: train_loss -0.9615 
2024-11-11 16:06:05.789635: val_loss -0.8698 
2024-11-11 16:06:05.792359: Pseudo dice [np.float32(0.9383), np.float32(0.9039)] 
2024-11-11 16:06:05.794739: Epoch time: 39.54 s 
2024-11-11 16:06:06.975098:  
2024-11-11 16:06:06.977928: Epoch 960 
2024-11-11 16:06:06.980661: Current learning rate: 0.00055 
2024-11-11 16:06:46.496592: train_loss -0.9537 
2024-11-11 16:06:46.501577: val_loss -0.8898 
2024-11-11 16:06:46.504168: Pseudo dice [np.float32(0.9421), np.float32(0.8982)] 
2024-11-11 16:06:46.506565: Epoch time: 39.52 s 
2024-11-11 16:06:47.689041:  
2024-11-11 16:06:47.691657: Epoch 961 
2024-11-11 16:06:47.694456: Current learning rate: 0.00054 
2024-11-11 16:07:27.197894: train_loss -0.9573 
2024-11-11 16:07:27.202044: val_loss -0.8671 
2024-11-11 16:07:27.204667: Pseudo dice [np.float32(0.942), np.float32(0.8854)] 
2024-11-11 16:07:27.207285: Epoch time: 39.51 s 
2024-11-11 16:07:28.386234:  
2024-11-11 16:07:28.389208: Epoch 962 
2024-11-11 16:07:28.391720: Current learning rate: 0.00053 
2024-11-11 16:08:07.921784: train_loss -0.959 
2024-11-11 16:08:07.926743: val_loss -0.8834 
2024-11-11 16:08:07.929160: Pseudo dice [np.float32(0.9495), np.float32(0.9023)] 
2024-11-11 16:08:07.931361: Epoch time: 39.54 s 
2024-11-11 16:08:09.110266:  
2024-11-11 16:08:09.112878: Epoch 963 
2024-11-11 16:08:09.115392: Current learning rate: 0.00051 
2024-11-11 16:08:48.621676: train_loss -0.958 
2024-11-11 16:08:48.625291: val_loss -0.8757 
2024-11-11 16:08:48.628562: Pseudo dice [np.float32(0.9437), np.float32(0.9088)] 
2024-11-11 16:08:48.630938: Epoch time: 39.51 s 
2024-11-11 16:08:49.809628:  
2024-11-11 16:08:49.812052: Epoch 964 
2024-11-11 16:08:49.814394: Current learning rate: 0.0005 
2024-11-11 16:09:29.319302: train_loss -0.9578 
2024-11-11 16:09:29.324487: val_loss -0.9006 
2024-11-11 16:09:29.326814: Pseudo dice [np.float32(0.9507), np.float32(0.914)] 
2024-11-11 16:09:29.329079: Epoch time: 39.51 s 
2024-11-11 16:09:30.511148:  
2024-11-11 16:09:30.513424: Epoch 965 
2024-11-11 16:09:30.515641: Current learning rate: 0.00049 
2024-11-11 16:10:10.030338: train_loss -0.9542 
2024-11-11 16:10:10.033690: val_loss -0.8791 
2024-11-11 16:10:10.035966: Pseudo dice [np.float32(0.9483), np.float32(0.8928)] 
2024-11-11 16:10:10.038091: Epoch time: 39.52 s 
2024-11-11 16:10:11.217931:  
2024-11-11 16:10:11.220210: Epoch 966 
2024-11-11 16:10:11.222958: Current learning rate: 0.00048 
2024-11-11 16:10:50.717068: train_loss -0.9584 
2024-11-11 16:10:50.722060: val_loss -0.8912 
2024-11-11 16:10:50.725161: Pseudo dice [np.float32(0.946), np.float32(0.9147)] 
2024-11-11 16:10:50.727521: Epoch time: 39.5 s 
2024-11-11 16:10:51.906742:  
2024-11-11 16:10:51.909247: Epoch 967 
2024-11-11 16:10:51.911999: Current learning rate: 0.00046 
2024-11-11 16:11:31.424243: train_loss -0.9598 
2024-11-11 16:11:31.427764: val_loss -0.8874 
2024-11-11 16:11:31.430712: Pseudo dice [np.float32(0.9488), np.float32(0.8994)] 
2024-11-11 16:11:31.433162: Epoch time: 39.52 s 
2024-11-11 16:11:32.617194:  
2024-11-11 16:11:32.619466: Epoch 968 
2024-11-11 16:11:32.621686: Current learning rate: 0.00045 
2024-11-11 16:12:12.136450: train_loss -0.9583 
2024-11-11 16:12:12.141833: val_loss -0.8791 
2024-11-11 16:12:12.144049: Pseudo dice [np.float32(0.9535), np.float32(0.8862)] 
2024-11-11 16:12:12.146068: Epoch time: 39.52 s 
2024-11-11 16:12:13.329512:  
2024-11-11 16:12:13.332177: Epoch 969 
2024-11-11 16:12:13.334628: Current learning rate: 0.00044 
2024-11-11 16:12:52.849172: train_loss -0.9585 
2024-11-11 16:12:52.854559: val_loss -0.9048 
2024-11-11 16:12:52.856805: Pseudo dice [np.float32(0.9486), np.float32(0.9214)] 
2024-11-11 16:12:52.859423: Epoch time: 39.52 s 
2024-11-11 16:12:54.041137:  
2024-11-11 16:12:54.044361: Epoch 970 
2024-11-11 16:12:54.046883: Current learning rate: 0.00043 
2024-11-11 16:13:33.569242: train_loss -0.9533 
2024-11-11 16:13:33.574980: val_loss -0.8644 
2024-11-11 16:13:33.577762: Pseudo dice [np.float32(0.9416), np.float32(0.8857)] 
2024-11-11 16:13:33.580458: Epoch time: 39.53 s 
2024-11-11 16:13:34.763981:  
2024-11-11 16:13:34.766249: Epoch 971 
2024-11-11 16:13:34.768855: Current learning rate: 0.00041 
2024-11-11 16:14:14.298915: train_loss -0.9485 
2024-11-11 16:14:14.302572: val_loss -0.8711 
2024-11-11 16:14:14.305120: Pseudo dice [np.float32(0.948), np.float32(0.9097)] 
2024-11-11 16:14:14.307482: Epoch time: 39.54 s 
2024-11-11 16:14:16.120170:  
2024-11-11 16:14:16.122804: Epoch 972 
2024-11-11 16:14:16.125293: Current learning rate: 0.0004 
2024-11-11 16:14:55.693812: train_loss -0.9619 
2024-11-11 16:14:55.703113: val_loss -0.8701 
2024-11-11 16:14:55.705589: Pseudo dice [np.float32(0.9382), np.float32(0.892)] 
2024-11-11 16:14:55.707944: Epoch time: 39.57 s 
2024-11-11 16:14:56.894635:  
2024-11-11 16:14:56.897273: Epoch 973 
2024-11-11 16:14:56.899385: Current learning rate: 0.00039 
2024-11-11 16:15:36.463773: train_loss -0.9552 
2024-11-11 16:15:36.467521: val_loss -0.8676 
2024-11-11 16:15:36.469935: Pseudo dice [np.float32(0.9375), np.float32(0.8754)] 
2024-11-11 16:15:36.472210: Epoch time: 39.57 s 
2024-11-11 16:15:37.657384:  
2024-11-11 16:15:37.660052: Epoch 974 
2024-11-11 16:15:37.662399: Current learning rate: 0.00037 
2024-11-11 16:16:17.233129: train_loss -0.9613 
2024-11-11 16:16:17.238248: val_loss -0.8891 
2024-11-11 16:16:17.240760: Pseudo dice [np.float32(0.94), np.float32(0.9078)] 
2024-11-11 16:16:17.243044: Epoch time: 39.58 s 
2024-11-11 16:16:18.429115:  
2024-11-11 16:16:18.431442: Epoch 975 
2024-11-11 16:16:18.434818: Current learning rate: 0.00036 
2024-11-11 16:16:57.995249: train_loss -0.9602 
2024-11-11 16:16:57.998938: val_loss -0.875 
2024-11-11 16:16:58.001286: Pseudo dice [np.float32(0.9374), np.float32(0.9016)] 
2024-11-11 16:16:58.003397: Epoch time: 39.57 s 
2024-11-11 16:16:59.188403:  
2024-11-11 16:16:59.190927: Epoch 976 
2024-11-11 16:16:59.193433: Current learning rate: 0.00035 
2024-11-11 16:17:38.774063: train_loss -0.9597 
2024-11-11 16:17:38.779095: val_loss -0.896 
2024-11-11 16:17:38.781506: Pseudo dice [np.float32(0.9504), np.float32(0.9025)] 
2024-11-11 16:17:38.783845: Epoch time: 39.59 s 
2024-11-11 16:17:39.969410:  
2024-11-11 16:17:39.972157: Epoch 977 
2024-11-11 16:17:39.974806: Current learning rate: 0.00034 
2024-11-11 16:18:19.567643: train_loss -0.9583 
2024-11-11 16:18:19.571614: val_loss -0.883 
2024-11-11 16:18:19.574276: Pseudo dice [np.float32(0.9478), np.float32(0.9174)] 
2024-11-11 16:18:19.576601: Epoch time: 39.6 s 
2024-11-11 16:18:20.765893:  
2024-11-11 16:18:20.768497: Epoch 978 
2024-11-11 16:18:20.770953: Current learning rate: 0.00032 
2024-11-11 16:19:00.353398: train_loss -0.9615 
2024-11-11 16:19:00.358189: val_loss -0.8784 
2024-11-11 16:19:00.360796: Pseudo dice [np.float32(0.9469), np.float32(0.8942)] 
2024-11-11 16:19:00.363102: Epoch time: 39.59 s 
2024-11-11 16:19:01.547483:  
2024-11-11 16:19:01.550831: Epoch 979 
2024-11-11 16:19:01.553553: Current learning rate: 0.00031 
2024-11-11 16:19:41.153901: train_loss -0.959 
2024-11-11 16:19:41.157612: val_loss -0.8869 
2024-11-11 16:19:41.160356: Pseudo dice [np.float32(0.9429), np.float32(0.9004)] 
2024-11-11 16:19:41.162842: Epoch time: 39.61 s 
2024-11-11 16:19:42.346422:  
2024-11-11 16:19:42.348934: Epoch 980 
2024-11-11 16:19:42.351410: Current learning rate: 0.0003 
2024-11-11 16:20:21.964882: train_loss -0.9628 
2024-11-11 16:20:21.970067: val_loss -0.8468 
2024-11-11 16:20:21.972626: Pseudo dice [np.float32(0.9349), np.float32(0.8725)] 
2024-11-11 16:20:21.974846: Epoch time: 39.62 s 
2024-11-11 16:20:23.200051:  
2024-11-11 16:20:23.202465: Epoch 981 
2024-11-11 16:20:23.204923: Current learning rate: 0.00028 
2024-11-11 16:21:02.811908: train_loss -0.9577 
2024-11-11 16:21:02.815719: val_loss -0.8767 
2024-11-11 16:21:02.818320: Pseudo dice [np.float32(0.9377), np.float32(0.9029)] 
2024-11-11 16:21:02.820772: Epoch time: 39.61 s 
2024-11-11 16:21:04.011203:  
2024-11-11 16:21:04.013670: Epoch 982 
2024-11-11 16:21:04.017491: Current learning rate: 0.00027 
2024-11-11 16:21:43.605932: train_loss -0.9614 
2024-11-11 16:21:43.611029: val_loss -0.8883 
2024-11-11 16:21:43.614416: Pseudo dice [np.float32(0.9483), np.float32(0.8861)] 
2024-11-11 16:21:43.616734: Epoch time: 39.6 s 
2024-11-11 16:21:44.799871:  
2024-11-11 16:21:44.802289: Epoch 983 
2024-11-11 16:21:44.804752: Current learning rate: 0.00026 
2024-11-11 16:22:24.407309: train_loss -0.9603 
2024-11-11 16:22:24.411236: val_loss -0.9048 
2024-11-11 16:22:24.413819: Pseudo dice [np.float32(0.9487), np.float32(0.9174)] 
2024-11-11 16:22:24.416492: Epoch time: 39.61 s 
2024-11-11 16:22:25.605148:  
2024-11-11 16:22:25.607833: Epoch 984 
2024-11-11 16:22:25.610493: Current learning rate: 0.00024 
2024-11-11 16:23:05.203844: train_loss -0.9562 
2024-11-11 16:23:05.212815: val_loss -0.8753 
2024-11-11 16:23:05.215286: Pseudo dice [np.float32(0.9484), np.float32(0.8897)] 
2024-11-11 16:23:05.217795: Epoch time: 39.6 s 
2024-11-11 16:23:06.409930:  
2024-11-11 16:23:06.412384: Epoch 985 
2024-11-11 16:23:06.414912: Current learning rate: 0.00023 
2024-11-11 16:23:46.013858: train_loss -0.9606 
2024-11-11 16:23:46.017821: val_loss -0.8832 
2024-11-11 16:23:46.020209: Pseudo dice [np.float32(0.9439), np.float32(0.8857)] 
2024-11-11 16:23:46.022633: Epoch time: 39.61 s 
2024-11-11 16:23:47.212563:  
2024-11-11 16:23:47.215177: Epoch 986 
2024-11-11 16:23:47.217872: Current learning rate: 0.00021 
2024-11-11 16:24:26.819824: train_loss -0.9577 
2024-11-11 16:24:26.825232: val_loss -0.8752 
2024-11-11 16:24:26.827915: Pseudo dice [np.float32(0.9472), np.float32(0.8972)] 
2024-11-11 16:24:26.830307: Epoch time: 39.61 s 
2024-11-11 16:24:28.033348:  
2024-11-11 16:24:28.035707: Epoch 987 
2024-11-11 16:24:28.038430: Current learning rate: 0.0002 
2024-11-11 16:25:07.636024: train_loss -0.9592 
2024-11-11 16:25:07.640450: val_loss -0.8902 
2024-11-11 16:25:07.642910: Pseudo dice [np.float32(0.9436), np.float32(0.8938)] 
2024-11-11 16:25:07.645490: Epoch time: 39.6 s 
2024-11-11 16:25:08.829088:  
2024-11-11 16:25:08.831515: Epoch 988 
2024-11-11 16:25:08.834086: Current learning rate: 0.00019 
2024-11-11 16:25:48.416380: train_loss -0.9551 
2024-11-11 16:25:48.421688: val_loss -0.8667 
2024-11-11 16:25:48.424227: Pseudo dice [np.float32(0.9399), np.float32(0.8839)] 
2024-11-11 16:25:48.426838: Epoch time: 39.59 s 
2024-11-11 16:25:49.621542:  
2024-11-11 16:25:49.624019: Epoch 989 
2024-11-11 16:25:49.626358: Current learning rate: 0.00017 
2024-11-11 16:26:29.099447: train_loss -0.9602 
2024-11-11 16:26:29.107629: val_loss -0.8751 
2024-11-11 16:26:29.110038: Pseudo dice [np.float32(0.9364), np.float32(0.9012)] 
2024-11-11 16:26:29.112389: Epoch time: 39.48 s 
2024-11-11 16:26:30.299781:  
2024-11-11 16:26:30.302398: Epoch 990 
2024-11-11 16:26:30.304620: Current learning rate: 0.00016 
2024-11-11 16:27:09.806639: train_loss -0.9576 
2024-11-11 16:27:09.811723: val_loss -0.8767 
2024-11-11 16:27:09.814156: Pseudo dice [np.float32(0.9295), np.float32(0.9013)] 
2024-11-11 16:27:09.816402: Epoch time: 39.51 s 
2024-11-11 16:27:11.616024:  
2024-11-11 16:27:11.618658: Epoch 991 
2024-11-11 16:27:11.620963: Current learning rate: 0.00014 
2024-11-11 16:27:51.124809: train_loss -0.9631 
2024-11-11 16:27:51.128454: val_loss -0.875 
2024-11-11 16:27:51.130774: Pseudo dice [np.float32(0.9455), np.float32(0.8811)] 
2024-11-11 16:27:51.133363: Epoch time: 39.51 s 
2024-11-11 16:27:52.316386:  
2024-11-11 16:27:52.318985: Epoch 992 
2024-11-11 16:27:52.321365: Current learning rate: 0.00013 
2024-11-11 16:28:31.836297: train_loss -0.9546 
2024-11-11 16:28:31.841705: val_loss -0.8849 
2024-11-11 16:28:31.844154: Pseudo dice [np.float32(0.9444), np.float32(0.9022)] 
2024-11-11 16:28:31.846627: Epoch time: 39.52 s 
2024-11-11 16:28:33.030733:  
2024-11-11 16:28:33.033261: Epoch 993 
2024-11-11 16:28:33.035659: Current learning rate: 0.00011 
2024-11-11 16:29:12.553799: train_loss -0.9579 
2024-11-11 16:29:12.557313: val_loss -0.8831 
2024-11-11 16:29:12.559438: Pseudo dice [np.float32(0.9481), np.float32(0.8808)] 
2024-11-11 16:29:12.561904: Epoch time: 39.52 s 
2024-11-11 16:29:13.755738:  
2024-11-11 16:29:13.758335: Epoch 994 
2024-11-11 16:29:13.760940: Current learning rate: 0.0001 
2024-11-11 16:29:53.286364: train_loss -0.962 
2024-11-11 16:29:53.291470: val_loss -0.8806 
2024-11-11 16:29:53.293889: Pseudo dice [np.float32(0.9448), np.float32(0.8911)] 
2024-11-11 16:29:53.296279: Epoch time: 39.53 s 
2024-11-11 16:29:54.487600:  
2024-11-11 16:29:54.490154: Epoch 995 
2024-11-11 16:29:54.492441: Current learning rate: 8e-05 
2024-11-11 16:30:34.023625: train_loss -0.9599 
2024-11-11 16:30:34.026926: val_loss -0.9025 
2024-11-11 16:30:34.029616: Pseudo dice [np.float32(0.9528), np.float32(0.915)] 
2024-11-11 16:30:34.032038: Epoch time: 39.54 s 
2024-11-11 16:30:35.225202:  
2024-11-11 16:30:35.227615: Epoch 996 
2024-11-11 16:30:35.230186: Current learning rate: 7e-05 
2024-11-11 16:31:14.772141: train_loss -0.9597 
2024-11-11 16:31:14.777387: val_loss -0.863 
2024-11-11 16:31:14.780070: Pseudo dice [np.float32(0.9275), np.float32(0.9051)] 
2024-11-11 16:31:14.782418: Epoch time: 39.55 s 
2024-11-11 16:31:15.973907:  
2024-11-11 16:31:15.976721: Epoch 997 
2024-11-11 16:31:15.979510: Current learning rate: 5e-05 
2024-11-11 16:31:55.517458: train_loss -0.9576 
2024-11-11 16:31:55.521046: val_loss -0.8918 
2024-11-11 16:31:55.523436: Pseudo dice [np.float32(0.9436), np.float32(0.9074)] 
2024-11-11 16:31:55.528043: Epoch time: 39.54 s 
2024-11-11 16:31:56.711908:  
2024-11-11 16:31:56.714768: Epoch 998 
2024-11-11 16:31:56.717440: Current learning rate: 4e-05 
2024-11-11 16:32:36.288333: train_loss -0.9586 
2024-11-11 16:32:36.293425: val_loss -0.8883 
2024-11-11 16:32:36.296113: Pseudo dice [np.float32(0.9469), np.float32(0.8785)] 
2024-11-11 16:32:36.298534: Epoch time: 39.58 s 
2024-11-11 16:32:37.480294:  
2024-11-11 16:32:37.482735: Epoch 999 
2024-11-11 16:32:37.485278: Current learning rate: 2e-05 
2024-11-11 16:33:17.048519: train_loss -0.9579 
2024-11-11 16:33:17.052060: val_loss -0.8643 
2024-11-11 16:33:17.054450: Pseudo dice [np.float32(0.9331), np.float32(0.8932)] 
2024-11-11 16:33:17.057173: Epoch time: 39.57 s 
2024-11-11 16:33:19.058516: Training done. 
2024-11-11 16:33:19.109053: Using splits from existing split file: /srv/scratch/z5362216/kits19/nnUNet_db/nnUNet_preprocessed/Dataset001_Kits19/splits_final.json 
2024-11-11 16:33:19.112348: The split file contains 5 splits. 
2024-11-11 16:33:19.114835: Desired fold for training: 3 
2024-11-11 16:33:19.117843: This split has 80 training and 20 validation cases. 
2024-11-11 16:33:19.120367: predicting imaging_003 
2024-11-11 16:33:19.128988: imaging_003, shape torch.Size([1, 270, 552, 552]), rank 0 
2024-11-11 16:34:02.673555: predicting imaging_005 
2024-11-11 16:34:02.693291: imaging_005, shape torch.Size([1, 834, 630, 630]), rank 0 
2024-11-11 16:35:03.710312: predicting imaging_015 
2024-11-11 16:35:03.760219: imaging_015, shape torch.Size([1, 75, 455, 455]), rank 0 
2024-11-11 16:35:18.944385: predicting imaging_023 
2024-11-11 16:35:18.957524: imaging_023, shape torch.Size([1, 107, 504, 504]), rank 0 
2024-11-11 16:35:21.555593: predicting imaging_031 
2024-11-11 16:35:21.576053: imaging_031, shape torch.Size([1, 117, 334, 334]), rank 0 
2024-11-11 16:35:24.396303: predicting imaging_044 
2024-11-11 16:35:24.407624: imaging_044, shape torch.Size([1, 101, 623, 623]), rank 0 
2024-11-11 16:35:41.066973: predicting imaging_053 
2024-11-11 16:35:41.078169: imaging_053, shape torch.Size([1, 553, 499, 499]), rank 0 
2024-11-11 16:36:18.277219: predicting imaging_055 
2024-11-11 16:36:18.299907: imaging_055, shape torch.Size([1, 101, 567, 567]), rank 0 
2024-11-11 16:36:27.536952: predicting imaging_056 
2024-11-11 16:36:27.546606: imaging_056, shape torch.Size([1, 90, 504, 504]), rank 0 
2024-11-11 16:36:29.684023: predicting imaging_061 
2024-11-11 16:36:29.762379: imaging_061, shape torch.Size([1, 29, 630, 630]), rank 0 
2024-11-11 16:36:32.358611: predicting imaging_064 
2024-11-11 16:36:32.387042: imaging_064, shape torch.Size([1, 53, 453, 453]), rank 0 
2024-11-11 16:36:34.909589: predicting imaging_066 
2024-11-11 16:36:34.943195: imaging_066, shape torch.Size([1, 445, 557, 557]), rank 0 
2024-11-11 16:37:30.839152: predicting imaging_067 
2024-11-11 16:37:30.865396: imaging_067, shape torch.Size([1, 285, 491, 491]), rank 0 
2024-11-11 16:37:41.927402: predicting imaging_070 
2024-11-11 16:37:41.942019: imaging_070, shape torch.Size([1, 57, 472, 472]), rank 0 
2024-11-11 16:37:43.275102: predicting imaging_072 
2024-11-11 16:37:43.295201: imaging_072, shape torch.Size([1, 164, 428, 428]), rank 0 
2024-11-11 16:37:47.974505: predicting imaging_090 
2024-11-11 16:37:47.983616: imaging_090, shape torch.Size([1, 76, 495, 495]), rank 0 
2024-11-11 16:37:49.753547: predicting imaging_091 
2024-11-11 16:37:49.764192: imaging_091, shape torch.Size([1, 735, 509, 509]), rank 0 
2024-11-11 16:38:26.891994: predicting imaging_093 
2024-11-11 16:38:26.916599: imaging_093, shape torch.Size([1, 787, 587, 587]), rank 0 
2024-11-11 16:40:12.159311: predicting imaging_094 
2024-11-11 16:40:12.194540: imaging_094, shape torch.Size([1, 43, 453, 453]), rank 0 
2024-11-11 16:40:13.244018: predicting imaging_096 
2024-11-11 16:40:13.252947: imaging_096, shape torch.Size([1, 683, 476, 476]), rank 0 
2024-11-11 16:42:44.425455: Validation complete 
2024-11-11 16:42:44.435852: Mean Validation Dice:  0.8090781024475693 
