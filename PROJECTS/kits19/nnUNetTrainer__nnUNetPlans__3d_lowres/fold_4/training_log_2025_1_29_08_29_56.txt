
#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################
 
2025-01-29 08:29:57.965247: Using torch.compile... 
2025-01-29 08:30:02.890298: do_dummy_2d_data_aug: False 
2025-01-29 08:30:02.956247: Using splits from existing split file: /srv/scratch/z5362216/kits19/nnUNet_db/nnUNet_preprocessed/Dataset001_Kits19/splits_final.json 
2025-01-29 08:30:03.010806: The split file contains 5 splits. 
2025-01-29 08:30:03.013623: Desired fold for training: 4 
2025-01-29 08:30:03.016518: This split has 80 training and 20 validation cases. 

This is the configuration used by this training:
Configuration name: 3d_lowres
 {'data_identifier': 'nnUNetPlans_3d_lowres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 2, 'patch_size': [128, 128, 128], 'median_image_size_in_voxels': [200, 205, 205], 'spacing': [1.9849520718478983, 1.9849270710444444, 1.9849270710444444], 'normalization_schemes': ['CTNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': False, 'next_stage': '3d_cascade_fullres'} 
 
These are the global plan.json settings:
 {'dataset_name': 'Dataset001_Kits19', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [3.0, 0.7939453125, 0.7939453125], 'original_median_shape_after_transp': [104, 512, 512], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [2, 0, 1], 'transpose_backward': [1, 2, 0], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 2553.0, 'mean': 104.46720886230469, 'median': 104.0, 'min': -277.0, 'percentile_00_5': -73.0, 'percentile_99_5': 292.0, 'std': 74.68063354492188}}} 
 
2025-01-29 08:30:05.514271: unpacking dataset... 
2025-01-29 08:30:14.063277: unpacking done... 
2025-01-29 08:30:14.104595: 
printing the network instead:
 
2025-01-29 08:30:14.107647: OptimizedModule(
  (_orig_mod): PlainConvUNet(
    (encoder): PlainConvEncoder(
      (stages): Sequential(
        (0): Sequential(
          (0): StackedConvBlocks(
            (convs): Sequential(
              (0): ConvDropoutNormReLU(
                (conv): Conv3d(1, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(1, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (1): ConvDropoutNormReLU(
                (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
            )
          )
        )
        (1): Sequential(
          (0): StackedConvBlocks(
            (convs): Sequential(
              (0): ConvDropoutNormReLU(
                (conv): Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                  (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (1): ConvDropoutNormReLU(
                (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
            )
          )
        )
        (2): Sequential(
          (0): StackedConvBlocks(
            (convs): Sequential(
              (0): ConvDropoutNormReLU(
                (conv): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                  (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (1): ConvDropoutNormReLU(
                (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
            )
          )
        )
        (3): Sequential(
          (0): StackedConvBlocks(
            (convs): Sequential(
              (0): ConvDropoutNormReLU(
                (conv): Conv3d(128, 256, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                (norm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(128, 256, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                  (1): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (1): ConvDropoutNormReLU(
                (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
            )
          )
        )
        (4): Sequential(
          (0): StackedConvBlocks(
            (convs): Sequential(
              (0): ConvDropoutNormReLU(
                (conv): Conv3d(256, 320, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                (norm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(256, 320, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                  (1): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (1): ConvDropoutNormReLU(
                (conv): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
            )
          )
        )
        (5): Sequential(
          (0): StackedConvBlocks(
            (convs): Sequential(
              (0): ConvDropoutNormReLU(
                (conv): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                (norm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                  (1): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (1): ConvDropoutNormReLU(
                (conv): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
            )
          )
        )
      )
    )
    (decoder): UNetDecoder(
      (encoder): PlainConvEncoder(
        (stages): Sequential(
          (0): Sequential(
            (0): StackedConvBlocks(
              (convs): Sequential(
                (0): ConvDropoutNormReLU(
                  (conv): Conv3d(1, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                  (all_modules): Sequential(
                    (0): Conv3d(1, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                    (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                    (2): LeakyReLU(negative_slope=0.01, inplace=True)
                  )
                )
                (1): ConvDropoutNormReLU(
                  (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                  (all_modules): Sequential(
                    (0): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                    (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                    (2): LeakyReLU(negative_slope=0.01, inplace=True)
                  )
                )
              )
            )
          )
          (1): Sequential(
            (0): StackedConvBlocks(
              (convs): Sequential(
                (0): ConvDropoutNormReLU(
                  (conv): Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                  (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                  (all_modules): Sequential(
                    (0): Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                    (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                    (2): LeakyReLU(negative_slope=0.01, inplace=True)
                  )
                )
                (1): ConvDropoutNormReLU(
                  (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                  (all_modules): Sequential(
                    (0): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                    (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                    (2): LeakyReLU(negative_slope=0.01, inplace=True)
                  )
                )
              )
            )
          )
          (2): Sequential(
            (0): StackedConvBlocks(
              (convs): Sequential(
                (0): ConvDropoutNormReLU(
                  (conv): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                  (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                  (all_modules): Sequential(
                    (0): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                    (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                    (2): LeakyReLU(negative_slope=0.01, inplace=True)
                  )
                )
                (1): ConvDropoutNormReLU(
                  (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                  (all_modules): Sequential(
                    (0): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                    (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                    (2): LeakyReLU(negative_slope=0.01, inplace=True)
                  )
                )
              )
            )
          )
          (3): Sequential(
            (0): StackedConvBlocks(
              (convs): Sequential(
                (0): ConvDropoutNormReLU(
                  (conv): Conv3d(128, 256, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                  (norm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                  (all_modules): Sequential(
                    (0): Conv3d(128, 256, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                    (1): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                    (2): LeakyReLU(negative_slope=0.01, inplace=True)
                  )
                )
                (1): ConvDropoutNormReLU(
                  (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (norm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                  (all_modules): Sequential(
                    (0): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                    (1): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                    (2): LeakyReLU(negative_slope=0.01, inplace=True)
                  )
                )
              )
            )
          )
          (4): Sequential(
            (0): StackedConvBlocks(
              (convs): Sequential(
                (0): ConvDropoutNormReLU(
                  (conv): Conv3d(256, 320, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                  (norm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                  (all_modules): Sequential(
                    (0): Conv3d(256, 320, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                    (1): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                    (2): LeakyReLU(negative_slope=0.01, inplace=True)
                  )
                )
                (1): ConvDropoutNormReLU(
                  (conv): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (norm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                  (all_modules): Sequential(
                    (0): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                    (1): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                    (2): LeakyReLU(negative_slope=0.01, inplace=True)
                  )
                )
              )
            )
          )
          (5): Sequential(
            (0): StackedConvBlocks(
              (convs): Sequential(
                (0): ConvDropoutNormReLU(
                  (conv): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                  (norm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                  (all_modules): Sequential(
                    (0): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                    (1): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                    (2): LeakyReLU(negative_slope=0.01, inplace=True)
                  )
                )
                (1): ConvDropoutNormReLU(
                  (conv): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (norm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                  (all_modules): Sequential(
                    (0): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                    (1): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                    (2): LeakyReLU(negative_slope=0.01, inplace=True)
                  )
                )
              )
            )
          )
        )
      )
      (stages): ModuleList(
        (0): StackedConvBlocks(
          (convs): Sequential(
            (0): ConvDropoutNormReLU(
              (conv): Conv3d(640, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
              (norm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(640, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (1): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (1): ConvDropoutNormReLU(
              (conv): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
              (norm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (1): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
          )
        )
        (1): StackedConvBlocks(
          (convs): Sequential(
            (0): ConvDropoutNormReLU(
              (conv): Conv3d(512, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
              (norm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(512, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (1): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (1): ConvDropoutNormReLU(
              (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
              (norm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (1): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
          )
        )
        (2): StackedConvBlocks(
          (convs): Sequential(
            (0): ConvDropoutNormReLU(
              (conv): Conv3d(256, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
              (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(256, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (1): ConvDropoutNormReLU(
              (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
              (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
          )
        )
        (3): StackedConvBlocks(
          (convs): Sequential(
            (0): ConvDropoutNormReLU(
              (conv): Conv3d(128, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
              (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(128, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (1): ConvDropoutNormReLU(
              (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
              (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
          )
        )
        (4): StackedConvBlocks(
          (convs): Sequential(
            (0): ConvDropoutNormReLU(
              (conv): Conv3d(64, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
              (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(64, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (1): ConvDropoutNormReLU(
              (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
              (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
          )
        )
      )
      (transpconvs): ModuleList(
        (0): ConvTranspose3d(320, 320, kernel_size=(2, 2, 2), stride=(2, 2, 2))
        (1): ConvTranspose3d(320, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2))
        (2): ConvTranspose3d(256, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2))
        (3): ConvTranspose3d(128, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2))
        (4): ConvTranspose3d(64, 32, kernel_size=(2, 2, 2), stride=(2, 2, 2))
      )
      (seg_layers): ModuleList(
        (0): Conv3d(320, 3, kernel_size=(1, 1, 1), stride=(1, 1, 1))
        (1): Conv3d(256, 3, kernel_size=(1, 1, 1), stride=(1, 1, 1))
        (2): Conv3d(128, 3, kernel_size=(1, 1, 1), stride=(1, 1, 1))
        (3): Conv3d(64, 3, kernel_size=(1, 1, 1), stride=(1, 1, 1))
        (4): Conv3d(32, 3, kernel_size=(1, 1, 1), stride=(1, 1, 1))
      )
    )
  )
) 
2025-01-29 08:30:14.115977: 
 
2025-01-29 08:30:14.118414: Unable to plot network architecture: nnUNet_compile is enabled! 
2025-01-29 08:30:14.219391:  
2025-01-29 08:30:14.221898: Epoch 400 
2025-01-29 08:30:14.224452: Current learning rate: 0.00631 
2025-01-29 08:32:29.383305: train_loss -0.806 
2025-01-29 08:32:29.391067: val_loss -0.7764 
2025-01-29 08:32:29.398358: Pseudo dice [np.float32(0.9666), np.float32(0.8853)] 
2025-01-29 08:32:29.401256: Epoch time: 135.17 s 
2025-01-29 08:32:31.105240:  
2025-01-29 08:32:31.107967: Epoch 401 
2025-01-29 08:32:31.110653: Current learning rate: 0.0063 
2025-01-29 08:33:19.806636: train_loss -0.8103 
2025-01-29 08:33:19.811347: val_loss -0.7948 
2025-01-29 08:33:19.813906: Pseudo dice [np.float32(0.9625), np.float32(0.8325)] 
2025-01-29 08:33:19.816638: Epoch time: 48.7 s 
2025-01-29 08:33:21.013359:  
2025-01-29 08:33:21.016222: Epoch 402 
2025-01-29 08:33:21.019241: Current learning rate: 0.0063 
2025-01-29 08:34:09.455335: train_loss -0.8084 
2025-01-29 08:34:09.461925: val_loss -0.793 
2025-01-29 08:34:09.464858: Pseudo dice [np.float32(0.9678), np.float32(0.8882)] 
2025-01-29 08:34:09.467563: Epoch time: 48.44 s 
2025-01-29 08:34:10.661376:  
2025-01-29 08:34:10.664133: Epoch 403 
2025-01-29 08:34:10.667034: Current learning rate: 0.00629 
2025-01-29 08:34:59.020182: train_loss -0.818 
2025-01-29 08:34:59.028430: val_loss -0.768 
2025-01-29 08:34:59.031311: Pseudo dice [np.float32(0.9654), np.float32(0.8774)] 
2025-01-29 08:34:59.034401: Epoch time: 48.36 s 
2025-01-29 08:35:00.225264:  
2025-01-29 08:35:00.228288: Epoch 404 
2025-01-29 08:35:00.231254: Current learning rate: 0.00628 
2025-01-29 08:35:48.487848: train_loss -0.8114 
2025-01-29 08:35:48.494439: val_loss -0.7827 
2025-01-29 08:35:48.502473: Pseudo dice [np.float32(0.9637), np.float32(0.8714)] 
2025-01-29 08:35:48.505175: Epoch time: 48.26 s 
2025-01-29 08:35:49.693711:  
2025-01-29 08:35:49.696998: Epoch 405 
2025-01-29 08:35:49.699970: Current learning rate: 0.00627 
2025-01-29 08:36:37.928388: train_loss -0.8159 
2025-01-29 08:36:37.932085: val_loss -0.7563 
2025-01-29 08:36:37.934762: Pseudo dice [np.float32(0.9622), np.float32(0.8517)] 
2025-01-29 08:36:37.937454: Epoch time: 48.24 s 
2025-01-29 08:36:39.160477:  
2025-01-29 08:36:39.163177: Epoch 406 
2025-01-29 08:36:39.165499: Current learning rate: 0.00626 
2025-01-29 08:37:27.806775: train_loss -0.8184 
2025-01-29 08:37:27.813392: val_loss -0.7744 
2025-01-29 08:37:27.816202: Pseudo dice [np.float32(0.9639), np.float32(0.8833)] 
2025-01-29 08:37:27.818656: Epoch time: 48.65 s 
2025-01-29 08:37:29.048186:  
2025-01-29 08:37:29.051197: Epoch 407 
2025-01-29 08:37:29.053940: Current learning rate: 0.00625 
2025-01-29 08:38:17.882230: train_loss -0.8157 
2025-01-29 08:38:17.885908: val_loss -0.7591 
2025-01-29 08:38:17.888688: Pseudo dice [np.float32(0.9659), np.float32(0.8765)] 
2025-01-29 08:38:17.891442: Epoch time: 48.84 s 
2025-01-29 08:38:19.121067:  
2025-01-29 08:38:19.124185: Epoch 408 
2025-01-29 08:38:19.126847: Current learning rate: 0.00624 
2025-01-29 08:39:07.277003: train_loss -0.8218 
2025-01-29 08:39:07.284378: val_loss -0.7832 
2025-01-29 08:39:07.293802: Pseudo dice [np.float32(0.9625), np.float32(0.8832)] 
2025-01-29 08:39:07.296906: Epoch time: 48.16 s 
2025-01-29 08:39:08.481456:  
2025-01-29 08:39:08.484736: Epoch 409 
2025-01-29 08:39:08.487793: Current learning rate: 0.00623 
2025-01-29 08:39:56.789111: train_loss -0.8249 
2025-01-29 08:39:56.792780: val_loss -0.7967 
2025-01-29 08:39:56.796014: Pseudo dice [np.float32(0.9682), np.float32(0.8842)] 
2025-01-29 08:39:56.798911: Epoch time: 48.31 s 
2025-01-29 08:39:57.984740:  
2025-01-29 08:39:57.988000: Epoch 410 
2025-01-29 08:39:57.990963: Current learning rate: 0.00622 
2025-01-29 08:40:46.259705: train_loss -0.8251 
2025-01-29 08:40:46.265763: val_loss -0.7907 
2025-01-29 08:40:46.268360: Pseudo dice [np.float32(0.964), np.float32(0.8895)] 
2025-01-29 08:40:46.270817: Epoch time: 48.28 s 
2025-01-29 08:40:47.394179:  
2025-01-29 08:40:47.397029: Epoch 411 
2025-01-29 08:40:47.400017: Current learning rate: 0.00621 
2025-01-29 08:41:35.731082: train_loss -0.8175 
2025-01-29 08:41:35.734523: val_loss -0.741 
2025-01-29 08:41:35.737215: Pseudo dice [np.float32(0.9686), np.float32(0.8726)] 
2025-01-29 08:41:35.739666: Epoch time: 48.34 s 
2025-01-29 08:41:35.742180: Yayy! New best EMA pseudo Dice: 0.9168000221252441 
2025-01-29 08:41:37.380512:  
2025-01-29 08:41:37.383708: Epoch 412 
2025-01-29 08:41:37.386715: Current learning rate: 0.0062 
2025-01-29 08:42:25.652181: train_loss -0.8122 
2025-01-29 08:42:25.657769: val_loss -0.7282 
2025-01-29 08:42:25.660716: Pseudo dice [np.float32(0.9647), np.float32(0.8399)] 
2025-01-29 08:42:25.663444: Epoch time: 48.27 s 
2025-01-29 08:42:26.788069:  
2025-01-29 08:42:26.790656: Epoch 413 
2025-01-29 08:42:26.793367: Current learning rate: 0.00619 
2025-01-29 08:43:15.315155: train_loss -0.7868 
2025-01-29 08:43:15.321731: val_loss -0.7615 
2025-01-29 08:43:15.329452: Pseudo dice [np.float32(0.9649), np.float32(0.8594)] 
2025-01-29 08:43:15.332227: Epoch time: 48.53 s 
2025-01-29 08:43:16.486889:  
2025-01-29 08:43:16.489699: Epoch 414 
2025-01-29 08:43:16.492558: Current learning rate: 0.00618 
2025-01-29 08:44:05.111457: train_loss -0.793 
2025-01-29 08:44:05.117175: val_loss -0.7216 
2025-01-29 08:44:05.120165: Pseudo dice [np.float32(0.9602), np.float32(0.8198)] 
2025-01-29 08:44:05.122901: Epoch time: 48.63 s 
2025-01-29 08:44:06.251532:  
2025-01-29 08:44:06.254958: Epoch 415 
2025-01-29 08:44:06.257821: Current learning rate: 0.00617 
2025-01-29 08:44:54.851459: train_loss -0.7979 
2025-01-29 08:44:54.858462: val_loss -0.7827 
2025-01-29 08:44:54.861460: Pseudo dice [np.float32(0.9645), np.float32(0.895)] 
2025-01-29 08:44:54.863955: Epoch time: 48.6 s 
2025-01-29 08:44:56.022530:  
2025-01-29 08:44:56.025675: Epoch 416 
2025-01-29 08:44:56.028571: Current learning rate: 0.00616 
2025-01-29 08:45:44.390044: train_loss -0.8116 
2025-01-29 08:45:44.396549: val_loss -0.7366 
2025-01-29 08:45:44.399016: Pseudo dice [np.float32(0.95), np.float32(0.8128)] 
2025-01-29 08:45:44.401299: Epoch time: 48.37 s 
2025-01-29 08:45:46.188086:  
2025-01-29 08:45:46.191462: Epoch 417 
2025-01-29 08:45:46.194491: Current learning rate: 0.00615 
2025-01-29 08:46:34.191551: train_loss -0.8122 
2025-01-29 08:46:34.198977: val_loss -0.7499 
2025-01-29 08:46:34.206954: Pseudo dice [np.float32(0.9626), np.float32(0.8618)] 
2025-01-29 08:46:34.209844: Epoch time: 48.0 s 
2025-01-29 08:46:35.336468:  
2025-01-29 08:46:35.339890: Epoch 418 
2025-01-29 08:46:35.342660: Current learning rate: 0.00614 
2025-01-29 08:47:23.932751: train_loss -0.831 
2025-01-29 08:47:23.938291: val_loss -0.76 
2025-01-29 08:47:23.940965: Pseudo dice [np.float32(0.9632), np.float32(0.8189)] 
2025-01-29 08:47:23.943403: Epoch time: 48.6 s 
2025-01-29 08:47:25.074268:  
2025-01-29 08:47:25.077451: Epoch 419 
2025-01-29 08:47:25.080136: Current learning rate: 0.00613 
2025-01-29 08:48:13.534186: train_loss -0.806 
2025-01-29 08:48:13.538030: val_loss -0.7454 
2025-01-29 08:48:13.540701: Pseudo dice [np.float32(0.9676), np.float32(0.8602)] 
2025-01-29 08:48:13.543311: Epoch time: 48.46 s 
2025-01-29 08:48:14.673113:  
2025-01-29 08:48:14.676178: Epoch 420 
2025-01-29 08:48:14.678931: Current learning rate: 0.00612 
2025-01-29 08:49:02.496299: train_loss -0.8087 
2025-01-29 08:49:02.501906: val_loss -0.7529 
2025-01-29 08:49:02.504375: Pseudo dice [np.float32(0.9606), np.float32(0.8569)] 
2025-01-29 08:49:02.506711: Epoch time: 47.82 s 
2025-01-29 08:49:03.639596:  
2025-01-29 08:49:03.642448: Epoch 421 
2025-01-29 08:49:03.645054: Current learning rate: 0.00612 
2025-01-29 08:49:52.323077: train_loss -0.8151 
2025-01-29 08:49:52.330506: val_loss -0.7957 
2025-01-29 08:49:52.339190: Pseudo dice [np.float32(0.9619), np.float32(0.8849)] 
2025-01-29 08:49:52.341953: Epoch time: 48.68 s 
2025-01-29 08:49:53.479131:  
2025-01-29 08:49:53.481897: Epoch 422 
2025-01-29 08:49:53.484360: Current learning rate: 0.00611 
2025-01-29 08:50:41.373769: train_loss -0.8164 
2025-01-29 08:50:41.382340: val_loss -0.7587 
2025-01-29 08:50:41.384928: Pseudo dice [np.float32(0.9661), np.float32(0.853)] 
2025-01-29 08:50:41.387350: Epoch time: 47.9 s 
2025-01-29 08:50:42.515364:  
2025-01-29 08:50:42.518315: Epoch 423 
2025-01-29 08:50:42.521075: Current learning rate: 0.0061 
2025-01-29 08:51:30.966057: train_loss -0.8 
2025-01-29 08:51:30.970300: val_loss -0.8295 
2025-01-29 08:51:30.973023: Pseudo dice [np.float32(0.9644), np.float32(0.8798)] 
2025-01-29 08:51:30.975902: Epoch time: 48.45 s 
2025-01-29 08:51:32.113503:  
2025-01-29 08:51:32.116544: Epoch 424 
2025-01-29 08:51:32.119197: Current learning rate: 0.00609 
2025-01-29 08:52:20.709981: train_loss -0.7923 
2025-01-29 08:52:20.715899: val_loss -0.742 
2025-01-29 08:52:20.718481: Pseudo dice [np.float32(0.9579), np.float32(0.8623)] 
2025-01-29 08:52:20.721157: Epoch time: 48.6 s 
2025-01-29 08:52:21.852209:  
2025-01-29 08:52:21.855181: Epoch 425 
2025-01-29 08:52:21.857776: Current learning rate: 0.00608 
2025-01-29 08:53:10.532344: train_loss -0.8062 
2025-01-29 08:53:10.536295: val_loss -0.7841 
2025-01-29 08:53:10.543825: Pseudo dice [np.float32(0.9595), np.float32(0.8345)] 
2025-01-29 08:53:10.546311: Epoch time: 48.68 s 
2025-01-29 08:53:11.681410:  
2025-01-29 08:53:11.684225: Epoch 426 
2025-01-29 08:53:11.686844: Current learning rate: 0.00607 
2025-01-29 08:53:59.913790: train_loss -0.8037 
2025-01-29 08:53:59.919181: val_loss -0.727 
2025-01-29 08:53:59.921811: Pseudo dice [np.float32(0.9651), np.float32(0.812)] 
2025-01-29 08:53:59.924102: Epoch time: 48.23 s 
2025-01-29 08:54:01.059137:  
2025-01-29 08:54:01.061723: Epoch 427 
2025-01-29 08:54:01.064619: Current learning rate: 0.00606 
2025-01-29 08:54:49.163856: train_loss -0.8028 
2025-01-29 08:54:49.168038: val_loss -0.7654 
2025-01-29 08:54:49.170713: Pseudo dice [np.float32(0.964), np.float32(0.8384)] 
2025-01-29 08:54:49.173322: Epoch time: 48.11 s 
2025-01-29 08:54:50.305170:  
2025-01-29 08:54:50.308021: Epoch 428 
2025-01-29 08:54:50.310872: Current learning rate: 0.00605 
2025-01-29 08:55:38.647557: train_loss -0.8057 
2025-01-29 08:55:38.653205: val_loss -0.7567 
2025-01-29 08:55:38.655794: Pseudo dice [np.float32(0.9619), np.float32(0.8645)] 
2025-01-29 08:55:38.658584: Epoch time: 48.34 s 
2025-01-29 08:55:39.784710:  
2025-01-29 08:55:39.787515: Epoch 429 
2025-01-29 08:55:39.790149: Current learning rate: 0.00604 
2025-01-29 08:56:28.148246: train_loss -0.7972 
2025-01-29 08:56:28.152248: val_loss -0.7296 
2025-01-29 08:56:28.161090: Pseudo dice [np.float32(0.9639), np.float32(0.8412)] 
2025-01-29 08:56:28.163896: Epoch time: 48.36 s 
2025-01-29 08:56:29.300846:  
2025-01-29 08:56:29.303663: Epoch 430 
2025-01-29 08:56:29.306507: Current learning rate: 0.00603 
2025-01-29 08:57:17.693995: train_loss -0.7975 
2025-01-29 08:57:17.700086: val_loss -0.7365 
2025-01-29 08:57:17.702658: Pseudo dice [np.float32(0.9629), np.float32(0.7642)] 
2025-01-29 08:57:17.705242: Epoch time: 48.39 s 
2025-01-29 08:57:18.837591:  
2025-01-29 08:57:18.840340: Epoch 431 
2025-01-29 08:57:18.843355: Current learning rate: 0.00602 
2025-01-29 08:58:07.374789: train_loss -0.8084 
2025-01-29 08:58:07.380642: val_loss -0.749 
2025-01-29 08:58:07.383234: Pseudo dice [np.float32(0.9668), np.float32(0.8423)] 
2025-01-29 08:58:07.385908: Epoch time: 48.54 s 
2025-01-29 08:58:08.521457:  
2025-01-29 08:58:08.524670: Epoch 432 
2025-01-29 08:58:08.527528: Current learning rate: 0.00601 
2025-01-29 08:58:56.800632: train_loss -0.7644 
2025-01-29 08:58:56.806345: val_loss -0.6659 
2025-01-29 08:58:56.809176: Pseudo dice [np.float32(0.9572), np.float32(0.7309)] 
2025-01-29 08:58:56.811990: Epoch time: 48.28 s 
2025-01-29 08:58:57.946179:  
2025-01-29 08:58:57.949773: Epoch 433 
2025-01-29 08:58:57.952639: Current learning rate: 0.006 
2025-01-29 08:59:46.492273: train_loss -0.7775 
2025-01-29 08:59:46.496781: val_loss -0.7384 
2025-01-29 08:59:46.499922: Pseudo dice [np.float32(0.9552), np.float32(0.8409)] 
2025-01-29 08:59:46.502385: Epoch time: 48.55 s 
2025-01-29 08:59:47.630269:  
2025-01-29 08:59:47.633136: Epoch 434 
2025-01-29 08:59:47.636014: Current learning rate: 0.00599 
2025-01-29 09:00:36.289283: train_loss -0.7961 
2025-01-29 09:00:36.296011: val_loss -0.7253 
2025-01-29 09:00:36.299093: Pseudo dice [np.float32(0.9625), np.float32(0.8301)] 
2025-01-29 09:00:36.301755: Epoch time: 48.66 s 
2025-01-29 09:00:37.988132:  
2025-01-29 09:00:37.991003: Epoch 435 
2025-01-29 09:00:37.993572: Current learning rate: 0.00598 
2025-01-29 09:01:26.451887: train_loss -0.7985 
2025-01-29 09:01:26.457873: val_loss -0.777 
2025-01-29 09:01:26.460591: Pseudo dice [np.float32(0.9638), np.float32(0.7701)] 
2025-01-29 09:01:26.463395: Epoch time: 48.46 s 
2025-01-29 09:01:27.593980:  
2025-01-29 09:01:27.596844: Epoch 436 
2025-01-29 09:01:27.599458: Current learning rate: 0.00597 
2025-01-29 09:02:16.283756: train_loss -0.8038 
2025-01-29 09:02:16.289659: val_loss -0.7722 
2025-01-29 09:02:16.292477: Pseudo dice [np.float32(0.9596), np.float32(0.8413)] 
2025-01-29 09:02:16.295084: Epoch time: 48.69 s 
2025-01-29 09:02:17.431119:  
2025-01-29 09:02:17.434209: Epoch 437 
2025-01-29 09:02:17.436987: Current learning rate: 0.00596 
2025-01-29 09:03:05.923717: train_loss -0.8162 
2025-01-29 09:03:05.928247: val_loss -0.7428 
2025-01-29 09:03:05.931026: Pseudo dice [np.float32(0.9583), np.float32(0.8317)] 
2025-01-29 09:03:05.933791: Epoch time: 48.49 s 
2025-01-29 09:03:07.072203:  
2025-01-29 09:03:07.074936: Epoch 438 
2025-01-29 09:03:07.077413: Current learning rate: 0.00595 
2025-01-29 09:03:55.521363: train_loss -0.8108 
2025-01-29 09:03:55.526836: val_loss -0.7654 
2025-01-29 09:03:55.529457: Pseudo dice [np.float32(0.9609), np.float32(0.8178)] 
2025-01-29 09:03:55.531654: Epoch time: 48.45 s 
2025-01-29 09:03:56.668355:  
2025-01-29 09:03:56.671172: Epoch 439 
2025-01-29 09:03:56.674237: Current learning rate: 0.00594 
2025-01-29 09:04:44.976260: train_loss -0.81 
2025-01-29 09:04:44.980567: val_loss -0.7536 
2025-01-29 09:04:44.983182: Pseudo dice [np.float32(0.9585), np.float32(0.8581)] 
2025-01-29 09:04:44.985938: Epoch time: 48.31 s 
2025-01-29 09:04:46.120425:  
2025-01-29 09:04:46.123288: Epoch 440 
2025-01-29 09:04:46.126142: Current learning rate: 0.00593 
2025-01-29 09:05:34.246653: train_loss -0.8088 
2025-01-29 09:05:34.253881: val_loss -0.8372 
2025-01-29 09:05:34.261864: Pseudo dice [np.float32(0.9629), np.float32(0.8899)] 
2025-01-29 09:05:34.264436: Epoch time: 48.13 s 
2025-01-29 09:05:35.393573:  
2025-01-29 09:05:35.396478: Epoch 441 
2025-01-29 09:05:35.399409: Current learning rate: 0.00592 
2025-01-29 09:06:23.530239: train_loss -0.8237 
2025-01-29 09:06:23.534204: val_loss -0.8207 
2025-01-29 09:06:23.536586: Pseudo dice [np.float32(0.968), np.float32(0.8904)] 
2025-01-29 09:06:23.538928: Epoch time: 48.14 s 
2025-01-29 09:06:24.678401:  
2025-01-29 09:06:24.681569: Epoch 442 
2025-01-29 09:06:24.684402: Current learning rate: 0.00592 
2025-01-29 09:07:12.587564: train_loss -0.8171 
2025-01-29 09:07:12.593165: val_loss -0.7696 
2025-01-29 09:07:12.595963: Pseudo dice [np.float32(0.9639), np.float32(0.798)] 
2025-01-29 09:07:12.598480: Epoch time: 47.91 s 
2025-01-29 09:07:13.731550:  
2025-01-29 09:07:13.734571: Epoch 443 
2025-01-29 09:07:13.737473: Current learning rate: 0.00591 
2025-01-29 09:08:02.285966: train_loss -0.8097 
2025-01-29 09:08:02.290137: val_loss -0.7695 
2025-01-29 09:08:02.293326: Pseudo dice [np.float32(0.9664), np.float32(0.8558)] 
2025-01-29 09:08:02.296225: Epoch time: 48.56 s 
2025-01-29 09:08:03.416441:  
2025-01-29 09:08:03.419680: Epoch 444 
2025-01-29 09:08:03.422580: Current learning rate: 0.0059 
2025-01-29 09:08:51.935797: train_loss -0.8283 
2025-01-29 09:08:51.942851: val_loss -0.781 
2025-01-29 09:08:51.950397: Pseudo dice [np.float32(0.9648), np.float32(0.8412)] 
2025-01-29 09:08:51.953256: Epoch time: 48.52 s 
2025-01-29 09:08:53.184547:  
2025-01-29 09:08:53.187307: Epoch 445 
2025-01-29 09:08:53.190302: Current learning rate: 0.00589 
2025-01-29 09:09:41.507671: train_loss -0.7992 
2025-01-29 09:09:41.511867: val_loss -0.7697 
2025-01-29 09:09:41.514739: Pseudo dice [np.float32(0.9635), np.float32(0.8749)] 
2025-01-29 09:09:41.517422: Epoch time: 48.32 s 
2025-01-29 09:09:42.638162:  
2025-01-29 09:09:42.641330: Epoch 446 
2025-01-29 09:09:42.644359: Current learning rate: 0.00588 
2025-01-29 09:10:31.145609: train_loss -0.8249 
2025-01-29 09:10:31.150886: val_loss -0.7698 
2025-01-29 09:10:31.153400: Pseudo dice [np.float32(0.9632), np.float32(0.8789)] 
2025-01-29 09:10:31.155828: Epoch time: 48.51 s 
2025-01-29 09:10:32.279562:  
2025-01-29 09:10:32.282370: Epoch 447 
2025-01-29 09:10:32.285149: Current learning rate: 0.00587 
2025-01-29 09:11:21.161827: train_loss -0.8315 
2025-01-29 09:11:21.166125: val_loss -0.7827 
2025-01-29 09:11:21.169023: Pseudo dice [np.float32(0.9662), np.float32(0.8751)] 
2025-01-29 09:11:21.171671: Epoch time: 48.88 s 
2025-01-29 09:11:22.299717:  
2025-01-29 09:11:22.302821: Epoch 448 
2025-01-29 09:11:22.305792: Current learning rate: 0.00586 
2025-01-29 09:12:10.706871: train_loss -0.8336 
2025-01-29 09:12:10.712351: val_loss -0.7752 
2025-01-29 09:12:10.715070: Pseudo dice [np.float32(0.9617), np.float32(0.8563)] 
2025-01-29 09:12:10.717699: Epoch time: 48.41 s 
2025-01-29 09:12:11.836729:  
2025-01-29 09:12:11.839285: Epoch 449 
2025-01-29 09:12:11.841724: Current learning rate: 0.00585 
2025-01-29 09:13:00.141484: train_loss -0.8206 
2025-01-29 09:13:00.145640: val_loss -0.7436 
2025-01-29 09:13:00.154356: Pseudo dice [np.float32(0.9613), np.float32(0.8078)] 
2025-01-29 09:13:00.156782: Epoch time: 48.31 s 
2025-01-29 09:13:01.809071:  
2025-01-29 09:13:01.812107: Epoch 450 
2025-01-29 09:13:01.815141: Current learning rate: 0.00584 
2025-01-29 09:13:50.173357: train_loss -0.8185 
2025-01-29 09:13:50.179228: val_loss -0.7234 
2025-01-29 09:13:50.182289: Pseudo dice [np.float32(0.964), np.float32(0.8239)] 
2025-01-29 09:13:50.184894: Epoch time: 48.37 s 
2025-01-29 09:13:51.307139:  
2025-01-29 09:13:51.310022: Epoch 451 
2025-01-29 09:13:51.312591: Current learning rate: 0.00583 
2025-01-29 09:14:39.586971: train_loss -0.8127 
2025-01-29 09:14:39.591127: val_loss -0.7725 
2025-01-29 09:14:39.593709: Pseudo dice [np.float32(0.9654), np.float32(0.8742)] 
2025-01-29 09:14:39.596162: Epoch time: 48.28 s 
2025-01-29 09:14:40.716208:  
2025-01-29 09:14:40.719036: Epoch 452 
2025-01-29 09:14:40.721917: Current learning rate: 0.00582 
2025-01-29 09:15:29.155765: train_loss -0.7992 
2025-01-29 09:15:29.161380: val_loss -0.7703 
2025-01-29 09:15:29.169765: Pseudo dice [np.float32(0.9587), np.float32(0.8379)] 
2025-01-29 09:15:29.172353: Epoch time: 48.44 s 
2025-01-29 09:15:30.300204:  
2025-01-29 09:15:30.303028: Epoch 453 
2025-01-29 09:15:30.306037: Current learning rate: 0.00581 
2025-01-29 09:16:18.768156: train_loss -0.8148 
2025-01-29 09:16:18.772553: val_loss -0.7794 
2025-01-29 09:16:18.775343: Pseudo dice [np.float32(0.9616), np.float32(0.8777)] 
2025-01-29 09:16:18.778136: Epoch time: 48.47 s 
2025-01-29 09:16:20.483253:  
2025-01-29 09:16:20.486427: Epoch 454 
2025-01-29 09:16:20.489336: Current learning rate: 0.0058 
2025-01-29 09:17:09.130846: train_loss -0.8127 
2025-01-29 09:17:09.136516: val_loss -0.7248 
2025-01-29 09:17:09.139332: Pseudo dice [np.float32(0.956), np.float32(0.844)] 
2025-01-29 09:17:09.141892: Epoch time: 48.65 s 
2025-01-29 09:17:10.269043:  
2025-01-29 09:17:10.272227: Epoch 455 
2025-01-29 09:17:10.275385: Current learning rate: 0.00579 
2025-01-29 09:17:58.559532: train_loss -0.8174 
2025-01-29 09:17:58.563930: val_loss -0.8053 
2025-01-29 09:17:58.566852: Pseudo dice [np.float32(0.9684), np.float32(0.8827)] 
2025-01-29 09:17:58.569539: Epoch time: 48.29 s 
2025-01-29 09:17:59.695296:  
2025-01-29 09:17:59.698690: Epoch 456 
2025-01-29 09:17:59.701610: Current learning rate: 0.00578 
2025-01-29 09:18:47.682863: train_loss -0.8344 
2025-01-29 09:18:47.689100: val_loss -0.7602 
2025-01-29 09:18:47.697363: Pseudo dice [np.float32(0.9631), np.float32(0.8602)] 
2025-01-29 09:18:47.700161: Epoch time: 47.99 s 
2025-01-29 09:18:48.828105:  
2025-01-29 09:18:48.831364: Epoch 457 
2025-01-29 09:18:48.834058: Current learning rate: 0.00577 
2025-01-29 09:19:37.459619: train_loss -0.8197 
2025-01-29 09:19:37.465739: val_loss -0.778 
2025-01-29 09:19:37.468441: Pseudo dice [np.float32(0.9629), np.float32(0.8592)] 
2025-01-29 09:19:37.471232: Epoch time: 48.63 s 
2025-01-29 09:19:38.591943:  
2025-01-29 09:19:38.595059: Epoch 458 
2025-01-29 09:19:38.598239: Current learning rate: 0.00576 
2025-01-29 09:20:26.931072: train_loss -0.8251 
2025-01-29 09:20:26.936738: val_loss -0.7826 
2025-01-29 09:20:26.939267: Pseudo dice [np.float32(0.9654), np.float32(0.8426)] 
2025-01-29 09:20:26.941754: Epoch time: 48.34 s 
2025-01-29 09:20:28.064945:  
2025-01-29 09:20:28.067713: Epoch 459 
2025-01-29 09:20:28.070581: Current learning rate: 0.00575 
2025-01-29 09:21:16.336411: train_loss -0.8109 
2025-01-29 09:21:16.341954: val_loss -0.7945 
2025-01-29 09:21:16.344970: Pseudo dice [np.float32(0.9678), np.float32(0.8258)] 
2025-01-29 09:21:16.347756: Epoch time: 48.27 s 
2025-01-29 09:21:17.472690:  
2025-01-29 09:21:17.475992: Epoch 460 
2025-01-29 09:21:17.478898: Current learning rate: 0.00574 
2025-01-29 09:22:05.649881: train_loss -0.8117 
2025-01-29 09:22:05.655644: val_loss -0.7372 
2025-01-29 09:22:05.658256: Pseudo dice [np.float32(0.9614), np.float32(0.7999)] 
2025-01-29 09:22:05.660651: Epoch time: 48.18 s 
2025-01-29 09:22:06.772463:  
2025-01-29 09:22:06.775520: Epoch 461 
2025-01-29 09:22:06.778529: Current learning rate: 0.00573 
2025-01-29 09:22:54.842063: train_loss -0.8221 
2025-01-29 09:22:54.846251: val_loss -0.7649 
2025-01-29 09:22:54.854405: Pseudo dice [np.float32(0.9673), np.float32(0.8741)] 
2025-01-29 09:22:54.857105: Epoch time: 48.07 s 
2025-01-29 09:22:55.983076:  
2025-01-29 09:22:55.986291: Epoch 462 
2025-01-29 09:22:55.989365: Current learning rate: 0.00572 
2025-01-29 09:23:44.244833: train_loss -0.833 
2025-01-29 09:23:44.250865: val_loss -0.7797 
2025-01-29 09:23:44.253499: Pseudo dice [np.float32(0.9645), np.float32(0.865)] 
2025-01-29 09:23:44.255862: Epoch time: 48.26 s 
2025-01-29 09:23:45.384882:  
2025-01-29 09:23:45.387898: Epoch 463 
2025-01-29 09:23:45.390884: Current learning rate: 0.00571 
2025-01-29 09:24:33.900374: train_loss -0.8109 
2025-01-29 09:24:33.904482: val_loss -0.8014 
2025-01-29 09:24:33.907051: Pseudo dice [np.float32(0.9627), np.float32(0.883)] 
2025-01-29 09:24:33.909405: Epoch time: 48.52 s 
2025-01-29 09:24:35.035893:  
2025-01-29 09:24:35.038690: Epoch 464 
2025-01-29 09:24:35.041426: Current learning rate: 0.0057 
2025-01-29 09:25:23.195128: train_loss -0.7964 
2025-01-29 09:25:23.200490: val_loss -0.7633 
2025-01-29 09:25:23.203293: Pseudo dice [np.float32(0.9651), np.float32(0.8585)] 
2025-01-29 09:25:23.206164: Epoch time: 48.16 s 
2025-01-29 09:25:24.331057:  
2025-01-29 09:25:24.333976: Epoch 465 
2025-01-29 09:25:24.336607: Current learning rate: 0.0057 
2025-01-29 09:26:12.523224: train_loss -0.8273 
2025-01-29 09:26:12.527284: val_loss -0.7701 
2025-01-29 09:26:12.534372: Pseudo dice [np.float32(0.965), np.float32(0.8489)] 
2025-01-29 09:26:12.537205: Epoch time: 48.19 s 
2025-01-29 09:26:13.682693:  
2025-01-29 09:26:13.685795: Epoch 466 
2025-01-29 09:26:13.688852: Current learning rate: 0.00569 
2025-01-29 09:27:02.013911: train_loss -0.8091 
2025-01-29 09:27:02.020257: val_loss -0.7802 
2025-01-29 09:27:02.022897: Pseudo dice [np.float32(0.9641), np.float32(0.8801)] 
2025-01-29 09:27:02.025697: Epoch time: 48.33 s 
2025-01-29 09:27:03.154497:  
2025-01-29 09:27:03.157327: Epoch 467 
2025-01-29 09:27:03.160004: Current learning rate: 0.00568 
2025-01-29 09:27:51.344735: train_loss -0.8193 
2025-01-29 09:27:51.348548: val_loss -0.7712 
2025-01-29 09:27:51.351111: Pseudo dice [np.float32(0.9664), np.float32(0.828)] 
2025-01-29 09:27:51.353407: Epoch time: 48.19 s 
2025-01-29 09:27:52.492280:  
2025-01-29 09:27:52.495205: Epoch 468 
2025-01-29 09:27:52.497911: Current learning rate: 0.00567 
2025-01-29 09:28:40.902179: train_loss -0.8244 
2025-01-29 09:28:40.907275: val_loss -0.7805 
2025-01-29 09:28:40.909800: Pseudo dice [np.float32(0.9614), np.float32(0.8783)] 
2025-01-29 09:28:40.912336: Epoch time: 48.41 s 
2025-01-29 09:28:42.051647:  
2025-01-29 09:28:42.054454: Epoch 469 
2025-01-29 09:28:42.057109: Current learning rate: 0.00566 
2025-01-29 09:29:30.706403: train_loss -0.8065 
2025-01-29 09:29:30.710660: val_loss -0.7519 
2025-01-29 09:29:30.719805: Pseudo dice [np.float32(0.9618), np.float32(0.8225)] 
2025-01-29 09:29:30.722805: Epoch time: 48.66 s 
2025-01-29 09:29:31.860511:  
2025-01-29 09:29:31.863578: Epoch 470 
2025-01-29 09:29:31.866302: Current learning rate: 0.00565 
2025-01-29 09:30:20.414140: train_loss -0.8168 
2025-01-29 09:30:20.419897: val_loss -0.7683 
2025-01-29 09:30:20.422612: Pseudo dice [np.float32(0.9628), np.float32(0.8435)] 
2025-01-29 09:30:20.425173: Epoch time: 48.55 s 
2025-01-29 09:30:21.583374:  
2025-01-29 09:30:21.586658: Epoch 471 
2025-01-29 09:30:21.589394: Current learning rate: 0.00564 
2025-01-29 09:31:09.857007: train_loss -0.7971 
2025-01-29 09:31:09.862350: val_loss -0.7842 
2025-01-29 09:31:09.870562: Pseudo dice [np.float32(0.9636), np.float32(0.8388)] 
2025-01-29 09:31:09.873740: Epoch time: 48.27 s 
2025-01-29 09:31:10.995521:  
2025-01-29 09:31:10.998292: Epoch 472 
2025-01-29 09:31:11.001139: Current learning rate: 0.00563 
2025-01-29 09:31:59.016635: train_loss -0.795 
2025-01-29 09:31:59.024735: val_loss -0.7663 
2025-01-29 09:31:59.027390: Pseudo dice [np.float32(0.9631), np.float32(0.8544)] 
2025-01-29 09:31:59.030025: Epoch time: 48.02 s 
2025-01-29 09:32:00.824216:  
2025-01-29 09:32:00.827125: Epoch 473 
2025-01-29 09:32:00.829585: Current learning rate: 0.00562 
2025-01-29 09:32:48.979442: train_loss -0.8199 
2025-01-29 09:32:48.984243: val_loss -0.7702 
2025-01-29 09:32:48.987155: Pseudo dice [np.float32(0.9669), np.float32(0.8322)] 
2025-01-29 09:32:48.989815: Epoch time: 48.16 s 
2025-01-29 09:32:50.138446:  
2025-01-29 09:32:50.141637: Epoch 474 
2025-01-29 09:32:50.144646: Current learning rate: 0.00561 
2025-01-29 09:33:38.446806: train_loss -0.8055 
2025-01-29 09:33:38.452818: val_loss -0.7092 
2025-01-29 09:33:38.455905: Pseudo dice [np.float32(0.9626), np.float32(0.8089)] 
2025-01-29 09:33:38.458767: Epoch time: 48.31 s 
2025-01-29 09:33:39.590046:  
2025-01-29 09:33:39.593010: Epoch 475 
2025-01-29 09:33:39.595788: Current learning rate: 0.0056 
2025-01-29 09:34:28.151547: train_loss -0.8011 
2025-01-29 09:34:28.156023: val_loss -0.7832 
2025-01-29 09:34:28.164781: Pseudo dice [np.float32(0.9663), np.float32(0.8726)] 
2025-01-29 09:34:28.167565: Epoch time: 48.56 s 
2025-01-29 09:34:29.292560:  
2025-01-29 09:34:29.295336: Epoch 476 
2025-01-29 09:34:29.297940: Current learning rate: 0.00559 
2025-01-29 09:35:17.544084: train_loss -0.8124 
2025-01-29 09:35:17.551728: val_loss -0.8022 
2025-01-29 09:35:17.554451: Pseudo dice [np.float32(0.9665), np.float32(0.8841)] 
2025-01-29 09:35:17.557079: Epoch time: 48.25 s 
2025-01-29 09:35:18.678782:  
2025-01-29 09:35:18.682036: Epoch 477 
2025-01-29 09:35:18.685069: Current learning rate: 0.00558 
2025-01-29 09:36:06.767704: train_loss -0.799 
2025-01-29 09:36:06.772704: val_loss -0.7607 
2025-01-29 09:36:06.775594: Pseudo dice [np.float32(0.9672), np.float32(0.8782)] 
2025-01-29 09:36:06.778445: Epoch time: 48.09 s 
2025-01-29 09:36:07.954452:  
2025-01-29 09:36:07.957783: Epoch 478 
2025-01-29 09:36:07.960727: Current learning rate: 0.00557 
2025-01-29 09:36:56.451716: train_loss -0.8085 
2025-01-29 09:36:56.457543: val_loss -0.7578 
2025-01-29 09:36:56.460261: Pseudo dice [np.float32(0.9648), np.float32(0.8228)] 
2025-01-29 09:36:56.462931: Epoch time: 48.5 s 
2025-01-29 09:36:57.598081:  
2025-01-29 09:36:57.601238: Epoch 479 
2025-01-29 09:36:57.604062: Current learning rate: 0.00556 
2025-01-29 09:37:46.364830: train_loss -0.824 
2025-01-29 09:37:46.369011: val_loss -0.7614 
2025-01-29 09:37:46.371768: Pseudo dice [np.float32(0.965), np.float32(0.8575)] 
2025-01-29 09:37:46.374430: Epoch time: 48.77 s 
2025-01-29 09:37:47.547425:  
2025-01-29 09:37:47.550411: Epoch 480 
2025-01-29 09:37:47.553316: Current learning rate: 0.00555 
2025-01-29 09:38:36.066126: train_loss -0.7978 
2025-01-29 09:38:36.074021: val_loss -0.7643 
2025-01-29 09:38:36.076770: Pseudo dice [np.float32(0.9621), np.float32(0.8276)] 
2025-01-29 09:38:36.079443: Epoch time: 48.52 s 
2025-01-29 09:38:37.252925:  
2025-01-29 09:38:37.255882: Epoch 481 
2025-01-29 09:38:37.258598: Current learning rate: 0.00554 
2025-01-29 09:39:26.377868: train_loss -0.8207 
2025-01-29 09:39:26.381991: val_loss -0.7993 
2025-01-29 09:39:26.384644: Pseudo dice [np.float32(0.9686), np.float32(0.8909)] 
2025-01-29 09:39:26.387259: Epoch time: 49.13 s 
2025-01-29 09:39:27.520277:  
2025-01-29 09:39:27.523065: Epoch 482 
2025-01-29 09:39:27.525985: Current learning rate: 0.00553 
2025-01-29 09:40:16.327895: train_loss -0.8066 
2025-01-29 09:40:16.334051: val_loss -0.7928 
2025-01-29 09:40:16.336864: Pseudo dice [np.float32(0.9623), np.float32(0.8803)] 
2025-01-29 09:40:16.339552: Epoch time: 48.81 s 
2025-01-29 09:40:17.474523:  
2025-01-29 09:40:17.477551: Epoch 483 
2025-01-29 09:40:17.480556: Current learning rate: 0.00552 
2025-01-29 09:41:06.344452: train_loss -0.7924 
2025-01-29 09:41:06.349734: val_loss -0.7648 
2025-01-29 09:41:06.360041: Pseudo dice [np.float32(0.9604), np.float32(0.8572)] 
2025-01-29 09:41:06.363410: Epoch time: 48.87 s 
2025-01-29 09:41:07.492225:  
2025-01-29 09:41:07.495259: Epoch 484 
2025-01-29 09:41:07.498263: Current learning rate: 0.00551 
2025-01-29 09:41:56.202863: train_loss -0.8021 
2025-01-29 09:41:56.208623: val_loss -0.7291 
2025-01-29 09:41:56.211319: Pseudo dice [np.float32(0.9625), np.float32(0.8319)] 
2025-01-29 09:41:56.213761: Epoch time: 48.71 s 
2025-01-29 09:41:57.343158:  
2025-01-29 09:41:57.346059: Epoch 485 
2025-01-29 09:41:57.348798: Current learning rate: 0.0055 
2025-01-29 09:42:46.168969: train_loss -0.817 
2025-01-29 09:42:46.173359: val_loss -0.8085 
2025-01-29 09:42:46.176380: Pseudo dice [np.float32(0.9645), np.float32(0.8736)] 
2025-01-29 09:42:46.179065: Epoch time: 48.83 s 
2025-01-29 09:42:47.330895:  
2025-01-29 09:42:47.334068: Epoch 486 
2025-01-29 09:42:47.336788: Current learning rate: 0.00549 
2025-01-29 09:43:35.817299: train_loss -0.8109 
2025-01-29 09:43:35.822977: val_loss -0.7249 
2025-01-29 09:43:35.825685: Pseudo dice [np.float32(0.9662), np.float32(0.8662)] 
2025-01-29 09:43:35.828542: Epoch time: 48.49 s 
2025-01-29 09:43:37.002482:  
2025-01-29 09:43:37.005874: Epoch 487 
2025-01-29 09:43:37.008907: Current learning rate: 0.00548 
2025-01-29 09:44:25.416085: train_loss -0.8074 
2025-01-29 09:44:25.422563: val_loss -0.7426 
2025-01-29 09:44:25.425471: Pseudo dice [np.float32(0.9637), np.float32(0.8403)] 
2025-01-29 09:44:25.428586: Epoch time: 48.41 s 
2025-01-29 09:44:26.565687:  
2025-01-29 09:44:26.568697: Epoch 488 
2025-01-29 09:44:26.571704: Current learning rate: 0.00547 
2025-01-29 09:45:15.036602: train_loss -0.8016 
2025-01-29 09:45:15.046365: val_loss -0.7483 
2025-01-29 09:45:15.055053: Pseudo dice [np.float32(0.9561), np.float32(0.8564)] 
2025-01-29 09:45:15.057713: Epoch time: 48.47 s 
2025-01-29 09:45:16.263992:  
2025-01-29 09:45:16.267096: Epoch 489 
2025-01-29 09:45:16.270164: Current learning rate: 0.00546 
2025-01-29 09:46:04.661030: train_loss -0.7968 
2025-01-29 09:46:04.665800: val_loss -0.7898 
2025-01-29 09:46:04.668775: Pseudo dice [np.float32(0.9624), np.float32(0.8719)] 
2025-01-29 09:46:04.671450: Epoch time: 48.4 s 
2025-01-29 09:46:05.812369:  
2025-01-29 09:46:05.815372: Epoch 490 
2025-01-29 09:46:05.818118: Current learning rate: 0.00546 
2025-01-29 09:46:54.172678: train_loss -0.8105 
2025-01-29 09:46:54.178509: val_loss -0.8015 
2025-01-29 09:46:54.181362: Pseudo dice [np.float32(0.9659), np.float32(0.8832)] 
2025-01-29 09:46:54.184180: Epoch time: 48.36 s 
2025-01-29 09:46:55.314806:  
2025-01-29 09:46:55.317768: Epoch 491 
2025-01-29 09:46:55.320481: Current learning rate: 0.00545 
2025-01-29 09:47:43.599811: train_loss -0.8209 
2025-01-29 09:47:43.604474: val_loss -0.7901 
2025-01-29 09:47:43.607373: Pseudo dice [np.float32(0.964), np.float32(0.8646)] 
2025-01-29 09:47:43.610318: Epoch time: 48.29 s 
2025-01-29 09:47:45.402656:  
2025-01-29 09:47:45.405480: Epoch 492 
2025-01-29 09:47:45.408236: Current learning rate: 0.00544 
2025-01-29 09:48:33.872362: train_loss -0.8038 
2025-01-29 09:48:33.880185: val_loss -0.7992 
2025-01-29 09:48:33.888125: Pseudo dice [np.float32(0.9667), np.float32(0.8714)] 
2025-01-29 09:48:33.890889: Epoch time: 48.47 s 
2025-01-29 09:48:35.030574:  
2025-01-29 09:48:35.033566: Epoch 493 
2025-01-29 09:48:35.036145: Current learning rate: 0.00543 
2025-01-29 09:49:23.565893: train_loss -0.8149 
2025-01-29 09:49:23.570642: val_loss -0.7583 
2025-01-29 09:49:23.573616: Pseudo dice [np.float32(0.961), np.float32(0.8634)] 
2025-01-29 09:49:23.576336: Epoch time: 48.54 s 
2025-01-29 09:49:24.707704:  
2025-01-29 09:49:24.710902: Epoch 494 
2025-01-29 09:49:24.713572: Current learning rate: 0.00542 
2025-01-29 09:50:13.363616: train_loss -0.8134 
2025-01-29 09:50:13.368937: val_loss -0.822 
2025-01-29 09:50:13.371647: Pseudo dice [np.float32(0.9638), np.float32(0.8806)] 
2025-01-29 09:50:13.374000: Epoch time: 48.66 s 
2025-01-29 09:50:14.499736:  
2025-01-29 09:50:14.502329: Epoch 495 
2025-01-29 09:50:14.505113: Current learning rate: 0.00541 
2025-01-29 09:51:03.504821: train_loss -0.8086 
2025-01-29 09:51:03.510864: val_loss -0.7313 
2025-01-29 09:51:03.514010: Pseudo dice [np.float32(0.9647), np.float32(0.8332)] 
2025-01-29 09:51:03.516655: Epoch time: 49.01 s 
2025-01-29 09:51:04.651044:  
2025-01-29 09:51:04.654118: Epoch 496 
2025-01-29 09:51:04.656869: Current learning rate: 0.0054 
2025-01-29 09:51:53.411684: train_loss -0.8187 
2025-01-29 09:51:53.417113: val_loss -0.7584 
2025-01-29 09:51:53.425198: Pseudo dice [np.float32(0.9657), np.float32(0.8555)] 
2025-01-29 09:51:53.427906: Epoch time: 48.76 s 
2025-01-29 09:51:54.560208:  
2025-01-29 09:51:54.563116: Epoch 497 
2025-01-29 09:51:54.566447: Current learning rate: 0.00539 
2025-01-29 09:52:43.308689: train_loss -0.8141 
2025-01-29 09:52:43.313253: val_loss -0.8089 
2025-01-29 09:52:43.316324: Pseudo dice [np.float32(0.9639), np.float32(0.8877)] 
2025-01-29 09:52:43.318874: Epoch time: 48.75 s 
2025-01-29 09:52:44.464352:  
2025-01-29 09:52:44.466972: Epoch 498 
2025-01-29 09:52:44.469409: Current learning rate: 0.00538 
2025-01-29 09:53:33.141223: train_loss -0.8165 
2025-01-29 09:53:33.147344: val_loss -0.7961 
2025-01-29 09:53:33.156419: Pseudo dice [np.float32(0.9637), np.float32(0.8443)] 
2025-01-29 09:53:33.159487: Epoch time: 48.68 s 
2025-01-29 09:53:34.286367:  
2025-01-29 09:53:34.289543: Epoch 499 
2025-01-29 09:53:34.292653: Current learning rate: 0.00537 
2025-01-29 09:54:23.160632: train_loss -0.8069 
2025-01-29 09:54:23.164891: val_loss -0.7489 
2025-01-29 09:54:23.167735: Pseudo dice [np.float32(0.9646), np.float32(0.8251)] 
2025-01-29 09:54:23.170695: Epoch time: 48.88 s 
2025-01-29 09:54:24.827419:  
2025-01-29 09:54:24.830272: Epoch 500 
2025-01-29 09:54:24.833173: Current learning rate: 0.00536 
2025-01-29 09:55:13.656919: train_loss -0.8094 
2025-01-29 09:55:13.662439: val_loss -0.7881 
2025-01-29 09:55:13.665010: Pseudo dice [np.float32(0.9654), np.float32(0.8982)] 
2025-01-29 09:55:13.667600: Epoch time: 48.83 s 
2025-01-29 09:55:14.803653:  
2025-01-29 09:55:14.806576: Epoch 501 
2025-01-29 09:55:14.809216: Current learning rate: 0.00535 
2025-01-29 09:56:03.488394: train_loss -0.8031 
2025-01-29 09:56:03.492969: val_loss -0.7654 
2025-01-29 09:56:03.495774: Pseudo dice [np.float32(0.9585), np.float32(0.8119)] 
2025-01-29 09:56:03.498711: Epoch time: 48.69 s 
2025-01-29 09:56:04.640995:  
2025-01-29 09:56:04.644055: Epoch 502 
2025-01-29 09:56:04.646754: Current learning rate: 0.00534 
2025-01-29 09:56:53.329849: train_loss -0.8107 
2025-01-29 09:56:53.337677: val_loss -0.7865 
2025-01-29 09:56:53.351955: Pseudo dice [np.float32(0.9579), np.float32(0.8694)] 
2025-01-29 09:56:53.355056: Epoch time: 48.69 s 
2025-01-29 09:56:54.527037:  
2025-01-29 09:56:54.530401: Epoch 503 
2025-01-29 09:56:54.533543: Current learning rate: 0.00533 
2025-01-29 09:57:43.405749: train_loss -0.7863 
2025-01-29 09:57:43.409585: val_loss -0.8039 
2025-01-29 09:57:43.412347: Pseudo dice [np.float32(0.9634), np.float32(0.8704)] 
2025-01-29 09:57:43.414930: Epoch time: 48.88 s 
2025-01-29 09:57:44.632036:  
2025-01-29 09:57:44.635474: Epoch 504 
2025-01-29 09:57:44.638819: Current learning rate: 0.00532 
2025-01-29 09:58:33.595746: train_loss -0.8057 
2025-01-29 09:58:33.601699: val_loss -0.7558 
2025-01-29 09:58:33.604338: Pseudo dice [np.float32(0.9634), np.float32(0.8749)] 
2025-01-29 09:58:33.607043: Epoch time: 48.96 s 
2025-01-29 09:58:34.776287:  
2025-01-29 09:58:34.779006: Epoch 505 
2025-01-29 09:58:34.781644: Current learning rate: 0.00531 
2025-01-29 09:59:23.537909: train_loss -0.8365 
2025-01-29 09:59:23.541738: val_loss -0.7998 
2025-01-29 09:59:23.544355: Pseudo dice [np.float32(0.9698), np.float32(0.9007)] 
2025-01-29 09:59:23.546569: Epoch time: 48.76 s 
2025-01-29 09:59:24.680856:  
2025-01-29 09:59:24.683457: Epoch 506 
2025-01-29 09:59:24.685869: Current learning rate: 0.0053 
2025-01-29 10:00:13.269624: train_loss -0.8244 
2025-01-29 10:00:13.276642: val_loss -0.7549 
2025-01-29 10:00:13.286581: Pseudo dice [np.float32(0.9657), np.float32(0.8222)] 
2025-01-29 10:00:13.289753: Epoch time: 48.59 s 
2025-01-29 10:00:14.430533:  
2025-01-29 10:00:14.433537: Epoch 507 
2025-01-29 10:00:14.436235: Current learning rate: 0.00529 
2025-01-29 10:01:02.946470: train_loss -0.8179 
2025-01-29 10:01:02.950989: val_loss -0.7958 
2025-01-29 10:01:02.954063: Pseudo dice [np.float32(0.9682), np.float32(0.8813)] 
2025-01-29 10:01:02.957067: Epoch time: 48.52 s 
2025-01-29 10:01:04.128679:  
2025-01-29 10:01:04.131804: Epoch 508 
2025-01-29 10:01:04.134673: Current learning rate: 0.00528 
2025-01-29 10:01:52.268187: train_loss -0.8314 
2025-01-29 10:01:52.273520: val_loss -0.7619 
2025-01-29 10:01:52.275820: Pseudo dice [np.float32(0.9685), np.float32(0.848)] 
2025-01-29 10:01:52.278093: Epoch time: 48.14 s 
2025-01-29 10:01:53.417189:  
2025-01-29 10:01:53.420010: Epoch 509 
2025-01-29 10:01:53.422768: Current learning rate: 0.00527 
2025-01-29 10:02:41.887348: train_loss -0.8209 
2025-01-29 10:02:41.891733: val_loss -0.7825 
2025-01-29 10:02:41.894522: Pseudo dice [np.float32(0.9658), np.float32(0.8594)] 
2025-01-29 10:02:41.897315: Epoch time: 48.47 s 
2025-01-29 10:02:43.029018:  
2025-01-29 10:02:43.031670: Epoch 510 
2025-01-29 10:02:43.034184: Current learning rate: 0.00526 
2025-01-29 10:03:31.320297: train_loss -0.8195 
2025-01-29 10:03:31.325822: val_loss -0.7775 
2025-01-29 10:03:31.335433: Pseudo dice [np.float32(0.9651), np.float32(0.8646)] 
2025-01-29 10:03:31.338057: Epoch time: 48.29 s 
2025-01-29 10:03:33.031845:  
2025-01-29 10:03:33.034894: Epoch 511 
2025-01-29 10:03:33.037660: Current learning rate: 0.00525 
2025-01-29 10:04:21.624207: train_loss -0.8326 
2025-01-29 10:04:21.628626: val_loss -0.7908 
2025-01-29 10:04:21.631485: Pseudo dice [np.float32(0.9601), np.float32(0.8427)] 
2025-01-29 10:04:21.634229: Epoch time: 48.59 s 
2025-01-29 10:04:22.771240:  
2025-01-29 10:04:22.774678: Epoch 512 
2025-01-29 10:04:22.777965: Current learning rate: 0.00524 
2025-01-29 10:05:11.043323: train_loss -0.8233 
2025-01-29 10:05:11.050586: val_loss -0.7068 
2025-01-29 10:05:11.053416: Pseudo dice [np.float32(0.9615), np.float32(0.8085)] 
2025-01-29 10:05:11.056254: Epoch time: 48.27 s 
2025-01-29 10:05:12.227972:  
2025-01-29 10:05:12.231182: Epoch 513 
2025-01-29 10:05:12.234081: Current learning rate: 0.00523 
2025-01-29 10:06:00.569829: train_loss -0.8276 
2025-01-29 10:06:00.573709: val_loss -0.7489 
2025-01-29 10:06:00.576444: Pseudo dice [np.float32(0.9601), np.float32(0.8351)] 
2025-01-29 10:06:00.579047: Epoch time: 48.34 s 
2025-01-29 10:06:01.717446:  
2025-01-29 10:06:01.721689: Epoch 514 
2025-01-29 10:06:01.724687: Current learning rate: 0.00522 
2025-01-29 10:06:49.905748: train_loss -0.8206 
2025-01-29 10:06:49.911427: val_loss -0.7771 
2025-01-29 10:06:49.913978: Pseudo dice [np.float32(0.9663), np.float32(0.8471)] 
2025-01-29 10:06:49.916517: Epoch time: 48.19 s 
2025-01-29 10:06:51.057768:  
2025-01-29 10:06:51.061298: Epoch 515 
2025-01-29 10:06:51.064267: Current learning rate: 0.00521 
2025-01-29 10:07:39.207750: train_loss -0.8419 
2025-01-29 10:07:39.215482: val_loss -0.7429 
2025-01-29 10:07:39.224138: Pseudo dice [np.float32(0.9601), np.float32(0.867)] 
2025-01-29 10:07:39.226950: Epoch time: 48.15 s 
2025-01-29 10:07:40.400312:  
2025-01-29 10:07:40.403886: Epoch 516 
2025-01-29 10:07:40.407269: Current learning rate: 0.0052 
2025-01-29 10:08:28.652047: train_loss -0.8203 
2025-01-29 10:08:28.657909: val_loss -0.8125 
2025-01-29 10:08:28.660769: Pseudo dice [np.float32(0.9653), np.float32(0.8936)] 
2025-01-29 10:08:28.663438: Epoch time: 48.25 s 
2025-01-29 10:08:29.807202:  
2025-01-29 10:08:29.810200: Epoch 517 
2025-01-29 10:08:29.813149: Current learning rate: 0.00519 
2025-01-29 10:09:17.873352: train_loss -0.8311 
2025-01-29 10:09:17.877030: val_loss -0.7459 
2025-01-29 10:09:17.879776: Pseudo dice [np.float32(0.9614), np.float32(0.8198)] 
2025-01-29 10:09:17.882177: Epoch time: 48.07 s 
2025-01-29 10:09:19.014519:  
2025-01-29 10:09:19.017092: Epoch 518 
2025-01-29 10:09:19.019892: Current learning rate: 0.00518 
2025-01-29 10:10:07.395114: train_loss -0.8364 
2025-01-29 10:10:07.420908: val_loss -0.7771 
2025-01-29 10:10:07.424852: Pseudo dice [np.float32(0.965), np.float32(0.8768)] 
2025-01-29 10:10:07.427568: Epoch time: 48.38 s 
2025-01-29 10:10:08.566622:  
2025-01-29 10:10:08.569709: Epoch 519 
2025-01-29 10:10:08.572652: Current learning rate: 0.00518 
2025-01-29 10:10:57.206053: train_loss -0.8219 
2025-01-29 10:10:57.210518: val_loss -0.792 
2025-01-29 10:10:57.220352: Pseudo dice [np.float32(0.9632), np.float32(0.8428)] 
2025-01-29 10:10:57.223040: Epoch time: 48.64 s 
2025-01-29 10:10:58.356756:  
2025-01-29 10:10:58.360048: Epoch 520 
2025-01-29 10:10:58.363070: Current learning rate: 0.00517 
2025-01-29 10:11:46.890687: train_loss -0.8293 
2025-01-29 10:11:46.896253: val_loss -0.7832 
2025-01-29 10:11:46.899079: Pseudo dice [np.float32(0.9708), np.float32(0.8997)] 
2025-01-29 10:11:46.901613: Epoch time: 48.53 s 
2025-01-29 10:11:48.077871:  
2025-01-29 10:11:48.080866: Epoch 521 
2025-01-29 10:11:48.084033: Current learning rate: 0.00516 
2025-01-29 10:12:36.362011: train_loss -0.8344 
2025-01-29 10:12:36.366150: val_loss -0.7937 
2025-01-29 10:12:36.369101: Pseudo dice [np.float32(0.9668), np.float32(0.8874)] 
2025-01-29 10:12:36.371715: Epoch time: 48.29 s 
2025-01-29 10:12:37.509361:  
2025-01-29 10:12:37.512733: Epoch 522 
2025-01-29 10:12:37.515800: Current learning rate: 0.00515 
2025-01-29 10:13:25.834639: train_loss -0.8335 
2025-01-29 10:13:25.840690: val_loss -0.788 
2025-01-29 10:13:25.843694: Pseudo dice [np.float32(0.9667), np.float32(0.8997)] 
2025-01-29 10:13:25.846382: Epoch time: 48.33 s 
2025-01-29 10:13:26.978393:  
2025-01-29 10:13:26.981403: Epoch 523 
2025-01-29 10:13:26.984333: Current learning rate: 0.00514 
2025-01-29 10:14:15.535165: train_loss -0.8111 
2025-01-29 10:14:15.541564: val_loss -0.7712 
2025-01-29 10:14:15.550885: Pseudo dice [np.float32(0.9604), np.float32(0.8884)] 
2025-01-29 10:14:15.553818: Epoch time: 48.56 s 
2025-01-29 10:14:16.692752:  
2025-01-29 10:14:16.695855: Epoch 524 
2025-01-29 10:14:16.698813: Current learning rate: 0.00513 
2025-01-29 10:15:05.157872: train_loss -0.8154 
2025-01-29 10:15:05.163733: val_loss -0.8254 
2025-01-29 10:15:05.166257: Pseudo dice [np.float32(0.9638), np.float32(0.8647)] 
2025-01-29 10:15:05.169057: Epoch time: 48.47 s 
2025-01-29 10:15:06.306935:  
2025-01-29 10:15:06.309858: Epoch 525 
2025-01-29 10:15:06.312912: Current learning rate: 0.00512 
2025-01-29 10:15:54.991286: train_loss -0.8146 
2025-01-29 10:15:54.995629: val_loss -0.764 
2025-01-29 10:15:55.004888: Pseudo dice [np.float32(0.9652), np.float32(0.8661)] 
2025-01-29 10:15:55.007901: Epoch time: 48.69 s 
2025-01-29 10:15:56.145805:  
2025-01-29 10:15:56.148967: Epoch 526 
2025-01-29 10:15:56.152127: Current learning rate: 0.00511 
2025-01-29 10:16:44.464850: train_loss -0.8368 
2025-01-29 10:16:44.470554: val_loss -0.7727 
2025-01-29 10:16:44.473518: Pseudo dice [np.float32(0.9635), np.float32(0.8679)] 
2025-01-29 10:16:44.476188: Epoch time: 48.32 s 
2025-01-29 10:16:45.659698:  
2025-01-29 10:16:45.662827: Epoch 527 
2025-01-29 10:16:45.666050: Current learning rate: 0.0051 
2025-01-29 10:17:34.059122: train_loss -0.8159 
2025-01-29 10:17:34.063121: val_loss -0.7606 
2025-01-29 10:17:34.065697: Pseudo dice [np.float32(0.9619), np.float32(0.8705)] 
2025-01-29 10:17:34.068364: Epoch time: 48.4 s 
2025-01-29 10:17:35.214075:  
2025-01-29 10:17:35.217174: Epoch 528 
2025-01-29 10:17:35.220510: Current learning rate: 0.00509 
2025-01-29 10:18:23.862879: train_loss -0.8242 
2025-01-29 10:18:23.868594: val_loss -0.7988 
2025-01-29 10:18:23.871423: Pseudo dice [np.float32(0.9624), np.float32(0.8599)] 
2025-01-29 10:18:23.873983: Epoch time: 48.65 s 
2025-01-29 10:18:25.055441:  
2025-01-29 10:18:25.058287: Epoch 529 
2025-01-29 10:18:25.061054: Current learning rate: 0.00508 
2025-01-29 10:19:13.812234: train_loss -0.8365 
2025-01-29 10:19:13.816257: val_loss -0.7791 
2025-01-29 10:19:13.819203: Pseudo dice [np.float32(0.9623), np.float32(0.8594)] 
2025-01-29 10:19:13.821567: Epoch time: 48.76 s 
2025-01-29 10:19:15.563724:  
2025-01-29 10:19:15.566401: Epoch 530 
2025-01-29 10:19:15.568966: Current learning rate: 0.00507 
2025-01-29 10:20:04.268259: train_loss -0.8242 
2025-01-29 10:20:04.276096: val_loss -0.811 
2025-01-29 10:20:04.284197: Pseudo dice [np.float32(0.9609), np.float32(0.8728)] 
2025-01-29 10:20:04.287120: Epoch time: 48.71 s 
2025-01-29 10:20:05.449993:  
2025-01-29 10:20:05.452873: Epoch 531 
2025-01-29 10:20:05.455646: Current learning rate: 0.00506 
2025-01-29 10:20:53.708804: train_loss -0.8194 
2025-01-29 10:20:53.715227: val_loss -0.7671 
2025-01-29 10:20:53.718212: Pseudo dice [np.float32(0.9594), np.float32(0.8379)] 
2025-01-29 10:20:53.721166: Epoch time: 48.26 s 
2025-01-29 10:20:54.864184:  
2025-01-29 10:20:54.867298: Epoch 532 
2025-01-29 10:20:54.869969: Current learning rate: 0.00505 
2025-01-29 10:21:43.248230: train_loss -0.811 
2025-01-29 10:21:43.254025: val_loss -0.7972 
2025-01-29 10:21:43.256698: Pseudo dice [np.float32(0.9609), np.float32(0.8949)] 
2025-01-29 10:21:43.259459: Epoch time: 48.38 s 
2025-01-29 10:21:44.439099:  
2025-01-29 10:21:44.442218: Epoch 533 
2025-01-29 10:21:44.445322: Current learning rate: 0.00504 
2025-01-29 10:22:33.077751: train_loss -0.8285 
2025-01-29 10:22:33.084707: val_loss -0.8033 
2025-01-29 10:22:33.087691: Pseudo dice [np.float32(0.9614), np.float32(0.8612)] 
2025-01-29 10:22:33.090177: Epoch time: 48.64 s 
2025-01-29 10:22:34.226084:  
2025-01-29 10:22:34.229037: Epoch 534 
2025-01-29 10:22:34.231980: Current learning rate: 0.00503 
2025-01-29 10:23:23.065105: train_loss -0.81 
2025-01-29 10:23:23.070879: val_loss -0.7333 
2025-01-29 10:23:23.080216: Pseudo dice [np.float32(0.9647), np.float32(0.8337)] 
2025-01-29 10:23:23.082935: Epoch time: 48.84 s 
2025-01-29 10:23:24.240558:  
2025-01-29 10:23:24.243683: Epoch 535 
2025-01-29 10:23:24.246364: Current learning rate: 0.00502 
2025-01-29 10:24:12.673478: train_loss -0.8226 
2025-01-29 10:24:12.677566: val_loss -0.8019 
2025-01-29 10:24:12.680254: Pseudo dice [np.float32(0.9632), np.float32(0.8876)] 
2025-01-29 10:24:12.682960: Epoch time: 48.43 s 
2025-01-29 10:24:13.821959:  
2025-01-29 10:24:13.824873: Epoch 536 
2025-01-29 10:24:13.827880: Current learning rate: 0.00501 
2025-01-29 10:25:01.850361: train_loss -0.8113 
2025-01-29 10:25:01.856452: val_loss -0.7086 
2025-01-29 10:25:01.859322: Pseudo dice [np.float32(0.9615), np.float32(0.5918)] 
2025-01-29 10:25:01.861797: Epoch time: 48.03 s 
2025-01-29 10:25:03.005774:  
2025-01-29 10:25:03.009616: Epoch 537 
2025-01-29 10:25:03.012129: Current learning rate: 0.005 
2025-01-29 10:25:51.585083: train_loss -0.804 
2025-01-29 10:25:51.591013: val_loss -0.7729 
2025-01-29 10:25:51.593957: Pseudo dice [np.float32(0.9617), np.float32(0.8791)] 
2025-01-29 10:25:51.596697: Epoch time: 48.58 s 
2025-01-29 10:25:52.769373:  
2025-01-29 10:25:52.772680: Epoch 538 
2025-01-29 10:25:52.775772: Current learning rate: 0.00499 
2025-01-29 10:26:41.085251: train_loss -0.8097 
2025-01-29 10:26:41.091785: val_loss -0.8232 
2025-01-29 10:26:41.094160: Pseudo dice [np.float32(0.9603), np.float32(0.8915)] 
2025-01-29 10:26:41.096571: Epoch time: 48.32 s 
2025-01-29 10:26:42.241145:  
2025-01-29 10:26:42.244059: Epoch 539 
2025-01-29 10:26:42.246685: Current learning rate: 0.00498 
2025-01-29 10:27:30.665440: train_loss -0.815 
2025-01-29 10:27:30.669580: val_loss -0.7392 
2025-01-29 10:27:30.672295: Pseudo dice [np.float32(0.9573), np.float32(0.7908)] 
2025-01-29 10:27:30.675044: Epoch time: 48.43 s 
2025-01-29 10:27:31.855796:  
2025-01-29 10:27:31.858917: Epoch 540 
2025-01-29 10:27:31.861620: Current learning rate: 0.00497 
2025-01-29 10:28:20.187459: train_loss -0.8253 
2025-01-29 10:28:20.193088: val_loss -0.7495 
2025-01-29 10:28:20.195676: Pseudo dice [np.float32(0.9695), np.float32(0.8808)] 
2025-01-29 10:28:20.198065: Epoch time: 48.33 s 
2025-01-29 10:28:21.333597:  
2025-01-29 10:28:21.336358: Epoch 541 
2025-01-29 10:28:21.338968: Current learning rate: 0.00496 
2025-01-29 10:29:09.623066: train_loss -0.8183 
2025-01-29 10:29:09.627741: val_loss -0.7287 
2025-01-29 10:29:09.630939: Pseudo dice [np.float32(0.9647), np.float32(0.8102)] 
2025-01-29 10:29:09.633757: Epoch time: 48.29 s 
2025-01-29 10:29:10.767739:  
2025-01-29 10:29:10.770532: Epoch 542 
2025-01-29 10:29:10.773033: Current learning rate: 0.00495 
2025-01-29 10:29:59.517463: train_loss -0.8149 
2025-01-29 10:29:59.530281: val_loss -0.7474 
2025-01-29 10:29:59.533460: Pseudo dice [np.float32(0.9554), np.float32(0.8642)] 
2025-01-29 10:29:59.536536: Epoch time: 48.75 s 
2025-01-29 10:30:00.675310:  
2025-01-29 10:30:00.678513: Epoch 543 
2025-01-29 10:30:00.681462: Current learning rate: 0.00494 
2025-01-29 10:30:49.031338: train_loss -0.8147 
2025-01-29 10:30:49.035822: val_loss -0.763 
2025-01-29 10:30:49.038334: Pseudo dice [np.float32(0.9595), np.float32(0.8512)] 
2025-01-29 10:30:49.040831: Epoch time: 48.36 s 
2025-01-29 10:30:50.185618:  
2025-01-29 10:30:50.188560: Epoch 544 
2025-01-29 10:30:50.191258: Current learning rate: 0.00493 
2025-01-29 10:31:38.484409: train_loss -0.7925 
2025-01-29 10:31:38.489708: val_loss -0.7546 
2025-01-29 10:31:38.492340: Pseudo dice [np.float32(0.964), np.float32(0.8359)] 
2025-01-29 10:31:38.494619: Epoch time: 48.3 s 
2025-01-29 10:31:39.667773:  
2025-01-29 10:31:39.671256: Epoch 545 
2025-01-29 10:31:39.674220: Current learning rate: 0.00492 
2025-01-29 10:32:28.012744: train_loss -0.8306 
2025-01-29 10:32:28.016709: val_loss -0.7452 
2025-01-29 10:32:28.019597: Pseudo dice [np.float32(0.9569), np.float32(0.8066)] 
2025-01-29 10:32:28.022123: Epoch time: 48.35 s 
2025-01-29 10:32:29.162222:  
2025-01-29 10:32:29.165814: Epoch 546 
2025-01-29 10:32:29.168752: Current learning rate: 0.00491 
2025-01-29 10:33:17.555082: train_loss -0.8078 
2025-01-29 10:33:17.564517: val_loss -0.7754 
2025-01-29 10:33:17.574663: Pseudo dice [np.float32(0.9674), np.float32(0.8663)] 
2025-01-29 10:33:17.577559: Epoch time: 48.39 s 
2025-01-29 10:33:18.708351:  
2025-01-29 10:33:18.710954: Epoch 547 
2025-01-29 10:33:18.713603: Current learning rate: 0.0049 
2025-01-29 10:34:07.174178: train_loss -0.8131 
2025-01-29 10:34:07.178519: val_loss -0.7342 
2025-01-29 10:34:07.181514: Pseudo dice [np.float32(0.9646), np.float32(0.823)] 
2025-01-29 10:34:07.184125: Epoch time: 48.47 s 
2025-01-29 10:34:08.313043:  
2025-01-29 10:34:08.315959: Epoch 548 
2025-01-29 10:34:08.318674: Current learning rate: 0.00489 
2025-01-29 10:34:56.600277: train_loss -0.8025 
2025-01-29 10:34:56.606116: val_loss -0.8026 
2025-01-29 10:34:56.613729: Pseudo dice [np.float32(0.9629), np.float32(0.8674)] 
2025-01-29 10:34:56.616760: Epoch time: 48.29 s 
2025-01-29 10:34:58.341258:  
2025-01-29 10:34:58.344565: Epoch 549 
2025-01-29 10:34:58.347334: Current learning rate: 0.00488 
2025-01-29 10:35:46.735671: train_loss -0.8109 
2025-01-29 10:35:46.740131: val_loss -0.7825 
2025-01-29 10:35:46.742844: Pseudo dice [np.float32(0.9607), np.float32(0.8768)] 
2025-01-29 10:35:46.745210: Epoch time: 48.4 s 
2025-01-29 10:35:48.427473:  
2025-01-29 10:35:48.430676: Epoch 550 
2025-01-29 10:35:48.433471: Current learning rate: 0.00487 
2025-01-29 10:36:37.161128: train_loss -0.822 
2025-01-29 10:36:37.167319: val_loss -0.7922 
2025-01-29 10:36:37.170085: Pseudo dice [np.float32(0.9658), np.float32(0.7643)] 
2025-01-29 10:36:37.172689: Epoch time: 48.73 s 
2025-01-29 10:36:38.346919:  
2025-01-29 10:36:38.349836: Epoch 551 
2025-01-29 10:36:38.352286: Current learning rate: 0.00486 
2025-01-29 10:37:26.875666: train_loss -0.8133 
2025-01-29 10:37:26.880593: val_loss -0.8059 
2025-01-29 10:37:26.883605: Pseudo dice [np.float32(0.9624), np.float32(0.8925)] 
2025-01-29 10:37:26.886055: Epoch time: 48.53 s 
2025-01-29 10:37:28.036404:  
2025-01-29 10:37:28.039463: Epoch 552 
2025-01-29 10:37:28.042304: Current learning rate: 0.00485 
2025-01-29 10:38:16.748287: train_loss -0.8188 
2025-01-29 10:38:16.754040: val_loss -0.7859 
2025-01-29 10:38:16.765151: Pseudo dice [np.float32(0.9605), np.float32(0.8512)] 
2025-01-29 10:38:16.768035: Epoch time: 48.71 s 
2025-01-29 10:38:17.913493:  
2025-01-29 10:38:17.916710: Epoch 553 
2025-01-29 10:38:17.919618: Current learning rate: 0.00484 
2025-01-29 10:39:06.406582: train_loss -0.8131 
2025-01-29 10:39:06.411318: val_loss -0.7896 
2025-01-29 10:39:06.414183: Pseudo dice [np.float32(0.9649), np.float32(0.8825)] 
2025-01-29 10:39:06.416927: Epoch time: 48.49 s 
2025-01-29 10:39:07.602655:  
2025-01-29 10:39:07.605965: Epoch 554 
2025-01-29 10:39:07.609027: Current learning rate: 0.00484 
2025-01-29 10:39:56.047702: train_loss -0.8258 
2025-01-29 10:39:56.053769: val_loss -0.8186 
2025-01-29 10:39:56.056293: Pseudo dice [np.float32(0.9693), np.float32(0.8938)] 
2025-01-29 10:39:56.058786: Epoch time: 48.45 s 
2025-01-29 10:39:57.224465:  
2025-01-29 10:39:57.227656: Epoch 555 
2025-01-29 10:39:57.230561: Current learning rate: 0.00483 
2025-01-29 10:40:45.265074: train_loss -0.8213 
2025-01-29 10:40:45.269620: val_loss -0.7603 
2025-01-29 10:40:45.272300: Pseudo dice [np.float32(0.9677), np.float32(0.8332)] 
2025-01-29 10:40:45.275181: Epoch time: 48.04 s 
2025-01-29 10:40:46.431915:  
2025-01-29 10:40:46.434452: Epoch 556 
2025-01-29 10:40:46.437292: Current learning rate: 0.00482 
2025-01-29 10:41:34.285651: train_loss -0.8166 
2025-01-29 10:41:34.294631: val_loss -0.8493 
2025-01-29 10:41:34.303676: Pseudo dice [np.float32(0.9675), np.float32(0.9049)] 
2025-01-29 10:41:34.306629: Epoch time: 47.85 s 
2025-01-29 10:41:35.453907:  
2025-01-29 10:41:35.457231: Epoch 557 
2025-01-29 10:41:35.460200: Current learning rate: 0.00481 
2025-01-29 10:42:23.483737: train_loss -0.8222 
2025-01-29 10:42:23.488376: val_loss -0.7905 
2025-01-29 10:42:23.491374: Pseudo dice [np.float32(0.9676), np.float32(0.8905)] 
2025-01-29 10:42:23.494227: Epoch time: 48.03 s 
2025-01-29 10:42:24.646208:  
2025-01-29 10:42:24.648692: Epoch 558 
2025-01-29 10:42:24.651459: Current learning rate: 0.0048 
2025-01-29 10:43:12.954664: train_loss -0.8299 
2025-01-29 10:43:12.960808: val_loss -0.8142 
2025-01-29 10:43:12.963429: Pseudo dice [np.float32(0.9697), np.float32(0.8754)] 
2025-01-29 10:43:12.966079: Epoch time: 48.31 s 
2025-01-29 10:43:14.112318:  
2025-01-29 10:43:14.115297: Epoch 559 
2025-01-29 10:43:14.118044: Current learning rate: 0.00479 
2025-01-29 10:44:02.530821: train_loss -0.8135 
2025-01-29 10:44:02.534948: val_loss -0.8008 
2025-01-29 10:44:02.537582: Pseudo dice [np.float32(0.958), np.float32(0.8903)] 
2025-01-29 10:44:02.540238: Epoch time: 48.42 s 
2025-01-29 10:44:03.699053:  
2025-01-29 10:44:03.701931: Epoch 560 
2025-01-29 10:44:03.704660: Current learning rate: 0.00478 
2025-01-29 10:44:52.075921: train_loss -0.8269 
2025-01-29 10:44:52.081913: val_loss -0.7798 
2025-01-29 10:44:52.092870: Pseudo dice [np.float32(0.9685), np.float32(0.8543)] 
2025-01-29 10:44:52.096075: Epoch time: 48.38 s 
2025-01-29 10:44:53.243412:  
2025-01-29 10:44:53.246011: Epoch 561 
2025-01-29 10:44:53.248805: Current learning rate: 0.00477 
2025-01-29 10:45:41.684199: train_loss -0.8136 
2025-01-29 10:45:41.688308: val_loss -0.7038 
2025-01-29 10:45:41.691003: Pseudo dice [np.float32(0.956), np.float32(0.7992)] 
2025-01-29 10:45:41.693777: Epoch time: 48.44 s 
2025-01-29 10:45:42.839171:  
2025-01-29 10:45:42.841831: Epoch 562 
2025-01-29 10:45:42.844412: Current learning rate: 0.00476 
2025-01-29 10:46:31.220070: train_loss -0.8271 
2025-01-29 10:46:31.226176: val_loss -0.7843 
2025-01-29 10:46:31.229175: Pseudo dice [np.float32(0.9647), np.float32(0.8698)] 
2025-01-29 10:46:31.231728: Epoch time: 48.38 s 
2025-01-29 10:46:32.377124:  
2025-01-29 10:46:32.379818: Epoch 563 
2025-01-29 10:46:32.382581: Current learning rate: 0.00475 
2025-01-29 10:47:20.692206: train_loss -0.8264 
2025-01-29 10:47:20.696405: val_loss -0.7502 
2025-01-29 10:47:20.699091: Pseudo dice [np.float32(0.9667), np.float32(0.8583)] 
2025-01-29 10:47:20.701745: Epoch time: 48.32 s 
2025-01-29 10:47:21.855778:  
2025-01-29 10:47:21.858803: Epoch 564 
2025-01-29 10:47:21.861298: Current learning rate: 0.00474 
2025-01-29 10:48:09.993368: train_loss -0.8273 
2025-01-29 10:48:10.000400: val_loss -0.7805 
2025-01-29 10:48:10.003419: Pseudo dice [np.float32(0.9632), np.float32(0.8578)] 
2025-01-29 10:48:10.006200: Epoch time: 48.14 s 
2025-01-29 10:48:11.149333:  
2025-01-29 10:48:11.152597: Epoch 565 
2025-01-29 10:48:11.155667: Current learning rate: 0.00473 
2025-01-29 10:48:59.145684: train_loss -0.8189 
2025-01-29 10:48:59.149829: val_loss -0.7648 
2025-01-29 10:48:59.152535: Pseudo dice [np.float32(0.9598), np.float32(0.8264)] 
2025-01-29 10:48:59.155227: Epoch time: 48.0 s 
2025-01-29 10:49:00.308431:  
2025-01-29 10:49:00.311287: Epoch 566 
2025-01-29 10:49:00.313931: Current learning rate: 0.00472 
2025-01-29 10:49:48.383135: train_loss -0.8121 
2025-01-29 10:49:48.391281: val_loss -0.7442 
2025-01-29 10:49:48.393920: Pseudo dice [np.float32(0.9671), np.float32(0.8042)] 
2025-01-29 10:49:48.396631: Epoch time: 48.08 s 
2025-01-29 10:49:49.547864:  
2025-01-29 10:49:49.550883: Epoch 567 
2025-01-29 10:49:49.553797: Current learning rate: 0.00471 
2025-01-29 10:50:38.215172: train_loss -0.8188 
2025-01-29 10:50:38.218658: val_loss -0.7495 
2025-01-29 10:50:38.221132: Pseudo dice [np.float32(0.9633), np.float32(0.8001)] 
2025-01-29 10:50:38.223515: Epoch time: 48.67 s 
2025-01-29 10:50:40.035447:  
2025-01-29 10:50:40.038563: Epoch 568 
2025-01-29 10:50:40.041443: Current learning rate: 0.0047 
2025-01-29 10:51:27.972832: train_loss -0.8006 
2025-01-29 10:51:27.977964: val_loss -0.7704 
2025-01-29 10:51:27.980788: Pseudo dice [np.float32(0.965), np.float32(0.8665)] 
2025-01-29 10:51:27.983121: Epoch time: 47.94 s 
2025-01-29 10:51:29.130238:  
2025-01-29 10:51:29.133102: Epoch 569 
2025-01-29 10:51:29.135912: Current learning rate: 0.00469 
2025-01-29 10:52:17.174907: train_loss -0.8115 
2025-01-29 10:52:17.180160: val_loss -0.8058 
2025-01-29 10:52:17.189977: Pseudo dice [np.float32(0.9625), np.float32(0.8935)] 
2025-01-29 10:52:17.193294: Epoch time: 48.05 s 
2025-01-29 10:52:18.345531:  
2025-01-29 10:52:18.348248: Epoch 570 
2025-01-29 10:52:18.351069: Current learning rate: 0.00468 
2025-01-29 10:53:06.428646: train_loss -0.8211 
2025-01-29 10:53:06.433613: val_loss -0.8129 
2025-01-29 10:53:06.436256: Pseudo dice [np.float32(0.9645), np.float32(0.8918)] 
2025-01-29 10:53:06.438604: Epoch time: 48.08 s 
2025-01-29 10:53:07.590141:  
2025-01-29 10:53:07.592725: Epoch 571 
2025-01-29 10:53:07.595421: Current learning rate: 0.00467 
2025-01-29 10:53:56.109302: train_loss -0.7949 
2025-01-29 10:53:56.113042: val_loss -0.795 
2025-01-29 10:53:56.116077: Pseudo dice [np.float32(0.9615), np.float32(0.8356)] 
2025-01-29 10:53:56.118633: Epoch time: 48.52 s 
2025-01-29 10:53:57.265357:  
2025-01-29 10:53:57.268270: Epoch 572 
2025-01-29 10:53:57.270916: Current learning rate: 0.00466 
2025-01-29 10:54:45.758261: train_loss -0.7888 
2025-01-29 10:54:45.765280: val_loss -0.8074 
2025-01-29 10:54:45.767751: Pseudo dice [np.float32(0.9632), np.float32(0.9039)] 
2025-01-29 10:54:45.770139: Epoch time: 48.49 s 
2025-01-29 10:54:46.930729:  
2025-01-29 10:54:46.933342: Epoch 573 
2025-01-29 10:54:46.935755: Current learning rate: 0.00465 
2025-01-29 10:55:35.239455: train_loss -0.8307 
2025-01-29 10:55:35.243778: val_loss -0.7179 
2025-01-29 10:55:35.252968: Pseudo dice [np.float32(0.9674), np.float32(0.8616)] 
2025-01-29 10:55:35.255805: Epoch time: 48.31 s 
2025-01-29 10:55:36.420138:  
2025-01-29 10:55:36.422989: Epoch 574 
2025-01-29 10:55:36.425405: Current learning rate: 0.00464 
2025-01-29 10:56:24.711440: train_loss -0.8149 
2025-01-29 10:56:24.734600: val_loss -0.789 
2025-01-29 10:56:24.737500: Pseudo dice [np.float32(0.9645), np.float32(0.8692)] 
2025-01-29 10:56:24.740219: Epoch time: 48.29 s 
2025-01-29 10:56:25.895334:  
2025-01-29 10:56:25.897999: Epoch 575 
2025-01-29 10:56:25.900814: Current learning rate: 0.00463 
2025-01-29 10:57:13.999097: train_loss -0.8067 
2025-01-29 10:57:14.004976: val_loss -0.7891 
2025-01-29 10:57:14.007727: Pseudo dice [np.float32(0.9672), np.float32(0.8713)] 
2025-01-29 10:57:14.010249: Epoch time: 48.1 s 
2025-01-29 10:57:15.172212:  
2025-01-29 10:57:15.174932: Epoch 576 
2025-01-29 10:57:15.177677: Current learning rate: 0.00462 
2025-01-29 10:58:03.056047: train_loss -0.8199 
2025-01-29 10:58:03.061381: val_loss -0.7575 
2025-01-29 10:58:03.063962: Pseudo dice [np.float32(0.9609), np.float32(0.7972)] 
2025-01-29 10:58:03.066661: Epoch time: 47.88 s 
2025-01-29 10:58:04.230432:  
2025-01-29 10:58:04.232850: Epoch 577 
2025-01-29 10:58:04.235633: Current learning rate: 0.00461 
2025-01-29 10:58:52.149183: train_loss -0.8225 
2025-01-29 10:58:52.153960: val_loss -0.8154 
2025-01-29 10:58:52.156902: Pseudo dice [np.float32(0.964), np.float32(0.8619)] 
2025-01-29 10:58:52.159658: Epoch time: 47.92 s 
2025-01-29 10:58:53.331095:  
2025-01-29 10:58:53.333759: Epoch 578 
2025-01-29 10:58:53.336550: Current learning rate: 0.0046 
2025-01-29 10:59:41.502042: train_loss -0.8227 
2025-01-29 10:59:41.509246: val_loss -0.7399 
2025-01-29 10:59:41.511849: Pseudo dice [np.float32(0.9641), np.float32(0.8149)] 
2025-01-29 10:59:41.514225: Epoch time: 48.17 s 
2025-01-29 10:59:42.682323:  
2025-01-29 10:59:42.685038: Epoch 579 
2025-01-29 10:59:42.687827: Current learning rate: 0.00459 
2025-01-29 11:00:30.733429: train_loss -0.8044 
2025-01-29 11:00:30.737802: val_loss -0.7522 
2025-01-29 11:00:30.741085: Pseudo dice [np.float32(0.9645), np.float32(0.7908)] 
2025-01-29 11:00:30.743944: Epoch time: 48.05 s 
2025-01-29 11:00:31.914229:  
2025-01-29 11:00:31.917439: Epoch 580 
2025-01-29 11:00:31.920300: Current learning rate: 0.00458 
2025-01-29 11:01:20.050840: train_loss -0.8313 
2025-01-29 11:01:20.058279: val_loss -0.7894 
2025-01-29 11:01:20.060896: Pseudo dice [np.float32(0.9648), np.float32(0.8534)] 
2025-01-29 11:01:20.063701: Epoch time: 48.14 s 
2025-01-29 11:01:21.223027:  
2025-01-29 11:01:21.226172: Epoch 581 
2025-01-29 11:01:21.229276: Current learning rate: 0.00457 
2025-01-29 11:02:09.205442: train_loss -0.8169 
2025-01-29 11:02:09.211569: val_loss -0.7456 
2025-01-29 11:02:09.214132: Pseudo dice [np.float32(0.9669), np.float32(0.8721)] 
2025-01-29 11:02:09.216700: Epoch time: 47.98 s 
2025-01-29 11:02:10.385363:  
2025-01-29 11:02:10.387974: Epoch 582 
2025-01-29 11:02:10.390511: Current learning rate: 0.00456 
2025-01-29 11:02:58.862985: train_loss -0.8179 
2025-01-29 11:02:58.868989: val_loss -0.8018 
2025-01-29 11:02:58.871738: Pseudo dice [np.float32(0.9645), np.float32(0.8827)] 
2025-01-29 11:02:58.874566: Epoch time: 48.48 s 
2025-01-29 11:03:00.042559:  
2025-01-29 11:03:00.045498: Epoch 583 
2025-01-29 11:03:00.048562: Current learning rate: 0.00455 
2025-01-29 11:03:47.978517: train_loss -0.8295 
2025-01-29 11:03:47.983635: val_loss -0.7574 
2025-01-29 11:03:47.986352: Pseudo dice [np.float32(0.9648), np.float32(0.8575)] 
2025-01-29 11:03:47.988898: Epoch time: 47.94 s 
2025-01-29 11:03:49.154498:  
2025-01-29 11:03:49.156979: Epoch 584 
2025-01-29 11:03:49.159295: Current learning rate: 0.00454 
2025-01-29 11:04:37.358505: train_loss -0.8368 
2025-01-29 11:04:37.364270: val_loss -0.8012 
2025-01-29 11:04:37.367070: Pseudo dice [np.float32(0.9676), np.float32(0.8852)] 
2025-01-29 11:04:37.369475: Epoch time: 48.21 s 
2025-01-29 11:04:38.538222:  
2025-01-29 11:04:38.540880: Epoch 585 
2025-01-29 11:04:38.543444: Current learning rate: 0.00453 
2025-01-29 11:05:26.723674: train_loss -0.8125 
2025-01-29 11:05:26.727773: val_loss -0.7736 
2025-01-29 11:05:26.730059: Pseudo dice [np.float32(0.9646), np.float32(0.8927)] 
2025-01-29 11:05:26.732626: Epoch time: 48.19 s 
2025-01-29 11:05:28.572388:  
2025-01-29 11:05:28.575610: Epoch 586 
2025-01-29 11:05:28.578366: Current learning rate: 0.00452 
2025-01-29 11:06:16.735295: train_loss -0.8235 
2025-01-29 11:06:16.740794: val_loss -0.7782 
2025-01-29 11:06:16.748777: Pseudo dice [np.float32(0.9684), np.float32(0.8729)] 
2025-01-29 11:06:16.751909: Epoch time: 48.16 s 
2025-01-29 11:06:17.922595:  
2025-01-29 11:06:17.925448: Epoch 587 
2025-01-29 11:06:17.928238: Current learning rate: 0.00451 
2025-01-29 11:07:06.203813: train_loss -0.8149 
2025-01-29 11:07:06.207892: val_loss -0.7898 
2025-01-29 11:07:06.210485: Pseudo dice [np.float32(0.9658), np.float32(0.8908)] 
2025-01-29 11:07:06.213156: Epoch time: 48.28 s 
2025-01-29 11:07:07.385875:  
2025-01-29 11:07:07.388573: Epoch 588 
2025-01-29 11:07:07.391239: Current learning rate: 0.0045 
2025-01-29 11:07:55.735200: train_loss -0.8347 
2025-01-29 11:07:55.740905: val_loss -0.7418 
2025-01-29 11:07:55.743609: Pseudo dice [np.float32(0.9626), np.float32(0.8398)] 
2025-01-29 11:07:55.746509: Epoch time: 48.35 s 
2025-01-29 11:07:56.914663:  
2025-01-29 11:07:56.917425: Epoch 589 
2025-01-29 11:07:56.920134: Current learning rate: 0.00449 
2025-01-29 11:08:44.587971: train_loss -0.8111 
2025-01-29 11:08:44.594455: val_loss -0.8051 
2025-01-29 11:08:44.597471: Pseudo dice [np.float32(0.9673), np.float32(0.889)] 
2025-01-29 11:08:44.600041: Epoch time: 47.67 s 
2025-01-29 11:08:45.765008:  
2025-01-29 11:08:45.768383: Epoch 590 
2025-01-29 11:08:45.771597: Current learning rate: 0.00448 
2025-01-29 11:09:34.078061: train_loss -0.8399 
2025-01-29 11:09:34.083453: val_loss -0.8232 
2025-01-29 11:09:34.091997: Pseudo dice [np.float32(0.9661), np.float32(0.9022)] 
2025-01-29 11:09:34.094756: Epoch time: 48.31 s 
2025-01-29 11:09:35.257162:  
2025-01-29 11:09:35.260762: Epoch 591 
2025-01-29 11:09:35.263387: Current learning rate: 0.00447 
2025-01-29 11:10:23.517519: train_loss -0.8179 
2025-01-29 11:10:23.522093: val_loss -0.7693 
2025-01-29 11:10:23.524628: Pseudo dice [np.float32(0.9647), np.float32(0.8718)] 
2025-01-29 11:10:23.527160: Epoch time: 48.26 s 
2025-01-29 11:10:24.696680:  
2025-01-29 11:10:24.699634: Epoch 592 
2025-01-29 11:10:24.702514: Current learning rate: 0.00446 
2025-01-29 11:11:12.800555: train_loss -0.8032 
2025-01-29 11:11:12.806506: val_loss -0.7065 
2025-01-29 11:11:12.809205: Pseudo dice [np.float32(0.9635), np.float32(0.8092)] 
2025-01-29 11:11:12.811976: Epoch time: 48.1 s 
2025-01-29 11:11:13.976341:  
2025-01-29 11:11:13.979032: Epoch 593 
2025-01-29 11:11:13.981845: Current learning rate: 0.00445 
2025-01-29 11:12:02.447658: train_loss -0.817 
2025-01-29 11:12:02.452197: val_loss -0.7951 
2025-01-29 11:12:02.455372: Pseudo dice [np.float32(0.9704), np.float32(0.879)] 
2025-01-29 11:12:02.457989: Epoch time: 48.47 s 
2025-01-29 11:12:03.621971:  
2025-01-29 11:12:03.625223: Epoch 594 
2025-01-29 11:12:03.627961: Current learning rate: 0.00444 
2025-01-29 11:12:51.668473: train_loss -0.8277 
2025-01-29 11:12:51.674576: val_loss -0.7646 
2025-01-29 11:12:51.683429: Pseudo dice [np.float32(0.9663), np.float32(0.8474)] 
2025-01-29 11:12:51.686033: Epoch time: 48.05 s 
2025-01-29 11:12:52.844376:  
2025-01-29 11:12:52.847302: Epoch 595 
2025-01-29 11:12:52.850075: Current learning rate: 0.00443 
2025-01-29 11:13:41.405497: train_loss -0.8266 
2025-01-29 11:13:41.409616: val_loss -0.7364 
2025-01-29 11:13:41.412156: Pseudo dice [np.float32(0.9606), np.float32(0.773)] 
2025-01-29 11:13:41.414790: Epoch time: 48.56 s 
2025-01-29 11:13:42.582232:  
2025-01-29 11:13:42.585002: Epoch 596 
2025-01-29 11:13:42.587553: Current learning rate: 0.00442 
2025-01-29 11:14:30.667130: train_loss -0.8127 
2025-01-29 11:14:30.672193: val_loss -0.7466 
2025-01-29 11:14:30.681123: Pseudo dice [np.float32(0.9656), np.float32(0.8306)] 
2025-01-29 11:14:30.683647: Epoch time: 48.09 s 
2025-01-29 11:14:31.848768:  
2025-01-29 11:14:31.851444: Epoch 597 
2025-01-29 11:14:31.854182: Current learning rate: 0.00441 
2025-01-29 11:15:20.037007: train_loss -0.8136 
2025-01-29 11:15:20.040960: val_loss -0.7613 
2025-01-29 11:15:20.043615: Pseudo dice [np.float32(0.9646), np.float32(0.8681)] 
2025-01-29 11:15:20.046145: Epoch time: 48.19 s 
2025-01-29 11:15:21.213379:  
2025-01-29 11:15:21.216508: Epoch 598 
2025-01-29 11:15:21.219337: Current learning rate: 0.0044 
2025-01-29 11:16:09.518669: train_loss -0.8007 
2025-01-29 11:16:09.524779: val_loss -0.8021 
2025-01-29 11:16:09.527583: Pseudo dice [np.float32(0.9652), np.float32(0.8453)] 
2025-01-29 11:16:09.530542: Epoch time: 48.31 s 
2025-01-29 11:16:10.700848:  
2025-01-29 11:16:10.703720: Epoch 599 
2025-01-29 11:16:10.706343: Current learning rate: 0.00439 
2025-01-29 11:16:58.794680: train_loss -0.8292 
2025-01-29 11:16:58.801339: val_loss -0.7674 
2025-01-29 11:16:58.803892: Pseudo dice [np.float32(0.9662), np.float32(0.8215)] 
2025-01-29 11:16:58.806879: Epoch time: 48.09 s 
2025-01-29 11:17:00.604023:  
2025-01-29 11:17:00.607263: Epoch 600 
2025-01-29 11:17:00.610097: Current learning rate: 0.00438 
2025-01-29 11:17:48.987941: train_loss -0.8169 
2025-01-29 11:17:48.993298: val_loss -0.7512 
2025-01-29 11:17:48.996065: Pseudo dice [np.float32(0.9672), np.float32(0.8022)] 
2025-01-29 11:17:48.998513: Epoch time: 48.39 s 
2025-01-29 11:17:50.161741:  
2025-01-29 11:17:50.164509: Epoch 601 
2025-01-29 11:17:50.167227: Current learning rate: 0.00437 
2025-01-29 11:18:38.254595: train_loss -0.8251 
2025-01-29 11:18:38.260850: val_loss -0.7917 
2025-01-29 11:18:38.269578: Pseudo dice [np.float32(0.965), np.float32(0.8746)] 
2025-01-29 11:18:38.272575: Epoch time: 48.09 s 
2025-01-29 11:18:39.440244:  
2025-01-29 11:18:39.443451: Epoch 602 
2025-01-29 11:18:39.446190: Current learning rate: 0.00436 
2025-01-29 11:19:27.905911: train_loss -0.8077 
2025-01-29 11:19:27.912643: val_loss -0.7292 
2025-01-29 11:19:27.915355: Pseudo dice [np.float32(0.9623), np.float32(0.8391)] 
2025-01-29 11:19:27.918084: Epoch time: 48.47 s 
2025-01-29 11:19:29.087460:  
2025-01-29 11:19:29.090262: Epoch 603 
2025-01-29 11:19:29.092992: Current learning rate: 0.00435 
2025-01-29 11:20:17.701424: train_loss -0.8045 
2025-01-29 11:20:17.705714: val_loss -0.7451 
2025-01-29 11:20:17.708490: Pseudo dice [np.float32(0.9687), np.float32(0.8675)] 
2025-01-29 11:20:17.710866: Epoch time: 48.61 s 
2025-01-29 11:20:18.875887:  
2025-01-29 11:20:18.878829: Epoch 604 
2025-01-29 11:20:18.881652: Current learning rate: 0.00434 
2025-01-29 11:21:07.458001: train_loss -0.8271 
2025-01-29 11:21:07.465717: val_loss -0.7608 
2025-01-29 11:21:07.468662: Pseudo dice [np.float32(0.9599), np.float32(0.8861)] 
2025-01-29 11:21:07.471295: Epoch time: 48.58 s 
2025-01-29 11:21:09.208601:  
2025-01-29 11:21:09.211792: Epoch 605 
2025-01-29 11:21:09.214595: Current learning rate: 0.00433 
2025-01-29 11:21:57.892552: train_loss -0.8202 
2025-01-29 11:21:57.896839: val_loss -0.7283 
2025-01-29 11:21:57.905270: Pseudo dice [np.float32(0.9619), np.float32(0.8653)] 
2025-01-29 11:21:57.908075: Epoch time: 48.68 s 
2025-01-29 11:21:59.068227:  
2025-01-29 11:21:59.071352: Epoch 606 
2025-01-29 11:21:59.074072: Current learning rate: 0.00432 
2025-01-29 11:22:47.580673: train_loss -0.8214 
2025-01-29 11:22:47.588039: val_loss -0.8002 
2025-01-29 11:22:47.590833: Pseudo dice [np.float32(0.965), np.float32(0.8899)] 
2025-01-29 11:22:47.593399: Epoch time: 48.51 s 
2025-01-29 11:22:48.754463:  
2025-01-29 11:22:48.757626: Epoch 607 
2025-01-29 11:22:48.760489: Current learning rate: 0.00431 
2025-01-29 11:23:37.088364: train_loss -0.8228 
2025-01-29 11:23:37.092657: val_loss -0.7818 
2025-01-29 11:23:37.095495: Pseudo dice [np.float32(0.9668), np.float32(0.8922)] 
2025-01-29 11:23:37.097816: Epoch time: 48.33 s 
2025-01-29 11:23:38.263327:  
2025-01-29 11:23:38.266236: Epoch 608 
2025-01-29 11:23:38.269356: Current learning rate: 0.0043 
2025-01-29 11:24:26.507167: train_loss -0.8129 
2025-01-29 11:24:26.514219: val_loss -0.7752 
2025-01-29 11:24:26.516930: Pseudo dice [np.float32(0.967), np.float32(0.8576)] 
2025-01-29 11:24:26.519382: Epoch time: 48.24 s 
2025-01-29 11:24:27.685769:  
2025-01-29 11:24:27.688813: Epoch 609 
2025-01-29 11:24:27.691549: Current learning rate: 0.00429 
2025-01-29 11:25:16.270379: train_loss -0.8158 
2025-01-29 11:25:16.274774: val_loss -0.8124 
2025-01-29 11:25:16.283212: Pseudo dice [np.float32(0.9669), np.float32(0.8855)] 
2025-01-29 11:25:16.286258: Epoch time: 48.59 s 
2025-01-29 11:25:17.443222:  
2025-01-29 11:25:17.445889: Epoch 610 
2025-01-29 11:25:17.448623: Current learning rate: 0.00429 
2025-01-29 11:26:05.587870: train_loss -0.806 
2025-01-29 11:26:05.593097: val_loss -0.738 
2025-01-29 11:26:05.642102: Pseudo dice [np.float32(0.9654), np.float32(0.8095)] 
2025-01-29 11:26:05.645375: Epoch time: 48.15 s 
2025-01-29 11:26:06.814585:  
2025-01-29 11:26:06.817489: Epoch 611 
2025-01-29 11:26:06.820316: Current learning rate: 0.00428 
2025-01-29 11:26:54.953483: train_loss -0.8364 
2025-01-29 11:26:54.957909: val_loss -0.8 
2025-01-29 11:26:54.960754: Pseudo dice [np.float32(0.9692), np.float32(0.8854)] 
2025-01-29 11:26:54.963517: Epoch time: 48.14 s 
2025-01-29 11:26:56.132820:  
2025-01-29 11:26:56.135729: Epoch 612 
2025-01-29 11:26:56.138687: Current learning rate: 0.00427 
2025-01-29 11:27:43.952158: train_loss -0.825 
2025-01-29 11:27:43.957858: val_loss -0.7808 
2025-01-29 11:27:43.960717: Pseudo dice [np.float32(0.9645), np.float32(0.8866)] 
2025-01-29 11:27:43.963541: Epoch time: 47.82 s 
2025-01-29 11:27:45.124987:  
2025-01-29 11:27:45.127339: Epoch 613 
2025-01-29 11:27:45.130046: Current learning rate: 0.00426 
2025-01-29 11:28:33.156029: train_loss -0.8209 
2025-01-29 11:28:33.160580: val_loss -0.7919 
2025-01-29 11:28:33.163607: Pseudo dice [np.float32(0.9661), np.float32(0.877)] 
2025-01-29 11:28:33.166567: Epoch time: 48.03 s 
2025-01-29 11:28:34.327067:  
2025-01-29 11:28:34.330122: Epoch 614 
2025-01-29 11:28:34.332908: Current learning rate: 0.00425 
2025-01-29 11:29:22.689656: train_loss -0.8287 
2025-01-29 11:29:22.695766: val_loss -0.8165 
2025-01-29 11:29:22.698793: Pseudo dice [np.float32(0.9648), np.float32(0.8969)] 
2025-01-29 11:29:22.701600: Epoch time: 48.36 s 
2025-01-29 11:29:23.865511:  
2025-01-29 11:29:23.868608: Epoch 615 
2025-01-29 11:29:23.871534: Current learning rate: 0.00424 
2025-01-29 11:30:11.978130: train_loss -0.8041 
2025-01-29 11:30:11.982257: val_loss -0.7772 
2025-01-29 11:30:11.985108: Pseudo dice [np.float32(0.9628), np.float32(0.8689)] 
2025-01-29 11:30:11.987873: Epoch time: 48.11 s 
2025-01-29 11:30:13.146762:  
2025-01-29 11:30:13.149637: Epoch 616 
2025-01-29 11:30:13.152524: Current learning rate: 0.00423 
2025-01-29 11:31:00.985511: train_loss -0.8299 
2025-01-29 11:31:00.990725: val_loss -0.7832 
2025-01-29 11:31:00.993304: Pseudo dice [np.float32(0.9643), np.float32(0.8812)] 
2025-01-29 11:31:00.996014: Epoch time: 47.84 s 
2025-01-29 11:31:00.998260: Yayy! New best EMA pseudo Dice: 0.9172000288963318 
2025-01-29 11:31:02.698299:  
2025-01-29 11:31:02.701367: Epoch 617 
2025-01-29 11:31:02.703975: Current learning rate: 0.00422 
2025-01-29 11:31:50.923972: train_loss -0.832 
2025-01-29 11:31:50.928641: val_loss -0.773 
2025-01-29 11:31:50.931731: Pseudo dice [np.float32(0.9666), np.float32(0.8825)] 
2025-01-29 11:31:50.934446: Epoch time: 48.23 s 
2025-01-29 11:31:50.937433: Yayy! New best EMA pseudo Dice: 0.917900025844574 
2025-01-29 11:31:52.683842:  
2025-01-29 11:31:52.686772: Epoch 618 
2025-01-29 11:31:52.689587: Current learning rate: 0.00421 
2025-01-29 11:32:40.894700: train_loss -0.8344 
2025-01-29 11:32:40.899721: val_loss -0.7869 
2025-01-29 11:32:40.902240: Pseudo dice [np.float32(0.968), np.float32(0.8985)] 
2025-01-29 11:32:40.904616: Epoch time: 48.21 s 
2025-01-29 11:32:40.906993: Yayy! New best EMA pseudo Dice: 0.9193999767303467 
2025-01-29 11:32:42.619321:  
2025-01-29 11:32:42.622363: Epoch 619 
2025-01-29 11:32:42.625144: Current learning rate: 0.0042 
2025-01-29 11:33:31.159343: train_loss -0.8057 
2025-01-29 11:33:31.163327: val_loss -0.7848 
2025-01-29 11:33:31.165838: Pseudo dice [np.float32(0.9683), np.float32(0.8901)] 
2025-01-29 11:33:31.168431: Epoch time: 48.54 s 
2025-01-29 11:33:31.170696: Yayy! New best EMA pseudo Dice: 0.9204000234603882 
2025-01-29 11:33:32.944701:  
2025-01-29 11:33:32.947765: Epoch 620 
2025-01-29 11:33:32.950327: Current learning rate: 0.00419 
2025-01-29 11:34:21.364714: train_loss -0.8297 
2025-01-29 11:34:21.370103: val_loss -0.7595 
2025-01-29 11:34:21.372849: Pseudo dice [np.float32(0.9694), np.float32(0.8951)] 
2025-01-29 11:34:21.375678: Epoch time: 48.42 s 
2025-01-29 11:34:21.378228: Yayy! New best EMA pseudo Dice: 0.9215999841690063 
2025-01-29 11:34:23.123120:  
2025-01-29 11:34:23.125782: Epoch 621 
2025-01-29 11:34:23.128679: Current learning rate: 0.00418 
2025-01-29 11:35:11.545899: train_loss -0.8241 
2025-01-29 11:35:11.549862: val_loss -0.7861 
2025-01-29 11:35:11.552470: Pseudo dice [np.float32(0.9646), np.float32(0.8883)] 
2025-01-29 11:35:11.554983: Epoch time: 48.42 s 
2025-01-29 11:35:11.557279: Yayy! New best EMA pseudo Dice: 0.9221000075340271 
2025-01-29 11:35:13.839690:  
2025-01-29 11:35:13.842134: Epoch 622 
2025-01-29 11:35:13.844771: Current learning rate: 0.00417 
2025-01-29 11:36:01.894778: train_loss -0.8193 
2025-01-29 11:36:01.899937: val_loss -0.8204 
2025-01-29 11:36:01.902390: Pseudo dice [np.float32(0.968), np.float32(0.8975)] 
2025-01-29 11:36:01.905149: Epoch time: 48.06 s 
2025-01-29 11:36:01.907459: Yayy! New best EMA pseudo Dice: 0.9230999946594238 
2025-01-29 11:36:03.630865:  
2025-01-29 11:36:03.633829: Epoch 623 
2025-01-29 11:36:03.636709: Current learning rate: 0.00416 
2025-01-29 11:36:52.107290: train_loss -0.8211 
2025-01-29 11:36:52.111374: val_loss -0.8455 
2025-01-29 11:36:52.113708: Pseudo dice [np.float32(0.9677), np.float32(0.8945)] 
2025-01-29 11:36:52.115977: Epoch time: 48.48 s 
2025-01-29 11:36:52.118081: Yayy! New best EMA pseudo Dice: 0.9239000082015991 
2025-01-29 11:36:53.825001:  
2025-01-29 11:36:53.827854: Epoch 624 
2025-01-29 11:36:53.830514: Current learning rate: 0.00415 
2025-01-29 11:37:41.958087: train_loss -0.8217 
2025-01-29 11:37:41.963047: val_loss -0.7647 
2025-01-29 11:37:41.965456: Pseudo dice [np.float32(0.9669), np.float32(0.8767)] 
2025-01-29 11:37:41.967776: Epoch time: 48.13 s 
2025-01-29 11:37:43.131264:  
2025-01-29 11:37:43.134259: Epoch 625 
2025-01-29 11:37:43.137023: Current learning rate: 0.00414 
2025-01-29 11:38:31.381068: train_loss -0.8321 
2025-01-29 11:38:31.385400: val_loss -0.8046 
2025-01-29 11:38:31.388165: Pseudo dice [np.float32(0.9636), np.float32(0.8793)] 
2025-01-29 11:38:31.390965: Epoch time: 48.25 s 
2025-01-29 11:38:32.552864:  
2025-01-29 11:38:32.555954: Epoch 626 
2025-01-29 11:38:32.558615: Current learning rate: 0.00413 
2025-01-29 11:39:20.928758: train_loss -0.8387 
2025-01-29 11:39:20.935434: val_loss -0.7804 
2025-01-29 11:39:20.944511: Pseudo dice [np.float32(0.9675), np.float32(0.8947)] 
2025-01-29 11:39:20.948288: Epoch time: 48.38 s 
2025-01-29 11:39:20.951748: Yayy! New best EMA pseudo Dice: 0.9243000149726868 
2025-01-29 11:39:22.630471:  
2025-01-29 11:39:22.633109: Epoch 627 
2025-01-29 11:39:22.635835: Current learning rate: 0.00412 
2025-01-29 11:40:11.484327: train_loss -0.8349 
2025-01-29 11:40:11.488271: val_loss -0.7968 
2025-01-29 11:40:11.490817: Pseudo dice [np.float32(0.9632), np.float32(0.8885)] 
2025-01-29 11:40:11.493240: Epoch time: 48.85 s 
2025-01-29 11:40:11.495533: Yayy! New best EMA pseudo Dice: 0.9243999719619751 
2025-01-29 11:40:13.214309:  
2025-01-29 11:40:13.217371: Epoch 628 
2025-01-29 11:40:13.220364: Current learning rate: 0.00411 
2025-01-29 11:41:01.391430: train_loss -0.8175 
2025-01-29 11:41:01.397112: val_loss -0.7912 
2025-01-29 11:41:01.399796: Pseudo dice [np.float32(0.9675), np.float32(0.8767)] 
2025-01-29 11:41:01.402959: Epoch time: 48.18 s 
2025-01-29 11:41:02.563508:  
2025-01-29 11:41:02.566506: Epoch 629 
2025-01-29 11:41:02.569765: Current learning rate: 0.0041 
2025-01-29 11:41:50.855917: train_loss -0.8062 
2025-01-29 11:41:50.860130: val_loss -0.7811 
2025-01-29 11:41:50.862754: Pseudo dice [np.float32(0.9611), np.float32(0.822)] 
2025-01-29 11:41:50.865446: Epoch time: 48.29 s 
2025-01-29 11:41:52.023135:  
2025-01-29 11:41:52.026368: Epoch 630 
2025-01-29 11:41:52.029161: Current learning rate: 0.00409 
2025-01-29 11:42:40.071461: train_loss -0.8248 
2025-01-29 11:42:40.077304: val_loss -0.7885 
2025-01-29 11:42:40.080218: Pseudo dice [np.float32(0.9681), np.float32(0.8724)] 
2025-01-29 11:42:40.082944: Epoch time: 48.05 s 
2025-01-29 11:42:41.238080:  
2025-01-29 11:42:41.241466: Epoch 631 
2025-01-29 11:42:41.244425: Current learning rate: 0.00408 
2025-01-29 11:43:29.443876: train_loss -0.8347 
2025-01-29 11:43:29.448163: val_loss -0.7826 
2025-01-29 11:43:29.451144: Pseudo dice [np.float32(0.9632), np.float32(0.8765)] 
2025-01-29 11:43:29.453718: Epoch time: 48.21 s 
2025-01-29 11:43:30.603849:  
2025-01-29 11:43:30.606598: Epoch 632 
2025-01-29 11:43:30.609254: Current learning rate: 0.00407 
2025-01-29 11:44:18.610863: train_loss -0.8364 
2025-01-29 11:44:18.618698: val_loss -0.7685 
2025-01-29 11:44:18.621224: Pseudo dice [np.float32(0.9688), np.float32(0.896)] 
2025-01-29 11:44:18.623894: Epoch time: 48.01 s 
2025-01-29 11:44:19.785182:  
2025-01-29 11:44:19.788174: Epoch 633 
2025-01-29 11:44:19.791281: Current learning rate: 0.00406 
2025-01-29 11:45:07.948028: train_loss -0.8221 
2025-01-29 11:45:07.952148: val_loss -0.8085 
2025-01-29 11:45:07.954998: Pseudo dice [np.float32(0.9633), np.float32(0.8816)] 
2025-01-29 11:45:07.958188: Epoch time: 48.16 s 
2025-01-29 11:45:09.118171:  
2025-01-29 11:45:09.121128: Epoch 634 
2025-01-29 11:45:09.123942: Current learning rate: 0.00405 
2025-01-29 11:45:57.234056: train_loss -0.827 
2025-01-29 11:45:57.240511: val_loss -0.8028 
2025-01-29 11:45:57.243421: Pseudo dice [np.float32(0.9691), np.float32(0.8699)] 
2025-01-29 11:45:57.245880: Epoch time: 48.12 s 
2025-01-29 11:45:58.404063:  
2025-01-29 11:45:58.406959: Epoch 635 
2025-01-29 11:45:58.409882: Current learning rate: 0.00404 
2025-01-29 11:46:46.825624: train_loss -0.8308 
2025-01-29 11:46:46.831892: val_loss -0.7556 
2025-01-29 11:46:46.842219: Pseudo dice [np.float32(0.9633), np.float32(0.7852)] 
2025-01-29 11:46:46.846987: Epoch time: 48.42 s 
2025-01-29 11:46:48.001190:  
2025-01-29 11:46:48.004160: Epoch 636 
2025-01-29 11:46:48.006781: Current learning rate: 0.00403 
2025-01-29 11:47:36.249780: train_loss -0.816 
2025-01-29 11:47:36.255023: val_loss -0.7671 
2025-01-29 11:47:36.257834: Pseudo dice [np.float32(0.9614), np.float32(0.8981)] 
2025-01-29 11:47:36.260491: Epoch time: 48.25 s 
2025-01-29 11:47:37.415146:  
2025-01-29 11:47:37.419004: Epoch 637 
2025-01-29 11:47:37.421989: Current learning rate: 0.00402 
2025-01-29 11:48:25.756315: train_loss -0.8159 
2025-01-29 11:48:25.760508: val_loss -0.7985 
2025-01-29 11:48:25.770064: Pseudo dice [np.float32(0.9668), np.float32(0.8808)] 
2025-01-29 11:48:25.772878: Epoch time: 48.34 s 
2025-01-29 11:48:26.925185:  
2025-01-29 11:48:26.928201: Epoch 638 
2025-01-29 11:48:26.930932: Current learning rate: 0.00401 
2025-01-29 11:49:14.798183: train_loss -0.832 
2025-01-29 11:49:14.803776: val_loss -0.8067 
2025-01-29 11:49:14.806579: Pseudo dice [np.float32(0.9657), np.float32(0.8552)] 
2025-01-29 11:49:14.809159: Epoch time: 47.87 s 
2025-01-29 11:49:15.963340:  
2025-01-29 11:49:15.966133: Epoch 639 
2025-01-29 11:49:15.968867: Current learning rate: 0.004 
2025-01-29 11:50:03.859661: train_loss -0.8339 
2025-01-29 11:50:03.863756: val_loss -0.8053 
2025-01-29 11:50:03.866382: Pseudo dice [np.float32(0.9625), np.float32(0.8813)] 
2025-01-29 11:50:03.868726: Epoch time: 47.9 s 
2025-01-29 11:50:05.545273:  
2025-01-29 11:50:05.548872: Epoch 640 
2025-01-29 11:50:05.551614: Current learning rate: 0.00399 
2025-01-29 11:50:53.906611: train_loss -0.8305 
2025-01-29 11:50:53.912363: val_loss -0.7744 
2025-01-29 11:50:53.914796: Pseudo dice [np.float32(0.9672), np.float32(0.8719)] 
2025-01-29 11:50:53.917163: Epoch time: 48.36 s 
2025-01-29 11:50:55.071602:  
2025-01-29 11:50:55.073951: Epoch 641 
2025-01-29 11:50:55.076176: Current learning rate: 0.00398 
2025-01-29 11:51:43.171992: train_loss -0.8144 
2025-01-29 11:51:43.176321: val_loss -0.7958 
2025-01-29 11:51:43.187664: Pseudo dice [np.float32(0.9668), np.float32(0.8966)] 
2025-01-29 11:51:43.190738: Epoch time: 48.1 s 
2025-01-29 11:51:44.348393:  
2025-01-29 11:51:44.351289: Epoch 642 
2025-01-29 11:51:44.353992: Current learning rate: 0.00397 
2025-01-29 11:52:32.773291: train_loss -0.8164 
2025-01-29 11:52:32.779257: val_loss -0.7876 
2025-01-29 11:52:32.781746: Pseudo dice [np.float32(0.9676), np.float32(0.855)] 
2025-01-29 11:52:32.784439: Epoch time: 48.43 s 
2025-01-29 11:52:33.943393:  
2025-01-29 11:52:33.946581: Epoch 643 
2025-01-29 11:52:33.949295: Current learning rate: 0.00396 
2025-01-29 11:53:22.576717: train_loss -0.8208 
2025-01-29 11:53:22.580564: val_loss -0.7517 
2025-01-29 11:53:22.583386: Pseudo dice [np.float32(0.9609), np.float32(0.8452)] 
2025-01-29 11:53:22.585940: Epoch time: 48.63 s 
2025-01-29 11:53:23.744100:  
2025-01-29 11:53:23.747307: Epoch 644 
2025-01-29 11:53:23.750192: Current learning rate: 0.00395 
2025-01-29 11:54:11.900359: train_loss -0.8117 
2025-01-29 11:54:11.906652: val_loss -0.7941 
2025-01-29 11:54:11.909582: Pseudo dice [np.float32(0.9676), np.float32(0.9086)] 
2025-01-29 11:54:11.912480: Epoch time: 48.16 s 
2025-01-29 11:54:13.070212:  
2025-01-29 11:54:13.073225: Epoch 645 
2025-01-29 11:54:13.076100: Current learning rate: 0.00394 
2025-01-29 11:55:01.765597: train_loss -0.7956 
2025-01-29 11:55:01.771230: val_loss -0.7924 
2025-01-29 11:55:01.774265: Pseudo dice [np.float32(0.9662), np.float32(0.8987)] 
2025-01-29 11:55:01.776758: Epoch time: 48.7 s 
2025-01-29 11:55:02.968280:  
2025-01-29 11:55:02.971222: Epoch 646 
2025-01-29 11:55:02.973843: Current learning rate: 0.00393 
2025-01-29 11:55:51.220568: train_loss -0.8154 
2025-01-29 11:55:51.228809: val_loss -0.7709 
2025-01-29 11:55:51.232010: Pseudo dice [np.float32(0.9671), np.float32(0.8744)] 
2025-01-29 11:55:51.235150: Epoch time: 48.25 s 
2025-01-29 11:55:52.380940:  
2025-01-29 11:55:52.384010: Epoch 647 
2025-01-29 11:55:52.386812: Current learning rate: 0.00392 
2025-01-29 11:56:40.591982: train_loss -0.8078 
2025-01-29 11:56:40.596339: val_loss -0.7483 
2025-01-29 11:56:40.599207: Pseudo dice [np.float32(0.9581), np.float32(0.8588)] 
2025-01-29 11:56:40.601897: Epoch time: 48.21 s 
2025-01-29 11:56:41.759933:  
2025-01-29 11:56:41.762845: Epoch 648 
2025-01-29 11:56:41.765535: Current learning rate: 0.00391 
2025-01-29 11:57:30.311139: train_loss -0.8003 
2025-01-29 11:57:30.316948: val_loss -0.769 
2025-01-29 11:57:30.319810: Pseudo dice [np.float32(0.964), np.float32(0.8507)] 
2025-01-29 11:57:30.322564: Epoch time: 48.55 s 
2025-01-29 11:57:31.486953:  
2025-01-29 11:57:31.489840: Epoch 649 
2025-01-29 11:57:31.492643: Current learning rate: 0.0039 
2025-01-29 11:58:19.947912: train_loss -0.8139 
2025-01-29 11:58:19.954435: val_loss -0.8189 
2025-01-29 11:58:19.963051: Pseudo dice [np.float32(0.965), np.float32(0.8861)] 
2025-01-29 11:58:19.965857: Epoch time: 48.46 s 
2025-01-29 11:58:21.678516:  
2025-01-29 11:58:21.681240: Epoch 650 
2025-01-29 11:58:21.683749: Current learning rate: 0.00389 
2025-01-29 11:59:09.924481: train_loss -0.7996 
2025-01-29 11:59:09.930356: val_loss -0.7395 
2025-01-29 11:59:09.933205: Pseudo dice [np.float32(0.9674), np.float32(0.8286)] 
2025-01-29 11:59:09.935860: Epoch time: 48.25 s 
2025-01-29 11:59:11.125979:  
2025-01-29 11:59:11.129027: Epoch 651 
2025-01-29 11:59:11.131872: Current learning rate: 0.00388 
2025-01-29 11:59:59.063776: train_loss -0.8328 
2025-01-29 11:59:59.070469: val_loss -0.7951 
2025-01-29 11:59:59.073185: Pseudo dice [np.float32(0.9709), np.float32(0.8829)] 
2025-01-29 11:59:59.075799: Epoch time: 47.94 s 
2025-01-29 12:00:00.226804:  
2025-01-29 12:00:00.229906: Epoch 652 
2025-01-29 12:00:00.232709: Current learning rate: 0.00387 
2025-01-29 12:00:48.308213: train_loss -0.8128 
2025-01-29 12:00:48.315777: val_loss -0.7917 
2025-01-29 12:00:48.318867: Pseudo dice [np.float32(0.9603), np.float32(0.858)] 
2025-01-29 12:00:48.321391: Epoch time: 48.08 s 
2025-01-29 12:00:49.470876:  
2025-01-29 12:00:49.473782: Epoch 653 
2025-01-29 12:00:49.476558: Current learning rate: 0.00386 
2025-01-29 12:01:37.573742: train_loss -0.8285 
2025-01-29 12:01:37.578594: val_loss -0.7528 
2025-01-29 12:01:37.587234: Pseudo dice [np.float32(0.9655), np.float32(0.8719)] 
2025-01-29 12:01:37.590111: Epoch time: 48.1 s 
2025-01-29 12:01:38.738848:  
2025-01-29 12:01:38.742736: Epoch 654 
2025-01-29 12:01:38.745379: Current learning rate: 0.00385 
2025-01-29 12:02:27.273177: train_loss -0.8236 
2025-01-29 12:02:27.279098: val_loss -0.7486 
2025-01-29 12:02:27.282005: Pseudo dice [np.float32(0.9647), np.float32(0.8416)] 
2025-01-29 12:02:27.284554: Epoch time: 48.54 s 
2025-01-29 12:02:28.439500:  
2025-01-29 12:02:28.442316: Epoch 655 
2025-01-29 12:02:28.444833: Current learning rate: 0.00384 
2025-01-29 12:03:17.050894: train_loss -0.8332 
2025-01-29 12:03:17.054866: val_loss -0.7678 
2025-01-29 12:03:17.057658: Pseudo dice [np.float32(0.9654), np.float32(0.8764)] 
2025-01-29 12:03:17.060314: Epoch time: 48.61 s 
2025-01-29 12:03:18.205090:  
2025-01-29 12:03:18.208005: Epoch 656 
2025-01-29 12:03:18.210600: Current learning rate: 0.00383 
2025-01-29 12:04:06.548279: train_loss -0.8169 
2025-01-29 12:04:06.553512: val_loss -0.7503 
2025-01-29 12:04:06.556453: Pseudo dice [np.float32(0.9645), np.float32(0.8509)] 
2025-01-29 12:04:06.558985: Epoch time: 48.34 s 
2025-01-29 12:04:07.746696:  
2025-01-29 12:04:07.750644: Epoch 657 
2025-01-29 12:04:07.753539: Current learning rate: 0.00382 
2025-01-29 12:04:56.129588: train_loss -0.8192 
2025-01-29 12:04:56.133871: val_loss -0.7397 
2025-01-29 12:04:56.136489: Pseudo dice [np.float32(0.965), np.float32(0.8617)] 
2025-01-29 12:04:56.139179: Epoch time: 48.38 s 
2025-01-29 12:04:57.859609:  
2025-01-29 12:04:57.862792: Epoch 658 
2025-01-29 12:04:57.865462: Current learning rate: 0.00381 
2025-01-29 12:05:46.284950: train_loss -0.8072 
2025-01-29 12:05:46.290520: val_loss -0.8041 
2025-01-29 12:05:46.300390: Pseudo dice [np.float32(0.9664), np.float32(0.867)] 
2025-01-29 12:05:46.303116: Epoch time: 48.43 s 
2025-01-29 12:05:47.457764:  
2025-01-29 12:05:47.460581: Epoch 659 
2025-01-29 12:05:47.463436: Current learning rate: 0.0038 
2025-01-29 12:06:35.642216: train_loss -0.8092 
2025-01-29 12:06:35.646410: val_loss -0.7793 
2025-01-29 12:06:35.649119: Pseudo dice [np.float32(0.9622), np.float32(0.8945)] 
2025-01-29 12:06:35.652126: Epoch time: 48.19 s 
2025-01-29 12:06:36.797617:  
2025-01-29 12:06:36.800833: Epoch 660 
2025-01-29 12:06:36.804118: Current learning rate: 0.00379 
2025-01-29 12:07:25.077620: train_loss -0.8053 
2025-01-29 12:07:25.083244: val_loss -0.724 
2025-01-29 12:07:25.085887: Pseudo dice [np.float32(0.9682), np.float32(0.8819)] 
2025-01-29 12:07:25.088439: Epoch time: 48.28 s 
2025-01-29 12:07:26.278511:  
2025-01-29 12:07:26.281231: Epoch 661 
2025-01-29 12:07:26.284020: Current learning rate: 0.00378 
2025-01-29 12:08:14.722010: train_loss -0.8176 
2025-01-29 12:08:14.726043: val_loss -0.7709 
2025-01-29 12:08:14.729016: Pseudo dice [np.float32(0.9668), np.float32(0.8923)] 
2025-01-29 12:08:14.731798: Epoch time: 48.44 s 
2025-01-29 12:08:15.919970:  
2025-01-29 12:08:15.923271: Epoch 662 
2025-01-29 12:08:15.926021: Current learning rate: 0.00377 
2025-01-29 12:09:04.440608: train_loss -0.8272 
2025-01-29 12:09:04.446199: val_loss -0.7929 
2025-01-29 12:09:04.456033: Pseudo dice [np.float32(0.966), np.float32(0.8791)] 
2025-01-29 12:09:04.458755: Epoch time: 48.52 s 
2025-01-29 12:09:05.617090:  
2025-01-29 12:09:05.619915: Epoch 663 
2025-01-29 12:09:05.622560: Current learning rate: 0.00376 
2025-01-29 12:09:53.914122: train_loss -0.8195 
2025-01-29 12:09:53.917961: val_loss -0.788 
2025-01-29 12:09:53.920493: Pseudo dice [np.float32(0.9684), np.float32(0.8969)] 
2025-01-29 12:09:53.923203: Epoch time: 48.3 s 
2025-01-29 12:09:55.069019:  
2025-01-29 12:09:55.072481: Epoch 664 
2025-01-29 12:09:55.075494: Current learning rate: 0.00375 
2025-01-29 12:10:43.183897: train_loss -0.8325 
2025-01-29 12:10:43.189411: val_loss -0.801 
2025-01-29 12:10:43.192111: Pseudo dice [np.float32(0.9647), np.float32(0.8893)] 
2025-01-29 12:10:43.194685: Epoch time: 48.12 s 
2025-01-29 12:10:44.339763:  
2025-01-29 12:10:44.342549: Epoch 665 
2025-01-29 12:10:44.345340: Current learning rate: 0.00374 
2025-01-29 12:11:32.669465: train_loss -0.8392 
2025-01-29 12:11:32.673961: val_loss -0.7648 
2025-01-29 12:11:32.676882: Pseudo dice [np.float32(0.9726), np.float32(0.8857)] 
2025-01-29 12:11:32.679559: Epoch time: 48.33 s 
2025-01-29 12:11:33.822769:  
2025-01-29 12:11:33.825768: Epoch 666 
2025-01-29 12:11:33.828860: Current learning rate: 0.00373 
2025-01-29 12:12:21.789447: train_loss -0.8458 
2025-01-29 12:12:21.795592: val_loss -0.7774 
2025-01-29 12:12:21.798493: Pseudo dice [np.float32(0.9661), np.float32(0.891)] 
2025-01-29 12:12:21.801110: Epoch time: 47.97 s 
2025-01-29 12:12:22.949563:  
2025-01-29 12:12:22.952810: Epoch 667 
2025-01-29 12:12:22.955607: Current learning rate: 0.00372 
2025-01-29 12:13:11.165931: train_loss -0.8281 
2025-01-29 12:13:11.171561: val_loss -0.8104 
2025-01-29 12:13:11.178885: Pseudo dice [np.float32(0.9684), np.float32(0.8963)] 
2025-01-29 12:13:11.181754: Epoch time: 48.22 s 
2025-01-29 12:13:12.379415:  
2025-01-29 12:13:12.382080: Epoch 668 
2025-01-29 12:13:12.384796: Current learning rate: 0.00371 
2025-01-29 12:14:00.738107: train_loss -0.8269 
2025-01-29 12:14:00.743757: val_loss -0.7583 
2025-01-29 12:14:00.746593: Pseudo dice [np.float32(0.9595), np.float32(0.8343)] 
2025-01-29 12:14:00.749352: Epoch time: 48.36 s 
2025-01-29 12:14:01.954540:  
2025-01-29 12:14:01.957523: Epoch 669 
2025-01-29 12:14:01.960224: Current learning rate: 0.0037 
2025-01-29 12:14:50.270073: train_loss -0.8287 
2025-01-29 12:14:50.275419: val_loss -0.7776 
2025-01-29 12:14:50.278283: Pseudo dice [np.float32(0.9682), np.float32(0.8554)] 
2025-01-29 12:14:50.281086: Epoch time: 48.32 s 
2025-01-29 12:14:51.450947:  
2025-01-29 12:14:51.453758: Epoch 670 
2025-01-29 12:14:51.456553: Current learning rate: 0.00369 
2025-01-29 12:15:39.798798: train_loss -0.8487 
2025-01-29 12:15:39.811798: val_loss -0.7399 
2025-01-29 12:15:39.814854: Pseudo dice [np.float32(0.9639), np.float32(0.8751)] 
2025-01-29 12:15:39.817558: Epoch time: 48.35 s 
2025-01-29 12:15:40.978223:  
2025-01-29 12:15:40.981556: Epoch 671 
2025-01-29 12:15:40.984539: Current learning rate: 0.00368 
2025-01-29 12:16:29.301730: train_loss -0.8319 
2025-01-29 12:16:29.308061: val_loss -0.82 
2025-01-29 12:16:29.315351: Pseudo dice [np.float32(0.9612), np.float32(0.8724)] 
2025-01-29 12:16:29.318084: Epoch time: 48.32 s 
2025-01-29 12:16:30.521427:  
2025-01-29 12:16:30.524532: Epoch 672 
2025-01-29 12:16:30.527226: Current learning rate: 0.00367 
2025-01-29 12:17:19.128129: train_loss -0.8374 
2025-01-29 12:17:19.134428: val_loss -0.8125 
2025-01-29 12:17:19.137507: Pseudo dice [np.float32(0.9618), np.float32(0.8696)] 
2025-01-29 12:17:19.140234: Epoch time: 48.61 s 
2025-01-29 12:17:20.346854:  
2025-01-29 12:17:20.349847: Epoch 673 
2025-01-29 12:17:20.352709: Current learning rate: 0.00366 
2025-01-29 12:18:08.598283: train_loss -0.8404 
2025-01-29 12:18:08.602355: val_loss -0.8248 
2025-01-29 12:18:08.605102: Pseudo dice [np.float32(0.9655), np.float32(0.8944)] 
2025-01-29 12:18:08.607515: Epoch time: 48.25 s 
2025-01-29 12:18:09.774463:  
2025-01-29 12:18:09.777833: Epoch 674 
2025-01-29 12:18:09.781084: Current learning rate: 0.00365 
2025-01-29 12:18:57.992008: train_loss -0.8243 
2025-01-29 12:18:57.997646: val_loss -0.7803 
2025-01-29 12:18:58.000305: Pseudo dice [np.float32(0.9708), np.float32(0.9047)] 
2025-01-29 12:18:58.002808: Epoch time: 48.22 s 
2025-01-29 12:18:59.168423:  
2025-01-29 12:18:59.171353: Epoch 675 
2025-01-29 12:18:59.174422: Current learning rate: 0.00364 
2025-01-29 12:19:47.802576: train_loss -0.8354 
2025-01-29 12:19:47.807210: val_loss -0.8272 
2025-01-29 12:19:47.814361: Pseudo dice [np.float32(0.9702), np.float32(0.8925)] 
2025-01-29 12:19:47.817109: Epoch time: 48.64 s 
2025-01-29 12:19:49.020174:  
2025-01-29 12:19:49.023078: Epoch 676 
2025-01-29 12:19:49.025943: Current learning rate: 0.00363 
2025-01-29 12:20:37.470692: train_loss -0.8238 
2025-01-29 12:20:37.476734: val_loss -0.8017 
2025-01-29 12:20:37.479591: Pseudo dice [np.float32(0.9663), np.float32(0.9019)] 
2025-01-29 12:20:37.482159: Epoch time: 48.45 s 
2025-01-29 12:20:39.297562:  
2025-01-29 12:20:39.300701: Epoch 677 
2025-01-29 12:20:39.303508: Current learning rate: 0.00362 
2025-01-29 12:21:27.499334: train_loss -0.8264 
2025-01-29 12:21:27.503824: val_loss -0.7895 
2025-01-29 12:21:27.506645: Pseudo dice [np.float32(0.9673), np.float32(0.8978)] 
2025-01-29 12:21:27.509136: Epoch time: 48.2 s 
2025-01-29 12:21:27.511632: Yayy! New best EMA pseudo Dice: 0.9248999953269958 
2025-01-29 12:21:29.242150:  
2025-01-29 12:21:29.245102: Epoch 678 
2025-01-29 12:21:29.247897: Current learning rate: 0.00361 
2025-01-29 12:22:17.668866: train_loss -0.7992 
2025-01-29 12:22:17.674503: val_loss -0.7887 
2025-01-29 12:22:17.677361: Pseudo dice [np.float32(0.9666), np.float32(0.869)] 
2025-01-29 12:22:17.679988: Epoch time: 48.43 s 
2025-01-29 12:22:18.837126:  
2025-01-29 12:22:18.840586: Epoch 679 
2025-01-29 12:22:18.843752: Current learning rate: 0.0036 
2025-01-29 12:23:07.132382: train_loss -0.8308 
2025-01-29 12:23:07.136295: val_loss -0.7586 
2025-01-29 12:23:07.139119: Pseudo dice [np.float32(0.9643), np.float32(0.8058)] 
2025-01-29 12:23:07.141591: Epoch time: 48.3 s 
2025-01-29 12:23:08.306570:  
2025-01-29 12:23:08.309677: Epoch 680 
2025-01-29 12:23:08.312444: Current learning rate: 0.00359 
2025-01-29 12:23:56.797039: train_loss -0.7937 
2025-01-29 12:23:56.804264: val_loss -0.8219 
2025-01-29 12:23:56.811577: Pseudo dice [np.float32(0.965), np.float32(0.9089)] 
2025-01-29 12:23:56.814503: Epoch time: 48.49 s 
2025-01-29 12:23:57.977654:  
2025-01-29 12:23:57.980978: Epoch 681 
2025-01-29 12:23:57.983672: Current learning rate: 0.00358 
2025-01-29 12:24:46.530748: train_loss -0.8192 
2025-01-29 12:24:46.534734: val_loss -0.753 
2025-01-29 12:24:46.537522: Pseudo dice [np.float32(0.9652), np.float32(0.8871)] 
2025-01-29 12:24:46.539979: Epoch time: 48.55 s 
2025-01-29 12:24:47.710205:  
2025-01-29 12:24:47.713335: Epoch 682 
2025-01-29 12:24:47.716290: Current learning rate: 0.00357 
2025-01-29 12:25:36.039557: train_loss -0.8345 
2025-01-29 12:25:36.048120: val_loss -0.7726 
2025-01-29 12:25:36.050948: Pseudo dice [np.float32(0.9596), np.float32(0.8812)] 
2025-01-29 12:25:36.053546: Epoch time: 48.33 s 
2025-01-29 12:25:37.207980:  
2025-01-29 12:25:37.211380: Epoch 683 
2025-01-29 12:25:37.214163: Current learning rate: 0.00356 
2025-01-29 12:26:25.495119: train_loss -0.8187 
2025-01-29 12:26:25.499336: val_loss -0.8258 
2025-01-29 12:26:25.502209: Pseudo dice [np.float32(0.9658), np.float32(0.8979)] 
2025-01-29 12:26:25.505149: Epoch time: 48.29 s 
2025-01-29 12:26:26.710695:  
2025-01-29 12:26:26.713635: Epoch 684 
2025-01-29 12:26:26.716481: Current learning rate: 0.00355 
2025-01-29 12:27:15.145983: train_loss -0.8131 
2025-01-29 12:27:15.151782: val_loss -0.7582 
2025-01-29 12:27:15.154905: Pseudo dice [np.float32(0.9673), np.float32(0.9008)] 
2025-01-29 12:27:15.157595: Epoch time: 48.44 s 
2025-01-29 12:27:16.323231:  
2025-01-29 12:27:16.326329: Epoch 685 
2025-01-29 12:27:16.329118: Current learning rate: 0.00354 
2025-01-29 12:28:04.861633: train_loss -0.8129 
2025-01-29 12:28:04.867626: val_loss -0.7886 
2025-01-29 12:28:04.869965: Pseudo dice [np.float32(0.9669), np.float32(0.846)] 
2025-01-29 12:28:04.872577: Epoch time: 48.54 s 
2025-01-29 12:28:06.032832:  
2025-01-29 12:28:06.035767: Epoch 686 
2025-01-29 12:28:06.038347: Current learning rate: 0.00353 
2025-01-29 12:28:53.975282: train_loss -0.8165 
2025-01-29 12:28:53.980941: val_loss -0.7825 
2025-01-29 12:28:53.983780: Pseudo dice [np.float32(0.9687), np.float32(0.8725)] 
2025-01-29 12:28:53.986233: Epoch time: 47.94 s 
2025-01-29 12:28:55.150338:  
2025-01-29 12:28:55.153364: Epoch 687 
2025-01-29 12:28:55.156160: Current learning rate: 0.00352 
2025-01-29 12:29:43.074317: train_loss -0.8315 
2025-01-29 12:29:43.078661: val_loss -0.8062 
2025-01-29 12:29:43.081670: Pseudo dice [np.float32(0.9639), np.float32(0.8944)] 
2025-01-29 12:29:43.084331: Epoch time: 47.92 s 
2025-01-29 12:29:44.251408:  
2025-01-29 12:29:44.254365: Epoch 688 
2025-01-29 12:29:44.257243: Current learning rate: 0.00351 
2025-01-29 12:30:32.715838: train_loss -0.8158 
2025-01-29 12:30:32.721414: val_loss -0.826 
2025-01-29 12:30:32.728704: Pseudo dice [np.float32(0.9698), np.float32(0.8727)] 
2025-01-29 12:30:32.731295: Epoch time: 48.47 s 
2025-01-29 12:30:33.927624:  
2025-01-29 12:30:33.930757: Epoch 689 
2025-01-29 12:30:33.933530: Current learning rate: 0.0035 
2025-01-29 12:31:22.545658: train_loss -0.8297 
2025-01-29 12:31:22.550308: val_loss -0.8164 
2025-01-29 12:31:22.553275: Pseudo dice [np.float32(0.9704), np.float32(0.8925)] 
2025-01-29 12:31:22.555862: Epoch time: 48.62 s 
2025-01-29 12:31:23.718127:  
2025-01-29 12:31:23.721202: Epoch 690 
2025-01-29 12:31:23.724355: Current learning rate: 0.00349 
2025-01-29 12:32:12.089356: train_loss -0.8389 
2025-01-29 12:32:12.112337: val_loss -0.7809 
2025-01-29 12:32:12.115356: Pseudo dice [np.float32(0.9661), np.float32(0.8524)] 
2025-01-29 12:32:12.118042: Epoch time: 48.37 s 
2025-01-29 12:32:13.264485:  
2025-01-29 12:32:13.267560: Epoch 691 
2025-01-29 12:32:13.270252: Current learning rate: 0.00348 
2025-01-29 12:33:01.478491: train_loss -0.8085 
2025-01-29 12:33:01.483638: val_loss -0.77 
2025-01-29 12:33:01.486382: Pseudo dice [np.float32(0.966), np.float32(0.8709)] 
2025-01-29 12:33:01.489091: Epoch time: 48.21 s 
2025-01-29 12:33:02.653061:  
2025-01-29 12:33:02.656213: Epoch 692 
2025-01-29 12:33:02.659129: Current learning rate: 0.00346 
2025-01-29 12:33:51.386382: train_loss -0.8237 
2025-01-29 12:33:51.394267: val_loss -0.7342 
2025-01-29 12:33:51.397178: Pseudo dice [np.float32(0.9661), np.float32(0.862)] 
2025-01-29 12:33:51.399671: Epoch time: 48.73 s 
2025-01-29 12:33:52.561065:  
2025-01-29 12:33:52.563944: Epoch 693 
2025-01-29 12:33:52.566744: Current learning rate: 0.00345 
2025-01-29 12:34:40.839948: train_loss -0.8183 
2025-01-29 12:34:40.844668: val_loss -0.7451 
2025-01-29 12:34:40.847288: Pseudo dice [np.float32(0.9638), np.float32(0.8312)] 
2025-01-29 12:34:40.849909: Epoch time: 48.28 s 
2025-01-29 12:34:42.618439:  
2025-01-29 12:34:42.621243: Epoch 694 
2025-01-29 12:34:42.623965: Current learning rate: 0.00344 
2025-01-29 12:35:31.184849: train_loss -0.8063 
2025-01-29 12:35:31.190594: val_loss -0.7806 
2025-01-29 12:35:31.193318: Pseudo dice [np.float32(0.9641), np.float32(0.8289)] 
2025-01-29 12:35:31.195859: Epoch time: 48.57 s 
2025-01-29 12:35:32.377456:  
2025-01-29 12:35:32.380143: Epoch 695 
2025-01-29 12:35:32.382890: Current learning rate: 0.00343 
2025-01-29 12:36:20.269592: train_loss -0.8271 
2025-01-29 12:36:20.273928: val_loss -0.7857 
2025-01-29 12:36:20.283109: Pseudo dice [np.float32(0.9691), np.float32(0.9095)] 
2025-01-29 12:36:20.285957: Epoch time: 47.89 s 
2025-01-29 12:36:21.485804:  
2025-01-29 12:36:21.488868: Epoch 696 
2025-01-29 12:36:21.491468: Current learning rate: 0.00342 
2025-01-29 12:37:09.494053: train_loss -0.8301 
2025-01-29 12:37:09.499998: val_loss -0.8035 
2025-01-29 12:37:09.502610: Pseudo dice [np.float32(0.9712), np.float32(0.896)] 
2025-01-29 12:37:09.505175: Epoch time: 48.01 s 
2025-01-29 12:37:10.676291:  
2025-01-29 12:37:10.678883: Epoch 697 
2025-01-29 12:37:10.681494: Current learning rate: 0.00341 
2025-01-29 12:37:58.625026: train_loss -0.8233 
2025-01-29 12:37:58.628787: val_loss -0.8157 
2025-01-29 12:37:58.631243: Pseudo dice [np.float32(0.9676), np.float32(0.8884)] 
2025-01-29 12:37:58.633430: Epoch time: 47.95 s 
2025-01-29 12:37:59.808034:  
2025-01-29 12:37:59.810653: Epoch 698 
2025-01-29 12:37:59.813256: Current learning rate: 0.0034 
2025-01-29 12:38:47.649414: train_loss -0.8242 
2025-01-29 12:38:47.654815: val_loss -0.7376 
2025-01-29 12:38:47.657457: Pseudo dice [np.float32(0.9639), np.float32(0.8543)] 
2025-01-29 12:38:47.659829: Epoch time: 47.84 s 
2025-01-29 12:38:48.835514:  
2025-01-29 12:38:48.838354: Epoch 699 
2025-01-29 12:38:48.840878: Current learning rate: 0.00339 
2025-01-29 12:39:36.856309: train_loss -0.8497 
2025-01-29 12:39:36.860732: val_loss -0.7813 
2025-01-29 12:39:36.868009: Pseudo dice [np.float32(0.9657), np.float32(0.9102)] 
2025-01-29 12:39:36.870651: Epoch time: 48.02 s 
2025-01-29 12:39:38.602211:  
2025-01-29 12:39:38.605353: Epoch 700 
2025-01-29 12:39:38.608327: Current learning rate: 0.00338 
2025-01-29 12:40:26.704661: train_loss -0.8301 
2025-01-29 12:40:26.709766: val_loss -0.7732 
2025-01-29 12:40:26.712249: Pseudo dice [np.float32(0.9689), np.float32(0.8868)] 
2025-01-29 12:40:26.714741: Epoch time: 48.1 s 
2025-01-29 12:40:27.897800:  
2025-01-29 12:40:27.900719: Epoch 701 
2025-01-29 12:40:27.903601: Current learning rate: 0.00337 
2025-01-29 12:41:16.235109: train_loss -0.8311 
2025-01-29 12:41:16.238903: val_loss -0.8515 
2025-01-29 12:41:16.241597: Pseudo dice [np.float32(0.9674), np.float32(0.9098)] 
2025-01-29 12:41:16.244378: Epoch time: 48.34 s 
2025-01-29 12:41:17.418213:  
2025-01-29 12:41:17.421268: Epoch 702 
2025-01-29 12:41:17.424579: Current learning rate: 0.00336 
2025-01-29 12:42:05.471304: train_loss -0.8392 
2025-01-29 12:42:05.476766: val_loss -0.8313 
2025-01-29 12:42:05.478981: Pseudo dice [np.float32(0.9677), np.float32(0.8937)] 
2025-01-29 12:42:05.481425: Epoch time: 48.05 s 
2025-01-29 12:42:06.655279:  
2025-01-29 12:42:06.657619: Epoch 703 
2025-01-29 12:42:06.659791: Current learning rate: 0.00335 
2025-01-29 12:42:55.030980: train_loss -0.8261 
2025-01-29 12:42:55.037464: val_loss -0.8012 
2025-01-29 12:42:55.040556: Pseudo dice [np.float32(0.9705), np.float32(0.8841)] 
2025-01-29 12:42:55.047551: Epoch time: 48.38 s 
2025-01-29 12:42:56.223813:  
2025-01-29 12:42:56.226974: Epoch 704 
2025-01-29 12:42:56.229822: Current learning rate: 0.00334 
2025-01-29 12:43:44.279944: train_loss -0.8199 
2025-01-29 12:43:44.285051: val_loss -0.808 
2025-01-29 12:43:44.287865: Pseudo dice [np.float32(0.9682), np.float32(0.8951)] 
2025-01-29 12:43:44.290455: Epoch time: 48.06 s 
2025-01-29 12:43:44.292994: Yayy! New best EMA pseudo Dice: 0.9254999756813049 
2025-01-29 12:43:46.014491:  
2025-01-29 12:43:46.017812: Epoch 705 
2025-01-29 12:43:46.020725: Current learning rate: 0.00333 
2025-01-29 12:44:34.116761: train_loss -0.8175 
2025-01-29 12:44:34.120390: val_loss -0.7913 
2025-01-29 12:44:34.123252: Pseudo dice [np.float32(0.9646), np.float32(0.8481)] 
2025-01-29 12:44:34.125618: Epoch time: 48.1 s 
2025-01-29 12:44:35.301759:  
2025-01-29 12:44:35.304779: Epoch 706 
2025-01-29 12:44:35.307833: Current learning rate: 0.00332 
2025-01-29 12:45:23.530928: train_loss -0.8247 
2025-01-29 12:45:23.536702: val_loss -0.7609 
2025-01-29 12:45:23.539670: Pseudo dice [np.float32(0.9631), np.float32(0.8791)] 
2025-01-29 12:45:23.542640: Epoch time: 48.23 s 
2025-01-29 12:45:24.719682:  
2025-01-29 12:45:24.722676: Epoch 707 
2025-01-29 12:45:24.725565: Current learning rate: 0.00331 
2025-01-29 12:46:13.160150: train_loss -0.8296 
2025-01-29 12:46:13.163864: val_loss -0.7866 
2025-01-29 12:46:13.166483: Pseudo dice [np.float32(0.9673), np.float32(0.876)] 
2025-01-29 12:46:13.169096: Epoch time: 48.44 s 
2025-01-29 12:46:14.382693:  
2025-01-29 12:46:14.385358: Epoch 708 
2025-01-29 12:46:14.388075: Current learning rate: 0.0033 
2025-01-29 12:47:02.338803: train_loss -0.8312 
2025-01-29 12:47:02.343483: val_loss -0.8031 
2025-01-29 12:47:02.346085: Pseudo dice [np.float32(0.9665), np.float32(0.8773)] 
2025-01-29 12:47:02.348375: Epoch time: 47.96 s 
2025-01-29 12:47:03.525991:  
2025-01-29 12:47:03.528663: Epoch 709 
2025-01-29 12:47:03.531334: Current learning rate: 0.00329 
2025-01-29 12:47:51.979436: train_loss -0.819 
2025-01-29 12:47:51.984114: val_loss -0.7412 
2025-01-29 12:47:51.986999: Pseudo dice [np.float32(0.9689), np.float32(0.8272)] 
2025-01-29 12:47:51.989913: Epoch time: 48.45 s 
2025-01-29 12:47:53.169458:  
2025-01-29 12:47:53.172137: Epoch 710 
2025-01-29 12:47:53.174814: Current learning rate: 0.00328 
2025-01-29 12:48:41.364257: train_loss -0.8407 
2025-01-29 12:48:41.368754: val_loss -0.7551 
2025-01-29 12:48:41.371063: Pseudo dice [np.float32(0.9721), np.float32(0.8911)] 
2025-01-29 12:48:41.373481: Epoch time: 48.2 s 
2025-01-29 12:48:42.548568:  
2025-01-29 12:48:42.551266: Epoch 711 
2025-01-29 12:48:42.553671: Current learning rate: 0.00327 
2025-01-29 12:49:30.589515: train_loss -0.8343 
2025-01-29 12:49:30.594386: val_loss -0.7801 
2025-01-29 12:49:30.601917: Pseudo dice [np.float32(0.9646), np.float32(0.8671)] 
2025-01-29 12:49:30.604881: Epoch time: 48.04 s 
2025-01-29 12:49:32.362341:  
2025-01-29 12:49:32.365105: Epoch 712 
2025-01-29 12:49:32.367766: Current learning rate: 0.00326 
2025-01-29 12:50:20.268485: train_loss -0.8416 
2025-01-29 12:50:20.273677: val_loss -0.7657 
2025-01-29 12:50:20.276079: Pseudo dice [np.float32(0.9657), np.float32(0.894)] 
2025-01-29 12:50:20.278556: Epoch time: 47.91 s 
2025-01-29 12:50:21.453000:  
2025-01-29 12:50:21.455935: Epoch 713 
2025-01-29 12:50:21.458577: Current learning rate: 0.00325 
2025-01-29 12:51:09.833811: train_loss -0.8075 
2025-01-29 12:51:09.837375: val_loss -0.7825 
2025-01-29 12:51:09.839686: Pseudo dice [np.float32(0.9656), np.float32(0.8064)] 
2025-01-29 12:51:09.842287: Epoch time: 48.38 s 
2025-01-29 12:51:11.020602:  
2025-01-29 12:51:11.023706: Epoch 714 
2025-01-29 12:51:11.026608: Current learning rate: 0.00324 
2025-01-29 12:51:59.116597: train_loss -0.8318 
2025-01-29 12:51:59.122651: val_loss -0.7492 
2025-01-29 12:51:59.125139: Pseudo dice [np.float32(0.9638), np.float32(0.8616)] 
2025-01-29 12:51:59.127699: Epoch time: 48.1 s 
2025-01-29 12:52:00.303776:  
2025-01-29 12:52:00.306649: Epoch 715 
2025-01-29 12:52:00.309262: Current learning rate: 0.00323 
2025-01-29 12:52:48.374919: train_loss -0.836 
2025-01-29 12:52:48.379730: val_loss -0.7942 
2025-01-29 12:52:48.382572: Pseudo dice [np.float32(0.9689), np.float32(0.8795)] 
2025-01-29 12:52:48.384984: Epoch time: 48.07 s 
2025-01-29 12:52:49.592206:  
2025-01-29 12:52:49.594845: Epoch 716 
2025-01-29 12:52:49.597547: Current learning rate: 0.00322 
2025-01-29 12:53:38.069292: train_loss -0.8264 
2025-01-29 12:53:38.075140: val_loss -0.7782 
2025-01-29 12:53:38.086064: Pseudo dice [np.float32(0.9668), np.float32(0.8873)] 
2025-01-29 12:53:38.088866: Epoch time: 48.48 s 
2025-01-29 12:53:39.293726:  
2025-01-29 12:53:39.296570: Epoch 717 
2025-01-29 12:53:39.299176: Current learning rate: 0.00321 
2025-01-29 12:54:26.900747: train_loss -0.8302 
2025-01-29 12:54:26.907033: val_loss -0.7795 
2025-01-29 12:54:26.909768: Pseudo dice [np.float32(0.9645), np.float32(0.8923)] 
2025-01-29 12:54:26.912729: Epoch time: 47.61 s 
2025-01-29 12:54:28.088131:  
2025-01-29 12:54:28.091503: Epoch 718 
2025-01-29 12:54:28.094406: Current learning rate: 0.0032 
2025-01-29 12:55:16.153709: train_loss -0.8258 
2025-01-29 12:55:16.158954: val_loss -0.7865 
2025-01-29 12:55:16.161608: Pseudo dice [np.float32(0.9656), np.float32(0.8903)] 
2025-01-29 12:55:16.164114: Epoch time: 48.07 s 
2025-01-29 12:55:17.343743:  
2025-01-29 12:55:17.346821: Epoch 719 
2025-01-29 12:55:17.349179: Current learning rate: 0.00319 
2025-01-29 12:56:05.254504: train_loss -0.8397 
2025-01-29 12:56:05.260534: val_loss -0.7966 
2025-01-29 12:56:05.267609: Pseudo dice [np.float32(0.9693), np.float32(0.9019)] 
2025-01-29 12:56:05.270655: Epoch time: 47.91 s 
2025-01-29 12:56:06.456506:  
2025-01-29 12:56:06.459918: Epoch 720 
2025-01-29 12:56:06.462706: Current learning rate: 0.00318 
2025-01-29 12:56:54.812594: train_loss -0.8323 
2025-01-29 12:56:54.819604: val_loss -0.8006 
2025-01-29 12:56:54.822106: Pseudo dice [np.float32(0.9693), np.float32(0.8982)] 
2025-01-29 12:56:54.824619: Epoch time: 48.36 s 
2025-01-29 12:56:56.026919:  
2025-01-29 12:56:56.029971: Epoch 721 
2025-01-29 12:56:56.032879: Current learning rate: 0.00317 
2025-01-29 12:57:44.252960: train_loss -0.8401 
2025-01-29 12:57:44.259506: val_loss -0.7809 
2025-01-29 12:57:44.262233: Pseudo dice [np.float32(0.9677), np.float32(0.8866)] 
2025-01-29 12:57:44.264863: Epoch time: 48.23 s 
2025-01-29 12:57:45.446864:  
2025-01-29 12:57:45.449522: Epoch 722 
2025-01-29 12:57:45.452054: Current learning rate: 0.00316 
2025-01-29 12:58:33.596838: train_loss -0.851 
2025-01-29 12:58:33.603668: val_loss -0.8193 
2025-01-29 12:58:33.606237: Pseudo dice [np.float32(0.9683), np.float32(0.8523)] 
2025-01-29 12:58:33.608748: Epoch time: 48.15 s 
2025-01-29 12:58:34.790750:  
2025-01-29 12:58:34.793738: Epoch 723 
2025-01-29 12:58:34.796493: Current learning rate: 0.00315 
2025-01-29 12:59:23.190394: train_loss -0.8376 
2025-01-29 12:59:23.196241: val_loss -0.8124 
2025-01-29 12:59:23.198948: Pseudo dice [np.float32(0.9693), np.float32(0.887)] 
2025-01-29 12:59:23.201767: Epoch time: 48.4 s 
2025-01-29 12:59:24.390610:  
2025-01-29 12:59:24.393452: Epoch 724 
2025-01-29 12:59:24.396096: Current learning rate: 0.00314 
2025-01-29 13:00:12.704053: train_loss -0.8362 
2025-01-29 13:00:12.709755: val_loss -0.7827 
2025-01-29 13:00:12.716856: Pseudo dice [np.float32(0.9702), np.float32(0.8779)] 
2025-01-29 13:00:12.719756: Epoch time: 48.31 s 
2025-01-29 13:00:13.901112:  
2025-01-29 13:00:13.903897: Epoch 725 
2025-01-29 13:00:13.906431: Current learning rate: 0.00313 
2025-01-29 13:01:01.925062: train_loss -0.8282 
2025-01-29 13:01:01.930348: val_loss -0.7726 
2025-01-29 13:01:01.933312: Pseudo dice [np.float32(0.9679), np.float32(0.8934)] 
2025-01-29 13:01:01.936335: Epoch time: 48.02 s 
2025-01-29 13:01:03.111492:  
2025-01-29 13:01:03.114193: Epoch 726 
2025-01-29 13:01:03.117212: Current learning rate: 0.00312 
2025-01-29 13:01:50.870701: train_loss -0.8394 
2025-01-29 13:01:50.876267: val_loss -0.8153 
2025-01-29 13:01:50.879103: Pseudo dice [np.float32(0.9688), np.float32(0.888)] 
2025-01-29 13:01:50.881663: Epoch time: 47.76 s 
2025-01-29 13:01:52.057003:  
2025-01-29 13:01:52.059895: Epoch 727 
2025-01-29 13:01:52.062612: Current learning rate: 0.00311 
2025-01-29 13:02:40.160454: train_loss -0.845 
2025-01-29 13:02:40.164323: val_loss -0.7858 
2025-01-29 13:02:40.166903: Pseudo dice [np.float32(0.9677), np.float32(0.9089)] 
2025-01-29 13:02:40.169470: Epoch time: 48.1 s 
2025-01-29 13:02:40.171868: Yayy! New best EMA pseudo Dice: 0.9258000254631042 
2025-01-29 13:02:41.876034:  
2025-01-29 13:02:41.879629: Epoch 728 
2025-01-29 13:02:41.882353: Current learning rate: 0.0031 
2025-01-29 13:03:30.503159: train_loss -0.8515 
2025-01-29 13:03:30.510021: val_loss -0.7599 
2025-01-29 13:03:30.512923: Pseudo dice [np.float32(0.9696), np.float32(0.8602)] 
2025-01-29 13:03:30.515598: Epoch time: 48.63 s 
2025-01-29 13:03:31.707294:  
2025-01-29 13:03:31.710117: Epoch 729 
2025-01-29 13:03:31.712883: Current learning rate: 0.00309 
2025-01-29 13:04:19.738595: train_loss -0.8275 
2025-01-29 13:04:19.744595: val_loss -0.7945 
2025-01-29 13:04:19.747489: Pseudo dice [np.float32(0.9667), np.float32(0.884)] 
2025-01-29 13:04:19.750370: Epoch time: 48.03 s 
2025-01-29 13:04:21.457109:  
2025-01-29 13:04:21.459878: Epoch 730 
2025-01-29 13:04:21.462664: Current learning rate: 0.00308 
2025-01-29 13:05:09.205181: train_loss -0.8406 
2025-01-29 13:05:09.211408: val_loss -0.7797 
2025-01-29 13:05:09.213987: Pseudo dice [np.float32(0.9603), np.float32(0.8661)] 
2025-01-29 13:05:09.216394: Epoch time: 47.75 s 
2025-01-29 13:05:10.395272:  
2025-01-29 13:05:10.399042: Epoch 731 
2025-01-29 13:05:10.401624: Current learning rate: 0.00307 
2025-01-29 13:05:58.877389: train_loss -0.824 
2025-01-29 13:05:58.880995: val_loss -0.7945 
2025-01-29 13:05:58.883427: Pseudo dice [np.float32(0.966), np.float32(0.8833)] 
2025-01-29 13:05:58.885850: Epoch time: 48.48 s 
2025-01-29 13:06:00.059956:  
2025-01-29 13:06:00.062721: Epoch 732 
2025-01-29 13:06:00.065294: Current learning rate: 0.00306 
2025-01-29 13:06:48.148907: train_loss -0.8362 
2025-01-29 13:06:48.156126: val_loss -0.8081 
2025-01-29 13:06:48.158914: Pseudo dice [np.float32(0.9695), np.float32(0.8741)] 
2025-01-29 13:06:48.161740: Epoch time: 48.09 s 
2025-01-29 13:06:49.346708:  
2025-01-29 13:06:49.349658: Epoch 733 
2025-01-29 13:06:49.352651: Current learning rate: 0.00305 
2025-01-29 13:07:37.367015: train_loss -0.8259 
2025-01-29 13:07:37.371469: val_loss -0.826 
2025-01-29 13:07:37.378447: Pseudo dice [np.float32(0.971), np.float32(0.8989)] 
2025-01-29 13:07:37.381043: Epoch time: 48.02 s 
2025-01-29 13:07:38.561779:  
2025-01-29 13:07:38.564515: Epoch 734 
2025-01-29 13:07:38.567508: Current learning rate: 0.00304 
2025-01-29 13:08:26.686306: train_loss -0.8381 
2025-01-29 13:08:26.692500: val_loss -0.7507 
2025-01-29 13:08:26.695334: Pseudo dice [np.float32(0.9688), np.float32(0.874)] 
2025-01-29 13:08:26.698546: Epoch time: 48.13 s 
2025-01-29 13:08:27.883657:  
2025-01-29 13:08:27.886682: Epoch 735 
2025-01-29 13:08:27.889648: Current learning rate: 0.00303 
2025-01-29 13:09:16.082644: train_loss -0.8359 
2025-01-29 13:09:16.088123: val_loss -0.78 
2025-01-29 13:09:16.090806: Pseudo dice [np.float32(0.9679), np.float32(0.8507)] 
2025-01-29 13:09:16.093299: Epoch time: 48.2 s 
2025-01-29 13:09:17.273854:  
2025-01-29 13:09:17.276731: Epoch 736 
2025-01-29 13:09:17.279250: Current learning rate: 0.00302 
2025-01-29 13:10:05.347782: train_loss -0.8503 
2025-01-29 13:10:05.353229: val_loss -0.7707 
2025-01-29 13:10:05.355810: Pseudo dice [np.float32(0.9689), np.float32(0.9017)] 
2025-01-29 13:10:05.358568: Epoch time: 48.07 s 
2025-01-29 13:10:06.533628:  
2025-01-29 13:10:06.536844: Epoch 737 
2025-01-29 13:10:06.539788: Current learning rate: 0.00301 
2025-01-29 13:10:54.960607: train_loss -0.8219 
2025-01-29 13:10:54.966563: val_loss -0.8301 
2025-01-29 13:10:54.969592: Pseudo dice [np.float32(0.9638), np.float32(0.87)] 
2025-01-29 13:10:54.972450: Epoch time: 48.43 s 
2025-01-29 13:10:56.149780:  
2025-01-29 13:10:56.152707: Epoch 738 
2025-01-29 13:10:56.155543: Current learning rate: 0.003 
2025-01-29 13:11:44.179586: train_loss -0.8276 
2025-01-29 13:11:44.184494: val_loss -0.7473 
2025-01-29 13:11:44.191753: Pseudo dice [np.float32(0.966), np.float32(0.8924)] 
2025-01-29 13:11:44.194202: Epoch time: 48.03 s 
2025-01-29 13:11:45.370364:  
2025-01-29 13:11:45.372874: Epoch 739 
2025-01-29 13:11:45.375265: Current learning rate: 0.00299 
2025-01-29 13:12:33.471908: train_loss -0.8136 
2025-01-29 13:12:33.476060: val_loss -0.7435 
2025-01-29 13:12:33.478688: Pseudo dice [np.float32(0.9633), np.float32(0.8115)] 
2025-01-29 13:12:33.481158: Epoch time: 48.1 s 
2025-01-29 13:12:34.660951:  
2025-01-29 13:12:34.663589: Epoch 740 
2025-01-29 13:12:34.666114: Current learning rate: 0.00297 
2025-01-29 13:13:22.610103: train_loss -0.8373 
2025-01-29 13:13:22.615699: val_loss -0.7529 
2025-01-29 13:13:22.618562: Pseudo dice [np.float32(0.9668), np.float32(0.8384)] 
2025-01-29 13:13:22.621289: Epoch time: 47.95 s 
2025-01-29 13:13:23.795632:  
2025-01-29 13:13:23.798651: Epoch 741 
2025-01-29 13:13:23.801455: Current learning rate: 0.00296 
2025-01-29 13:14:12.008667: train_loss -0.8241 
2025-01-29 13:14:12.012853: val_loss -0.6992 
2025-01-29 13:14:12.015694: Pseudo dice [np.float32(0.9602), np.float32(0.7842)] 
2025-01-29 13:14:12.018275: Epoch time: 48.21 s 
2025-01-29 13:14:13.196151:  
2025-01-29 13:14:13.199108: Epoch 742 
2025-01-29 13:14:13.201400: Current learning rate: 0.00295 
2025-01-29 13:15:01.282367: train_loss -0.8176 
2025-01-29 13:15:01.288102: val_loss -0.787 
2025-01-29 13:15:01.295259: Pseudo dice [np.float32(0.9667), np.float32(0.8993)] 
2025-01-29 13:15:01.297572: Epoch time: 48.09 s 
2025-01-29 13:15:02.485214:  
2025-01-29 13:15:02.487928: Epoch 743 
2025-01-29 13:15:02.490815: Current learning rate: 0.00294 
2025-01-29 13:15:50.676462: train_loss -0.8336 
2025-01-29 13:15:50.680870: val_loss -0.7534 
2025-01-29 13:15:50.683966: Pseudo dice [np.float32(0.9681), np.float32(0.8971)] 
2025-01-29 13:15:50.686343: Epoch time: 48.19 s 
2025-01-29 13:15:51.864824:  
2025-01-29 13:15:51.867778: Epoch 744 
2025-01-29 13:15:51.870353: Current learning rate: 0.00293 
2025-01-29 13:16:39.781437: train_loss -0.8411 
2025-01-29 13:16:39.787357: val_loss -0.7555 
2025-01-29 13:16:39.790166: Pseudo dice [np.float32(0.9668), np.float32(0.8709)] 
2025-01-29 13:16:39.792622: Epoch time: 47.92 s 
2025-01-29 13:16:40.975036:  
2025-01-29 13:16:40.978070: Epoch 745 
2025-01-29 13:16:40.980911: Current learning rate: 0.00292 
2025-01-29 13:17:29.606686: train_loss -0.8122 
2025-01-29 13:17:29.614010: val_loss -0.8016 
2025-01-29 13:17:29.616539: Pseudo dice [np.float32(0.9672), np.float32(0.8903)] 
2025-01-29 13:17:29.618937: Epoch time: 48.63 s 
2025-01-29 13:17:30.811052:  
2025-01-29 13:17:30.813665: Epoch 746 
2025-01-29 13:17:30.816571: Current learning rate: 0.00291 
2025-01-29 13:18:19.337981: train_loss -0.8318 
2025-01-29 13:18:19.343806: val_loss -0.7773 
2025-01-29 13:18:19.351055: Pseudo dice [np.float32(0.9679), np.float32(0.883)] 
2025-01-29 13:18:19.353660: Epoch time: 48.53 s 
2025-01-29 13:18:21.053971:  
2025-01-29 13:18:21.056521: Epoch 747 
2025-01-29 13:18:21.058806: Current learning rate: 0.0029 
2025-01-29 13:19:09.321135: train_loss -0.8354 
2025-01-29 13:19:09.325556: val_loss -0.8054 
2025-01-29 13:19:09.328454: Pseudo dice [np.float32(0.9687), np.float32(0.8944)] 
2025-01-29 13:19:09.331123: Epoch time: 48.27 s 
2025-01-29 13:19:10.508188:  
2025-01-29 13:19:10.510758: Epoch 748 
2025-01-29 13:19:10.513317: Current learning rate: 0.00289 
2025-01-29 13:19:58.800543: train_loss -0.8211 
2025-01-29 13:19:58.806783: val_loss -0.785 
2025-01-29 13:19:58.809473: Pseudo dice [np.float32(0.9608), np.float32(0.8583)] 
2025-01-29 13:19:58.812045: Epoch time: 48.29 s 
2025-01-29 13:19:59.989218:  
2025-01-29 13:19:59.992336: Epoch 749 
2025-01-29 13:19:59.995330: Current learning rate: 0.00288 
2025-01-29 13:20:48.122445: train_loss -0.8307 
2025-01-29 13:20:48.127965: val_loss -0.7787 
2025-01-29 13:20:48.130637: Pseudo dice [np.float32(0.9692), np.float32(0.9018)] 
2025-01-29 13:20:48.133323: Epoch time: 48.13 s 
2025-01-29 13:20:49.826584:  
2025-01-29 13:20:49.829772: Epoch 750 
2025-01-29 13:20:49.832530: Current learning rate: 0.00287 
2025-01-29 13:21:37.816028: train_loss -0.8299 
2025-01-29 13:21:37.824204: val_loss -0.7806 
2025-01-29 13:21:37.826830: Pseudo dice [np.float32(0.9624), np.float32(0.8558)] 
2025-01-29 13:21:37.829422: Epoch time: 47.99 s 
2025-01-29 13:21:39.010644:  
2025-01-29 13:21:39.013626: Epoch 751 
2025-01-29 13:21:39.016745: Current learning rate: 0.00286 
2025-01-29 13:22:27.145362: train_loss -0.8489 
2025-01-29 13:22:27.149729: val_loss -0.8389 
2025-01-29 13:22:27.152601: Pseudo dice [np.float32(0.9664), np.float32(0.8749)] 
2025-01-29 13:22:27.155256: Epoch time: 48.14 s 
2025-01-29 13:22:28.339755:  
2025-01-29 13:22:28.342749: Epoch 752 
2025-01-29 13:22:28.345493: Current learning rate: 0.00285 
2025-01-29 13:23:16.627292: train_loss -0.8349 
2025-01-29 13:23:16.633374: val_loss -0.8073 
2025-01-29 13:23:16.636019: Pseudo dice [np.float32(0.9682), np.float32(0.904)] 
2025-01-29 13:23:16.638328: Epoch time: 48.29 s 
2025-01-29 13:23:17.816617:  
2025-01-29 13:23:17.819778: Epoch 753 
2025-01-29 13:23:17.822482: Current learning rate: 0.00284 
2025-01-29 13:24:06.062892: train_loss -0.8476 
2025-01-29 13:24:06.068640: val_loss -0.8231 
2025-01-29 13:24:06.071378: Pseudo dice [np.float32(0.9695), np.float32(0.8845)] 
2025-01-29 13:24:06.074328: Epoch time: 48.25 s 
2025-01-29 13:24:07.258380:  
2025-01-29 13:24:07.261598: Epoch 754 
2025-01-29 13:24:07.264412: Current learning rate: 0.00283 
2025-01-29 13:24:55.610518: train_loss -0.8397 
2025-01-29 13:24:55.616204: val_loss -0.8143 
2025-01-29 13:24:55.619065: Pseudo dice [np.float32(0.9654), np.float32(0.8907)] 
2025-01-29 13:24:55.621868: Epoch time: 48.35 s 
2025-01-29 13:24:56.806454:  
2025-01-29 13:24:56.809022: Epoch 755 
2025-01-29 13:24:56.811677: Current learning rate: 0.00282 
2025-01-29 13:25:45.230601: train_loss -0.8199 
2025-01-29 13:25:45.236978: val_loss -0.8054 
2025-01-29 13:25:45.239809: Pseudo dice [np.float32(0.9705), np.float32(0.898)] 
2025-01-29 13:25:45.242600: Epoch time: 48.43 s 
2025-01-29 13:25:46.419738:  
2025-01-29 13:25:46.422709: Epoch 756 
2025-01-29 13:25:46.425314: Current learning rate: 0.00281 
2025-01-29 13:26:34.920243: train_loss -0.8291 
2025-01-29 13:26:35.136049: val_loss -0.8269 
2025-01-29 13:26:35.138973: Pseudo dice [np.float32(0.9666), np.float32(0.8715)] 
2025-01-29 13:26:35.141698: Epoch time: 48.5 s 
2025-01-29 13:26:36.309094:  
2025-01-29 13:26:36.312118: Epoch 757 
2025-01-29 13:26:36.314810: Current learning rate: 0.0028 
2025-01-29 13:27:24.344985: train_loss -0.8297 
2025-01-29 13:27:24.350093: val_loss -0.81 
2025-01-29 13:27:24.352955: Pseudo dice [np.float32(0.9703), np.float32(0.9052)] 
2025-01-29 13:27:24.355410: Epoch time: 48.04 s 
2025-01-29 13:27:25.522593:  
2025-01-29 13:27:25.525676: Epoch 758 
2025-01-29 13:27:25.528431: Current learning rate: 0.00279 
2025-01-29 13:28:13.689502: train_loss -0.8419 
2025-01-29 13:28:13.696719: val_loss -0.8282 
2025-01-29 13:28:13.699401: Pseudo dice [np.float32(0.9655), np.float32(0.9025)] 
2025-01-29 13:28:13.701931: Epoch time: 48.17 s 
2025-01-29 13:28:14.869102:  
2025-01-29 13:28:14.871853: Epoch 759 
2025-01-29 13:28:14.874400: Current learning rate: 0.00278 
2025-01-29 13:29:03.027071: train_loss -0.8447 
2025-01-29 13:29:03.032668: val_loss -0.8217 
2025-01-29 13:29:03.035691: Pseudo dice [np.float32(0.9683), np.float32(0.9061)] 
2025-01-29 13:29:03.039850: Epoch time: 48.16 s 
2025-01-29 13:29:03.042994: Yayy! New best EMA pseudo Dice: 0.9269000291824341 
2025-01-29 13:29:04.793939:  
2025-01-29 13:29:04.797032: Epoch 760 
2025-01-29 13:29:04.799952: Current learning rate: 0.00277 
2025-01-29 13:29:52.900521: train_loss -0.8302 
2025-01-29 13:29:52.908066: val_loss -0.8362 
2025-01-29 13:29:52.910760: Pseudo dice [np.float32(0.9689), np.float32(0.9043)] 
2025-01-29 13:29:52.913129: Epoch time: 48.11 s 
2025-01-29 13:29:52.915553: Yayy! New best EMA pseudo Dice: 0.9279000163078308 
2025-01-29 13:29:54.650044:  
2025-01-29 13:29:54.653006: Epoch 761 
2025-01-29 13:29:54.655976: Current learning rate: 0.00276 
2025-01-29 13:30:42.943874: train_loss -0.848 
2025-01-29 13:30:42.950596: val_loss -0.7968 
2025-01-29 13:30:42.953783: Pseudo dice [np.float32(0.9656), np.float32(0.8822)] 
2025-01-29 13:30:42.956681: Epoch time: 48.29 s 
2025-01-29 13:30:44.140561:  
2025-01-29 13:30:44.144699: Epoch 762 
2025-01-29 13:30:44.147474: Current learning rate: 0.00275 
2025-01-29 13:31:32.548457: train_loss -0.8368 
2025-01-29 13:31:32.553958: val_loss -0.8148 
2025-01-29 13:31:32.556418: Pseudo dice [np.float32(0.9653), np.float32(0.8809)] 
2025-01-29 13:31:32.558640: Epoch time: 48.41 s 
2025-01-29 13:31:33.757741:  
2025-01-29 13:31:33.761457: Epoch 763 
2025-01-29 13:31:33.764813: Current learning rate: 0.00274 
2025-01-29 13:32:21.775480: train_loss -0.8259 
2025-01-29 13:32:21.783967: val_loss -0.7995 
2025-01-29 13:32:21.793219: Pseudo dice [np.float32(0.9677), np.float32(0.888)] 
2025-01-29 13:32:21.796098: Epoch time: 48.02 s 
2025-01-29 13:32:23.590254:  
2025-01-29 13:32:23.593317: Epoch 764 
2025-01-29 13:32:23.596044: Current learning rate: 0.00273 
2025-01-29 13:33:11.906534: train_loss -0.8352 
2025-01-29 13:33:11.916862: val_loss -0.8254 
2025-01-29 13:33:11.919785: Pseudo dice [np.float32(0.9697), np.float32(0.8897)] 
2025-01-29 13:33:11.922576: Epoch time: 48.32 s 
2025-01-29 13:33:13.154198:  
2025-01-29 13:33:13.157320: Epoch 765 
2025-01-29 13:33:13.160124: Current learning rate: 0.00272 
2025-01-29 13:34:01.425179: train_loss -0.8429 
2025-01-29 13:34:01.433890: val_loss -0.8226 
2025-01-29 13:34:01.436594: Pseudo dice [np.float32(0.9726), np.float32(0.888)] 
2025-01-29 13:34:01.440092: Epoch time: 48.27 s 
2025-01-29 13:34:02.672541:  
2025-01-29 13:34:02.675815: Epoch 766 
2025-01-29 13:34:02.678932: Current learning rate: 0.00271 
2025-01-29 13:34:51.088972: train_loss -0.8237 
2025-01-29 13:34:51.097759: val_loss -0.799 
2025-01-29 13:34:51.100754: Pseudo dice [np.float32(0.9678), np.float32(0.8952)] 
2025-01-29 13:34:51.103283: Epoch time: 48.42 s 
2025-01-29 13:34:51.105751: Yayy! New best EMA pseudo Dice: 0.9280999898910522 
2025-01-29 13:34:52.840287:  
2025-01-29 13:34:52.843556: Epoch 767 
2025-01-29 13:34:52.846247: Current learning rate: 0.0027 
2025-01-29 13:35:41.027277: train_loss -0.8333 
2025-01-29 13:35:41.035219: val_loss -0.7721 
2025-01-29 13:35:41.037786: Pseudo dice [np.float32(0.9687), np.float32(0.8963)] 
2025-01-29 13:35:41.040457: Epoch time: 48.19 s 
2025-01-29 13:35:41.042876: Yayy! New best EMA pseudo Dice: 0.9284999966621399 
2025-01-29 13:35:42.822828:  
2025-01-29 13:35:42.825811: Epoch 768 
2025-01-29 13:35:42.828717: Current learning rate: 0.00268 
2025-01-29 13:36:30.733782: train_loss -0.8487 
2025-01-29 13:36:30.741691: val_loss -0.8326 
2025-01-29 13:36:30.744347: Pseudo dice [np.float32(0.9689), np.float32(0.908)] 
2025-01-29 13:36:30.746840: Epoch time: 47.91 s 
2025-01-29 13:36:30.749244: Yayy! New best EMA pseudo Dice: 0.9294999837875366 
2025-01-29 13:36:32.536423:  
2025-01-29 13:36:32.539511: Epoch 769 
2025-01-29 13:36:32.542490: Current learning rate: 0.00267 
2025-01-29 13:37:20.757291: train_loss -0.8413 
2025-01-29 13:37:20.764224: val_loss -0.8272 
2025-01-29 13:37:20.766626: Pseudo dice [np.float32(0.9682), np.float32(0.8541)] 
2025-01-29 13:37:20.768885: Epoch time: 48.22 s 
2025-01-29 13:37:21.953482:  
2025-01-29 13:37:21.956351: Epoch 770 
2025-01-29 13:37:21.958688: Current learning rate: 0.00266 
2025-01-29 13:38:10.267221: train_loss -0.8351 
2025-01-29 13:38:10.273475: val_loss -0.8052 
2025-01-29 13:38:10.276272: Pseudo dice [np.float32(0.9711), np.float32(0.8967)] 
2025-01-29 13:38:10.278963: Epoch time: 48.31 s 
2025-01-29 13:38:11.460159:  
2025-01-29 13:38:11.463013: Epoch 771 
2025-01-29 13:38:11.465824: Current learning rate: 0.00265 
2025-01-29 13:38:59.791101: train_loss -0.8499 
2025-01-29 13:38:59.798746: val_loss -0.8225 
2025-01-29 13:38:59.801981: Pseudo dice [np.float32(0.9709), np.float32(0.889)] 
2025-01-29 13:38:59.804925: Epoch time: 48.33 s 
2025-01-29 13:39:00.985842:  
2025-01-29 13:39:00.989168: Epoch 772 
2025-01-29 13:39:00.992699: Current learning rate: 0.00264 
2025-01-29 13:39:49.944686: train_loss -0.8484 
2025-01-29 13:39:49.951933: val_loss -0.8025 
2025-01-29 13:39:49.954287: Pseudo dice [np.float32(0.9684), np.float32(0.8929)] 
2025-01-29 13:39:49.956523: Epoch time: 48.96 s 
2025-01-29 13:39:51.139590:  
2025-01-29 13:39:51.142100: Epoch 773 
2025-01-29 13:39:51.144927: Current learning rate: 0.00263 
2025-01-29 13:40:39.409577: train_loss -0.8359 
2025-01-29 13:40:39.417742: val_loss -0.8126 
2025-01-29 13:40:39.420619: Pseudo dice [np.float32(0.971), np.float32(0.8921)] 
2025-01-29 13:40:39.423214: Epoch time: 48.27 s 
2025-01-29 13:40:40.604376:  
2025-01-29 13:40:40.607016: Epoch 774 
2025-01-29 13:40:40.609758: Current learning rate: 0.00262 
2025-01-29 13:41:28.693702: train_loss -0.8387 
2025-01-29 13:41:28.701925: val_loss -0.7567 
2025-01-29 13:41:28.704697: Pseudo dice [np.float32(0.9584), np.float32(0.8783)] 
2025-01-29 13:41:28.707573: Epoch time: 48.09 s 
2025-01-29 13:41:29.889067:  
2025-01-29 13:41:29.891878: Epoch 775 
2025-01-29 13:41:29.894632: Current learning rate: 0.00261 
2025-01-29 13:42:18.213571: train_loss -0.8493 
2025-01-29 13:42:18.224029: val_loss -0.8042 
2025-01-29 13:42:18.226999: Pseudo dice [np.float32(0.968), np.float32(0.901)] 
2025-01-29 13:42:18.229555: Epoch time: 48.33 s 
2025-01-29 13:42:19.421899:  
2025-01-29 13:42:19.424624: Epoch 776 
2025-01-29 13:42:19.427386: Current learning rate: 0.0026 
2025-01-29 13:43:07.643880: train_loss -0.8479 
2025-01-29 13:43:07.652418: val_loss -0.8332 
2025-01-29 13:43:07.655303: Pseudo dice [np.float32(0.9688), np.float32(0.9126)] 
2025-01-29 13:43:07.657711: Epoch time: 48.22 s 
2025-01-29 13:43:07.660127: Yayy! New best EMA pseudo Dice: 0.9297999739646912 
2025-01-29 13:43:09.439068:  
2025-01-29 13:43:09.442185: Epoch 777 
2025-01-29 13:43:09.445006: Current learning rate: 0.00259 
2025-01-29 13:43:57.584698: train_loss -0.8422 
2025-01-29 13:43:57.593431: val_loss -0.844 
2025-01-29 13:43:57.596241: Pseudo dice [np.float32(0.9691), np.float32(0.9052)] 
2025-01-29 13:43:57.598881: Epoch time: 48.15 s 
2025-01-29 13:43:57.601358: Yayy! New best EMA pseudo Dice: 0.9304999709129333 
2025-01-29 13:43:59.331370:  
2025-01-29 13:43:59.334501: Epoch 778 
2025-01-29 13:43:59.337489: Current learning rate: 0.00258 
2025-01-29 13:44:47.854210: train_loss -0.8526 
2025-01-29 13:44:47.862171: val_loss -0.7939 
2025-01-29 13:44:47.864959: Pseudo dice [np.float32(0.972), np.float32(0.9023)] 
2025-01-29 13:44:47.867572: Epoch time: 48.52 s 
2025-01-29 13:44:47.870142: Yayy! New best EMA pseudo Dice: 0.9312000274658203 
2025-01-29 13:44:49.635820:  
2025-01-29 13:44:49.638743: Epoch 779 
2025-01-29 13:44:49.641400: Current learning rate: 0.00257 
2025-01-29 13:45:37.515762: train_loss -0.8291 
2025-01-29 13:45:37.521430: val_loss -0.7648 
2025-01-29 13:45:37.524045: Pseudo dice [np.float32(0.9705), np.float32(0.8901)] 
2025-01-29 13:45:37.526426: Epoch time: 47.88 s 
2025-01-29 13:45:38.705896:  
2025-01-29 13:45:38.708721: Epoch 780 
2025-01-29 13:45:38.711473: Current learning rate: 0.00256 
2025-01-29 13:46:26.982209: train_loss -0.8576 
2025-01-29 13:46:26.991320: val_loss -0.8135 
2025-01-29 13:46:26.994070: Pseudo dice [np.float32(0.9723), np.float32(0.8848)] 
2025-01-29 13:46:26.996748: Epoch time: 48.28 s 
2025-01-29 13:46:28.750704:  
2025-01-29 13:46:28.753606: Epoch 781 
2025-01-29 13:46:28.756536: Current learning rate: 0.00255 
2025-01-29 13:47:16.830070: train_loss -0.8394 
2025-01-29 13:47:16.837864: val_loss -0.796 
2025-01-29 13:47:16.840765: Pseudo dice [np.float32(0.9703), np.float32(0.9004)] 
2025-01-29 13:47:16.843542: Epoch time: 48.08 s 
2025-01-29 13:47:16.846303: Yayy! New best EMA pseudo Dice: 0.9312999844551086 
2025-01-29 13:47:18.733756:  
2025-01-29 13:47:18.736699: Epoch 782 
2025-01-29 13:47:18.739751: Current learning rate: 0.00254 
2025-01-29 13:48:06.721558: train_loss -0.8403 
2025-01-29 13:48:06.729687: val_loss -0.8125 
2025-01-29 13:48:06.732483: Pseudo dice [np.float32(0.9684), np.float32(0.8916)] 
2025-01-29 13:48:06.735019: Epoch time: 47.99 s 
2025-01-29 13:48:07.948138:  
2025-01-29 13:48:07.950993: Epoch 783 
2025-01-29 13:48:07.953409: Current learning rate: 0.00253 
2025-01-29 13:48:56.177217: train_loss -0.8526 
2025-01-29 13:48:56.185542: val_loss -0.7445 
2025-01-29 13:48:56.188361: Pseudo dice [np.float32(0.9628), np.float32(0.8516)] 
2025-01-29 13:48:56.191108: Epoch time: 48.23 s 
2025-01-29 13:48:57.417954:  
2025-01-29 13:48:57.420876: Epoch 784 
2025-01-29 13:48:57.423621: Current learning rate: 0.00252 
2025-01-29 13:49:45.516442: train_loss -0.8354 
2025-01-29 13:49:45.522011: val_loss -0.8142 
2025-01-29 13:49:45.529926: Pseudo dice [np.float32(0.9705), np.float32(0.8879)] 
2025-01-29 13:49:45.533297: Epoch time: 48.1 s 
2025-01-29 13:49:46.756310:  
2025-01-29 13:49:46.759279: Epoch 785 
2025-01-29 13:49:46.762549: Current learning rate: 0.00251 
2025-01-29 13:50:34.985802: train_loss -0.8436 
2025-01-29 13:50:34.995007: val_loss -0.8045 
2025-01-29 13:50:34.998085: Pseudo dice [np.float32(0.97), np.float32(0.886)] 
2025-01-29 13:50:35.000909: Epoch time: 48.23 s 
2025-01-29 13:50:36.182441:  
2025-01-29 13:50:36.185857: Epoch 786 
2025-01-29 13:50:36.189042: Current learning rate: 0.0025 
2025-01-29 13:51:24.314066: train_loss -0.8404 
2025-01-29 13:51:24.319674: val_loss -0.7631 
2025-01-29 13:51:24.322310: Pseudo dice [np.float32(0.9685), np.float32(0.8757)] 
2025-01-29 13:51:24.324993: Epoch time: 48.13 s 
2025-01-29 13:51:25.518345:  
2025-01-29 13:51:25.521488: Epoch 787 
2025-01-29 13:51:25.524230: Current learning rate: 0.00249 
2025-01-29 13:52:13.436489: train_loss -0.8259 
2025-01-29 13:52:13.447867: val_loss -0.8047 
2025-01-29 13:52:13.450754: Pseudo dice [np.float32(0.9685), np.float32(0.9071)] 
2025-01-29 13:52:13.453425: Epoch time: 47.92 s 
2025-01-29 13:52:14.686819:  
2025-01-29 13:52:14.689974: Epoch 788 
2025-01-29 13:52:14.692788: Current learning rate: 0.00248 
2025-01-29 13:53:02.814026: train_loss -0.8318 
2025-01-29 13:53:02.820455: val_loss -0.8113 
2025-01-29 13:53:02.823624: Pseudo dice [np.float32(0.9711), np.float32(0.8954)] 
2025-01-29 13:53:02.826276: Epoch time: 48.13 s 
2025-01-29 13:53:04.009501:  
2025-01-29 13:53:04.012406: Epoch 789 
2025-01-29 13:53:04.015057: Current learning rate: 0.00247 
2025-01-29 13:53:52.401661: train_loss -0.8352 
2025-01-29 13:53:52.410650: val_loss -0.7717 
2025-01-29 13:53:52.413424: Pseudo dice [np.float32(0.9669), np.float32(0.8842)] 
2025-01-29 13:53:52.416297: Epoch time: 48.39 s 
2025-01-29 13:53:53.638011:  
2025-01-29 13:53:53.641328: Epoch 790 
2025-01-29 13:53:53.644204: Current learning rate: 0.00245 
2025-01-29 13:54:41.895062: train_loss -0.8295 
2025-01-29 13:54:41.905499: val_loss -0.8146 
2025-01-29 13:54:41.908552: Pseudo dice [np.float32(0.9702), np.float32(0.9028)] 
2025-01-29 13:54:41.911432: Epoch time: 48.26 s 
2025-01-29 13:54:43.113894:  
2025-01-29 13:54:43.116945: Epoch 791 
2025-01-29 13:54:43.119735: Current learning rate: 0.00244 
2025-01-29 13:55:31.453378: train_loss -0.8432 
2025-01-29 13:55:31.460399: val_loss -0.831 
2025-01-29 13:55:31.462764: Pseudo dice [np.float32(0.9698), np.float32(0.9069)] 
2025-01-29 13:55:31.465063: Epoch time: 48.34 s 
2025-01-29 13:55:32.659805:  
2025-01-29 13:55:32.662570: Epoch 792 
2025-01-29 13:55:32.665218: Current learning rate: 0.00243 
2025-01-29 13:56:21.208839: train_loss -0.8218 
2025-01-29 13:56:21.216998: val_loss -0.8136 
2025-01-29 13:56:21.219717: Pseudo dice [np.float32(0.9701), np.float32(0.902)] 
2025-01-29 13:56:21.222244: Epoch time: 48.55 s 
2025-01-29 13:56:22.448028:  
2025-01-29 13:56:22.451289: Epoch 793 
2025-01-29 13:56:22.454394: Current learning rate: 0.00242 
2025-01-29 13:57:10.848917: train_loss -0.8379 
2025-01-29 13:57:10.855843: val_loss -0.7607 
2025-01-29 13:57:10.858951: Pseudo dice [np.float32(0.9659), np.float32(0.8962)] 
2025-01-29 13:57:10.861463: Epoch time: 48.4 s 
2025-01-29 13:57:12.051537:  
2025-01-29 13:57:12.054143: Epoch 794 
2025-01-29 13:57:12.057284: Current learning rate: 0.00241 
2025-01-29 13:58:00.188787: train_loss -0.8442 
2025-01-29 13:58:00.196889: val_loss -0.7947 
2025-01-29 13:58:00.199469: Pseudo dice [np.float32(0.9707), np.float32(0.8573)] 
2025-01-29 13:58:00.201910: Epoch time: 48.14 s 
2025-01-29 13:58:01.410839:  
2025-01-29 13:58:01.413477: Epoch 795 
2025-01-29 13:58:01.416199: Current learning rate: 0.0024 
2025-01-29 13:58:49.441849: train_loss -0.8278 
2025-01-29 13:58:49.450066: val_loss -0.8079 
2025-01-29 13:58:49.453308: Pseudo dice [np.float32(0.962), np.float32(0.8902)] 
2025-01-29 13:58:49.456346: Epoch time: 48.03 s 
2025-01-29 13:58:50.676839:  
2025-01-29 13:58:50.679395: Epoch 796 
2025-01-29 13:58:50.682119: Current learning rate: 0.00239 
2025-01-29 13:59:38.791114: train_loss -0.8461 
2025-01-29 13:59:38.799321: val_loss -0.8107 
2025-01-29 13:59:38.802108: Pseudo dice [np.float32(0.9701), np.float32(0.8809)] 
2025-01-29 13:59:38.804997: Epoch time: 48.12 s 
2025-01-29 13:59:39.995779:  
2025-01-29 13:59:39.998557: Epoch 797 
2025-01-29 13:59:40.001405: Current learning rate: 0.00238 
2025-01-29 14:00:28.189947: train_loss -0.8463 
2025-01-29 14:00:28.196747: val_loss -0.779 
2025-01-29 14:00:28.199640: Pseudo dice [np.float32(0.9602), np.float32(0.8324)] 
2025-01-29 14:00:28.201993: Epoch time: 48.2 s 
2025-01-29 14:00:29.406112:  
2025-01-29 14:00:29.409400: Epoch 798 
2025-01-29 14:00:29.412238: Current learning rate: 0.00237 
2025-01-29 14:01:17.585624: train_loss -0.836 
2025-01-29 14:01:17.594018: val_loss -0.7938 
2025-01-29 14:01:17.596623: Pseudo dice [np.float32(0.9699), np.float32(0.8885)] 
2025-01-29 14:01:17.598883: Epoch time: 48.18 s 
2025-01-29 14:01:19.391475:  
2025-01-29 14:01:19.394350: Epoch 799 
2025-01-29 14:01:19.396984: Current learning rate: 0.00236 
2025-01-29 14:02:07.470879: train_loss -0.8393 
2025-01-29 14:02:07.478055: val_loss -0.8397 
2025-01-29 14:02:07.480827: Pseudo dice [np.float32(0.9677), np.float32(0.8879)] 
2025-01-29 14:02:07.483453: Epoch time: 48.08 s 
2025-01-29 14:02:09.212652:  
2025-01-29 14:02:09.215608: Epoch 800 
2025-01-29 14:02:09.218451: Current learning rate: 0.00235 
2025-01-29 14:02:57.146610: train_loss -0.848 
2025-01-29 14:02:57.155126: val_loss -0.7868 
2025-01-29 14:02:57.158403: Pseudo dice [np.float32(0.9666), np.float32(0.8507)] 
2025-01-29 14:02:57.161123: Epoch time: 47.93 s 
2025-01-29 14:02:58.363379:  
2025-01-29 14:02:58.366408: Epoch 801 
2025-01-29 14:02:58.369460: Current learning rate: 0.00234 
2025-01-29 14:03:46.495831: train_loss -0.8445 
2025-01-29 14:03:46.504905: val_loss -0.7822 
2025-01-29 14:03:46.507495: Pseudo dice [np.float32(0.9656), np.float32(0.8596)] 
2025-01-29 14:03:46.510113: Epoch time: 48.13 s 
2025-01-29 14:03:47.711767:  
2025-01-29 14:03:47.714615: Epoch 802 
2025-01-29 14:03:47.717242: Current learning rate: 0.00233 
2025-01-29 14:04:35.853964: train_loss -0.8417 
2025-01-29 14:04:35.862958: val_loss -0.8158 
2025-01-29 14:04:35.865441: Pseudo dice [np.float32(0.9638), np.float32(0.8729)] 
2025-01-29 14:04:35.867923: Epoch time: 48.14 s 
2025-01-29 14:04:37.064732:  
2025-01-29 14:04:37.067213: Epoch 803 
2025-01-29 14:04:37.069880: Current learning rate: 0.00232 
2025-01-29 14:05:25.385132: train_loss -0.856 
2025-01-29 14:05:25.393258: val_loss -0.783 
2025-01-29 14:05:25.395876: Pseudo dice [np.float32(0.9651), np.float32(0.8743)] 
2025-01-29 14:05:25.398339: Epoch time: 48.32 s 
2025-01-29 14:05:26.589691:  
2025-01-29 14:05:26.592725: Epoch 804 
2025-01-29 14:05:26.595773: Current learning rate: 0.00231 
2025-01-29 14:06:14.509083: train_loss -0.843 
2025-01-29 14:06:14.518196: val_loss -0.8178 
2025-01-29 14:06:14.521555: Pseudo dice [np.float32(0.9621), np.float32(0.8769)] 
2025-01-29 14:06:14.524402: Epoch time: 47.92 s 
2025-01-29 14:06:15.722378:  
2025-01-29 14:06:15.724972: Epoch 805 
2025-01-29 14:06:15.727540: Current learning rate: 0.0023 
2025-01-29 14:07:03.918627: train_loss -0.8257 
2025-01-29 14:07:03.926848: val_loss -0.8187 
2025-01-29 14:07:03.929630: Pseudo dice [np.float32(0.9692), np.float32(0.8897)] 
2025-01-29 14:07:03.932459: Epoch time: 48.2 s 
2025-01-29 14:07:05.131610:  
2025-01-29 14:07:05.135313: Epoch 806 
2025-01-29 14:07:05.138161: Current learning rate: 0.00229 
2025-01-29 14:07:53.642652: train_loss -0.847 
2025-01-29 14:07:53.650089: val_loss -0.7521 
2025-01-29 14:07:53.653086: Pseudo dice [np.float32(0.9693), np.float32(0.8616)] 
2025-01-29 14:07:53.655574: Epoch time: 48.51 s 
2025-01-29 14:07:54.854102:  
2025-01-29 14:07:54.857071: Epoch 807 
2025-01-29 14:07:54.859978: Current learning rate: 0.00228 
2025-01-29 14:08:43.281131: train_loss -0.8408 
2025-01-29 14:08:43.289113: val_loss -0.723 
2025-01-29 14:08:43.292188: Pseudo dice [np.float32(0.9684), np.float32(0.8512)] 
2025-01-29 14:08:43.294734: Epoch time: 48.43 s 
2025-01-29 14:08:44.492655:  
2025-01-29 14:08:44.495579: Epoch 808 
2025-01-29 14:08:44.498263: Current learning rate: 0.00226 
2025-01-29 14:09:32.668094: train_loss -0.8461 
2025-01-29 14:09:32.673956: val_loss -0.7965 
2025-01-29 14:09:32.676603: Pseudo dice [np.float32(0.9663), np.float32(0.8988)] 
2025-01-29 14:09:32.679069: Epoch time: 48.18 s 
2025-01-29 14:09:33.914426:  
2025-01-29 14:09:33.917267: Epoch 809 
2025-01-29 14:09:33.920292: Current learning rate: 0.00225 
2025-01-29 14:10:22.237118: train_loss -0.8378 
2025-01-29 14:10:22.246158: val_loss -0.7789 
2025-01-29 14:10:22.253920: Pseudo dice [np.float32(0.9687), np.float32(0.8731)] 
2025-01-29 14:10:22.256677: Epoch time: 48.32 s 
2025-01-29 14:10:23.444625:  
2025-01-29 14:10:23.447340: Epoch 810 
2025-01-29 14:10:23.449917: Current learning rate: 0.00224 
2025-01-29 14:11:11.560597: train_loss -0.8281 
2025-01-29 14:11:11.569238: val_loss -0.7888 
2025-01-29 14:11:11.572536: Pseudo dice [np.float32(0.9597), np.float32(0.866)] 
2025-01-29 14:11:11.575382: Epoch time: 48.12 s 
2025-01-29 14:11:12.768259:  
2025-01-29 14:11:12.771331: Epoch 811 
2025-01-29 14:11:12.774109: Current learning rate: 0.00223 
2025-01-29 14:12:01.196913: train_loss -0.8177 
2025-01-29 14:12:01.206296: val_loss -0.7757 
2025-01-29 14:12:01.209342: Pseudo dice [np.float32(0.9674), np.float32(0.8553)] 
2025-01-29 14:12:01.213056: Epoch time: 48.43 s 
2025-01-29 14:12:02.415500:  
2025-01-29 14:12:02.418426: Epoch 812 
2025-01-29 14:12:02.420944: Current learning rate: 0.00222 
2025-01-29 14:12:50.544973: train_loss -0.819 
2025-01-29 14:12:50.553017: val_loss -0.8156 
2025-01-29 14:12:50.555748: Pseudo dice [np.float32(0.9646), np.float32(0.8795)] 
2025-01-29 14:12:50.558220: Epoch time: 48.13 s 
2025-01-29 14:12:51.749232:  
2025-01-29 14:12:51.752092: Epoch 813 
2025-01-29 14:12:51.754935: Current learning rate: 0.00221 
2025-01-29 14:13:39.813774: train_loss -0.8314 
2025-01-29 14:13:39.822004: val_loss -0.8003 
2025-01-29 14:13:39.824723: Pseudo dice [np.float32(0.9663), np.float32(0.8921)] 
2025-01-29 14:13:39.832461: Epoch time: 48.07 s 
2025-01-29 14:13:41.030594:  
2025-01-29 14:13:41.033508: Epoch 814 
2025-01-29 14:13:41.036164: Current learning rate: 0.0022 
2025-01-29 14:14:29.049224: train_loss -0.8409 
2025-01-29 14:14:29.056411: val_loss -0.771 
2025-01-29 14:14:29.058926: Pseudo dice [np.float32(0.9705), np.float32(0.8737)] 
2025-01-29 14:14:29.061705: Epoch time: 48.02 s 
2025-01-29 14:14:30.252770:  
2025-01-29 14:14:30.255698: Epoch 815 
2025-01-29 14:14:30.258611: Current learning rate: 0.00219 
2025-01-29 14:15:18.390302: train_loss -0.8471 
2025-01-29 14:15:18.398212: val_loss -0.7604 
2025-01-29 14:15:18.401147: Pseudo dice [np.float32(0.9672), np.float32(0.889)] 
2025-01-29 14:15:18.403680: Epoch time: 48.14 s 
2025-01-29 14:15:20.188327:  
2025-01-29 14:15:20.191497: Epoch 816 
2025-01-29 14:15:20.194257: Current learning rate: 0.00218 
2025-01-29 14:16:08.255325: train_loss -0.8358 
2025-01-29 14:16:08.264087: val_loss -0.8061 
2025-01-29 14:16:08.266807: Pseudo dice [np.float32(0.9695), np.float32(0.8888)] 
2025-01-29 14:16:08.269493: Epoch time: 48.07 s 
2025-01-29 14:16:09.459971:  
2025-01-29 14:16:09.462790: Epoch 817 
2025-01-29 14:16:09.465429: Current learning rate: 0.00217 
2025-01-29 14:16:57.734110: train_loss -0.8298 
2025-01-29 14:16:57.750276: val_loss -0.7823 
2025-01-29 14:16:57.753489: Pseudo dice [np.float32(0.968), np.float32(0.8638)] 
2025-01-29 14:16:57.756271: Epoch time: 48.28 s 
2025-01-29 14:16:58.950664:  
2025-01-29 14:16:58.953848: Epoch 818 
2025-01-29 14:16:58.956685: Current learning rate: 0.00216 
2025-01-29 14:17:47.153319: train_loss -0.8309 
2025-01-29 14:17:47.161192: val_loss -0.7944 
2025-01-29 14:17:47.164239: Pseudo dice [np.float32(0.9695), np.float32(0.885)] 
2025-01-29 14:17:47.167019: Epoch time: 48.2 s 
2025-01-29 14:17:48.354490:  
2025-01-29 14:17:48.357541: Epoch 819 
2025-01-29 14:17:48.360437: Current learning rate: 0.00215 
2025-01-29 14:18:36.818125: train_loss -0.8224 
2025-01-29 14:18:36.824055: val_loss -0.7956 
2025-01-29 14:18:36.827159: Pseudo dice [np.float32(0.9635), np.float32(0.872)] 
2025-01-29 14:18:36.830160: Epoch time: 48.46 s 
2025-01-29 14:18:37.997273:  
2025-01-29 14:18:38.000366: Epoch 820 
2025-01-29 14:18:38.003423: Current learning rate: 0.00214 
2025-01-29 14:19:26.218719: train_loss -0.8439 
2025-01-29 14:19:26.226800: val_loss -0.7668 
2025-01-29 14:19:26.229299: Pseudo dice [np.float32(0.9672), np.float32(0.8723)] 
2025-01-29 14:19:26.231803: Epoch time: 48.22 s 
2025-01-29 14:19:27.361243:  
2025-01-29 14:19:27.364278: Epoch 821 
2025-01-29 14:19:27.367095: Current learning rate: 0.00213 
2025-01-29 14:20:15.632771: train_loss -0.8346 
2025-01-29 14:20:15.636845: val_loss -0.8036 
2025-01-29 14:20:15.643903: Pseudo dice [np.float32(0.9693), np.float32(0.8832)] 
2025-01-29 14:20:15.646643: Epoch time: 48.27 s 
2025-01-29 14:20:16.782151:  
2025-01-29 14:20:16.785191: Epoch 822 
2025-01-29 14:20:16.788013: Current learning rate: 0.00212 
2025-01-29 14:21:05.291123: train_loss -0.8345 
2025-01-29 14:21:05.301135: val_loss -0.775 
2025-01-29 14:21:05.303948: Pseudo dice [np.float32(0.9668), np.float32(0.8702)] 
2025-01-29 14:21:05.306724: Epoch time: 48.51 s 
2025-01-29 14:21:06.473885:  
2025-01-29 14:21:06.477004: Epoch 823 
2025-01-29 14:21:06.479856: Current learning rate: 0.0021 
2025-01-29 14:21:54.351310: train_loss -0.8499 
2025-01-29 14:21:54.356952: val_loss -0.8376 
2025-01-29 14:21:54.359542: Pseudo dice [np.float32(0.9707), np.float32(0.9029)] 
2025-01-29 14:21:54.361920: Epoch time: 47.88 s 
2025-01-29 14:21:55.526307:  
2025-01-29 14:21:55.529439: Epoch 824 
2025-01-29 14:21:55.532308: Current learning rate: 0.00209 
2025-01-29 14:22:43.720737: train_loss -0.8437 
2025-01-29 14:22:43.728987: val_loss -0.8021 
2025-01-29 14:22:43.731466: Pseudo dice [np.float32(0.9646), np.float32(0.8856)] 
2025-01-29 14:22:43.734050: Epoch time: 48.2 s 
2025-01-29 14:22:44.899420:  
2025-01-29 14:22:44.902200: Epoch 825 
2025-01-29 14:22:44.904834: Current learning rate: 0.00208 
2025-01-29 14:23:33.355408: train_loss -0.8344 
2025-01-29 14:23:33.359766: val_loss -0.7197 
2025-01-29 14:23:33.363203: Pseudo dice [np.float32(0.9682), np.float32(0.8718)] 
2025-01-29 14:23:33.366059: Epoch time: 48.46 s 
2025-01-29 14:23:34.489332:  
2025-01-29 14:23:34.492426: Epoch 826 
2025-01-29 14:23:34.495385: Current learning rate: 0.00207 
2025-01-29 14:24:22.868412: train_loss -0.8327 
2025-01-29 14:24:22.875693: val_loss -0.7586 
2025-01-29 14:24:22.878461: Pseudo dice [np.float32(0.9656), np.float32(0.8357)] 
2025-01-29 14:24:22.880957: Epoch time: 48.38 s 
2025-01-29 14:24:23.999126:  
2025-01-29 14:24:24.001885: Epoch 827 
2025-01-29 14:24:24.004407: Current learning rate: 0.00206 
2025-01-29 14:25:12.335166: train_loss -0.8505 
2025-01-29 14:25:12.339616: val_loss -0.7617 
2025-01-29 14:25:12.342283: Pseudo dice [np.float32(0.9689), np.float32(0.8925)] 
2025-01-29 14:25:12.345039: Epoch time: 48.34 s 
2025-01-29 14:25:13.466304:  
2025-01-29 14:25:13.469117: Epoch 828 
2025-01-29 14:25:13.471524: Current learning rate: 0.00205 
2025-01-29 14:26:01.971321: train_loss -0.8325 
2025-01-29 14:26:01.979910: val_loss -0.7722 
2025-01-29 14:26:01.982417: Pseudo dice [np.float32(0.9703), np.float32(0.8698)] 
2025-01-29 14:26:01.985096: Epoch time: 48.51 s 
2025-01-29 14:26:03.110491:  
2025-01-29 14:26:03.113451: Epoch 829 
2025-01-29 14:26:03.116457: Current learning rate: 0.00204 
2025-01-29 14:26:51.305762: train_loss -0.8324 
2025-01-29 14:26:51.310243: val_loss -0.7981 
2025-01-29 14:26:51.312987: Pseudo dice [np.float32(0.9671), np.float32(0.8829)] 
2025-01-29 14:26:51.315773: Epoch time: 48.2 s 
2025-01-29 14:26:52.450624:  
2025-01-29 14:26:52.453719: Epoch 830 
2025-01-29 14:26:52.456661: Current learning rate: 0.00203 
2025-01-29 14:27:40.552732: train_loss -0.8435 
2025-01-29 14:27:40.561469: val_loss -0.7552 
2025-01-29 14:27:40.564398: Pseudo dice [np.float32(0.9702), np.float32(0.8775)] 
2025-01-29 14:27:40.567384: Epoch time: 48.1 s 
2025-01-29 14:27:41.690368:  
2025-01-29 14:27:41.693331: Epoch 831 
2025-01-29 14:27:41.697663: Current learning rate: 0.00202 
2025-01-29 14:28:30.102206: train_loss -0.8421 
2025-01-29 14:28:30.107136: val_loss -0.7732 
2025-01-29 14:28:30.109756: Pseudo dice [np.float32(0.9681), np.float32(0.8642)] 
2025-01-29 14:28:30.112291: Epoch time: 48.41 s 
2025-01-29 14:28:31.277113:  
2025-01-29 14:28:31.280547: Epoch 832 
2025-01-29 14:28:31.283499: Current learning rate: 0.00201 
2025-01-29 14:29:19.700226: train_loss -0.833 
2025-01-29 14:29:19.708889: val_loss -0.7582 
2025-01-29 14:29:19.711417: Pseudo dice [np.float32(0.966), np.float32(0.8361)] 
2025-01-29 14:29:19.714036: Epoch time: 48.42 s 
2025-01-29 14:29:20.838200:  
2025-01-29 14:29:20.840726: Epoch 833 
2025-01-29 14:29:20.843178: Current learning rate: 0.002 
2025-01-29 14:30:09.334225: train_loss -0.8553 
2025-01-29 14:30:09.339174: val_loss -0.7885 
2025-01-29 14:30:09.346710: Pseudo dice [np.float32(0.9686), np.float32(0.874)] 
2025-01-29 14:30:09.349194: Epoch time: 48.5 s 
2025-01-29 14:30:10.998138:  
2025-01-29 14:30:11.000979: Epoch 834 
2025-01-29 14:30:11.003941: Current learning rate: 0.00199 
2025-01-29 14:30:59.702746: train_loss -0.8215 
2025-01-29 14:30:59.709521: val_loss -0.8227 
2025-01-29 14:30:59.713475: Pseudo dice [np.float32(0.969), np.float32(0.8974)] 
2025-01-29 14:30:59.716108: Epoch time: 48.71 s 
2025-01-29 14:31:00.840495:  
2025-01-29 14:31:00.843566: Epoch 835 
2025-01-29 14:31:00.846336: Current learning rate: 0.00198 
2025-01-29 14:31:49.286325: train_loss -0.8506 
2025-01-29 14:31:49.293737: val_loss -0.7441 
2025-01-29 14:31:49.296750: Pseudo dice [np.float32(0.9684), np.float32(0.8381)] 
2025-01-29 14:31:49.299314: Epoch time: 48.45 s 
2025-01-29 14:31:50.421685:  
2025-01-29 14:31:50.424954: Epoch 836 
2025-01-29 14:31:50.427887: Current learning rate: 0.00196 
2025-01-29 14:32:38.879335: train_loss -0.8313 
2025-01-29 14:32:38.887043: val_loss -0.7711 
2025-01-29 14:32:38.895373: Pseudo dice [np.float32(0.9705), np.float32(0.8801)] 
2025-01-29 14:32:38.898488: Epoch time: 48.46 s 
2025-01-29 14:32:40.034129:  
2025-01-29 14:32:40.037188: Epoch 837 
2025-01-29 14:32:40.039844: Current learning rate: 0.00195 
2025-01-29 14:33:28.184682: train_loss -0.8364 
2025-01-29 14:33:28.191669: val_loss -0.8094 
2025-01-29 14:33:28.194296: Pseudo dice [np.float32(0.9702), np.float32(0.8989)] 
2025-01-29 14:33:28.196719: Epoch time: 48.15 s 
2025-01-29 14:33:29.358455:  
2025-01-29 14:33:29.361240: Epoch 838 
2025-01-29 14:33:29.363914: Current learning rate: 0.00194 
2025-01-29 14:34:17.636114: train_loss -0.8407 
2025-01-29 14:34:17.641564: val_loss -0.7983 
2025-01-29 14:34:17.644320: Pseudo dice [np.float32(0.9711), np.float32(0.8824)] 
2025-01-29 14:34:17.647149: Epoch time: 48.28 s 
2025-01-29 14:34:18.783934:  
2025-01-29 14:34:18.787552: Epoch 839 
2025-01-29 14:34:18.791037: Current learning rate: 0.00193 
2025-01-29 14:35:07.502414: train_loss -0.8354 
2025-01-29 14:35:07.510334: val_loss -0.8142 
2025-01-29 14:35:07.513006: Pseudo dice [np.float32(0.9704), np.float32(0.8686)] 
2025-01-29 14:35:07.515501: Epoch time: 48.72 s 
2025-01-29 14:35:08.649922:  
2025-01-29 14:35:08.652740: Epoch 840 
2025-01-29 14:35:08.655578: Current learning rate: 0.00192 
2025-01-29 14:35:56.933843: train_loss -0.8494 
2025-01-29 14:35:56.940339: val_loss -0.7891 
2025-01-29 14:35:56.943426: Pseudo dice [np.float32(0.9681), np.float32(0.8653)] 
2025-01-29 14:35:56.946280: Epoch time: 48.28 s 
2025-01-29 14:35:58.098234:  
2025-01-29 14:35:58.101310: Epoch 841 
2025-01-29 14:35:58.104071: Current learning rate: 0.00191 
2025-01-29 14:36:46.386909: train_loss -0.8424 
2025-01-29 14:36:46.395009: val_loss -0.7451 
2025-01-29 14:36:46.397913: Pseudo dice [np.float32(0.9704), np.float32(0.864)] 
2025-01-29 14:36:46.400766: Epoch time: 48.29 s 
2025-01-29 14:36:47.523156:  
2025-01-29 14:36:47.527263: Epoch 842 
2025-01-29 14:36:47.529965: Current learning rate: 0.0019 
2025-01-29 14:37:35.703051: train_loss -0.8462 
2025-01-29 14:37:35.710568: val_loss -0.8008 
2025-01-29 14:37:35.713331: Pseudo dice [np.float32(0.968), np.float32(0.873)] 
2025-01-29 14:37:35.716212: Epoch time: 48.18 s 
2025-01-29 14:37:36.839613:  
2025-01-29 14:37:36.843453: Epoch 843 
2025-01-29 14:37:36.846340: Current learning rate: 0.00189 
2025-01-29 14:38:25.114774: train_loss -0.8471 
2025-01-29 14:38:25.120990: val_loss -0.8295 
2025-01-29 14:38:25.123511: Pseudo dice [np.float32(0.969), np.float32(0.897)] 
2025-01-29 14:38:25.126025: Epoch time: 48.28 s 
2025-01-29 14:38:26.263480:  
2025-01-29 14:38:26.266458: Epoch 844 
2025-01-29 14:38:26.269046: Current learning rate: 0.00188 
2025-01-29 14:39:14.620725: train_loss -0.8432 
2025-01-29 14:39:14.629680: val_loss -0.8003 
2025-01-29 14:39:14.633090: Pseudo dice [np.float32(0.9672), np.float32(0.8739)] 
2025-01-29 14:39:14.636388: Epoch time: 48.36 s 
2025-01-29 14:39:15.797567:  
2025-01-29 14:39:15.802816: Epoch 845 
2025-01-29 14:39:15.805813: Current learning rate: 0.00187 
2025-01-29 14:40:04.045113: train_loss -0.8558 
2025-01-29 14:40:04.053437: val_loss -0.821 
2025-01-29 14:40:04.055956: Pseudo dice [np.float32(0.9715), np.float32(0.9026)] 
2025-01-29 14:40:04.058555: Epoch time: 48.25 s 
2025-01-29 14:40:05.215873:  
2025-01-29 14:40:05.218613: Epoch 846 
2025-01-29 14:40:05.221886: Current learning rate: 0.00186 
2025-01-29 14:40:53.633741: train_loss -0.8456 
2025-01-29 14:40:53.642207: val_loss -0.7987 
2025-01-29 14:40:53.645070: Pseudo dice [np.float32(0.9659), np.float32(0.8586)] 
2025-01-29 14:40:53.647578: Epoch time: 48.42 s 
2025-01-29 14:40:54.814987:  
2025-01-29 14:40:54.819323: Epoch 847 
2025-01-29 14:40:54.822200: Current learning rate: 0.00185 
2025-01-29 14:41:43.491509: train_loss -0.8291 
2025-01-29 14:41:43.500118: val_loss -0.8252 
2025-01-29 14:41:43.502785: Pseudo dice [np.float32(0.9671), np.float32(0.8875)] 
2025-01-29 14:41:43.505860: Epoch time: 48.68 s 
2025-01-29 14:41:44.680935:  
2025-01-29 14:41:44.684263: Epoch 848 
2025-01-29 14:41:44.687200: Current learning rate: 0.00184 
2025-01-29 14:42:33.279635: train_loss -0.8389 
2025-01-29 14:42:33.288453: val_loss -0.8351 
2025-01-29 14:42:33.291245: Pseudo dice [np.float32(0.9629), np.float32(0.881)] 
2025-01-29 14:42:33.293956: Epoch time: 48.6 s 
2025-01-29 14:42:34.459661:  
2025-01-29 14:42:34.462647: Epoch 849 
2025-01-29 14:42:34.465549: Current learning rate: 0.00182 
2025-01-29 14:43:23.396813: train_loss -0.829 
2025-01-29 14:43:23.405642: val_loss -0.7851 
2025-01-29 14:43:23.408128: Pseudo dice [np.float32(0.9728), np.float32(0.9022)] 
2025-01-29 14:43:23.410313: Epoch time: 48.94 s 
2025-01-29 14:43:25.131280:  
2025-01-29 14:43:25.134322: Epoch 850 
2025-01-29 14:43:25.136901: Current learning rate: 0.00181 
2025-01-29 14:44:13.732113: train_loss -0.8189 
2025-01-29 14:44:13.741096: val_loss -0.7783 
2025-01-29 14:44:13.743814: Pseudo dice [np.float32(0.9657), np.float32(0.8915)] 
2025-01-29 14:44:13.746522: Epoch time: 48.6 s 
2025-01-29 14:44:14.897272:  
2025-01-29 14:44:14.900226: Epoch 851 
2025-01-29 14:44:14.902727: Current learning rate: 0.0018 
2025-01-29 14:45:03.134130: train_loss -0.8547 
2025-01-29 14:45:03.143808: val_loss -0.8158 
2025-01-29 14:45:03.146487: Pseudo dice [np.float32(0.9712), np.float32(0.8903)] 
2025-01-29 14:45:03.148948: Epoch time: 48.24 s 
2025-01-29 14:45:04.975545:  
2025-01-29 14:45:04.978040: Epoch 852 
2025-01-29 14:45:04.980684: Current learning rate: 0.00179 
2025-01-29 14:45:53.425666: train_loss -0.8315 
2025-01-29 14:45:53.434225: val_loss -0.7826 
2025-01-29 14:45:53.437092: Pseudo dice [np.float32(0.9671), np.float32(0.9043)] 
2025-01-29 14:45:53.439685: Epoch time: 48.45 s 
2025-01-29 14:45:54.569293:  
2025-01-29 14:45:54.572052: Epoch 853 
2025-01-29 14:45:54.574789: Current learning rate: 0.00178 
2025-01-29 14:46:42.871280: train_loss -0.836 
2025-01-29 14:46:42.878900: val_loss -0.8069 
2025-01-29 14:46:42.881340: Pseudo dice [np.float32(0.9724), np.float32(0.9014)] 
2025-01-29 14:46:42.883998: Epoch time: 48.3 s 
2025-01-29 14:46:44.024387:  
2025-01-29 14:46:44.026967: Epoch 854 
2025-01-29 14:46:44.029231: Current learning rate: 0.00177 
2025-01-29 14:47:32.622848: train_loss -0.8391 
2025-01-29 14:47:32.631430: val_loss -0.7876 
2025-01-29 14:47:32.634038: Pseudo dice [np.float32(0.9687), np.float32(0.8897)] 
2025-01-29 14:47:32.636748: Epoch time: 48.6 s 
2025-01-29 14:47:33.770386:  
2025-01-29 14:47:33.773633: Epoch 855 
2025-01-29 14:47:33.776642: Current learning rate: 0.00176 
2025-01-29 14:48:22.272910: train_loss -0.8502 
2025-01-29 14:48:22.281066: val_loss -0.7564 
2025-01-29 14:48:22.283731: Pseudo dice [np.float32(0.9687), np.float32(0.8872)] 
2025-01-29 14:48:22.286174: Epoch time: 48.5 s 
2025-01-29 14:48:23.416052:  
2025-01-29 14:48:23.418912: Epoch 856 
2025-01-29 14:48:23.421556: Current learning rate: 0.00175 
2025-01-29 14:49:12.140733: train_loss -0.8328 
2025-01-29 14:49:12.149567: val_loss -0.767 
2025-01-29 14:49:12.152453: Pseudo dice [np.float32(0.966), np.float32(0.8336)] 
2025-01-29 14:49:12.155481: Epoch time: 48.73 s 
2025-01-29 14:49:13.285235:  
2025-01-29 14:49:13.287943: Epoch 857 
2025-01-29 14:49:13.290635: Current learning rate: 0.00174 
2025-01-29 14:50:01.546666: train_loss -0.8486 
2025-01-29 14:50:01.554709: val_loss -0.7914 
2025-01-29 14:50:01.557252: Pseudo dice [np.float32(0.9703), np.float32(0.8562)] 
2025-01-29 14:50:01.559777: Epoch time: 48.26 s 
2025-01-29 14:50:02.685833:  
2025-01-29 14:50:02.688434: Epoch 858 
2025-01-29 14:50:02.691003: Current learning rate: 0.00173 
2025-01-29 14:50:50.980165: train_loss -0.8383 
2025-01-29 14:50:50.989130: val_loss -0.7907 
2025-01-29 14:50:50.997854: Pseudo dice [np.float32(0.9693), np.float32(0.886)] 
2025-01-29 14:50:51.000878: Epoch time: 48.3 s 
2025-01-29 14:50:52.128418:  
2025-01-29 14:50:52.131139: Epoch 859 
2025-01-29 14:50:52.133778: Current learning rate: 0.00172 
2025-01-29 14:51:41.137926: train_loss -0.8464 
2025-01-29 14:51:41.142193: val_loss -0.7795 
2025-01-29 14:51:41.145022: Pseudo dice [np.float32(0.9693), np.float32(0.8958)] 
2025-01-29 14:51:41.147627: Epoch time: 49.01 s 
2025-01-29 14:51:42.274732:  
2025-01-29 14:51:42.277363: Epoch 860 
2025-01-29 14:51:42.279905: Current learning rate: 0.0017 
2025-01-29 14:52:30.886702: train_loss -0.8355 
2025-01-29 14:52:30.896253: val_loss -0.7786 
2025-01-29 14:52:30.898742: Pseudo dice [np.float32(0.9698), np.float32(0.9052)] 
2025-01-29 14:52:30.901377: Epoch time: 48.61 s 
2025-01-29 14:52:32.025479:  
2025-01-29 14:52:32.028419: Epoch 861 
2025-01-29 14:52:32.031384: Current learning rate: 0.00169 
2025-01-29 14:53:20.612542: train_loss -0.8254 
2025-01-29 14:53:20.617164: val_loss -0.7416 
2025-01-29 14:53:20.619676: Pseudo dice [np.float32(0.9674), np.float32(0.8478)] 
2025-01-29 14:53:20.621678: Epoch time: 48.59 s 
2025-01-29 14:53:21.738374:  
2025-01-29 14:53:21.741033: Epoch 862 
2025-01-29 14:53:21.743403: Current learning rate: 0.00168 
2025-01-29 14:54:10.375505: train_loss -0.8418 
2025-01-29 14:54:10.381986: val_loss -0.833 
2025-01-29 14:54:10.384984: Pseudo dice [np.float32(0.9724), np.float32(0.882)] 
2025-01-29 14:54:10.387743: Epoch time: 48.64 s 
2025-01-29 14:54:11.511004:  
2025-01-29 14:54:11.513845: Epoch 863 
2025-01-29 14:54:11.516836: Current learning rate: 0.00167 
2025-01-29 14:54:59.622069: train_loss -0.8456 
2025-01-29 14:54:59.626658: val_loss -0.8186 
2025-01-29 14:54:59.634183: Pseudo dice [np.float32(0.9695), np.float32(0.894)] 
2025-01-29 14:54:59.637192: Epoch time: 48.11 s 
2025-01-29 14:55:00.765578:  
2025-01-29 14:55:00.768642: Epoch 864 
2025-01-29 14:55:00.771303: Current learning rate: 0.00166 
2025-01-29 14:55:49.243153: train_loss -0.8372 
2025-01-29 14:55:49.250927: val_loss -0.8076 
2025-01-29 14:55:49.253730: Pseudo dice [np.float32(0.9692), np.float32(0.8791)] 
2025-01-29 14:55:49.256358: Epoch time: 48.48 s 
2025-01-29 14:55:50.374323:  
2025-01-29 14:55:50.377355: Epoch 865 
2025-01-29 14:55:50.380315: Current learning rate: 0.00165 
2025-01-29 14:56:38.852968: train_loss -0.8473 
2025-01-29 14:56:38.857507: val_loss -0.7713 
2025-01-29 14:56:38.860305: Pseudo dice [np.float32(0.9701), np.float32(0.8921)] 
2025-01-29 14:56:38.863010: Epoch time: 48.48 s 
2025-01-29 14:56:39.982069:  
2025-01-29 14:56:39.984868: Epoch 866 
2025-01-29 14:56:39.987592: Current learning rate: 0.00164 
2025-01-29 14:57:28.523854: train_loss -0.8537 
2025-01-29 14:57:28.529151: val_loss -0.8186 
2025-01-29 14:57:28.531401: Pseudo dice [np.float32(0.9707), np.float32(0.8763)] 
2025-01-29 14:57:28.533498: Epoch time: 48.54 s 
2025-01-29 14:57:29.662331:  
2025-01-29 14:57:29.664840: Epoch 867 
2025-01-29 14:57:29.667030: Current learning rate: 0.00163 
2025-01-29 14:58:18.061725: train_loss -0.8453 
2025-01-29 14:58:18.065460: val_loss -0.7805 
2025-01-29 14:58:18.072303: Pseudo dice [np.float32(0.969), np.float32(0.8938)] 
2025-01-29 14:58:18.074823: Epoch time: 48.4 s 
2025-01-29 14:58:19.219727:  
2025-01-29 14:58:19.222270: Epoch 868 
2025-01-29 14:58:19.224545: Current learning rate: 0.00162 
2025-01-29 14:59:07.493924: train_loss -0.8453 
2025-01-29 14:59:07.499911: val_loss -0.8115 
2025-01-29 14:59:07.502578: Pseudo dice [np.float32(0.9642), np.float32(0.8981)] 
2025-01-29 14:59:07.505040: Epoch time: 48.28 s 
2025-01-29 14:59:08.672944:  
2025-01-29 14:59:08.675902: Epoch 869 
2025-01-29 14:59:08.678759: Current learning rate: 0.00161 
2025-01-29 14:59:57.391978: train_loss -0.8413 
2025-01-29 14:59:57.397374: val_loss -0.8313 
2025-01-29 14:59:57.400244: Pseudo dice [np.float32(0.9686), np.float32(0.89)] 
2025-01-29 14:59:57.402816: Epoch time: 48.72 s 
2025-01-29 14:59:58.529950:  
2025-01-29 14:59:58.532938: Epoch 870 
2025-01-29 14:59:58.535485: Current learning rate: 0.00159 
2025-01-29 15:00:46.990716: train_loss -0.8463 
2025-01-29 15:00:46.996117: val_loss -0.808 
2025-01-29 15:00:46.998470: Pseudo dice [np.float32(0.9693), np.float32(0.8857)] 
2025-01-29 15:00:47.000937: Epoch time: 48.46 s 
2025-01-29 15:00:48.123503:  
2025-01-29 15:00:48.126591: Epoch 871 
2025-01-29 15:00:48.129328: Current learning rate: 0.00158 
2025-01-29 15:01:37.061468: train_loss -0.841 
2025-01-29 15:01:37.065569: val_loss -0.8189 
2025-01-29 15:01:37.067829: Pseudo dice [np.float32(0.9661), np.float32(0.8931)] 
2025-01-29 15:01:37.070195: Epoch time: 48.94 s 
2025-01-29 15:01:38.795435:  
2025-01-29 15:01:38.798028: Epoch 872 
2025-01-29 15:01:38.800706: Current learning rate: 0.00157 
2025-01-29 15:02:27.149868: train_loss -0.8399 
2025-01-29 15:02:27.158193: val_loss -0.7642 
2025-01-29 15:02:27.168530: Pseudo dice [np.float32(0.9677), np.float32(0.8821)] 
2025-01-29 15:02:27.170991: Epoch time: 48.36 s 
2025-01-29 15:02:28.292988:  
2025-01-29 15:02:28.296431: Epoch 873 
2025-01-29 15:02:28.299211: Current learning rate: 0.00156 
2025-01-29 15:03:16.893746: train_loss -0.8551 
2025-01-29 15:03:16.898419: val_loss -0.7953 
2025-01-29 15:03:16.901390: Pseudo dice [np.float32(0.9663), np.float32(0.8883)] 
2025-01-29 15:03:16.903918: Epoch time: 48.6 s 
2025-01-29 15:03:18.031768:  
2025-01-29 15:03:18.034876: Epoch 874 
2025-01-29 15:03:18.037590: Current learning rate: 0.00155 
2025-01-29 15:04:07.260607: train_loss -0.8545 
2025-01-29 15:04:07.267789: val_loss -0.7864 
2025-01-29 15:04:07.270951: Pseudo dice [np.float32(0.9706), np.float32(0.8536)] 
2025-01-29 15:04:07.273643: Epoch time: 49.23 s 
2025-01-29 15:04:08.390634:  
2025-01-29 15:04:08.393193: Epoch 875 
2025-01-29 15:04:08.396313: Current learning rate: 0.00154 
2025-01-29 15:04:57.020625: train_loss -0.8382 
2025-01-29 15:04:57.025024: val_loss -0.7972 
2025-01-29 15:04:57.033014: Pseudo dice [np.float32(0.9665), np.float32(0.8442)] 
2025-01-29 15:04:57.035682: Epoch time: 48.63 s 
2025-01-29 15:04:58.158294:  
2025-01-29 15:04:58.161534: Epoch 876 
2025-01-29 15:04:58.164796: Current learning rate: 0.00153 
2025-01-29 15:05:46.542655: train_loss -0.8582 
2025-01-29 15:05:46.548487: val_loss -0.8064 
2025-01-29 15:05:46.551191: Pseudo dice [np.float32(0.9699), np.float32(0.8873)] 
2025-01-29 15:05:46.553600: Epoch time: 48.39 s 
2025-01-29 15:05:47.671215:  
2025-01-29 15:05:47.673984: Epoch 877 
2025-01-29 15:05:47.676311: Current learning rate: 0.00152 
2025-01-29 15:06:35.928462: train_loss -0.8507 
2025-01-29 15:06:35.933170: val_loss -0.8037 
2025-01-29 15:06:35.935817: Pseudo dice [np.float32(0.9697), np.float32(0.8916)] 
2025-01-29 15:06:35.938266: Epoch time: 48.26 s 
2025-01-29 15:06:37.062531:  
2025-01-29 15:06:37.065464: Epoch 878 
2025-01-29 15:06:37.068434: Current learning rate: 0.00151 
2025-01-29 15:07:26.232168: train_loss -0.8494 
2025-01-29 15:07:26.236839: val_loss -0.8197 
2025-01-29 15:07:26.239144: Pseudo dice [np.float32(0.9726), np.float32(0.8915)] 
2025-01-29 15:07:26.241376: Epoch time: 49.17 s 
2025-01-29 15:07:27.361154:  
2025-01-29 15:07:27.364118: Epoch 879 
2025-01-29 15:07:27.367290: Current learning rate: 0.00149 
2025-01-29 15:08:15.797133: train_loss -0.8279 
2025-01-29 15:08:15.803195: val_loss -0.8041 
2025-01-29 15:08:15.805944: Pseudo dice [np.float32(0.9665), np.float32(0.8687)] 
2025-01-29 15:08:15.808649: Epoch time: 48.44 s 
2025-01-29 15:08:16.933889:  
2025-01-29 15:08:16.937336: Epoch 880 
2025-01-29 15:08:16.940131: Current learning rate: 0.00148 
2025-01-29 15:09:05.436095: train_loss -0.8426 
2025-01-29 15:09:05.442451: val_loss -0.7722 
2025-01-29 15:09:05.449350: Pseudo dice [np.float32(0.9655), np.float32(0.8747)] 
2025-01-29 15:09:05.452127: Epoch time: 48.5 s 
2025-01-29 15:09:06.582870:  
2025-01-29 15:09:06.585723: Epoch 881 
2025-01-29 15:09:06.588860: Current learning rate: 0.00147 
2025-01-29 15:09:55.016913: train_loss -0.8353 
2025-01-29 15:09:55.021254: val_loss -0.769 
2025-01-29 15:09:55.024040: Pseudo dice [np.float32(0.9653), np.float32(0.8664)] 
2025-01-29 15:09:55.026544: Epoch time: 48.44 s 
2025-01-29 15:09:56.149305:  
2025-01-29 15:09:56.152444: Epoch 882 
2025-01-29 15:09:56.155253: Current learning rate: 0.00146 
2025-01-29 15:10:44.625734: train_loss -0.8516 
2025-01-29 15:10:44.631517: val_loss -0.7964 
2025-01-29 15:10:44.634408: Pseudo dice [np.float32(0.9709), np.float32(0.8785)] 
2025-01-29 15:10:44.636978: Epoch time: 48.48 s 
2025-01-29 15:10:45.756770:  
2025-01-29 15:10:45.759101: Epoch 883 
2025-01-29 15:10:45.761716: Current learning rate: 0.00145 
2025-01-29 15:11:34.465504: train_loss -0.8404 
2025-01-29 15:11:34.473616: val_loss -0.8042 
2025-01-29 15:11:34.476387: Pseudo dice [np.float32(0.9597), np.float32(0.8789)] 
2025-01-29 15:11:34.478854: Epoch time: 48.71 s 
2025-01-29 15:11:35.599646:  
2025-01-29 15:11:35.602915: Epoch 884 
2025-01-29 15:11:35.605560: Current learning rate: 0.00144 
2025-01-29 15:12:24.065736: train_loss -0.8531 
2025-01-29 15:12:24.071385: val_loss -0.7986 
2025-01-29 15:12:24.073960: Pseudo dice [np.float32(0.9681), np.float32(0.8951)] 
2025-01-29 15:12:24.076476: Epoch time: 48.47 s 
2025-01-29 15:12:25.196577:  
2025-01-29 15:12:25.199573: Epoch 885 
2025-01-29 15:12:25.202543: Current learning rate: 0.00143 
2025-01-29 15:13:13.656452: train_loss -0.8456 
2025-01-29 15:13:13.664045: val_loss -0.8264 
2025-01-29 15:13:13.666873: Pseudo dice [np.float32(0.9684), np.float32(0.8999)] 
2025-01-29 15:13:13.669323: Epoch time: 48.46 s 
2025-01-29 15:13:14.788341:  
2025-01-29 15:13:14.791044: Epoch 886 
2025-01-29 15:13:14.793721: Current learning rate: 0.00142 
2025-01-29 15:14:03.437833: train_loss -0.8487 
2025-01-29 15:14:03.443683: val_loss -0.8258 
2025-01-29 15:14:03.446552: Pseudo dice [np.float32(0.9703), np.float32(0.8859)] 
2025-01-29 15:14:03.449142: Epoch time: 48.65 s 
2025-01-29 15:14:04.571033:  
2025-01-29 15:14:04.574206: Epoch 887 
2025-01-29 15:14:04.576926: Current learning rate: 0.00141 
2025-01-29 15:14:53.056841: train_loss -0.8354 
2025-01-29 15:14:53.061946: val_loss -0.8277 
2025-01-29 15:14:53.064659: Pseudo dice [np.float32(0.9687), np.float32(0.8798)] 
2025-01-29 15:14:53.067074: Epoch time: 48.49 s 
2025-01-29 15:14:54.207130:  
2025-01-29 15:14:54.210196: Epoch 888 
2025-01-29 15:14:54.212892: Current learning rate: 0.00139 
2025-01-29 15:15:42.748829: train_loss -0.8351 
2025-01-29 15:15:42.757796: val_loss -0.8318 
2025-01-29 15:15:42.760345: Pseudo dice [np.float32(0.9713), np.float32(0.8921)] 
2025-01-29 15:15:42.762750: Epoch time: 48.54 s 
2025-01-29 15:15:43.882721:  
2025-01-29 15:15:43.885773: Epoch 889 
2025-01-29 15:15:43.888404: Current learning rate: 0.00138 
2025-01-29 15:16:32.460491: train_loss -0.8455 
2025-01-29 15:16:32.467175: val_loss -0.821 
2025-01-29 15:16:32.469821: Pseudo dice [np.float32(0.9676), np.float32(0.8775)] 
2025-01-29 15:16:32.472217: Epoch time: 48.58 s 
2025-01-29 15:16:33.593250:  
2025-01-29 15:16:33.596489: Epoch 890 
2025-01-29 15:16:33.599374: Current learning rate: 0.00137 
2025-01-29 15:17:22.304540: train_loss -0.8561 
2025-01-29 15:17:22.311716: val_loss -0.8239 
2025-01-29 15:17:22.320018: Pseudo dice [np.float32(0.967), np.float32(0.8922)] 
2025-01-29 15:17:22.322819: Epoch time: 48.71 s 
2025-01-29 15:17:23.982034:  
2025-01-29 15:17:23.984797: Epoch 891 
2025-01-29 15:17:23.987473: Current learning rate: 0.00136 
2025-01-29 15:18:12.295945: train_loss -0.8448 
2025-01-29 15:18:12.302771: val_loss -0.7734 
2025-01-29 15:18:12.305321: Pseudo dice [np.float32(0.9727), np.float32(0.877)] 
2025-01-29 15:18:12.308085: Epoch time: 48.31 s 
2025-01-29 15:18:13.439956:  
2025-01-29 15:18:13.442677: Epoch 892 
2025-01-29 15:18:13.445129: Current learning rate: 0.00135 
2025-01-29 15:19:01.567921: train_loss -0.8465 
2025-01-29 15:19:01.598595: val_loss -0.7768 
2025-01-29 15:19:01.601542: Pseudo dice [np.float32(0.9628), np.float32(0.8336)] 
2025-01-29 15:19:01.604141: Epoch time: 48.13 s 
2025-01-29 15:19:02.731719:  
2025-01-29 15:19:02.734693: Epoch 893 
2025-01-29 15:19:02.737464: Current learning rate: 0.00134 
2025-01-29 15:19:51.544481: train_loss -0.8468 
2025-01-29 15:19:51.550065: val_loss -0.7999 
2025-01-29 15:19:51.552770: Pseudo dice [np.float32(0.966), np.float32(0.8749)] 
2025-01-29 15:19:51.555181: Epoch time: 48.81 s 
2025-01-29 15:19:52.679796:  
2025-01-29 15:19:52.682402: Epoch 894 
2025-01-29 15:19:52.685167: Current learning rate: 0.00133 
2025-01-29 15:20:41.088360: train_loss -0.8403 
2025-01-29 15:20:41.093786: val_loss -0.7895 
2025-01-29 15:20:41.096626: Pseudo dice [np.float32(0.9719), np.float32(0.8768)] 
2025-01-29 15:20:41.098956: Epoch time: 48.41 s 
2025-01-29 15:20:42.219585:  
2025-01-29 15:20:42.222561: Epoch 895 
2025-01-29 15:20:42.225306: Current learning rate: 0.00132 
2025-01-29 15:21:30.628346: train_loss -0.8544 
2025-01-29 15:21:30.632438: val_loss -0.7996 
2025-01-29 15:21:30.639834: Pseudo dice [np.float32(0.968), np.float32(0.896)] 
2025-01-29 15:21:30.642224: Epoch time: 48.41 s 
2025-01-29 15:21:31.765545:  
2025-01-29 15:21:31.768078: Epoch 896 
2025-01-29 15:21:31.770634: Current learning rate: 0.0013 
2025-01-29 15:22:20.026683: train_loss -0.8606 
2025-01-29 15:22:20.032942: val_loss -0.8143 
2025-01-29 15:22:20.035736: Pseudo dice [np.float32(0.9659), np.float32(0.8763)] 
2025-01-29 15:22:20.038352: Epoch time: 48.26 s 
2025-01-29 15:22:21.159620:  
2025-01-29 15:22:21.162460: Epoch 897 
2025-01-29 15:22:21.165320: Current learning rate: 0.00129 
2025-01-29 15:23:09.855295: train_loss -0.8396 
2025-01-29 15:23:09.860056: val_loss -0.7911 
2025-01-29 15:23:09.862944: Pseudo dice [np.float32(0.9694), np.float32(0.8559)] 
2025-01-29 15:23:09.865433: Epoch time: 48.7 s 
2025-01-29 15:23:10.996379:  
2025-01-29 15:23:11.003654: Epoch 898 
2025-01-29 15:23:11.006273: Current learning rate: 0.00128 
2025-01-29 15:23:59.334844: train_loss -0.8503 
2025-01-29 15:23:59.343312: val_loss -0.8015 
2025-01-29 15:23:59.350959: Pseudo dice [np.float32(0.9737), np.float32(0.8895)] 
2025-01-29 15:23:59.353954: Epoch time: 48.34 s 
2025-01-29 15:24:00.476417:  
2025-01-29 15:24:00.479166: Epoch 899 
2025-01-29 15:24:00.481897: Current learning rate: 0.00127 
2025-01-29 15:24:49.587079: train_loss -0.838 
2025-01-29 15:24:49.590885: val_loss -0.7999 
2025-01-29 15:24:49.593255: Pseudo dice [np.float32(0.9679), np.float32(0.8718)] 
2025-01-29 15:24:49.595837: Epoch time: 49.11 s 
2025-01-29 15:24:51.264897:  
2025-01-29 15:24:51.267628: Epoch 900 
2025-01-29 15:24:51.270384: Current learning rate: 0.00126 
2025-01-29 15:25:40.099649: train_loss -0.8399 
2025-01-29 15:25:40.105436: val_loss -0.7903 
2025-01-29 15:25:40.108294: Pseudo dice [np.float32(0.9685), np.float32(0.8791)] 
2025-01-29 15:25:40.111072: Epoch time: 48.84 s 
2025-01-29 15:25:41.244294:  
2025-01-29 15:25:41.247344: Epoch 901 
2025-01-29 15:25:41.250005: Current learning rate: 0.00125 
2025-01-29 15:26:30.328859: train_loss -0.8332 
2025-01-29 15:26:30.332970: val_loss -0.7979 
2025-01-29 15:26:30.335737: Pseudo dice [np.float32(0.9627), np.float32(0.8767)] 
2025-01-29 15:26:30.338418: Epoch time: 49.09 s 
2025-01-29 15:26:31.463202:  
2025-01-29 15:26:31.466111: Epoch 902 
2025-01-29 15:26:31.468937: Current learning rate: 0.00124 
2025-01-29 15:27:19.797015: train_loss -0.8419 
2025-01-29 15:27:19.802672: val_loss -0.8098 
2025-01-29 15:27:19.805329: Pseudo dice [np.float32(0.9703), np.float32(0.8927)] 
2025-01-29 15:27:19.807916: Epoch time: 48.33 s 
2025-01-29 15:27:20.930120:  
2025-01-29 15:27:20.932765: Epoch 903 
2025-01-29 15:27:20.935185: Current learning rate: 0.00122 
2025-01-29 15:28:09.398769: train_loss -0.8556 
2025-01-29 15:28:09.403486: val_loss -0.7699 
2025-01-29 15:28:09.406253: Pseudo dice [np.float32(0.9682), np.float32(0.8613)] 
2025-01-29 15:28:09.408954: Epoch time: 48.47 s 
2025-01-29 15:28:10.532797:  
2025-01-29 15:28:10.535823: Epoch 904 
2025-01-29 15:28:10.538720: Current learning rate: 0.00121 
2025-01-29 15:28:59.246500: train_loss -0.8274 
2025-01-29 15:28:59.252259: val_loss -0.7904 
2025-01-29 15:28:59.254909: Pseudo dice [np.float32(0.9686), np.float32(0.8794)] 
2025-01-29 15:28:59.257396: Epoch time: 48.71 s 
2025-01-29 15:29:00.375808:  
2025-01-29 15:29:00.379062: Epoch 905 
2025-01-29 15:29:00.381622: Current learning rate: 0.0012 
2025-01-29 15:29:49.183469: train_loss -0.8503 
2025-01-29 15:29:49.188206: val_loss -0.7942 
2025-01-29 15:29:49.191682: Pseudo dice [np.float32(0.961), np.float32(0.8463)] 
2025-01-29 15:29:49.194889: Epoch time: 48.81 s 
2025-01-29 15:29:50.317068:  
2025-01-29 15:29:50.320319: Epoch 906 
2025-01-29 15:29:50.323328: Current learning rate: 0.00119 
2025-01-29 15:30:39.331192: train_loss -0.857 
2025-01-29 15:30:39.336287: val_loss -0.7981 
2025-01-29 15:30:39.338762: Pseudo dice [np.float32(0.9648), np.float32(0.8732)] 
2025-01-29 15:30:39.341321: Epoch time: 49.02 s 
2025-01-29 15:30:40.466236:  
2025-01-29 15:30:40.469038: Epoch 907 
2025-01-29 15:30:40.471931: Current learning rate: 0.00118 
2025-01-29 15:31:29.313383: train_loss -0.8509 
2025-01-29 15:31:29.317576: val_loss -0.8037 
2025-01-29 15:31:29.320142: Pseudo dice [np.float32(0.9696), np.float32(0.8754)] 
2025-01-29 15:31:29.322922: Epoch time: 48.85 s 
2025-01-29 15:31:30.440730:  
2025-01-29 15:31:30.443588: Epoch 908 
2025-01-29 15:31:30.446021: Current learning rate: 0.00117 
2025-01-29 15:32:19.072566: train_loss -0.8511 
2025-01-29 15:32:19.077979: val_loss -0.7933 
2025-01-29 15:32:19.080775: Pseudo dice [np.float32(0.9701), np.float32(0.8877)] 
2025-01-29 15:32:19.083404: Epoch time: 48.63 s 
2025-01-29 15:32:20.212632:  
2025-01-29 15:32:20.215455: Epoch 909 
2025-01-29 15:32:20.217987: Current learning rate: 0.00116 
2025-01-29 15:33:08.899775: train_loss -0.8366 
2025-01-29 15:33:08.905866: val_loss -0.7979 
2025-01-29 15:33:08.908616: Pseudo dice [np.float32(0.9678), np.float32(0.87)] 
2025-01-29 15:33:08.911252: Epoch time: 48.69 s 
2025-01-29 15:33:10.575173:  
2025-01-29 15:33:10.578453: Epoch 910 
2025-01-29 15:33:10.581360: Current learning rate: 0.00115 
2025-01-29 15:33:59.217389: train_loss -0.8605 
2025-01-29 15:33:59.222872: val_loss -0.7898 
2025-01-29 15:33:59.225398: Pseudo dice [np.float32(0.9633), np.float32(0.8502)] 
2025-01-29 15:33:59.227795: Epoch time: 48.64 s 
2025-01-29 15:34:00.345270:  
2025-01-29 15:34:00.348554: Epoch 911 
2025-01-29 15:34:00.351281: Current learning rate: 0.00113 
2025-01-29 15:34:48.552054: train_loss -0.8481 
2025-01-29 15:34:48.556138: val_loss -0.7985 
2025-01-29 15:34:48.558736: Pseudo dice [np.float32(0.9712), np.float32(0.8775)] 
2025-01-29 15:34:48.561408: Epoch time: 48.21 s 
2025-01-29 15:34:49.686765:  
2025-01-29 15:34:49.689825: Epoch 912 
2025-01-29 15:34:49.692662: Current learning rate: 0.00112 
2025-01-29 15:35:38.559558: train_loss -0.841 
2025-01-29 15:35:38.565012: val_loss -0.7818 
2025-01-29 15:35:38.567825: Pseudo dice [np.float32(0.9705), np.float32(0.8758)] 
2025-01-29 15:35:38.570285: Epoch time: 48.87 s 
2025-01-29 15:35:39.696847:  
2025-01-29 15:35:39.699912: Epoch 913 
2025-01-29 15:35:39.702796: Current learning rate: 0.00111 
2025-01-29 15:36:28.401011: train_loss -0.8581 
2025-01-29 15:36:28.407722: val_loss -0.7652 
2025-01-29 15:36:28.415806: Pseudo dice [np.float32(0.9712), np.float32(0.8894)] 
2025-01-29 15:36:28.419011: Epoch time: 48.71 s 
2025-01-29 15:36:29.545492:  
2025-01-29 15:36:29.548495: Epoch 914 
2025-01-29 15:36:29.551503: Current learning rate: 0.0011 
2025-01-29 15:37:17.656167: train_loss -0.8617 
2025-01-29 15:37:17.661801: val_loss -0.8069 
2025-01-29 15:37:17.664826: Pseudo dice [np.float32(0.9685), np.float32(0.8771)] 
2025-01-29 15:37:17.667451: Epoch time: 48.11 s 
2025-01-29 15:37:18.797499:  
2025-01-29 15:37:18.800549: Epoch 915 
2025-01-29 15:37:18.803334: Current learning rate: 0.00109 
2025-01-29 15:38:06.964202: train_loss -0.85 
2025-01-29 15:38:06.969200: val_loss -0.7954 
2025-01-29 15:38:06.972217: Pseudo dice [np.float32(0.9708), np.float32(0.887)] 
2025-01-29 15:38:06.974694: Epoch time: 48.17 s 
2025-01-29 15:38:08.101740:  
2025-01-29 15:38:08.104908: Epoch 916 
2025-01-29 15:38:08.107851: Current learning rate: 0.00108 
2025-01-29 15:38:56.895226: train_loss -0.851 
2025-01-29 15:38:56.902497: val_loss -0.8155 
2025-01-29 15:38:56.904822: Pseudo dice [np.float32(0.9694), np.float32(0.8963)] 
2025-01-29 15:38:56.907150: Epoch time: 48.79 s 
2025-01-29 15:38:58.028805:  
2025-01-29 15:38:58.031452: Epoch 917 
2025-01-29 15:38:58.034038: Current learning rate: 0.00106 
2025-01-29 15:39:46.500648: train_loss -0.8668 
2025-01-29 15:39:46.504910: val_loss -0.7872 
2025-01-29 15:39:46.507728: Pseudo dice [np.float32(0.9687), np.float32(0.8871)] 
2025-01-29 15:39:46.510337: Epoch time: 48.47 s 
2025-01-29 15:39:47.629884:  
2025-01-29 15:39:47.633320: Epoch 918 
2025-01-29 15:39:47.636272: Current learning rate: 0.00105 
2025-01-29 15:40:35.965750: train_loss -0.861 
2025-01-29 15:40:35.973228: val_loss -0.8003 
2025-01-29 15:40:35.981329: Pseudo dice [np.float32(0.9699), np.float32(0.8631)] 
2025-01-29 15:40:35.984257: Epoch time: 48.34 s 
2025-01-29 15:40:37.105304:  
2025-01-29 15:40:37.108103: Epoch 919 
2025-01-29 15:40:37.110725: Current learning rate: 0.00104 
2025-01-29 15:41:25.481285: train_loss -0.8566 
2025-01-29 15:41:25.485480: val_loss -0.8034 
2025-01-29 15:41:25.488383: Pseudo dice [np.float32(0.965), np.float32(0.8796)] 
2025-01-29 15:41:25.490940: Epoch time: 48.38 s 
2025-01-29 15:41:26.614567:  
2025-01-29 15:41:26.617461: Epoch 920 
2025-01-29 15:41:26.620513: Current learning rate: 0.00103 
2025-01-29 15:42:15.397182: train_loss -0.8713 
2025-01-29 15:42:15.404520: val_loss -0.8087 
2025-01-29 15:42:15.407176: Pseudo dice [np.float32(0.9683), np.float32(0.8937)] 
2025-01-29 15:42:15.409570: Epoch time: 48.78 s 
2025-01-29 15:42:16.535707:  
2025-01-29 15:42:16.539061: Epoch 921 
2025-01-29 15:42:16.541863: Current learning rate: 0.00102 
2025-01-29 15:43:05.081934: train_loss -0.8519 
2025-01-29 15:43:05.086737: val_loss -0.8144 
2025-01-29 15:43:05.090098: Pseudo dice [np.float32(0.9708), np.float32(0.8873)] 
2025-01-29 15:43:05.093482: Epoch time: 48.55 s 
2025-01-29 15:43:06.213599:  
2025-01-29 15:43:06.216572: Epoch 922 
2025-01-29 15:43:06.219487: Current learning rate: 0.00101 
2025-01-29 15:43:54.957053: train_loss -0.8421 
2025-01-29 15:43:54.962569: val_loss -0.8002 
2025-01-29 15:43:54.970065: Pseudo dice [np.float32(0.97), np.float32(0.8818)] 
2025-01-29 15:43:54.972676: Epoch time: 48.74 s 
2025-01-29 15:43:56.097135:  
2025-01-29 15:43:56.099849: Epoch 923 
2025-01-29 15:43:56.102968: Current learning rate: 0.001 
2025-01-29 15:44:44.737017: train_loss -0.8452 
2025-01-29 15:44:44.742034: val_loss -0.7784 
2025-01-29 15:44:44.744813: Pseudo dice [np.float32(0.9645), np.float32(0.8572)] 
2025-01-29 15:44:44.747652: Epoch time: 48.64 s 
2025-01-29 15:44:45.868716:  
2025-01-29 15:44:45.871654: Epoch 924 
2025-01-29 15:44:45.874382: Current learning rate: 0.00098 
2025-01-29 15:45:34.524291: train_loss -0.8447 
2025-01-29 15:45:34.529103: val_loss -0.8095 
2025-01-29 15:45:34.531410: Pseudo dice [np.float32(0.97), np.float32(0.9009)] 
2025-01-29 15:45:34.533702: Epoch time: 48.66 s 
2025-01-29 15:45:35.646225:  
2025-01-29 15:45:35.648696: Epoch 925 
2025-01-29 15:45:35.651051: Current learning rate: 0.00097 
2025-01-29 15:46:24.400916: train_loss -0.8603 
2025-01-29 15:46:24.405078: val_loss -0.8085 
2025-01-29 15:46:24.407875: Pseudo dice [np.float32(0.9709), np.float32(0.8919)] 
2025-01-29 15:46:24.410806: Epoch time: 48.76 s 
2025-01-29 15:46:25.536359:  
2025-01-29 15:46:25.539149: Epoch 926 
2025-01-29 15:46:25.541631: Current learning rate: 0.00096 
2025-01-29 15:47:14.548903: train_loss -0.849 
2025-01-29 15:47:14.553857: val_loss -0.8075 
2025-01-29 15:47:14.556434: Pseudo dice [np.float32(0.9695), np.float32(0.8975)] 
2025-01-29 15:47:14.558563: Epoch time: 49.01 s 
2025-01-29 15:47:15.677891:  
2025-01-29 15:47:15.680717: Epoch 927 
2025-01-29 15:47:15.683388: Current learning rate: 0.00095 
2025-01-29 15:48:04.313584: train_loss -0.86 
2025-01-29 15:48:04.317692: val_loss -0.7894 
2025-01-29 15:48:04.325582: Pseudo dice [np.float32(0.9697), np.float32(0.8814)] 
2025-01-29 15:48:04.328275: Epoch time: 48.64 s 
2025-01-29 15:48:05.443810:  
2025-01-29 15:48:05.446688: Epoch 928 
2025-01-29 15:48:05.449128: Current learning rate: 0.00094 
2025-01-29 15:48:54.441832: train_loss -0.8292 
2025-01-29 15:48:54.447611: val_loss -0.7991 
2025-01-29 15:48:54.449986: Pseudo dice [np.float32(0.9692), np.float32(0.8986)] 
2025-01-29 15:48:54.452445: Epoch time: 49.0 s 
2025-01-29 15:48:55.581569:  
2025-01-29 15:48:55.584495: Epoch 929 
2025-01-29 15:48:55.587848: Current learning rate: 0.00092 
2025-01-29 15:49:44.212370: train_loss -0.8579 
2025-01-29 15:49:44.217656: val_loss -0.7795 
2025-01-29 15:49:44.220352: Pseudo dice [np.float32(0.9715), np.float32(0.8948)] 
2025-01-29 15:49:44.222951: Epoch time: 48.63 s 
2025-01-29 15:49:45.983563:  
2025-01-29 15:49:45.986382: Epoch 930 
2025-01-29 15:49:45.989127: Current learning rate: 0.00091 
2025-01-29 15:50:34.705815: train_loss -0.8314 
2025-01-29 15:50:34.712701: val_loss -0.8133 
2025-01-29 15:50:34.720735: Pseudo dice [np.float32(0.9728), np.float32(0.8964)] 
2025-01-29 15:50:34.723664: Epoch time: 48.72 s 
2025-01-29 15:50:35.847358:  
2025-01-29 15:50:35.850582: Epoch 931 
2025-01-29 15:50:35.853593: Current learning rate: 0.0009 
2025-01-29 15:51:24.911769: train_loss -0.8549 
2025-01-29 15:51:24.915971: val_loss -0.8132 
2025-01-29 15:51:24.919927: Pseudo dice [np.float32(0.9638), np.float32(0.8748)] 
2025-01-29 15:51:24.922698: Epoch time: 49.07 s 
2025-01-29 15:51:26.042189:  
2025-01-29 15:51:26.045641: Epoch 932 
2025-01-29 15:51:26.048148: Current learning rate: 0.00089 
2025-01-29 15:52:14.663826: train_loss -0.8746 
2025-01-29 15:52:14.671276: val_loss -0.8176 
2025-01-29 15:52:14.673780: Pseudo dice [np.float32(0.9701), np.float32(0.8866)] 
2025-01-29 15:52:14.676301: Epoch time: 48.62 s 
2025-01-29 15:52:15.796733:  
2025-01-29 15:52:15.799918: Epoch 933 
2025-01-29 15:52:15.803058: Current learning rate: 0.00088 
2025-01-29 15:53:04.096552: train_loss -0.8606 
2025-01-29 15:53:04.100801: val_loss -0.7829 
2025-01-29 15:53:04.103796: Pseudo dice [np.float32(0.9667), np.float32(0.882)] 
2025-01-29 15:53:04.106406: Epoch time: 48.3 s 
2025-01-29 15:53:05.223879:  
2025-01-29 15:53:05.226761: Epoch 934 
2025-01-29 15:53:05.229574: Current learning rate: 0.00087 
2025-01-29 15:53:54.180198: train_loss -0.8477 
2025-01-29 15:53:54.185344: val_loss -0.8221 
2025-01-29 15:53:54.187819: Pseudo dice [np.float32(0.9695), np.float32(0.8923)] 
2025-01-29 15:53:54.190164: Epoch time: 48.96 s 
2025-01-29 15:53:55.305979:  
2025-01-29 15:53:55.308632: Epoch 935 
2025-01-29 15:53:55.311269: Current learning rate: 0.00085 
2025-01-29 15:54:44.231766: train_loss -0.8574 
2025-01-29 15:54:44.238332: val_loss -0.8042 
2025-01-29 15:54:44.241086: Pseudo dice [np.float32(0.9715), np.float32(0.8963)] 
2025-01-29 15:54:44.243831: Epoch time: 48.93 s 
2025-01-29 15:54:45.364952:  
2025-01-29 15:54:45.367696: Epoch 936 
2025-01-29 15:54:45.370228: Current learning rate: 0.00084 
2025-01-29 15:55:34.002839: train_loss -0.8452 
2025-01-29 15:55:34.008093: val_loss -0.8118 
2025-01-29 15:55:34.010650: Pseudo dice [np.float32(0.9688), np.float32(0.9055)] 
2025-01-29 15:55:34.013150: Epoch time: 48.64 s 
2025-01-29 15:55:35.143930:  
2025-01-29 15:55:35.147632: Epoch 937 
2025-01-29 15:55:35.150059: Current learning rate: 0.00083 
2025-01-29 15:56:23.998499: train_loss -0.8501 
2025-01-29 15:56:24.003266: val_loss -0.8157 
2025-01-29 15:56:24.011029: Pseudo dice [np.float32(0.9721), np.float32(0.8992)] 
2025-01-29 15:56:24.014090: Epoch time: 48.86 s 
2025-01-29 15:56:25.136039:  
2025-01-29 15:56:25.138849: Epoch 938 
2025-01-29 15:56:25.141539: Current learning rate: 0.00082 
2025-01-29 15:57:14.111820: train_loss -0.8407 
2025-01-29 15:57:14.117661: val_loss -0.8105 
2025-01-29 15:57:14.120238: Pseudo dice [np.float32(0.9728), np.float32(0.888)] 
2025-01-29 15:57:14.122807: Epoch time: 48.98 s 
2025-01-29 15:57:15.252655:  
2025-01-29 15:57:15.255403: Epoch 939 
2025-01-29 15:57:15.258090: Current learning rate: 0.00081 
2025-01-29 15:58:03.758785: train_loss -0.8596 
2025-01-29 15:58:03.763397: val_loss -0.8261 
2025-01-29 15:58:03.765957: Pseudo dice [np.float32(0.9658), np.float32(0.8911)] 
2025-01-29 15:58:03.768609: Epoch time: 48.51 s 
2025-01-29 15:58:04.887013:  
2025-01-29 15:58:04.889776: Epoch 940 
2025-01-29 15:58:04.892828: Current learning rate: 0.00079 
2025-01-29 15:58:53.279621: train_loss -0.8346 
2025-01-29 15:58:53.285451: val_loss -0.7781 
2025-01-29 15:58:53.288404: Pseudo dice [np.float32(0.9661), np.float32(0.8865)] 
2025-01-29 15:58:53.291425: Epoch time: 48.39 s 
2025-01-29 15:58:54.413317:  
2025-01-29 15:58:54.416324: Epoch 941 
2025-01-29 15:58:54.419404: Current learning rate: 0.00078 
2025-01-29 15:59:42.754996: train_loss -0.8356 
2025-01-29 15:59:42.758902: val_loss -0.7994 
2025-01-29 15:59:42.761466: Pseudo dice [np.float32(0.9713), np.float32(0.8942)] 
2025-01-29 15:59:42.764078: Epoch time: 48.34 s 
2025-01-29 15:59:43.881436:  
2025-01-29 15:59:43.884155: Epoch 942 
2025-01-29 15:59:43.886637: Current learning rate: 0.00077 
2025-01-29 16:00:32.206760: train_loss -0.8487 
2025-01-29 16:00:32.212697: val_loss -0.7757 
2025-01-29 16:00:32.215721: Pseudo dice [np.float32(0.9691), np.float32(0.8886)] 
2025-01-29 16:00:32.218122: Epoch time: 48.33 s 
2025-01-29 16:00:33.338826:  
2025-01-29 16:00:33.341864: Epoch 943 
2025-01-29 16:00:33.344823: Current learning rate: 0.00076 
2025-01-29 16:01:21.850918: train_loss -0.8494 
2025-01-29 16:01:21.855519: val_loss -0.7964 
2025-01-29 16:01:21.863247: Pseudo dice [np.float32(0.9731), np.float32(0.878)] 
2025-01-29 16:01:21.865939: Epoch time: 48.51 s 
2025-01-29 16:01:23.044400:  
2025-01-29 16:01:23.048351: Epoch 944 
2025-01-29 16:01:23.052181: Current learning rate: 0.00075 
2025-01-29 16:02:11.816300: train_loss -0.8407 
2025-01-29 16:02:11.822655: val_loss -0.7917 
2025-01-29 16:02:11.825228: Pseudo dice [np.float32(0.9686), np.float32(0.891)] 
2025-01-29 16:02:11.828004: Epoch time: 48.78 s 
2025-01-29 16:02:13.022225:  
2025-01-29 16:02:13.027621: Epoch 945 
2025-01-29 16:02:13.031112: Current learning rate: 0.00074 
2025-01-29 16:03:02.080723: train_loss -0.8451 
2025-01-29 16:03:02.085242: val_loss -0.8059 
2025-01-29 16:03:02.087878: Pseudo dice [np.float32(0.9708), np.float32(0.8921)] 
2025-01-29 16:03:02.090420: Epoch time: 49.06 s 
2025-01-29 16:03:03.226725:  
2025-01-29 16:03:03.229955: Epoch 946 
2025-01-29 16:03:03.233940: Current learning rate: 0.00072 
2025-01-29 16:03:51.932089: train_loss -0.8514 
2025-01-29 16:03:51.938356: val_loss -0.8093 
2025-01-29 16:03:51.949313: Pseudo dice [np.float32(0.9695), np.float32(0.8944)] 
2025-01-29 16:03:51.952067: Epoch time: 48.71 s 
2025-01-29 16:03:53.086682:  
2025-01-29 16:03:53.089669: Epoch 947 
2025-01-29 16:03:53.092439: Current learning rate: 0.00071 
2025-01-29 16:04:41.761517: train_loss -0.8567 
2025-01-29 16:04:41.765280: val_loss -0.8387 
2025-01-29 16:04:41.767695: Pseudo dice [np.float32(0.9727), np.float32(0.9058)] 
2025-01-29 16:04:41.770061: Epoch time: 48.68 s 
2025-01-29 16:04:42.895062:  
2025-01-29 16:04:42.898075: Epoch 948 
2025-01-29 16:04:42.900853: Current learning rate: 0.0007 
2025-01-29 16:05:31.248815: train_loss -0.8683 
2025-01-29 16:05:31.257327: val_loss -0.8223 
2025-01-29 16:05:31.260159: Pseudo dice [np.float32(0.9694), np.float32(0.8974)] 
2025-01-29 16:05:31.263142: Epoch time: 48.36 s 
2025-01-29 16:05:32.917078:  
2025-01-29 16:05:32.919843: Epoch 949 
2025-01-29 16:05:32.922698: Current learning rate: 0.00069 
2025-01-29 16:06:21.212363: train_loss -0.8567 
2025-01-29 16:06:21.216209: val_loss -0.8224 
2025-01-29 16:06:21.223924: Pseudo dice [np.float32(0.9675), np.float32(0.8928)] 
2025-01-29 16:06:21.226642: Epoch time: 48.3 s 
2025-01-29 16:06:22.884422:  
2025-01-29 16:06:22.886852: Epoch 950 
2025-01-29 16:06:22.889320: Current learning rate: 0.00067 
2025-01-29 16:07:11.409333: train_loss -0.8372 
2025-01-29 16:07:11.415349: val_loss -0.8325 
2025-01-29 16:07:11.418021: Pseudo dice [np.float32(0.9695), np.float32(0.8969)] 
2025-01-29 16:07:11.420537: Epoch time: 48.53 s 
2025-01-29 16:07:12.549837:  
2025-01-29 16:07:12.552851: Epoch 951 
2025-01-29 16:07:12.555497: Current learning rate: 0.00066 
2025-01-29 16:08:00.982794: train_loss -0.864 
2025-01-29 16:08:00.987329: val_loss -0.7958 
2025-01-29 16:08:00.989921: Pseudo dice [np.float32(0.9718), np.float32(0.898)] 
2025-01-29 16:08:00.992674: Epoch time: 48.43 s 
2025-01-29 16:08:00.995157: Yayy! New best EMA pseudo Dice: 0.9315000176429749 
2025-01-29 16:08:02.691327:  
2025-01-29 16:08:02.694051: Epoch 952 
2025-01-29 16:08:02.696628: Current learning rate: 0.00065 
2025-01-29 16:08:51.084567: train_loss -0.8501 
2025-01-29 16:08:51.089620: val_loss -0.806 
2025-01-29 16:08:51.092093: Pseudo dice [np.float32(0.9663), np.float32(0.8954)] 
2025-01-29 16:08:51.094446: Epoch time: 48.39 s 
2025-01-29 16:08:52.219836:  
2025-01-29 16:08:52.223014: Epoch 953 
2025-01-29 16:08:52.225782: Current learning rate: 0.00064 
2025-01-29 16:09:40.781669: train_loss -0.8493 
2025-01-29 16:09:40.786240: val_loss -0.8228 
2025-01-29 16:09:40.794699: Pseudo dice [np.float32(0.9726), np.float32(0.8951)] 
2025-01-29 16:09:40.797421: Epoch time: 48.56 s 
2025-01-29 16:09:40.800147: Yayy! New best EMA pseudo Dice: 0.9315999746322632 
2025-01-29 16:09:42.460654:  
2025-01-29 16:09:42.463788: Epoch 954 
2025-01-29 16:09:42.466542: Current learning rate: 0.00063 
2025-01-29 16:10:31.128254: train_loss -0.8457 
2025-01-29 16:10:31.133997: val_loss -0.7778 
2025-01-29 16:10:31.136946: Pseudo dice [np.float32(0.9711), np.float32(0.8808)] 
2025-01-29 16:10:31.139710: Epoch time: 48.67 s 
2025-01-29 16:10:32.280229:  
2025-01-29 16:10:32.282906: Epoch 955 
2025-01-29 16:10:32.285378: Current learning rate: 0.00061 
2025-01-29 16:11:20.940868: train_loss -0.8486 
2025-01-29 16:11:20.947289: val_loss -0.8267 
2025-01-29 16:11:20.950198: Pseudo dice [np.float32(0.9701), np.float32(0.903)] 
2025-01-29 16:11:20.953091: Epoch time: 48.66 s 
2025-01-29 16:11:22.092521:  
2025-01-29 16:11:22.095102: Epoch 956 
2025-01-29 16:11:22.097952: Current learning rate: 0.0006 
2025-01-29 16:12:10.669814: train_loss -0.8293 
2025-01-29 16:12:10.676785: val_loss -0.8129 
2025-01-29 16:12:10.679597: Pseudo dice [np.float32(0.9684), np.float32(0.889)] 
2025-01-29 16:12:10.682217: Epoch time: 48.58 s 
2025-01-29 16:12:11.816139:  
2025-01-29 16:12:11.819117: Epoch 957 
2025-01-29 16:12:11.821975: Current learning rate: 0.00059 
2025-01-29 16:13:00.173606: train_loss -0.861 
2025-01-29 16:13:00.177890: val_loss -0.7903 
2025-01-29 16:13:00.185856: Pseudo dice [np.float32(0.9652), np.float32(0.87)] 
2025-01-29 16:13:00.188689: Epoch time: 48.36 s 
2025-01-29 16:13:01.320292:  
2025-01-29 16:13:01.323410: Epoch 958 
2025-01-29 16:13:01.326228: Current learning rate: 0.00058 
2025-01-29 16:13:50.154416: train_loss -0.8577 
2025-01-29 16:13:50.161678: val_loss -0.7975 
2025-01-29 16:13:50.164647: Pseudo dice [np.float32(0.9698), np.float32(0.883)] 
2025-01-29 16:13:50.167218: Epoch time: 48.84 s 
2025-01-29 16:13:51.311837:  
2025-01-29 16:13:51.314826: Epoch 959 
2025-01-29 16:13:51.317567: Current learning rate: 0.00056 
2025-01-29 16:14:40.189791: train_loss -0.8546 
2025-01-29 16:14:40.193717: val_loss -0.811 
2025-01-29 16:14:40.196306: Pseudo dice [np.float32(0.9591), np.float32(0.8676)] 
2025-01-29 16:14:40.198961: Epoch time: 48.88 s 
2025-01-29 16:14:41.334399:  
2025-01-29 16:14:41.337621: Epoch 960 
2025-01-29 16:14:41.340425: Current learning rate: 0.00055 
2025-01-29 16:15:29.794855: train_loss -0.8719 
2025-01-29 16:15:29.801370: val_loss -0.8011 
2025-01-29 16:15:29.803769: Pseudo dice [np.float32(0.9737), np.float32(0.9059)] 
2025-01-29 16:15:29.806224: Epoch time: 48.46 s 
2025-01-29 16:15:30.934126:  
2025-01-29 16:15:30.936629: Epoch 961 
2025-01-29 16:15:30.938911: Current learning rate: 0.00054 
2025-01-29 16:16:19.579647: train_loss -0.8667 
2025-01-29 16:16:19.583604: val_loss -0.7944 
2025-01-29 16:16:19.586365: Pseudo dice [np.float32(0.9725), np.float32(0.9044)] 
2025-01-29 16:16:19.588788: Epoch time: 48.65 s 
2025-01-29 16:16:20.728940:  
2025-01-29 16:16:20.731687: Epoch 962 
2025-01-29 16:16:20.734291: Current learning rate: 0.00053 
2025-01-29 16:17:09.118771: train_loss -0.8544 
2025-01-29 16:17:09.126928: val_loss -0.8355 
2025-01-29 16:17:09.135722: Pseudo dice [np.float32(0.9698), np.float32(0.9025)] 
2025-01-29 16:17:09.138704: Epoch time: 48.39 s 
2025-01-29 16:17:10.273195:  
2025-01-29 16:17:10.276324: Epoch 963 
2025-01-29 16:17:10.279344: Current learning rate: 0.00051 
2025-01-29 16:17:59.744485: train_loss -0.8574 
2025-01-29 16:17:59.748608: val_loss -0.7999 
2025-01-29 16:17:59.751422: Pseudo dice [np.float32(0.9694), np.float32(0.886)] 
2025-01-29 16:17:59.753947: Epoch time: 49.47 s 
2025-01-29 16:18:00.894647:  
2025-01-29 16:18:00.897452: Epoch 964 
2025-01-29 16:18:00.900136: Current learning rate: 0.0005 
2025-01-29 16:18:49.376234: train_loss -0.8522 
2025-01-29 16:18:49.381874: val_loss -0.808 
2025-01-29 16:18:49.384676: Pseudo dice [np.float32(0.9684), np.float32(0.9029)] 
2025-01-29 16:18:49.387147: Epoch time: 48.48 s 
2025-01-29 16:18:50.528820:  
2025-01-29 16:18:50.531617: Epoch 965 
2025-01-29 16:18:50.534493: Current learning rate: 0.00049 
2025-01-29 16:19:38.997313: train_loss -0.8635 
2025-01-29 16:19:39.001714: val_loss -0.773 
2025-01-29 16:19:39.004560: Pseudo dice [np.float32(0.9705), np.float32(0.8758)] 
2025-01-29 16:19:39.007045: Epoch time: 48.47 s 
2025-01-29 16:19:40.142830:  
2025-01-29 16:19:40.145779: Epoch 966 
2025-01-29 16:19:40.148719: Current learning rate: 0.00048 
2025-01-29 16:20:28.683655: train_loss -0.8678 
2025-01-29 16:20:28.690949: val_loss -0.7682 
2025-01-29 16:20:28.693693: Pseudo dice [np.float32(0.9721), np.float32(0.8919)] 
2025-01-29 16:20:28.696674: Epoch time: 48.54 s 
2025-01-29 16:20:29.833803:  
2025-01-29 16:20:29.836842: Epoch 967 
2025-01-29 16:20:29.839984: Current learning rate: 0.00046 
2025-01-29 16:21:18.660769: train_loss -0.8566 
2025-01-29 16:21:18.667883: val_loss -0.7794 
2025-01-29 16:21:18.678810: Pseudo dice [np.float32(0.9696), np.float32(0.8881)] 
2025-01-29 16:21:18.681732: Epoch time: 48.83 s 
2025-01-29 16:21:20.361114:  
2025-01-29 16:21:20.363621: Epoch 968 
2025-01-29 16:21:20.366346: Current learning rate: 0.00045 
2025-01-29 16:22:08.932943: train_loss -0.8479 
2025-01-29 16:22:08.938574: val_loss -0.7816 
2025-01-29 16:22:08.940985: Pseudo dice [np.float32(0.9698), np.float32(0.8971)] 
2025-01-29 16:22:08.943459: Epoch time: 48.57 s 
2025-01-29 16:22:10.099306:  
2025-01-29 16:22:10.102362: Epoch 969 
2025-01-29 16:22:10.105190: Current learning rate: 0.00044 
2025-01-29 16:22:58.564277: train_loss -0.8582 
2025-01-29 16:22:58.568812: val_loss -0.7979 
2025-01-29 16:22:58.576971: Pseudo dice [np.float32(0.9695), np.float32(0.9026)] 
2025-01-29 16:22:58.579905: Epoch time: 48.47 s 
2025-01-29 16:22:59.747431:  
2025-01-29 16:22:59.751785: Epoch 970 
2025-01-29 16:22:59.754701: Current learning rate: 0.00043 
2025-01-29 16:23:48.557837: train_loss -0.8552 
2025-01-29 16:23:48.564296: val_loss -0.8109 
2025-01-29 16:23:48.567227: Pseudo dice [np.float32(0.9703), np.float32(0.8976)] 
2025-01-29 16:23:48.569767: Epoch time: 48.81 s 
2025-01-29 16:23:49.745482:  
2025-01-29 16:23:49.748460: Epoch 971 
2025-01-29 16:23:49.751609: Current learning rate: 0.00041 
2025-01-29 16:24:38.336207: train_loss -0.8716 
2025-01-29 16:24:38.340686: val_loss -0.7985 
2025-01-29 16:24:38.343297: Pseudo dice [np.float32(0.9698), np.float32(0.893)] 
2025-01-29 16:24:38.345915: Epoch time: 48.59 s 
2025-01-29 16:24:39.497340:  
2025-01-29 16:24:39.500318: Epoch 972 
2025-01-29 16:24:39.502930: Current learning rate: 0.0004 
2025-01-29 16:25:28.180105: train_loss -0.8346 
2025-01-29 16:25:28.185919: val_loss -0.8081 
2025-01-29 16:25:28.188564: Pseudo dice [np.float32(0.9715), np.float32(0.9046)] 
2025-01-29 16:25:28.191150: Epoch time: 48.68 s 
2025-01-29 16:25:28.193737: Yayy! New best EMA pseudo Dice: 0.9319999814033508 
2025-01-29 16:25:29.901418:  
2025-01-29 16:25:29.904587: Epoch 973 
2025-01-29 16:25:29.907444: Current learning rate: 0.00039 
2025-01-29 16:26:18.472262: train_loss -0.8634 
2025-01-29 16:26:18.476135: val_loss -0.8155 
2025-01-29 16:26:18.478826: Pseudo dice [np.float32(0.9729), np.float32(0.8835)] 
2025-01-29 16:26:18.481315: Epoch time: 48.57 s 
2025-01-29 16:26:19.623713:  
2025-01-29 16:26:19.626751: Epoch 974 
2025-01-29 16:26:19.629404: Current learning rate: 0.00037 
2025-01-29 16:27:07.762618: train_loss -0.8538 
2025-01-29 16:27:07.768994: val_loss -0.8067 
2025-01-29 16:27:07.772704: Pseudo dice [np.float32(0.9686), np.float32(0.8984)] 
2025-01-29 16:27:07.775278: Epoch time: 48.14 s 
2025-01-29 16:27:08.915207:  
2025-01-29 16:27:08.918203: Epoch 975 
2025-01-29 16:27:08.920880: Current learning rate: 0.00036 
2025-01-29 16:27:57.381770: train_loss -0.8584 
2025-01-29 16:27:57.385146: val_loss -0.8049 
2025-01-29 16:27:57.387371: Pseudo dice [np.float32(0.9659), np.float32(0.8699)] 
2025-01-29 16:27:57.389728: Epoch time: 48.47 s 
2025-01-29 16:27:58.541063:  
2025-01-29 16:27:58.543590: Epoch 976 
2025-01-29 16:27:58.545922: Current learning rate: 0.00035 
2025-01-29 16:28:47.106925: train_loss -0.8469 
2025-01-29 16:28:47.113118: val_loss -0.8429 
2025-01-29 16:28:47.115923: Pseudo dice [np.float32(0.9666), np.float32(0.8711)] 
2025-01-29 16:28:47.118497: Epoch time: 48.57 s 
2025-01-29 16:28:48.265248:  
2025-01-29 16:28:48.267996: Epoch 977 
2025-01-29 16:28:48.270908: Current learning rate: 0.00034 
2025-01-29 16:29:36.846767: train_loss -0.8497 
2025-01-29 16:29:36.851021: val_loss -0.8154 
2025-01-29 16:29:36.853405: Pseudo dice [np.float32(0.9635), np.float32(0.8727)] 
2025-01-29 16:29:36.856094: Epoch time: 48.58 s 
2025-01-29 16:29:37.992298:  
2025-01-29 16:29:37.995098: Epoch 978 
2025-01-29 16:29:37.998013: Current learning rate: 0.00032 
2025-01-29 16:30:26.143179: train_loss -0.8702 
2025-01-29 16:30:26.148396: val_loss -0.8015 
2025-01-29 16:30:26.155502: Pseudo dice [np.float32(0.9654), np.float32(0.8863)] 
2025-01-29 16:30:26.158236: Epoch time: 48.15 s 
2025-01-29 16:30:27.301758:  
2025-01-29 16:30:27.305045: Epoch 979 
2025-01-29 16:30:27.308055: Current learning rate: 0.00031 
2025-01-29 16:31:16.057918: train_loss -0.8506 
2025-01-29 16:31:16.062170: val_loss -0.8274 
2025-01-29 16:31:16.064687: Pseudo dice [np.float32(0.9703), np.float32(0.8984)] 
2025-01-29 16:31:16.067091: Epoch time: 48.76 s 
2025-01-29 16:31:17.205282:  
2025-01-29 16:31:17.207975: Epoch 980 
2025-01-29 16:31:17.210608: Current learning rate: 0.0003 
2025-01-29 16:32:05.722556: train_loss -0.8466 
2025-01-29 16:32:05.728473: val_loss -0.8096 
2025-01-29 16:32:05.730992: Pseudo dice [np.float32(0.9685), np.float32(0.8717)] 
2025-01-29 16:32:05.733721: Epoch time: 48.52 s 
2025-01-29 16:32:06.873291:  
2025-01-29 16:32:06.876027: Epoch 981 
2025-01-29 16:32:06.878699: Current learning rate: 0.00028 
2025-01-29 16:32:55.466208: train_loss -0.8645 
2025-01-29 16:32:55.471287: val_loss -0.8003 
2025-01-29 16:32:55.480053: Pseudo dice [np.float32(0.968), np.float32(0.8861)] 
2025-01-29 16:32:55.483214: Epoch time: 48.59 s 
2025-01-29 16:32:56.621028:  
2025-01-29 16:32:56.623917: Epoch 982 
2025-01-29 16:32:56.626671: Current learning rate: 0.00027 
2025-01-29 16:33:45.446566: train_loss -0.8436 
2025-01-29 16:33:45.452315: val_loss -0.8182 
2025-01-29 16:33:45.454929: Pseudo dice [np.float32(0.9686), np.float32(0.8841)] 
2025-01-29 16:33:45.457351: Epoch time: 48.83 s 
2025-01-29 16:33:46.680303:  
2025-01-29 16:33:46.683543: Epoch 983 
2025-01-29 16:33:46.686445: Current learning rate: 0.00026 
2025-01-29 16:34:34.864410: train_loss -0.847 
2025-01-29 16:34:34.868691: val_loss -0.8137 
2025-01-29 16:34:34.871496: Pseudo dice [np.float32(0.9743), np.float32(0.9079)] 
2025-01-29 16:34:34.874217: Epoch time: 48.19 s 
2025-01-29 16:34:36.022233:  
2025-01-29 16:34:36.025342: Epoch 984 
2025-01-29 16:34:36.028480: Current learning rate: 0.00024 
2025-01-29 16:35:24.448237: train_loss -0.8642 
2025-01-29 16:35:24.454918: val_loss -0.8152 
2025-01-29 16:35:24.457648: Pseudo dice [np.float32(0.9719), np.float32(0.8767)] 
2025-01-29 16:35:24.460019: Epoch time: 48.43 s 
2025-01-29 16:35:25.650099:  
2025-01-29 16:35:25.653366: Epoch 985 
2025-01-29 16:35:25.657428: Current learning rate: 0.00023 
2025-01-29 16:36:14.546974: train_loss -0.8528 
2025-01-29 16:36:14.551502: val_loss -0.7952 
2025-01-29 16:36:14.554219: Pseudo dice [np.float32(0.9734), np.float32(0.887)] 
2025-01-29 16:36:14.556912: Epoch time: 48.9 s 
2025-01-29 16:36:16.356513:  
2025-01-29 16:36:16.360353: Epoch 986 
2025-01-29 16:36:16.363117: Current learning rate: 0.00021 
2025-01-29 16:37:04.854822: train_loss -0.8615 
2025-01-29 16:37:04.860231: val_loss -0.8098 
2025-01-29 16:37:04.872365: Pseudo dice [np.float32(0.9664), np.float32(0.8747)] 
2025-01-29 16:37:04.875064: Epoch time: 48.5 s 
2025-01-29 16:37:06.049856:  
2025-01-29 16:37:06.053980: Epoch 987 
2025-01-29 16:37:06.057026: Current learning rate: 0.0002 
2025-01-29 16:37:54.511633: train_loss -0.8565 
2025-01-29 16:37:54.515987: val_loss -0.8134 
2025-01-29 16:37:54.519084: Pseudo dice [np.float32(0.9722), np.float32(0.8855)] 
2025-01-29 16:37:54.521656: Epoch time: 48.46 s 
2025-01-29 16:37:55.699483:  
2025-01-29 16:37:55.702368: Epoch 988 
2025-01-29 16:37:55.705622: Current learning rate: 0.00019 
2025-01-29 16:38:44.933855: train_loss -0.8647 
2025-01-29 16:38:44.939747: val_loss -0.7928 
2025-01-29 16:38:44.942734: Pseudo dice [np.float32(0.9701), np.float32(0.8898)] 
2025-01-29 16:38:44.945206: Epoch time: 49.24 s 
2025-01-29 16:38:46.111650:  
2025-01-29 16:38:46.114554: Epoch 989 
2025-01-29 16:38:46.117123: Current learning rate: 0.00017 
2025-01-29 16:39:34.470028: train_loss -0.8683 
2025-01-29 16:39:34.474633: val_loss -0.819 
2025-01-29 16:39:34.482606: Pseudo dice [np.float32(0.9725), np.float32(0.9011)] 
2025-01-29 16:39:34.485615: Epoch time: 48.36 s 
2025-01-29 16:39:35.700270:  
2025-01-29 16:39:35.704419: Epoch 990 
2025-01-29 16:39:35.707023: Current learning rate: 0.00016 
2025-01-29 16:40:24.323175: train_loss -0.8539 
2025-01-29 16:40:24.330226: val_loss -0.8318 
2025-01-29 16:40:24.332710: Pseudo dice [np.float32(0.9686), np.float32(0.8994)] 
2025-01-29 16:40:24.335216: Epoch time: 48.62 s 
2025-01-29 16:40:25.512938:  
2025-01-29 16:40:25.517124: Epoch 991 
2025-01-29 16:40:25.519879: Current learning rate: 0.00014 
2025-01-29 16:41:14.013126: train_loss -0.8571 
2025-01-29 16:41:14.018091: val_loss -0.8071 
2025-01-29 16:41:14.020823: Pseudo dice [np.float32(0.9703), np.float32(0.8932)] 
2025-01-29 16:41:14.023355: Epoch time: 48.5 s 
2025-01-29 16:41:15.201381:  
2025-01-29 16:41:15.204850: Epoch 992 
2025-01-29 16:41:15.207893: Current learning rate: 0.00013 
2025-01-29 16:42:04.188125: train_loss -0.8574 
2025-01-29 16:42:04.194025: val_loss -0.8565 
2025-01-29 16:42:04.196586: Pseudo dice [np.float32(0.9713), np.float32(0.8962)] 
2025-01-29 16:42:04.199194: Epoch time: 48.99 s 
2025-01-29 16:42:05.394852:  
2025-01-29 16:42:05.398303: Epoch 993 
2025-01-29 16:42:05.401036: Current learning rate: 0.00011 
2025-01-29 16:42:54.716396: train_loss -0.8473 
2025-01-29 16:42:54.720982: val_loss -0.7919 
2025-01-29 16:42:54.723869: Pseudo dice [np.float32(0.9642), np.float32(0.892)] 
2025-01-29 16:42:54.726724: Epoch time: 49.33 s 
2025-01-29 16:42:55.876521:  
2025-01-29 16:42:55.879366: Epoch 994 
2025-01-29 16:42:55.881786: Current learning rate: 0.0001 
2025-01-29 16:43:44.415918: train_loss -0.8438 
2025-01-29 16:43:44.420937: val_loss -0.8167 
2025-01-29 16:43:44.423335: Pseudo dice [np.float32(0.9738), np.float32(0.9012)] 
2025-01-29 16:43:44.425696: Epoch time: 48.54 s 
2025-01-29 16:43:45.562810:  
2025-01-29 16:43:45.565339: Epoch 995 
2025-01-29 16:43:45.567660: Current learning rate: 8e-05 
2025-01-29 16:44:34.106363: train_loss -0.8441 
2025-01-29 16:44:34.110184: val_loss -0.8027 
2025-01-29 16:44:34.112966: Pseudo dice [np.float32(0.9707), np.float32(0.8972)] 
2025-01-29 16:44:34.115330: Epoch time: 48.54 s 
2025-01-29 16:44:35.255025:  
2025-01-29 16:44:35.257730: Epoch 996 
2025-01-29 16:44:35.260561: Current learning rate: 7e-05 
2025-01-29 16:45:24.006534: train_loss -0.8539 
2025-01-29 16:45:24.012008: val_loss -0.8225 
2025-01-29 16:45:24.014809: Pseudo dice [np.float32(0.9665), np.float32(0.8941)] 
2025-01-29 16:45:24.017611: Epoch time: 48.75 s 
2025-01-29 16:45:25.155897:  
2025-01-29 16:45:25.159060: Epoch 997 
2025-01-29 16:45:25.162003: Current learning rate: 5e-05 
2025-01-29 16:46:13.597278: train_loss -0.8464 
2025-01-29 16:46:13.601403: val_loss -0.8017 
2025-01-29 16:46:13.604070: Pseudo dice [np.float32(0.9722), np.float32(0.8933)] 
2025-01-29 16:46:13.606630: Epoch time: 48.44 s 
2025-01-29 16:46:14.747301:  
2025-01-29 16:46:14.750466: Epoch 998 
2025-01-29 16:46:14.753443: Current learning rate: 4e-05 
2025-01-29 16:47:03.460135: train_loss -0.8528 
2025-01-29 16:47:03.466300: val_loss -0.8157 
2025-01-29 16:47:03.475426: Pseudo dice [np.float32(0.9711), np.float32(0.9024)] 
2025-01-29 16:47:03.478318: Epoch time: 48.71 s 
2025-01-29 16:47:04.616030:  
2025-01-29 16:47:04.619145: Epoch 999 
2025-01-29 16:47:04.622314: Current learning rate: 2e-05 
2025-01-29 16:47:53.311793: train_loss -0.8548 
2025-01-29 16:47:53.316295: val_loss -0.8152 
2025-01-29 16:47:53.319235: Pseudo dice [np.float32(0.9737), np.float32(0.8993)] 
2025-01-29 16:47:53.322038: Epoch time: 48.7 s 
2025-01-29 16:47:53.324819: Yayy! New best EMA pseudo Dice: 0.932200014591217 
2025-01-29 16:47:55.618515: Training done. 
2025-01-29 16:47:55.848913: Using splits from existing split file: /srv/scratch/z5362216/kits19/nnUNet_db/nnUNet_preprocessed/Dataset001_Kits19/splits_final.json 
2025-01-29 16:47:55.852558: The split file contains 5 splits. 
2025-01-29 16:47:55.855204: Desired fold for training: 4 
2025-01-29 16:47:55.857754: This split has 80 training and 20 validation cases. 
2025-01-29 16:47:55.961921: predicting imaging_001 
2025-01-29 16:47:55.971242: imaging_001, shape torch.Size([1, 152, 206, 206]), rank 0 
2025-01-29 16:48:17.413433: predicting imaging_007 
2025-01-29 16:48:17.423651: imaging_007, shape torch.Size([1, 92, 242, 242]), rank 0 
2025-01-29 16:48:19.594574: predicting imaging_010 
2025-01-29 16:48:19.602846: imaging_010, shape torch.Size([1, 76, 195, 195]), rank 0 
2025-01-29 16:48:21.732680: predicting imaging_011 
2025-01-29 16:48:21.741003: imaging_011, shape torch.Size([1, 202, 181, 181]), rank 0 
2025-01-29 16:48:24.527512: predicting imaging_014 
2025-01-29 16:48:24.536947: imaging_014, shape torch.Size([1, 221, 219, 219]), rank 0 
2025-01-29 16:48:41.867812: predicting imaging_029 
2025-01-29 16:48:41.878521: imaging_029, shape torch.Size([1, 330, 194, 194]), rank 0 
2025-01-29 16:48:52.011846: predicting imaging_034 
2025-01-29 16:48:52.028659: imaging_034, shape torch.Size([1, 277, 216, 216]), rank 0 
2025-01-29 16:49:00.222469: predicting imaging_036 
2025-01-29 16:49:00.240364: imaging_036, shape torch.Size([1, 246, 206, 206]), rank 0 
2025-01-29 16:49:07.212473: predicting imaging_038 
2025-01-29 16:49:07.223653: imaging_038, shape torch.Size([1, 81, 201, 201]), rank 0 
2025-01-29 16:49:32.927308: predicting imaging_041 
2025-01-29 16:49:32.936260: imaging_041, shape torch.Size([1, 79, 175, 175]), rank 0 
2025-01-29 16:49:44.442339: predicting imaging_043 
2025-01-29 16:49:44.452603: imaging_043, shape torch.Size([1, 260, 190, 190]), rank 0 
2025-01-29 16:49:48.211529: predicting imaging_059 
2025-01-29 16:49:48.226658: imaging_059, shape torch.Size([1, 186, 204, 204]), rank 0 
