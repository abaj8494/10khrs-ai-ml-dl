
#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################
 
2025-01-28 07:48:50.667758: Using torch.compile... 
2025-01-28 07:48:55.191485: do_dummy_2d_data_aug: False 
2025-01-28 07:48:55.268646: Using splits from existing split file: /srv/scratch/z5362216/kits19/nnUNet_db/nnUNet_preprocessed/Dataset001_Kits19/splits_final.json 
2025-01-28 07:48:55.318454: The split file contains 5 splits. 
2025-01-28 07:48:55.322033: Desired fold for training: 3 
2025-01-28 07:48:55.325307: This split has 80 training and 20 validation cases. 

This is the configuration used by this training:
Configuration name: 3d_lowres
 {'data_identifier': 'nnUNetPlans_3d_lowres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 2, 'patch_size': [128, 128, 128], 'median_image_size_in_voxels': [200, 205, 205], 'spacing': [1.9849520718478983, 1.9849270710444444, 1.9849270710444444], 'normalization_schemes': ['CTNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': False, 'next_stage': '3d_cascade_fullres'} 
 
These are the global plan.json settings:
 {'dataset_name': 'Dataset001_Kits19', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [3.0, 0.7939453125, 0.7939453125], 'original_median_shape_after_transp': [104, 512, 512], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [2, 0, 1], 'transpose_backward': [1, 2, 0], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 2553.0, 'mean': 104.46720886230469, 'median': 104.0, 'min': -277.0, 'percentile_00_5': -73.0, 'percentile_99_5': 292.0, 'std': 74.68063354492188}}} 
 
2025-01-28 07:48:57.654281: unpacking dataset... 
2025-01-28 07:49:09.479941: unpacking done... 
2025-01-28 07:49:09.573087: 
printing the network instead:
 
2025-01-28 07:49:09.576877: OptimizedModule(
  (_orig_mod): PlainConvUNet(
    (encoder): PlainConvEncoder(
      (stages): Sequential(
        (0): Sequential(
          (0): StackedConvBlocks(
            (convs): Sequential(
              (0): ConvDropoutNormReLU(
                (conv): Conv3d(1, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(1, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (1): ConvDropoutNormReLU(
                (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
            )
          )
        )
        (1): Sequential(
          (0): StackedConvBlocks(
            (convs): Sequential(
              (0): ConvDropoutNormReLU(
                (conv): Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                  (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (1): ConvDropoutNormReLU(
                (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
            )
          )
        )
        (2): Sequential(
          (0): StackedConvBlocks(
            (convs): Sequential(
              (0): ConvDropoutNormReLU(
                (conv): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                  (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (1): ConvDropoutNormReLU(
                (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
            )
          )
        )
        (3): Sequential(
          (0): StackedConvBlocks(
            (convs): Sequential(
              (0): ConvDropoutNormReLU(
                (conv): Conv3d(128, 256, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                (norm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(128, 256, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                  (1): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (1): ConvDropoutNormReLU(
                (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
            )
          )
        )
        (4): Sequential(
          (0): StackedConvBlocks(
            (convs): Sequential(
              (0): ConvDropoutNormReLU(
                (conv): Conv3d(256, 320, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                (norm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(256, 320, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                  (1): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (1): ConvDropoutNormReLU(
                (conv): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
            )
          )
        )
        (5): Sequential(
          (0): StackedConvBlocks(
            (convs): Sequential(
              (0): ConvDropoutNormReLU(
                (conv): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                (norm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                  (1): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (1): ConvDropoutNormReLU(
                (conv): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
            )
          )
        )
      )
    )
    (decoder): UNetDecoder(
      (encoder): PlainConvEncoder(
        (stages): Sequential(
          (0): Sequential(
            (0): StackedConvBlocks(
              (convs): Sequential(
                (0): ConvDropoutNormReLU(
                  (conv): Conv3d(1, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                  (all_modules): Sequential(
                    (0): Conv3d(1, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                    (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                    (2): LeakyReLU(negative_slope=0.01, inplace=True)
                  )
                )
                (1): ConvDropoutNormReLU(
                  (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                  (all_modules): Sequential(
                    (0): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                    (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                    (2): LeakyReLU(negative_slope=0.01, inplace=True)
                  )
                )
              )
            )
          )
          (1): Sequential(
            (0): StackedConvBlocks(
              (convs): Sequential(
                (0): ConvDropoutNormReLU(
                  (conv): Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                  (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                  (all_modules): Sequential(
                    (0): Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                    (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                    (2): LeakyReLU(negative_slope=0.01, inplace=True)
                  )
                )
                (1): ConvDropoutNormReLU(
                  (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                  (all_modules): Sequential(
                    (0): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                    (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                    (2): LeakyReLU(negative_slope=0.01, inplace=True)
                  )
                )
              )
            )
          )
          (2): Sequential(
            (0): StackedConvBlocks(
              (convs): Sequential(
                (0): ConvDropoutNormReLU(
                  (conv): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                  (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                  (all_modules): Sequential(
                    (0): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                    (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                    (2): LeakyReLU(negative_slope=0.01, inplace=True)
                  )
                )
                (1): ConvDropoutNormReLU(
                  (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                  (all_modules): Sequential(
                    (0): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                    (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                    (2): LeakyReLU(negative_slope=0.01, inplace=True)
                  )
                )
              )
            )
          )
          (3): Sequential(
            (0): StackedConvBlocks(
              (convs): Sequential(
                (0): ConvDropoutNormReLU(
                  (conv): Conv3d(128, 256, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                  (norm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                  (all_modules): Sequential(
                    (0): Conv3d(128, 256, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                    (1): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                    (2): LeakyReLU(negative_slope=0.01, inplace=True)
                  )
                )
                (1): ConvDropoutNormReLU(
                  (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (norm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                  (all_modules): Sequential(
                    (0): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                    (1): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                    (2): LeakyReLU(negative_slope=0.01, inplace=True)
                  )
                )
              )
            )
          )
          (4): Sequential(
            (0): StackedConvBlocks(
              (convs): Sequential(
                (0): ConvDropoutNormReLU(
                  (conv): Conv3d(256, 320, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                  (norm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                  (all_modules): Sequential(
                    (0): Conv3d(256, 320, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                    (1): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                    (2): LeakyReLU(negative_slope=0.01, inplace=True)
                  )
                )
                (1): ConvDropoutNormReLU(
                  (conv): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (norm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                  (all_modules): Sequential(
                    (0): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                    (1): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                    (2): LeakyReLU(negative_slope=0.01, inplace=True)
                  )
                )
              )
            )
          )
          (5): Sequential(
            (0): StackedConvBlocks(
              (convs): Sequential(
                (0): ConvDropoutNormReLU(
                  (conv): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                  (norm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                  (all_modules): Sequential(
                    (0): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                    (1): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                    (2): LeakyReLU(negative_slope=0.01, inplace=True)
                  )
                )
                (1): ConvDropoutNormReLU(
                  (conv): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (norm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                  (all_modules): Sequential(
                    (0): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                    (1): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                    (2): LeakyReLU(negative_slope=0.01, inplace=True)
                  )
                )
              )
            )
          )
        )
      )
      (stages): ModuleList(
        (0): StackedConvBlocks(
          (convs): Sequential(
            (0): ConvDropoutNormReLU(
              (conv): Conv3d(640, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
              (norm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(640, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (1): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (1): ConvDropoutNormReLU(
              (conv): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
              (norm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (1): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
          )
        )
        (1): StackedConvBlocks(
          (convs): Sequential(
            (0): ConvDropoutNormReLU(
              (conv): Conv3d(512, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
              (norm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(512, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (1): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (1): ConvDropoutNormReLU(
              (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
              (norm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (1): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
          )
        )
        (2): StackedConvBlocks(
          (convs): Sequential(
            (0): ConvDropoutNormReLU(
              (conv): Conv3d(256, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
              (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(256, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (1): ConvDropoutNormReLU(
              (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
              (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
          )
        )
        (3): StackedConvBlocks(
          (convs): Sequential(
            (0): ConvDropoutNormReLU(
              (conv): Conv3d(128, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
              (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(128, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (1): ConvDropoutNormReLU(
              (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
              (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
          )
        )
        (4): StackedConvBlocks(
          (convs): Sequential(
            (0): ConvDropoutNormReLU(
              (conv): Conv3d(64, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
              (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(64, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (1): ConvDropoutNormReLU(
              (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
              (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
          )
        )
      )
      (transpconvs): ModuleList(
        (0): ConvTranspose3d(320, 320, kernel_size=(2, 2, 2), stride=(2, 2, 2))
        (1): ConvTranspose3d(320, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2))
        (2): ConvTranspose3d(256, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2))
        (3): ConvTranspose3d(128, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2))
        (4): ConvTranspose3d(64, 32, kernel_size=(2, 2, 2), stride=(2, 2, 2))
      )
      (seg_layers): ModuleList(
        (0): Conv3d(320, 3, kernel_size=(1, 1, 1), stride=(1, 1, 1))
        (1): Conv3d(256, 3, kernel_size=(1, 1, 1), stride=(1, 1, 1))
        (2): Conv3d(128, 3, kernel_size=(1, 1, 1), stride=(1, 1, 1))
        (3): Conv3d(64, 3, kernel_size=(1, 1, 1), stride=(1, 1, 1))
        (4): Conv3d(32, 3, kernel_size=(1, 1, 1), stride=(1, 1, 1))
      )
    )
  )
) 
2025-01-28 07:49:09.584588: 
 
2025-01-28 07:49:09.587199: Unable to plot network architecture: nnUNet_compile is enabled! 
2025-01-28 07:49:09.693925:  
2025-01-28 07:49:09.696622: Epoch 650 
2025-01-28 07:49:09.699246: Current learning rate: 0.00389 
2025-01-28 07:51:18.575645: train_loss -0.8295 
2025-01-28 07:51:18.595462: val_loss -0.7828 
2025-01-28 07:51:18.602315: Pseudo dice [np.float32(0.9484), np.float32(0.9178)] 
2025-01-28 07:51:18.604960: Epoch time: 128.88 s 
2025-01-28 07:51:20.620063:  
2025-01-28 07:51:20.622721: Epoch 651 
2025-01-28 07:51:20.625161: Current learning rate: 0.00388 
2025-01-28 07:52:09.218357: train_loss -0.8203 
2025-01-28 07:52:09.222846: val_loss -0.719 
2025-01-28 07:52:09.225688: Pseudo dice [np.float32(0.9541), np.float32(0.9028)] 
2025-01-28 07:52:09.228627: Epoch time: 48.6 s 
2025-01-28 07:52:10.404356:  
2025-01-28 07:52:10.407566: Epoch 652 
2025-01-28 07:52:10.411022: Current learning rate: 0.00387 
2025-01-28 07:52:58.440463: train_loss -0.8026 
2025-01-28 07:52:58.446230: val_loss -0.7294 
2025-01-28 07:52:58.448833: Pseudo dice [np.float32(0.9478), np.float32(0.9294)] 
2025-01-28 07:52:58.451436: Epoch time: 48.04 s 
2025-01-28 07:52:59.600258:  
2025-01-28 07:52:59.603110: Epoch 653 
2025-01-28 07:52:59.606134: Current learning rate: 0.00386 
2025-01-28 07:53:47.887595: train_loss -0.8085 
2025-01-28 07:53:47.891478: val_loss -0.7274 
2025-01-28 07:53:47.894681: Pseudo dice [np.float32(0.9435), np.float32(0.9086)] 
2025-01-28 07:53:47.898004: Epoch time: 48.29 s 
2025-01-28 07:53:49.049949:  
2025-01-28 07:53:49.053300: Epoch 654 
2025-01-28 07:53:49.056558: Current learning rate: 0.00385 
2025-01-28 07:54:37.075680: train_loss -0.8223 
2025-01-28 07:54:37.081861: val_loss -0.7685 
2025-01-28 07:54:37.084614: Pseudo dice [np.float32(0.941), np.float32(0.9168)] 
2025-01-28 07:54:37.087424: Epoch time: 48.03 s 
2025-01-28 07:54:38.254426:  
2025-01-28 07:54:38.257594: Epoch 655 
2025-01-28 07:54:38.260346: Current learning rate: 0.00384 
2025-01-28 07:55:26.317312: train_loss -0.8103 
2025-01-28 07:55:26.321210: val_loss -0.7404 
2025-01-28 07:55:26.324281: Pseudo dice [np.float32(0.936), np.float32(0.9087)] 
2025-01-28 07:55:26.326952: Epoch time: 48.06 s 
2025-01-28 07:55:27.447002:  
2025-01-28 07:55:27.450366: Epoch 656 
2025-01-28 07:55:27.453408: Current learning rate: 0.00383 
2025-01-28 07:56:15.451481: train_loss -0.8412 
2025-01-28 07:56:15.457659: val_loss -0.7569 
2025-01-28 07:56:15.460268: Pseudo dice [np.float32(0.9588), np.float32(0.9114)] 
2025-01-28 07:56:15.462862: Epoch time: 48.01 s 
2025-01-28 07:56:16.634922:  
2025-01-28 07:56:16.638044: Epoch 657 
2025-01-28 07:56:16.641067: Current learning rate: 0.00382 
2025-01-28 07:57:05.019780: train_loss -0.8243 
2025-01-28 07:57:05.023661: val_loss -0.7264 
2025-01-28 07:57:05.026735: Pseudo dice [np.float32(0.9526), np.float32(0.9147)] 
2025-01-28 07:57:05.029579: Epoch time: 48.39 s 
2025-01-28 07:57:06.192296:  
2025-01-28 07:57:06.195642: Epoch 658 
2025-01-28 07:57:06.198783: Current learning rate: 0.00381 
2025-01-28 07:57:53.979483: train_loss -0.8324 
2025-01-28 07:57:53.987811: val_loss -0.7332 
2025-01-28 07:57:53.990855: Pseudo dice [np.float32(0.9521), np.float32(0.9167)] 
2025-01-28 07:57:53.993706: Epoch time: 47.79 s 
2025-01-28 07:57:55.175587:  
2025-01-28 07:57:55.178677: Epoch 659 
2025-01-28 07:57:55.181571: Current learning rate: 0.0038 
2025-01-28 07:58:42.679297: train_loss -0.8237 
2025-01-28 07:58:42.682953: val_loss -0.7159 
2025-01-28 07:58:42.685737: Pseudo dice [np.float32(0.9448), np.float32(0.9014)] 
2025-01-28 07:58:42.688576: Epoch time: 47.5 s 
2025-01-28 07:58:43.825578:  
2025-01-28 07:58:43.828577: Epoch 660 
2025-01-28 07:58:43.831221: Current learning rate: 0.00379 
2025-01-28 07:59:31.647212: train_loss -0.8305 
2025-01-28 07:59:31.653603: val_loss -0.6958 
2025-01-28 07:59:31.656553: Pseudo dice [np.float32(0.9619), np.float32(0.9386)] 
2025-01-28 07:59:31.659050: Epoch time: 47.82 s 
2025-01-28 07:59:32.835572:  
2025-01-28 07:59:32.839342: Epoch 661 
2025-01-28 07:59:32.842440: Current learning rate: 0.00378 
2025-01-28 08:00:20.896155: train_loss -0.847 
2025-01-28 08:00:20.899583: val_loss -0.7301 
2025-01-28 08:00:20.902537: Pseudo dice [np.float32(0.9544), np.float32(0.9347)] 
2025-01-28 08:00:20.905137: Epoch time: 48.06 s 
2025-01-28 08:00:22.062276:  
2025-01-28 08:00:22.065631: Epoch 662 
2025-01-28 08:00:22.068518: Current learning rate: 0.00377 
2025-01-28 08:01:10.347951: train_loss -0.8349 
2025-01-28 08:01:10.354971: val_loss -0.6982 
2025-01-28 08:01:10.357826: Pseudo dice [np.float32(0.9522), np.float32(0.8379)] 
2025-01-28 08:01:10.360351: Epoch time: 48.29 s 
2025-01-28 08:01:11.511798:  
2025-01-28 08:01:11.514662: Epoch 663 
2025-01-28 08:01:11.517789: Current learning rate: 0.00376 
2025-01-28 08:01:59.269494: train_loss -0.822 
2025-01-28 08:01:59.272731: val_loss -0.7642 
2025-01-28 08:01:59.275512: Pseudo dice [np.float32(0.956), np.float32(0.937)] 
2025-01-28 08:01:59.278251: Epoch time: 47.76 s 
2025-01-28 08:02:00.413499:  
2025-01-28 08:02:00.416522: Epoch 664 
2025-01-28 08:02:00.419186: Current learning rate: 0.00375 
2025-01-28 08:02:48.427226: train_loss -0.84 
2025-01-28 08:02:48.433744: val_loss -0.7427 
2025-01-28 08:02:48.436222: Pseudo dice [np.float32(0.9462), np.float32(0.9083)] 
2025-01-28 08:02:48.438766: Epoch time: 48.01 s 
2025-01-28 08:02:49.576089:  
2025-01-28 08:02:49.579055: Epoch 665 
2025-01-28 08:02:49.582221: Current learning rate: 0.00374 
2025-01-28 08:03:37.416550: train_loss -0.826 
2025-01-28 08:03:37.420292: val_loss -0.7508 
2025-01-28 08:03:37.423291: Pseudo dice [np.float32(0.9593), np.float32(0.9435)] 
2025-01-28 08:03:37.426090: Epoch time: 47.84 s 
2025-01-28 08:03:38.553863:  
2025-01-28 08:03:38.556996: Epoch 666 
2025-01-28 08:03:38.559819: Current learning rate: 0.00373 
2025-01-28 08:04:26.447558: train_loss -0.8322 
2025-01-28 08:04:26.454490: val_loss -0.7733 
2025-01-28 08:04:26.457581: Pseudo dice [np.float32(0.9528), np.float32(0.9292)] 
2025-01-28 08:04:26.460459: Epoch time: 47.89 s 
2025-01-28 08:04:27.586231:  
2025-01-28 08:04:27.589414: Epoch 667 
2025-01-28 08:04:27.592471: Current learning rate: 0.00372 
2025-01-28 08:05:15.455201: train_loss -0.8246 
2025-01-28 08:05:15.458544: val_loss -0.7211 
2025-01-28 08:05:15.461381: Pseudo dice [np.float32(0.9538), np.float32(0.9024)] 
2025-01-28 08:05:15.464177: Epoch time: 47.87 s 
2025-01-28 08:05:16.609741:  
2025-01-28 08:05:16.614511: Epoch 668 
2025-01-28 08:05:16.617169: Current learning rate: 0.00371 
2025-01-28 08:06:04.974754: train_loss -0.823 
2025-01-28 08:06:04.980983: val_loss -0.7325 
2025-01-28 08:06:04.983707: Pseudo dice [np.float32(0.954), np.float32(0.9248)] 
2025-01-28 08:06:04.986599: Epoch time: 48.37 s 
2025-01-28 08:06:06.695657:  
2025-01-28 08:06:06.698899: Epoch 669 
2025-01-28 08:06:06.701808: Current learning rate: 0.0037 
2025-01-28 08:06:54.508697: train_loss -0.835 
2025-01-28 08:06:54.512377: val_loss -0.7326 
2025-01-28 08:06:54.515132: Pseudo dice [np.float32(0.9534), np.float32(0.8344)] 
2025-01-28 08:06:54.517765: Epoch time: 47.81 s 
2025-01-28 08:06:55.670680:  
2025-01-28 08:06:55.673404: Epoch 670 
2025-01-28 08:06:55.676137: Current learning rate: 0.00369 
2025-01-28 08:07:43.508748: train_loss -0.8372 
2025-01-28 08:07:43.514330: val_loss -0.7361 
2025-01-28 08:07:43.516639: Pseudo dice [np.float32(0.9564), np.float32(0.894)] 
2025-01-28 08:07:43.518855: Epoch time: 47.84 s 
2025-01-28 08:07:44.683205:  
2025-01-28 08:07:44.686270: Epoch 671 
2025-01-28 08:07:44.689038: Current learning rate: 0.00368 
2025-01-28 08:08:33.141337: train_loss -0.8352 
2025-01-28 08:08:33.145308: val_loss -0.772 
2025-01-28 08:08:33.148033: Pseudo dice [np.float32(0.9501), np.float32(0.8945)] 
2025-01-28 08:08:33.150567: Epoch time: 48.46 s 
2025-01-28 08:08:34.297672:  
2025-01-28 08:08:34.300678: Epoch 672 
2025-01-28 08:08:34.303586: Current learning rate: 0.00367 
2025-01-28 08:09:22.405580: train_loss -0.8301 
2025-01-28 08:09:22.412250: val_loss -0.7162 
2025-01-28 08:09:22.414958: Pseudo dice [np.float32(0.94), np.float32(0.8281)] 
2025-01-28 08:09:22.417495: Epoch time: 48.11 s 
2025-01-28 08:09:23.582526:  
2025-01-28 08:09:23.585654: Epoch 673 
2025-01-28 08:09:23.588843: Current learning rate: 0.00366 
2025-01-28 08:10:11.596123: train_loss -0.7919 
2025-01-28 08:10:11.600037: val_loss -0.6789 
2025-01-28 08:10:11.603164: Pseudo dice [np.float32(0.9304), np.float32(0.8375)] 
2025-01-28 08:10:11.605577: Epoch time: 48.02 s 
2025-01-28 08:10:12.731626:  
2025-01-28 08:10:12.734641: Epoch 674 
2025-01-28 08:10:12.737381: Current learning rate: 0.00365 
2025-01-28 08:11:00.797922: train_loss -0.8027 
2025-01-28 08:11:00.803680: val_loss -0.7103 
2025-01-28 08:11:00.806274: Pseudo dice [np.float32(0.9532), np.float32(0.8324)] 
2025-01-28 08:11:00.808790: Epoch time: 48.07 s 
2025-01-28 08:11:01.983332:  
2025-01-28 08:11:01.986669: Epoch 675 
2025-01-28 08:11:01.989606: Current learning rate: 0.00364 
2025-01-28 08:11:50.143555: train_loss -0.8062 
2025-01-28 08:11:50.147007: val_loss -0.8018 
2025-01-28 08:11:50.150229: Pseudo dice [np.float32(0.9515), np.float32(0.9201)] 
2025-01-28 08:11:50.152972: Epoch time: 48.16 s 
2025-01-28 08:11:51.310595:  
2025-01-28 08:11:51.313595: Epoch 676 
2025-01-28 08:11:51.316501: Current learning rate: 0.00363 
2025-01-28 08:12:39.592795: train_loss -0.8275 
2025-01-28 08:12:39.600885: val_loss -0.7231 
2025-01-28 08:12:39.603573: Pseudo dice [np.float32(0.9547), np.float32(0.8181)] 
2025-01-28 08:12:39.606241: Epoch time: 48.28 s 
2025-01-28 08:12:40.768365:  
2025-01-28 08:12:40.771191: Epoch 677 
2025-01-28 08:12:40.774222: Current learning rate: 0.00362 
2025-01-28 08:13:28.138590: train_loss -0.8317 
2025-01-28 08:13:28.141995: val_loss -0.7172 
2025-01-28 08:13:28.144861: Pseudo dice [np.float32(0.9396), np.float32(0.8639)] 
2025-01-28 08:13:28.147757: Epoch time: 47.37 s 
2025-01-28 08:13:29.304055:  
2025-01-28 08:13:29.307293: Epoch 678 
2025-01-28 08:13:29.310179: Current learning rate: 0.00361 
2025-01-28 08:14:17.089119: train_loss -0.8312 
2025-01-28 08:14:17.097307: val_loss -0.7519 
2025-01-28 08:14:17.100520: Pseudo dice [np.float32(0.9366), np.float32(0.9201)] 
2025-01-28 08:14:17.102957: Epoch time: 47.79 s 
2025-01-28 08:14:18.256200:  
2025-01-28 08:14:18.259117: Epoch 679 
2025-01-28 08:14:18.261662: Current learning rate: 0.0036 
2025-01-28 08:15:06.398874: train_loss -0.818 
2025-01-28 08:15:06.403686: val_loss -0.7419 
2025-01-28 08:15:06.406674: Pseudo dice [np.float32(0.9568), np.float32(0.9148)] 
2025-01-28 08:15:06.409446: Epoch time: 48.14 s 
2025-01-28 08:15:07.567201:  
2025-01-28 08:15:07.570112: Epoch 680 
2025-01-28 08:15:07.573034: Current learning rate: 0.00359 
2025-01-28 08:15:55.794668: train_loss -0.8123 
2025-01-28 08:15:55.800713: val_loss -0.7003 
2025-01-28 08:15:55.803827: Pseudo dice [np.float32(0.9217), np.float32(0.9262)] 
2025-01-28 08:15:55.806841: Epoch time: 48.23 s 
2025-01-28 08:15:56.960055:  
2025-01-28 08:15:56.963485: Epoch 681 
2025-01-28 08:15:56.967547: Current learning rate: 0.00358 
2025-01-28 08:16:44.935032: train_loss -0.814 
2025-01-28 08:16:44.938539: val_loss -0.6933 
2025-01-28 08:16:44.941130: Pseudo dice [np.float32(0.9441), np.float32(0.8524)] 
2025-01-28 08:16:44.943701: Epoch time: 47.98 s 
2025-01-28 08:16:46.076878:  
2025-01-28 08:16:46.079971: Epoch 682 
2025-01-28 08:16:46.082900: Current learning rate: 0.00357 
2025-01-28 08:17:34.150215: train_loss -0.8144 
2025-01-28 08:17:34.156652: val_loss -0.728 
2025-01-28 08:17:34.159553: Pseudo dice [np.float32(0.9495), np.float32(0.9113)] 
2025-01-28 08:17:34.162062: Epoch time: 48.07 s 
2025-01-28 08:17:35.341808:  
2025-01-28 08:17:35.345557: Epoch 683 
2025-01-28 08:17:35.348469: Current learning rate: 0.00356 
2025-01-28 08:18:23.725333: train_loss -0.8212 
2025-01-28 08:18:23.729072: val_loss -0.7183 
2025-01-28 08:18:23.732001: Pseudo dice [np.float32(0.9455), np.float32(0.8549)] 
2025-01-28 08:18:23.734700: Epoch time: 48.38 s 
2025-01-28 08:18:24.906556:  
2025-01-28 08:18:24.909492: Epoch 684 
2025-01-28 08:18:24.912483: Current learning rate: 0.00355 
2025-01-28 08:19:13.257688: train_loss -0.811 
2025-01-28 08:19:13.263926: val_loss -0.7714 
2025-01-28 08:19:13.266871: Pseudo dice [np.float32(0.9486), np.float32(0.9253)] 
2025-01-28 08:19:13.269523: Epoch time: 48.35 s 
2025-01-28 08:19:14.442975:  
2025-01-28 08:19:14.445972: Epoch 685 
2025-01-28 08:19:14.448749: Current learning rate: 0.00354 
2025-01-28 08:20:02.592273: train_loss -0.8218 
2025-01-28 08:20:02.596141: val_loss -0.723 
2025-01-28 08:20:02.599070: Pseudo dice [np.float32(0.9552), np.float32(0.9132)] 
2025-01-28 08:20:02.602127: Epoch time: 48.15 s 
2025-01-28 08:20:03.764336:  
2025-01-28 08:20:03.767383: Epoch 686 
2025-01-28 08:20:03.770303: Current learning rate: 0.00353 
2025-01-28 08:20:51.866479: train_loss -0.8318 
2025-01-28 08:20:51.873022: val_loss -0.7821 
2025-01-28 08:20:51.875878: Pseudo dice [np.float32(0.9571), np.float32(0.9104)] 
2025-01-28 08:20:51.878497: Epoch time: 48.1 s 
2025-01-28 08:20:53.036649:  
2025-01-28 08:20:53.039469: Epoch 687 
2025-01-28 08:20:53.042410: Current learning rate: 0.00352 
2025-01-28 08:21:41.074729: train_loss -0.8374 
2025-01-28 08:21:41.079817: val_loss -0.7854 
2025-01-28 08:21:41.082316: Pseudo dice [np.float32(0.9527), np.float32(0.9429)] 
2025-01-28 08:21:41.085143: Epoch time: 48.04 s 
2025-01-28 08:21:42.786096:  
2025-01-28 08:21:42.789197: Epoch 688 
2025-01-28 08:21:42.791982: Current learning rate: 0.00351 
2025-01-28 08:22:31.042426: train_loss -0.8374 
2025-01-28 08:22:31.050299: val_loss -0.717 
2025-01-28 08:22:31.053015: Pseudo dice [np.float32(0.955), np.float32(0.9242)] 
2025-01-28 08:22:31.055623: Epoch time: 48.26 s 
2025-01-28 08:22:32.226403:  
2025-01-28 08:22:32.229335: Epoch 689 
2025-01-28 08:22:32.232170: Current learning rate: 0.0035 
2025-01-28 08:23:20.461744: train_loss -0.8397 
2025-01-28 08:23:20.466684: val_loss -0.7217 
2025-01-28 08:23:20.469072: Pseudo dice [np.float32(0.9551), np.float32(0.9133)] 
2025-01-28 08:23:20.471977: Epoch time: 48.24 s 
2025-01-28 08:23:21.637390:  
2025-01-28 08:23:21.640192: Epoch 690 
2025-01-28 08:23:21.642813: Current learning rate: 0.00349 
2025-01-28 08:24:09.868144: train_loss -0.826 
2025-01-28 08:24:09.875831: val_loss -0.7601 
2025-01-28 08:24:09.878376: Pseudo dice [np.float32(0.9484), np.float32(0.9213)] 
2025-01-28 08:24:09.880815: Epoch time: 48.23 s 
2025-01-28 08:24:11.050162:  
2025-01-28 08:24:11.052744: Epoch 691 
2025-01-28 08:24:11.055623: Current learning rate: 0.00348 
2025-01-28 08:24:59.686315: train_loss -0.8434 
2025-01-28 08:24:59.689787: val_loss -0.774 
2025-01-28 08:24:59.692711: Pseudo dice [np.float32(0.9507), np.float32(0.9341)] 
2025-01-28 08:24:59.695477: Epoch time: 48.64 s 
2025-01-28 08:25:00.860571:  
2025-01-28 08:25:00.863787: Epoch 692 
2025-01-28 08:25:00.866808: Current learning rate: 0.00346 
2025-01-28 08:25:48.908101: train_loss -0.8395 
2025-01-28 08:25:48.914143: val_loss -0.7645 
2025-01-28 08:25:48.916833: Pseudo dice [np.float32(0.9514), np.float32(0.9225)] 
2025-01-28 08:25:48.919540: Epoch time: 48.05 s 
2025-01-28 08:25:50.100135:  
2025-01-28 08:25:50.103321: Epoch 693 
2025-01-28 08:25:50.106293: Current learning rate: 0.00345 
2025-01-28 08:26:38.209003: train_loss -0.8372 
2025-01-28 08:26:38.212568: val_loss -0.7582 
2025-01-28 08:26:38.215216: Pseudo dice [np.float32(0.9459), np.float32(0.9252)] 
2025-01-28 08:26:38.217926: Epoch time: 48.11 s 
2025-01-28 08:26:39.389339:  
2025-01-28 08:26:39.392259: Epoch 694 
2025-01-28 08:26:39.395093: Current learning rate: 0.00344 
2025-01-28 08:27:27.848442: train_loss -0.8364 
2025-01-28 08:27:27.854726: val_loss -0.7434 
2025-01-28 08:27:27.857336: Pseudo dice [np.float32(0.9509), np.float32(0.9346)] 
2025-01-28 08:27:27.859642: Epoch time: 48.46 s 
2025-01-28 08:27:29.026398:  
2025-01-28 08:27:29.029541: Epoch 695 
2025-01-28 08:27:29.032243: Current learning rate: 0.00343 
2025-01-28 08:28:17.006696: train_loss -0.8174 
2025-01-28 08:28:17.010847: val_loss -0.7403 
2025-01-28 08:28:17.013562: Pseudo dice [np.float32(0.9487), np.float32(0.9273)] 
2025-01-28 08:28:17.016330: Epoch time: 47.99 s 
2025-01-28 08:28:18.178826:  
2025-01-28 08:28:18.184156: Epoch 696 
2025-01-28 08:28:18.187219: Current learning rate: 0.00342 
2025-01-28 08:29:06.449754: train_loss -0.8476 
2025-01-28 08:29:06.456132: val_loss -0.7738 
2025-01-28 08:29:06.458920: Pseudo dice [np.float32(0.9566), np.float32(0.9486)] 
2025-01-28 08:29:06.461813: Epoch time: 48.27 s 
2025-01-28 08:29:07.631643:  
2025-01-28 08:29:07.634899: Epoch 697 
2025-01-28 08:29:07.637910: Current learning rate: 0.00341 
2025-01-28 08:29:55.705647: train_loss -0.8263 
2025-01-28 08:29:55.709737: val_loss -0.7261 
2025-01-28 08:29:55.712614: Pseudo dice [np.float32(0.9544), np.float32(0.9366)] 
2025-01-28 08:29:55.715486: Epoch time: 48.07 s 
2025-01-28 08:29:56.898813:  
2025-01-28 08:29:56.901550: Epoch 698 
2025-01-28 08:29:56.904296: Current learning rate: 0.0034 
2025-01-28 08:30:45.014706: train_loss -0.8332 
2025-01-28 08:30:45.021226: val_loss -0.761 
2025-01-28 08:30:45.024106: Pseudo dice [np.float32(0.955), np.float32(0.9113)] 
2025-01-28 08:30:45.026853: Epoch time: 48.12 s 
2025-01-28 08:30:46.190041:  
2025-01-28 08:30:46.193151: Epoch 699 
2025-01-28 08:30:46.195948: Current learning rate: 0.00339 
2025-01-28 08:31:34.344403: train_loss -0.841 
2025-01-28 08:31:34.347976: val_loss -0.7212 
2025-01-28 08:31:34.350958: Pseudo dice [np.float32(0.9486), np.float32(0.9067)] 
2025-01-28 08:31:34.354008: Epoch time: 48.16 s 
2025-01-28 08:31:36.034113:  
2025-01-28 08:31:36.037211: Epoch 700 
2025-01-28 08:31:36.039936: Current learning rate: 0.00338 
2025-01-28 08:32:24.202619: train_loss -0.821 
2025-01-28 08:32:24.208854: val_loss -0.7103 
2025-01-28 08:32:24.211741: Pseudo dice [np.float32(0.9526), np.float32(0.9171)] 
2025-01-28 08:32:24.214326: Epoch time: 48.17 s 
2025-01-28 08:32:25.376440:  
2025-01-28 08:32:25.380484: Epoch 701 
2025-01-28 08:32:25.383458: Current learning rate: 0.00337 
2025-01-28 08:33:13.678548: train_loss -0.8197 
2025-01-28 08:33:13.682209: val_loss -0.722 
2025-01-28 08:33:13.684936: Pseudo dice [np.float32(0.9353), np.float32(0.8391)] 
2025-01-28 08:33:13.687566: Epoch time: 48.3 s 
2025-01-28 08:33:14.846270:  
2025-01-28 08:33:14.849313: Epoch 702 
2025-01-28 08:33:14.852250: Current learning rate: 0.00336 
2025-01-28 08:34:02.492009: train_loss -0.8158 
2025-01-28 08:34:02.498504: val_loss -0.7187 
2025-01-28 08:34:02.501266: Pseudo dice [np.float32(0.947), np.float32(0.911)] 
2025-01-28 08:34:02.503882: Epoch time: 47.65 s 
2025-01-28 08:34:03.667757:  
2025-01-28 08:34:03.671043: Epoch 703 
2025-01-28 08:34:03.674069: Current learning rate: 0.00335 
2025-01-28 08:34:52.018040: train_loss -0.8142 
2025-01-28 08:34:52.021746: val_loss -0.7466 
2025-01-28 08:34:52.024899: Pseudo dice [np.float32(0.9435), np.float32(0.9196)] 
2025-01-28 08:34:52.027909: Epoch time: 48.35 s 
2025-01-28 08:34:53.189084:  
2025-01-28 08:34:53.191854: Epoch 704 
2025-01-28 08:34:53.194876: Current learning rate: 0.00334 
2025-01-28 08:35:41.388114: train_loss -0.8263 
2025-01-28 08:35:41.395079: val_loss -0.7652 
2025-01-28 08:35:41.397723: Pseudo dice [np.float32(0.9504), np.float32(0.9317)] 
2025-01-28 08:35:41.400294: Epoch time: 48.2 s 
2025-01-28 08:35:42.570837:  
2025-01-28 08:35:42.573902: Epoch 705 
2025-01-28 08:35:42.576908: Current learning rate: 0.00333 
2025-01-28 08:36:30.988787: train_loss -0.8212 
2025-01-28 08:36:30.992660: val_loss -0.6911 
2025-01-28 08:36:30.995187: Pseudo dice [np.float32(0.9528), np.float32(0.8922)] 
2025-01-28 08:36:30.997896: Epoch time: 48.42 s 
2025-01-28 08:36:32.737062:  
2025-01-28 08:36:32.740388: Epoch 706 
2025-01-28 08:36:32.743563: Current learning rate: 0.00332 
2025-01-28 08:37:20.681601: train_loss -0.8273 
2025-01-28 08:37:20.687935: val_loss -0.752 
2025-01-28 08:37:20.690628: Pseudo dice [np.float32(0.9538), np.float32(0.9327)] 
2025-01-28 08:37:20.693133: Epoch time: 47.95 s 
2025-01-28 08:37:21.860888:  
2025-01-28 08:37:21.863994: Epoch 707 
2025-01-28 08:37:21.866831: Current learning rate: 0.00331 
2025-01-28 08:38:09.880295: train_loss -0.8255 
2025-01-28 08:38:09.883889: val_loss -0.7538 
2025-01-28 08:38:09.886607: Pseudo dice [np.float32(0.9508), np.float32(0.9324)] 
2025-01-28 08:38:09.889081: Epoch time: 48.02 s 
2025-01-28 08:38:11.057205:  
2025-01-28 08:38:11.060344: Epoch 708 
2025-01-28 08:38:11.063197: Current learning rate: 0.0033 
2025-01-28 08:38:59.068466: train_loss -0.8252 
2025-01-28 08:38:59.075032: val_loss -0.7885 
2025-01-28 08:38:59.077929: Pseudo dice [np.float32(0.9564), np.float32(0.9212)] 
2025-01-28 08:38:59.080869: Epoch time: 48.01 s 
2025-01-28 08:39:00.251253:  
2025-01-28 08:39:00.254203: Epoch 709 
2025-01-28 08:39:00.256980: Current learning rate: 0.00329 
2025-01-28 08:39:48.040182: train_loss -0.8442 
2025-01-28 08:39:48.043743: val_loss -0.7389 
2025-01-28 08:39:48.046550: Pseudo dice [np.float32(0.9454), np.float32(0.926)] 
2025-01-28 08:39:48.049198: Epoch time: 47.79 s 
2025-01-28 08:39:49.217158:  
2025-01-28 08:39:49.220211: Epoch 710 
2025-01-28 08:39:49.222790: Current learning rate: 0.00328 
2025-01-28 08:40:37.606392: train_loss -0.8232 
2025-01-28 08:40:37.613695: val_loss -0.7774 
2025-01-28 08:40:37.616467: Pseudo dice [np.float32(0.9568), np.float32(0.9413)] 
2025-01-28 08:40:37.619008: Epoch time: 48.39 s 
2025-01-28 08:40:38.787466:  
2025-01-28 08:40:38.790212: Epoch 711 
2025-01-28 08:40:38.792773: Current learning rate: 0.00327 
2025-01-28 08:41:26.781114: train_loss -0.8239 
2025-01-28 08:41:26.784337: val_loss -0.7223 
2025-01-28 08:41:26.786956: Pseudo dice [np.float32(0.9555), np.float32(0.942)] 
2025-01-28 08:41:26.789421: Epoch time: 47.99 s 
2025-01-28 08:41:26.792048: Yayy! New best EMA pseudo Dice: 0.9362999796867371 
2025-01-28 08:41:28.460171:  
2025-01-28 08:41:28.463447: Epoch 712 
2025-01-28 08:41:28.466594: Current learning rate: 0.00326 
2025-01-28 08:42:16.866750: train_loss -0.8283 
2025-01-28 08:42:16.872218: val_loss -0.7539 
2025-01-28 08:42:16.874738: Pseudo dice [np.float32(0.9559), np.float32(0.9378)] 
2025-01-28 08:42:16.877224: Epoch time: 48.41 s 
2025-01-28 08:42:16.879903: Yayy! New best EMA pseudo Dice: 0.9373000264167786 
2025-01-28 08:42:18.641466:  
2025-01-28 08:42:18.644650: Epoch 713 
2025-01-28 08:42:18.647542: Current learning rate: 0.00325 
2025-01-28 08:43:06.780571: train_loss -0.829 
2025-01-28 08:43:06.784878: val_loss -0.7318 
2025-01-28 08:43:06.787601: Pseudo dice [np.float32(0.9531), np.float32(0.9393)] 
2025-01-28 08:43:06.790200: Epoch time: 48.14 s 
2025-01-28 08:43:06.792743: Yayy! New best EMA pseudo Dice: 0.9381999969482422 
2025-01-28 08:43:08.549477:  
2025-01-28 08:43:08.552100: Epoch 714 
2025-01-28 08:43:08.555230: Current learning rate: 0.00324 
2025-01-28 08:43:56.492908: train_loss -0.8228 
2025-01-28 08:43:56.499199: val_loss -0.7406 
2025-01-28 08:43:56.501850: Pseudo dice [np.float32(0.9411), np.float32(0.9303)] 
2025-01-28 08:43:56.504454: Epoch time: 47.94 s 
2025-01-28 08:43:57.668491:  
2025-01-28 08:43:57.671396: Epoch 715 
2025-01-28 08:43:57.674190: Current learning rate: 0.00323 
2025-01-28 08:44:46.138803: train_loss -0.8361 
2025-01-28 08:44:46.143106: val_loss -0.7934 
2025-01-28 08:44:46.146111: Pseudo dice [np.float32(0.9466), np.float32(0.8768)] 
2025-01-28 08:44:46.148693: Epoch time: 48.47 s 
2025-01-28 08:44:47.324517:  
2025-01-28 08:44:47.328050: Epoch 716 
2025-01-28 08:44:47.331154: Current learning rate: 0.00322 
2025-01-28 08:45:35.478725: train_loss -0.8204 
2025-01-28 08:45:35.484738: val_loss -0.7684 
2025-01-28 08:45:35.487720: Pseudo dice [np.float32(0.9461), np.float32(0.9245)] 
2025-01-28 08:45:35.490378: Epoch time: 48.16 s 
2025-01-28 08:45:36.658928:  
2025-01-28 08:45:36.662455: Epoch 717 
2025-01-28 08:45:36.665443: Current learning rate: 0.00321 
2025-01-28 08:46:24.512096: train_loss -0.833 
2025-01-28 08:46:24.516167: val_loss -0.7838 
2025-01-28 08:46:24.518913: Pseudo dice [np.float32(0.9627), np.float32(0.9192)] 
2025-01-28 08:46:24.521513: Epoch time: 47.85 s 
2025-01-28 08:46:25.698901:  
2025-01-28 08:46:25.702377: Epoch 718 
2025-01-28 08:46:25.705460: Current learning rate: 0.0032 
2025-01-28 08:47:14.085901: train_loss -0.835 
2025-01-28 08:47:14.091608: val_loss -0.7876 
2025-01-28 08:47:14.094526: Pseudo dice [np.float32(0.9526), np.float32(0.933)] 
2025-01-28 08:47:14.097313: Epoch time: 48.39 s 
2025-01-28 08:47:15.280950:  
2025-01-28 08:47:15.284222: Epoch 719 
2025-01-28 08:47:15.287294: Current learning rate: 0.00319 
2025-01-28 08:48:03.698934: train_loss -0.8458 
2025-01-28 08:48:03.703641: val_loss -0.7495 
2025-01-28 08:48:03.706492: Pseudo dice [np.float32(0.9582), np.float32(0.9383)] 
2025-01-28 08:48:03.709559: Epoch time: 48.42 s 
2025-01-28 08:48:04.904301:  
2025-01-28 08:48:04.907872: Epoch 720 
2025-01-28 08:48:04.910918: Current learning rate: 0.00318 
2025-01-28 08:48:52.995354: train_loss -0.8298 
2025-01-28 08:48:53.001749: val_loss -0.7284 
2025-01-28 08:48:53.004600: Pseudo dice [np.float32(0.9485), np.float32(0.947)] 
2025-01-28 08:48:53.007438: Epoch time: 48.09 s 
2025-01-28 08:48:53.010130: Yayy! New best EMA pseudo Dice: 0.9387000203132629 
2025-01-28 08:48:54.773221:  
2025-01-28 08:48:54.776361: Epoch 721 
2025-01-28 08:48:54.779381: Current learning rate: 0.00317 
2025-01-28 08:49:43.264261: train_loss -0.8285 
2025-01-28 08:49:43.268862: val_loss -0.746 
2025-01-28 08:49:43.271765: Pseudo dice [np.float32(0.9548), np.float32(0.935)] 
2025-01-28 08:49:43.274582: Epoch time: 48.49 s 
2025-01-28 08:49:43.277145: Yayy! New best EMA pseudo Dice: 0.9394000172615051 
2025-01-28 08:49:45.024142:  
2025-01-28 08:49:45.027019: Epoch 722 
2025-01-28 08:49:45.029874: Current learning rate: 0.00316 
2025-01-28 08:50:33.460482: train_loss -0.8286 
2025-01-28 08:50:33.466634: val_loss -0.7291 
2025-01-28 08:50:33.469527: Pseudo dice [np.float32(0.9533), np.float32(0.9097)] 
2025-01-28 08:50:33.472312: Epoch time: 48.44 s 
2025-01-28 08:50:35.276456:  
2025-01-28 08:50:35.279601: Epoch 723 
2025-01-28 08:50:35.282507: Current learning rate: 0.00315 
2025-01-28 08:51:23.174501: train_loss -0.8226 
2025-01-28 08:51:23.180006: val_loss -0.7721 
2025-01-28 08:51:23.182818: Pseudo dice [np.float32(0.9435), np.float32(0.932)] 
2025-01-28 08:51:23.185315: Epoch time: 47.9 s 
2025-01-28 08:51:24.361931:  
2025-01-28 08:51:24.365397: Epoch 724 
2025-01-28 08:51:24.368317: Current learning rate: 0.00314 
2025-01-28 08:52:12.796634: train_loss -0.8371 
2025-01-28 08:52:12.802555: val_loss -0.7161 
2025-01-28 08:52:12.805395: Pseudo dice [np.float32(0.9564), np.float32(0.939)] 
2025-01-28 08:52:12.808198: Epoch time: 48.44 s 
2025-01-28 08:52:12.811112: Yayy! New best EMA pseudo Dice: 0.9394000172615051 
2025-01-28 08:52:14.619951:  
2025-01-28 08:52:14.624108: Epoch 725 
2025-01-28 08:52:14.626892: Current learning rate: 0.00313 
2025-01-28 08:53:02.611612: train_loss -0.8422 
2025-01-28 08:53:02.617679: val_loss -0.7401 
2025-01-28 08:53:02.621513: Pseudo dice [np.float32(0.9595), np.float32(0.9331)] 
2025-01-28 08:53:02.624667: Epoch time: 47.99 s 
2025-01-28 08:53:02.627319: Yayy! New best EMA pseudo Dice: 0.9401000142097473 
2025-01-28 08:53:04.400963:  
2025-01-28 08:53:04.404734: Epoch 726 
2025-01-28 08:53:04.407937: Current learning rate: 0.00312 
2025-01-28 08:53:52.916407: train_loss -0.8386 
2025-01-28 08:53:52.922439: val_loss -0.7755 
2025-01-28 08:53:52.925466: Pseudo dice [np.float32(0.954), np.float32(0.9332)] 
2025-01-28 08:53:52.928254: Epoch time: 48.52 s 
2025-01-28 08:53:52.930707: Yayy! New best EMA pseudo Dice: 0.940500020980835 
2025-01-28 08:53:54.690789:  
2025-01-28 08:53:54.693819: Epoch 727 
2025-01-28 08:53:54.696618: Current learning rate: 0.00311 
2025-01-28 08:54:42.757324: train_loss -0.8314 
2025-01-28 08:54:42.764127: val_loss -0.7101 
2025-01-28 08:54:42.767043: Pseudo dice [np.float32(0.9411), np.float32(0.9193)] 
2025-01-28 08:54:42.769552: Epoch time: 48.07 s 
2025-01-28 08:54:43.952009:  
2025-01-28 08:54:43.955165: Epoch 728 
2025-01-28 08:54:43.958124: Current learning rate: 0.0031 
2025-01-28 08:55:32.121318: train_loss -0.8395 
2025-01-28 08:55:32.127140: val_loss -0.7371 
2025-01-28 08:55:32.129645: Pseudo dice [np.float32(0.9366), np.float32(0.9325)] 
2025-01-28 08:55:32.132125: Epoch time: 48.17 s 
2025-01-28 08:55:33.302586:  
2025-01-28 08:55:33.305564: Epoch 729 
2025-01-28 08:55:33.308154: Current learning rate: 0.00309 
2025-01-28 08:56:21.044244: train_loss -0.8422 
2025-01-28 08:56:21.048685: val_loss -0.7564 
2025-01-28 08:56:21.051472: Pseudo dice [np.float32(0.9556), np.float32(0.9167)] 
2025-01-28 08:56:21.054259: Epoch time: 47.74 s 
2025-01-28 08:56:22.261240:  
2025-01-28 08:56:22.264174: Epoch 730 
2025-01-28 08:56:22.267408: Current learning rate: 0.00308 
2025-01-28 08:57:10.000427: train_loss -0.8211 
2025-01-28 08:57:10.006558: val_loss -0.7459 
2025-01-28 08:57:10.010866: Pseudo dice [np.float32(0.9491), np.float32(0.9223)] 
2025-01-28 08:57:10.013741: Epoch time: 47.74 s 
2025-01-28 08:57:11.185437:  
2025-01-28 08:57:11.188452: Epoch 731 
2025-01-28 08:57:11.191426: Current learning rate: 0.00307 
2025-01-28 08:57:59.387126: train_loss -0.8224 
2025-01-28 08:57:59.391101: val_loss -0.7414 
2025-01-28 08:57:59.394178: Pseudo dice [np.float32(0.9448), np.float32(0.8881)] 
2025-01-28 08:57:59.397224: Epoch time: 48.2 s 
2025-01-28 08:58:00.557635:  
2025-01-28 08:58:00.560861: Epoch 732 
2025-01-28 08:58:00.563991: Current learning rate: 0.00306 
2025-01-28 08:58:48.265218: train_loss -0.8359 
2025-01-28 08:58:48.271121: val_loss -0.757 
2025-01-28 08:58:48.274336: Pseudo dice [np.float32(0.9471), np.float32(0.9096)] 
2025-01-28 08:58:48.277164: Epoch time: 47.71 s 
2025-01-28 08:58:49.474402:  
2025-01-28 08:58:49.477653: Epoch 733 
2025-01-28 08:58:49.480500: Current learning rate: 0.00305 
2025-01-28 08:59:37.601122: train_loss -0.8142 
2025-01-28 08:59:37.607527: val_loss -0.7257 
2025-01-28 08:59:37.610592: Pseudo dice [np.float32(0.9477), np.float32(0.8891)] 
2025-01-28 08:59:37.613377: Epoch time: 48.13 s 
2025-01-28 08:59:38.784574:  
2025-01-28 08:59:38.787572: Epoch 734 
2025-01-28 08:59:38.790635: Current learning rate: 0.00304 
2025-01-28 09:00:26.572318: train_loss -0.8352 
2025-01-28 09:00:26.579119: val_loss -0.7756 
2025-01-28 09:00:26.582036: Pseudo dice [np.float32(0.9604), np.float32(0.9218)] 
2025-01-28 09:00:26.584806: Epoch time: 47.79 s 
2025-01-28 09:00:27.746274:  
2025-01-28 09:00:27.749809: Epoch 735 
2025-01-28 09:00:27.752589: Current learning rate: 0.00303 
2025-01-28 09:01:15.645837: train_loss -0.8253 
2025-01-28 09:01:15.649849: val_loss -0.7313 
2025-01-28 09:01:15.653251: Pseudo dice [np.float32(0.9543), np.float32(0.9215)] 
2025-01-28 09:01:15.656212: Epoch time: 47.9 s 
2025-01-28 09:01:16.815659:  
2025-01-28 09:01:16.818226: Epoch 736 
2025-01-28 09:01:16.821026: Current learning rate: 0.00302 
2025-01-28 09:02:04.716231: train_loss -0.8358 
2025-01-28 09:02:04.721763: val_loss -0.7427 
2025-01-28 09:02:04.724463: Pseudo dice [np.float32(0.9535), np.float32(0.9444)] 
2025-01-28 09:02:04.727494: Epoch time: 47.9 s 
2025-01-28 09:02:05.924759:  
2025-01-28 09:02:05.927582: Epoch 737 
2025-01-28 09:02:05.930257: Current learning rate: 0.00301 
2025-01-28 09:02:53.491428: train_loss -0.8504 
2025-01-28 09:02:53.495958: val_loss -0.7764 
2025-01-28 09:02:53.498804: Pseudo dice [np.float32(0.958), np.float32(0.9448)] 
2025-01-28 09:02:53.501549: Epoch time: 47.57 s 
2025-01-28 09:02:54.663653:  
2025-01-28 09:02:54.667024: Epoch 738 
2025-01-28 09:02:54.670126: Current learning rate: 0.003 
2025-01-28 09:03:42.583972: train_loss -0.8405 
2025-01-28 09:03:42.590074: val_loss -0.7725 
2025-01-28 09:03:42.592718: Pseudo dice [np.float32(0.9547), np.float32(0.9203)] 
2025-01-28 09:03:42.595501: Epoch time: 47.92 s 
2025-01-28 09:03:43.757587:  
2025-01-28 09:03:43.760872: Epoch 739 
2025-01-28 09:03:43.763748: Current learning rate: 0.00299 
2025-01-28 09:04:32.222697: train_loss -0.8358 
2025-01-28 09:04:32.226833: val_loss -0.7418 
2025-01-28 09:04:32.229552: Pseudo dice [np.float32(0.9512), np.float32(0.936)] 
2025-01-28 09:04:32.232144: Epoch time: 48.47 s 
2025-01-28 09:04:33.428557:  
2025-01-28 09:04:33.431344: Epoch 740 
2025-01-28 09:04:33.434108: Current learning rate: 0.00297 
2025-01-28 09:05:21.130131: train_loss -0.8254 
2025-01-28 09:05:21.135850: val_loss -0.7477 
2025-01-28 09:05:21.138612: Pseudo dice [np.float32(0.9516), np.float32(0.9365)] 
2025-01-28 09:05:21.141382: Epoch time: 47.7 s 
2025-01-28 09:05:22.934232:  
2025-01-28 09:05:22.937571: Epoch 741 
2025-01-28 09:05:22.940594: Current learning rate: 0.00296 
2025-01-28 09:06:10.811349: train_loss -0.8283 
2025-01-28 09:06:10.815464: val_loss -0.7689 
2025-01-28 09:06:10.818543: Pseudo dice [np.float32(0.9556), np.float32(0.9244)] 
2025-01-28 09:06:10.821281: Epoch time: 47.88 s 
2025-01-28 09:06:11.982813:  
2025-01-28 09:06:11.986097: Epoch 742 
2025-01-28 09:06:11.989271: Current learning rate: 0.00295 
2025-01-28 09:06:59.844284: train_loss -0.8556 
2025-01-28 09:06:59.849896: val_loss -0.7514 
2025-01-28 09:06:59.852681: Pseudo dice [np.float32(0.9514), np.float32(0.9261)] 
2025-01-28 09:06:59.854968: Epoch time: 47.86 s 
2025-01-28 09:07:01.039566:  
2025-01-28 09:07:01.042472: Epoch 743 
2025-01-28 09:07:01.045189: Current learning rate: 0.00294 
2025-01-28 09:07:48.894581: train_loss -0.8225 
2025-01-28 09:07:48.898826: val_loss -0.7412 
2025-01-28 09:07:48.901304: Pseudo dice [np.float32(0.9528), np.float32(0.9316)] 
2025-01-28 09:07:48.903960: Epoch time: 47.86 s 
2025-01-28 09:07:50.101668:  
2025-01-28 09:07:50.108565: Epoch 744 
2025-01-28 09:07:50.111442: Current learning rate: 0.00293 
2025-01-28 09:08:38.153898: train_loss -0.8213 
2025-01-28 09:08:38.159799: val_loss -0.729 
2025-01-28 09:08:38.163168: Pseudo dice [np.float32(0.9529), np.float32(0.9249)] 
2025-01-28 09:08:38.166021: Epoch time: 48.05 s 
2025-01-28 09:08:39.315255:  
2025-01-28 09:08:39.318298: Epoch 745 
2025-01-28 09:08:39.321264: Current learning rate: 0.00292 
2025-01-28 09:09:27.503643: train_loss -0.8287 
2025-01-28 09:09:27.507995: val_loss -0.7275 
2025-01-28 09:09:27.511107: Pseudo dice [np.float32(0.9463), np.float32(0.9235)] 
2025-01-28 09:09:27.513683: Epoch time: 48.19 s 
2025-01-28 09:09:28.685684:  
2025-01-28 09:09:28.688641: Epoch 746 
2025-01-28 09:09:28.691284: Current learning rate: 0.00291 
2025-01-28 09:10:17.120317: train_loss -0.8234 
2025-01-28 09:10:17.125993: val_loss -0.7514 
2025-01-28 09:10:17.128945: Pseudo dice [np.float32(0.9555), np.float32(0.9282)] 
2025-01-28 09:10:17.131734: Epoch time: 48.44 s 
2025-01-28 09:10:18.298062:  
2025-01-28 09:10:18.301347: Epoch 747 
2025-01-28 09:10:18.304083: Current learning rate: 0.0029 
2025-01-28 09:11:06.392983: train_loss -0.8391 
2025-01-28 09:11:06.397835: val_loss -0.6992 
2025-01-28 09:11:06.400640: Pseudo dice [np.float32(0.956), np.float32(0.9443)] 
2025-01-28 09:11:06.403538: Epoch time: 48.1 s 
2025-01-28 09:11:07.577888:  
2025-01-28 09:11:07.581400: Epoch 748 
2025-01-28 09:11:07.584541: Current learning rate: 0.00289 
2025-01-28 09:11:55.714204: train_loss -0.8491 
2025-01-28 09:11:55.719904: val_loss -0.7137 
2025-01-28 09:11:55.722626: Pseudo dice [np.float32(0.953), np.float32(0.9212)] 
2025-01-28 09:11:55.725145: Epoch time: 48.14 s 
2025-01-28 09:11:56.888247:  
2025-01-28 09:11:56.891463: Epoch 749 
2025-01-28 09:11:56.894662: Current learning rate: 0.00288 
2025-01-28 09:12:44.620686: train_loss -0.823 
2025-01-28 09:12:44.625010: val_loss -0.7943 
2025-01-28 09:12:44.628169: Pseudo dice [np.float32(0.9521), np.float32(0.9265)] 
2025-01-28 09:12:44.630830: Epoch time: 47.73 s 
2025-01-28 09:12:46.337934:  
2025-01-28 09:12:46.340836: Epoch 750 
2025-01-28 09:12:46.343527: Current learning rate: 0.00287 
2025-01-28 09:13:34.379990: train_loss -0.8467 
2025-01-28 09:13:34.385183: val_loss -0.7896 
2025-01-28 09:13:34.387883: Pseudo dice [np.float32(0.9551), np.float32(0.9182)] 
2025-01-28 09:13:34.390421: Epoch time: 48.04 s 
2025-01-28 09:13:35.566509:  
2025-01-28 09:13:35.569536: Epoch 751 
2025-01-28 09:13:35.572507: Current learning rate: 0.00286 
2025-01-28 09:14:23.797944: train_loss -0.8332 
2025-01-28 09:14:23.802264: val_loss -0.7412 
2025-01-28 09:14:23.805311: Pseudo dice [np.float32(0.9542), np.float32(0.8316)] 
2025-01-28 09:14:23.807931: Epoch time: 48.23 s 
2025-01-28 09:14:24.981062:  
2025-01-28 09:14:24.984276: Epoch 752 
2025-01-28 09:14:24.987380: Current learning rate: 0.00285 
2025-01-28 09:15:12.940611: train_loss -0.8413 
2025-01-28 09:15:12.946821: val_loss -0.78 
2025-01-28 09:15:12.950191: Pseudo dice [np.float32(0.9581), np.float32(0.9492)] 
2025-01-28 09:15:12.953016: Epoch time: 47.96 s 
2025-01-28 09:15:14.125093:  
2025-01-28 09:15:14.128060: Epoch 753 
2025-01-28 09:15:14.131049: Current learning rate: 0.00284 
2025-01-28 09:16:02.165996: train_loss -0.828 
2025-01-28 09:16:02.170559: val_loss -0.7307 
2025-01-28 09:16:02.173557: Pseudo dice [np.float32(0.9498), np.float32(0.9274)] 
2025-01-28 09:16:02.176675: Epoch time: 48.04 s 
2025-01-28 09:16:03.348164:  
2025-01-28 09:16:03.351156: Epoch 754 
2025-01-28 09:16:03.354073: Current learning rate: 0.00283 
2025-01-28 09:16:51.400890: train_loss -0.8288 
2025-01-28 09:16:51.406854: val_loss -0.7903 
2025-01-28 09:16:51.409722: Pseudo dice [np.float32(0.9558), np.float32(0.8846)] 
2025-01-28 09:16:51.412523: Epoch time: 48.05 s 
2025-01-28 09:16:52.581499:  
2025-01-28 09:16:52.584819: Epoch 755 
2025-01-28 09:16:52.587508: Current learning rate: 0.00282 
2025-01-28 09:17:40.604295: train_loss -0.8365 
2025-01-28 09:17:40.608485: val_loss -0.7456 
2025-01-28 09:17:40.611318: Pseudo dice [np.float32(0.9467), np.float32(0.9224)] 
2025-01-28 09:17:40.613741: Epoch time: 48.02 s 
2025-01-28 09:17:41.782799:  
2025-01-28 09:17:41.785527: Epoch 756 
2025-01-28 09:17:41.788114: Current learning rate: 0.00281 
2025-01-28 09:18:29.404250: train_loss -0.8376 
2025-01-28 09:18:29.410873: val_loss -0.7686 
2025-01-28 09:18:29.414045: Pseudo dice [np.float32(0.9578), np.float32(0.9399)] 
2025-01-28 09:18:29.417214: Epoch time: 47.62 s 
2025-01-28 09:18:30.588369:  
2025-01-28 09:18:30.591910: Epoch 757 
2025-01-28 09:18:30.594634: Current learning rate: 0.0028 
2025-01-28 09:19:18.640818: train_loss -0.8346 
2025-01-28 09:19:18.644979: val_loss -0.738 
2025-01-28 09:19:18.647738: Pseudo dice [np.float32(0.9503), np.float32(0.9247)] 
2025-01-28 09:19:18.650512: Epoch time: 48.05 s 
2025-01-28 09:19:19.825431:  
2025-01-28 09:19:19.828629: Epoch 758 
2025-01-28 09:19:19.832456: Current learning rate: 0.00279 
2025-01-28 09:20:07.600516: train_loss -0.8508 
2025-01-28 09:20:07.606287: val_loss -0.7642 
2025-01-28 09:20:07.609144: Pseudo dice [np.float32(0.96), np.float32(0.8746)] 
2025-01-28 09:20:07.612065: Epoch time: 47.78 s 
2025-01-28 09:20:09.361509:  
2025-01-28 09:20:09.364623: Epoch 759 
2025-01-28 09:20:09.367281: Current learning rate: 0.00278 
2025-01-28 09:20:57.747016: train_loss -0.8441 
2025-01-28 09:20:57.752698: val_loss -0.7649 
2025-01-28 09:20:57.755445: Pseudo dice [np.float32(0.9505), np.float32(0.9081)] 
2025-01-28 09:20:57.758114: Epoch time: 48.39 s 
2025-01-28 09:20:58.945061:  
2025-01-28 09:20:58.947837: Epoch 760 
2025-01-28 09:20:58.950744: Current learning rate: 0.00277 
2025-01-28 09:21:47.137467: train_loss -0.8407 
2025-01-28 09:21:47.143490: val_loss -0.7012 
2025-01-28 09:21:47.146218: Pseudo dice [np.float32(0.952), np.float32(0.9112)] 
2025-01-28 09:21:47.149027: Epoch time: 48.19 s 
2025-01-28 09:21:48.360769:  
2025-01-28 09:21:48.364084: Epoch 761 
2025-01-28 09:21:48.367206: Current learning rate: 0.00276 
2025-01-28 09:22:36.564016: train_loss -0.845 
2025-01-28 09:22:36.568712: val_loss -0.7714 
2025-01-28 09:22:36.571338: Pseudo dice [np.float32(0.9521), np.float32(0.9329)] 
2025-01-28 09:22:36.574237: Epoch time: 48.2 s 
2025-01-28 09:22:37.755698:  
2025-01-28 09:22:37.758513: Epoch 762 
2025-01-28 09:22:37.763596: Current learning rate: 0.00275 
2025-01-28 09:23:25.442256: train_loss -0.8298 
2025-01-28 09:23:25.448228: val_loss -0.7897 
2025-01-28 09:23:25.451126: Pseudo dice [np.float32(0.9541), np.float32(0.9331)] 
2025-01-28 09:23:25.453777: Epoch time: 47.69 s 
2025-01-28 09:23:26.635913:  
2025-01-28 09:23:26.639181: Epoch 763 
2025-01-28 09:23:26.642040: Current learning rate: 0.00274 
2025-01-28 09:24:14.370637: train_loss -0.8378 
2025-01-28 09:24:14.375515: val_loss -0.775 
2025-01-28 09:24:14.378394: Pseudo dice [np.float32(0.9456), np.float32(0.9212)] 
2025-01-28 09:24:14.381307: Epoch time: 47.74 s 
2025-01-28 09:24:15.603263:  
2025-01-28 09:24:15.606597: Epoch 764 
2025-01-28 09:24:15.609566: Current learning rate: 0.00273 
2025-01-28 09:25:03.794962: train_loss -0.8349 
2025-01-28 09:25:03.800655: val_loss -0.7425 
2025-01-28 09:25:03.803461: Pseudo dice [np.float32(0.9561), np.float32(0.925)] 
2025-01-28 09:25:03.806229: Epoch time: 48.19 s 
2025-01-28 09:25:05.012897:  
2025-01-28 09:25:05.016474: Epoch 765 
2025-01-28 09:25:05.019593: Current learning rate: 0.00272 
2025-01-28 09:25:53.181038: train_loss -0.8313 
2025-01-28 09:25:53.185667: val_loss -0.7332 
2025-01-28 09:25:53.188374: Pseudo dice [np.float32(0.9524), np.float32(0.9407)] 
2025-01-28 09:25:53.191236: Epoch time: 48.17 s 
2025-01-28 09:25:54.364290:  
2025-01-28 09:25:54.367282: Epoch 766 
2025-01-28 09:25:54.369951: Current learning rate: 0.00271 
2025-01-28 09:26:42.198775: train_loss -0.845 
2025-01-28 09:26:42.204762: val_loss -0.7855 
2025-01-28 09:26:42.207810: Pseudo dice [np.float32(0.949), np.float32(0.9323)] 
2025-01-28 09:26:42.210933: Epoch time: 47.84 s 
2025-01-28 09:26:43.423552:  
2025-01-28 09:26:43.426622: Epoch 767 
2025-01-28 09:26:43.429862: Current learning rate: 0.0027 
2025-01-28 09:27:31.453298: train_loss -0.8552 
2025-01-28 09:27:31.459184: val_loss -0.7697 
2025-01-28 09:27:31.461977: Pseudo dice [np.float32(0.9511), np.float32(0.9286)] 
2025-01-28 09:27:31.464599: Epoch time: 48.03 s 
2025-01-28 09:27:32.637992:  
2025-01-28 09:27:32.641128: Epoch 768 
2025-01-28 09:27:32.643934: Current learning rate: 0.00268 
2025-01-28 09:28:20.689111: train_loss -0.8341 
2025-01-28 09:28:20.694690: val_loss -0.7376 
2025-01-28 09:28:20.697664: Pseudo dice [np.float32(0.9377), np.float32(0.8992)] 
2025-01-28 09:28:20.700286: Epoch time: 48.05 s 
2025-01-28 09:28:21.877794:  
2025-01-28 09:28:21.880862: Epoch 769 
2025-01-28 09:28:21.883950: Current learning rate: 0.00267 
2025-01-28 09:29:09.929730: train_loss -0.8367 
2025-01-28 09:29:09.934174: val_loss -0.7507 
2025-01-28 09:29:09.937215: Pseudo dice [np.float32(0.9583), np.float32(0.9343)] 
2025-01-28 09:29:09.940168: Epoch time: 48.05 s 
2025-01-28 09:29:11.154818:  
2025-01-28 09:29:11.157987: Epoch 770 
2025-01-28 09:29:11.160818: Current learning rate: 0.00266 
2025-01-28 09:29:58.521768: train_loss -0.8446 
2025-01-28 09:29:58.527974: val_loss -0.7224 
2025-01-28 09:29:58.531025: Pseudo dice [np.float32(0.9561), np.float32(0.8691)] 
2025-01-28 09:29:58.533631: Epoch time: 47.37 s 
2025-01-28 09:29:59.708717:  
2025-01-28 09:29:59.711807: Epoch 771 
2025-01-28 09:29:59.714466: Current learning rate: 0.00265 
2025-01-28 09:30:47.603422: train_loss -0.8151 
2025-01-28 09:30:47.608482: val_loss -0.6899 
2025-01-28 09:30:47.611317: Pseudo dice [np.float32(0.9557), np.float32(0.909)] 
2025-01-28 09:30:47.614439: Epoch time: 47.9 s 
2025-01-28 09:30:48.828943:  
2025-01-28 09:30:48.832077: Epoch 772 
2025-01-28 09:30:48.835278: Current learning rate: 0.00264 
2025-01-28 09:31:36.460263: train_loss -0.8333 
2025-01-28 09:31:36.466249: val_loss -0.7461 
2025-01-28 09:31:36.469106: Pseudo dice [np.float32(0.9517), np.float32(0.9262)] 
2025-01-28 09:31:36.472153: Epoch time: 47.63 s 
2025-01-28 09:31:37.661847:  
2025-01-28 09:31:37.666151: Epoch 773 
2025-01-28 09:31:37.668889: Current learning rate: 0.00263 
2025-01-28 09:32:25.438628: train_loss -0.8389 
2025-01-28 09:32:25.444803: val_loss -0.7143 
2025-01-28 09:32:25.447614: Pseudo dice [np.float32(0.9514), np.float32(0.9139)] 
2025-01-28 09:32:25.450052: Epoch time: 47.78 s 
2025-01-28 09:32:26.621429:  
2025-01-28 09:32:26.626173: Epoch 774 
2025-01-28 09:32:26.629221: Current learning rate: 0.00262 
2025-01-28 09:33:14.493718: train_loss -0.8326 
2025-01-28 09:33:14.499359: val_loss -0.7652 
2025-01-28 09:33:14.502236: Pseudo dice [np.float32(0.9448), np.float32(0.9173)] 
2025-01-28 09:33:14.504847: Epoch time: 47.87 s 
2025-01-28 09:33:15.686871:  
2025-01-28 09:33:15.689676: Epoch 775 
2025-01-28 09:33:15.692442: Current learning rate: 0.00261 
2025-01-28 09:34:03.694765: train_loss -0.8414 
2025-01-28 09:34:03.699150: val_loss -0.7859 
2025-01-28 09:34:03.702233: Pseudo dice [np.float32(0.9527), np.float32(0.9382)] 
2025-01-28 09:34:03.705101: Epoch time: 48.01 s 
2025-01-28 09:34:04.921292:  
2025-01-28 09:34:04.924638: Epoch 776 
2025-01-28 09:34:04.927722: Current learning rate: 0.0026 
2025-01-28 09:34:53.574504: train_loss -0.8273 
2025-01-28 09:34:53.581927: val_loss -0.7384 
2025-01-28 09:34:53.584799: Pseudo dice [np.float32(0.9568), np.float32(0.9214)] 
2025-01-28 09:34:53.587572: Epoch time: 48.65 s 
2025-01-28 09:34:55.439402:  
2025-01-28 09:34:55.442511: Epoch 777 
2025-01-28 09:34:55.445426: Current learning rate: 0.00259 
2025-01-28 09:35:44.056010: train_loss -0.8296 
2025-01-28 09:35:44.061648: val_loss -0.7575 
2025-01-28 09:35:44.064475: Pseudo dice [np.float32(0.9527), np.float32(0.9167)] 
2025-01-28 09:35:44.067396: Epoch time: 48.62 s 
2025-01-28 09:35:45.278752:  
2025-01-28 09:35:45.282041: Epoch 778 
2025-01-28 09:35:45.285589: Current learning rate: 0.00258 
2025-01-28 09:36:33.279728: train_loss -0.8365 
2025-01-28 09:36:33.286254: val_loss -0.7678 
2025-01-28 09:36:33.289831: Pseudo dice [np.float32(0.9544), np.float32(0.9278)] 
2025-01-28 09:36:33.292827: Epoch time: 48.0 s 
2025-01-28 09:36:34.475718:  
2025-01-28 09:36:34.478852: Epoch 779 
2025-01-28 09:36:34.481709: Current learning rate: 0.00257 
2025-01-28 09:37:22.661232: train_loss -0.836 
2025-01-28 09:37:22.665771: val_loss -0.7747 
2025-01-28 09:37:22.668579: Pseudo dice [np.float32(0.9555), np.float32(0.8968)] 
2025-01-28 09:37:22.671516: Epoch time: 48.19 s 
2025-01-28 09:37:23.862100:  
2025-01-28 09:37:23.865232: Epoch 780 
2025-01-28 09:37:23.868068: Current learning rate: 0.00256 
2025-01-28 09:38:11.782864: train_loss -0.8421 
2025-01-28 09:38:11.788337: val_loss -0.7275 
2025-01-28 09:38:11.790868: Pseudo dice [np.float32(0.9534), np.float32(0.9348)] 
2025-01-28 09:38:11.793248: Epoch time: 47.92 s 
2025-01-28 09:38:12.974206:  
2025-01-28 09:38:12.977101: Epoch 781 
2025-01-28 09:38:12.979845: Current learning rate: 0.00255 
2025-01-28 09:39:01.224473: train_loss -0.8493 
2025-01-28 09:39:01.229340: val_loss -0.7509 
2025-01-28 09:39:01.232438: Pseudo dice [np.float32(0.9506), np.float32(0.943)] 
2025-01-28 09:39:01.235331: Epoch time: 48.25 s 
2025-01-28 09:39:02.425260:  
2025-01-28 09:39:02.428761: Epoch 782 
2025-01-28 09:39:02.431826: Current learning rate: 0.00254 
2025-01-28 09:39:50.437081: train_loss -0.8259 
2025-01-28 09:39:50.444240: val_loss -0.7794 
2025-01-28 09:39:50.447123: Pseudo dice [np.float32(0.9519), np.float32(0.9139)] 
2025-01-28 09:39:50.449929: Epoch time: 48.01 s 
2025-01-28 09:39:51.635504:  
2025-01-28 09:39:51.638529: Epoch 783 
2025-01-28 09:39:51.641209: Current learning rate: 0.00253 
2025-01-28 09:40:39.748585: train_loss -0.8504 
2025-01-28 09:40:39.753289: val_loss -0.7476 
2025-01-28 09:40:39.756296: Pseudo dice [np.float32(0.9487), np.float32(0.784)] 
2025-01-28 09:40:39.758760: Epoch time: 48.11 s 
2025-01-28 09:40:40.941887:  
2025-01-28 09:40:40.945075: Epoch 784 
2025-01-28 09:40:40.947854: Current learning rate: 0.00252 
2025-01-28 09:41:29.026345: train_loss -0.8391 
2025-01-28 09:41:29.032537: val_loss -0.7434 
2025-01-28 09:41:29.035572: Pseudo dice [np.float32(0.9552), np.float32(0.9357)] 
2025-01-28 09:41:29.038566: Epoch time: 48.09 s 
2025-01-28 09:41:30.224785:  
2025-01-28 09:41:30.228307: Epoch 785 
2025-01-28 09:41:30.230949: Current learning rate: 0.00251 
2025-01-28 09:42:18.250422: train_loss -0.8317 
2025-01-28 09:42:18.254904: val_loss -0.7529 
2025-01-28 09:42:18.257346: Pseudo dice [np.float32(0.9586), np.float32(0.9395)] 
2025-01-28 09:42:18.259811: Epoch time: 48.03 s 
2025-01-28 09:42:19.442497:  
2025-01-28 09:42:19.446462: Epoch 786 
2025-01-28 09:42:19.449744: Current learning rate: 0.0025 
2025-01-28 09:43:07.471462: train_loss -0.8285 
2025-01-28 09:43:07.477232: val_loss -0.7252 
2025-01-28 09:43:07.480132: Pseudo dice [np.float32(0.9535), np.float32(0.9199)] 
2025-01-28 09:43:07.482878: Epoch time: 48.03 s 
2025-01-28 09:43:08.667091:  
2025-01-28 09:43:08.675695: Epoch 787 
2025-01-28 09:43:08.678843: Current learning rate: 0.00249 
2025-01-28 09:43:56.893245: train_loss -0.8403 
2025-01-28 09:43:56.897730: val_loss -0.7082 
2025-01-28 09:43:56.900424: Pseudo dice [np.float32(0.9548), np.float32(0.9366)] 
2025-01-28 09:43:56.902951: Epoch time: 48.23 s 
2025-01-28 09:43:58.086264:  
2025-01-28 09:43:58.089437: Epoch 788 
2025-01-28 09:43:58.092413: Current learning rate: 0.00248 
2025-01-28 09:44:46.309099: train_loss -0.8395 
2025-01-28 09:44:46.317392: val_loss -0.7187 
2025-01-28 09:44:46.320101: Pseudo dice [np.float32(0.9578), np.float32(0.9297)] 
2025-01-28 09:44:46.322986: Epoch time: 48.22 s 
2025-01-28 09:44:47.537373:  
2025-01-28 09:44:47.540805: Epoch 789 
2025-01-28 09:44:47.544427: Current learning rate: 0.00247 
2025-01-28 09:45:36.292823: train_loss -0.8317 
2025-01-28 09:45:36.297834: val_loss -0.7086 
2025-01-28 09:45:36.300853: Pseudo dice [np.float32(0.9586), np.float32(0.9306)] 
2025-01-28 09:45:36.303651: Epoch time: 48.76 s 
2025-01-28 09:45:37.489280:  
2025-01-28 09:45:37.491939: Epoch 790 
2025-01-28 09:45:37.494794: Current learning rate: 0.00245 
2025-01-28 09:46:25.341111: train_loss -0.8219 
2025-01-28 09:46:25.347253: val_loss -0.7462 
2025-01-28 09:46:25.350110: Pseudo dice [np.float32(0.9628), np.float32(0.939)] 
2025-01-28 09:46:25.352879: Epoch time: 47.85 s 
2025-01-28 09:46:26.575729:  
2025-01-28 09:46:26.579373: Epoch 791 
2025-01-28 09:46:26.582592: Current learning rate: 0.00244 
2025-01-28 09:47:14.453277: train_loss -0.8455 
2025-01-28 09:47:14.457836: val_loss -0.7424 
2025-01-28 09:47:14.460733: Pseudo dice [np.float32(0.9623), np.float32(0.9296)] 
2025-01-28 09:47:14.463238: Epoch time: 47.88 s 
2025-01-28 09:47:15.646280:  
2025-01-28 09:47:15.649505: Epoch 792 
2025-01-28 09:47:15.652774: Current learning rate: 0.00243 
2025-01-28 09:48:03.108695: train_loss -0.8422 
2025-01-28 09:48:03.114973: val_loss -0.711 
2025-01-28 09:48:03.117963: Pseudo dice [np.float32(0.9599), np.float32(0.9342)] 
2025-01-28 09:48:03.120854: Epoch time: 47.46 s 
2025-01-28 09:48:04.336153:  
2025-01-28 09:48:04.339562: Epoch 793 
2025-01-28 09:48:04.342681: Current learning rate: 0.00242 
2025-01-28 09:48:52.496299: train_loss -0.8409 
2025-01-28 09:48:52.500883: val_loss -0.8061 
2025-01-28 09:48:52.504205: Pseudo dice [np.float32(0.9533), np.float32(0.9435)] 
2025-01-28 09:48:52.507086: Epoch time: 48.16 s 
2025-01-28 09:48:53.685322:  
2025-01-28 09:48:53.688375: Epoch 794 
2025-01-28 09:48:53.690957: Current learning rate: 0.00241 
2025-01-28 09:49:41.853161: train_loss -0.8328 
2025-01-28 09:49:41.859018: val_loss -0.7384 
2025-01-28 09:49:41.862161: Pseudo dice [np.float32(0.9568), np.float32(0.9074)] 
2025-01-28 09:49:41.864820: Epoch time: 48.17 s 
2025-01-28 09:49:43.642864:  
2025-01-28 09:49:43.646041: Epoch 795 
2025-01-28 09:49:43.648832: Current learning rate: 0.0024 
2025-01-28 09:50:31.792420: train_loss -0.8444 
2025-01-28 09:50:31.796822: val_loss -0.7448 
2025-01-28 09:50:31.799912: Pseudo dice [np.float32(0.9544), np.float32(0.9397)] 
2025-01-28 09:50:31.803062: Epoch time: 48.15 s 
2025-01-28 09:50:33.018418:  
2025-01-28 09:50:33.021704: Epoch 796 
2025-01-28 09:50:33.024770: Current learning rate: 0.00239 
2025-01-28 09:51:21.159792: train_loss -0.8437 
2025-01-28 09:51:21.165860: val_loss -0.8149 
2025-01-28 09:51:21.168976: Pseudo dice [np.float32(0.9524), np.float32(0.9474)] 
2025-01-28 09:51:21.171418: Epoch time: 48.14 s 
2025-01-28 09:51:21.173964: Yayy! New best EMA pseudo Dice: 0.9412999749183655 
2025-01-28 09:51:22.991899:  
2025-01-28 09:51:22.997281: Epoch 797 
2025-01-28 09:51:23.000495: Current learning rate: 0.00238 
2025-01-28 09:52:11.023082: train_loss -0.8345 
2025-01-28 09:52:11.027179: val_loss -0.7389 
2025-01-28 09:52:11.029846: Pseudo dice [np.float32(0.9545), np.float32(0.9351)] 
2025-01-28 09:52:11.032307: Epoch time: 48.03 s 
2025-01-28 09:52:11.034900: Yayy! New best EMA pseudo Dice: 0.9416000247001648 
2025-01-28 09:52:12.800551:  
2025-01-28 09:52:12.803540: Epoch 798 
2025-01-28 09:52:12.806515: Current learning rate: 0.00237 
2025-01-28 09:53:00.460244: train_loss -0.8401 
2025-01-28 09:53:00.470338: val_loss -0.7602 
2025-01-28 09:53:00.473634: Pseudo dice [np.float32(0.9544), np.float32(0.9376)] 
2025-01-28 09:53:00.476407: Epoch time: 47.66 s 
2025-01-28 09:53:00.479247: Yayy! New best EMA pseudo Dice: 0.9420999884605408 
2025-01-28 09:53:02.318889:  
2025-01-28 09:53:02.321989: Epoch 799 
2025-01-28 09:53:02.324796: Current learning rate: 0.00236 
2025-01-28 09:53:50.139890: train_loss -0.8331 
2025-01-28 09:53:50.146312: val_loss -0.7565 
2025-01-28 09:53:50.149626: Pseudo dice [np.float32(0.9561), np.float32(0.9334)] 
2025-01-28 09:53:50.153293: Epoch time: 47.82 s 
2025-01-28 09:53:50.772849: Yayy! New best EMA pseudo Dice: 0.942300021648407 
2025-01-28 09:53:52.651974:  
2025-01-28 09:53:52.656036: Epoch 800 
2025-01-28 09:53:52.659503: Current learning rate: 0.00235 
2025-01-28 09:54:40.505050: train_loss -0.8441 
2025-01-28 09:54:40.511944: val_loss -0.8044 
2025-01-28 09:54:40.514866: Pseudo dice [np.float32(0.9573), np.float32(0.9424)] 
2025-01-28 09:54:40.518028: Epoch time: 47.85 s 
2025-01-28 09:54:40.520756: Yayy! New best EMA pseudo Dice: 0.9430999755859375 
2025-01-28 09:54:42.452132:  
2025-01-28 09:54:42.455401: Epoch 801 
2025-01-28 09:54:42.458164: Current learning rate: 0.00234 
2025-01-28 09:55:30.575170: train_loss -0.8317 
2025-01-28 09:55:30.579299: val_loss -0.7998 
2025-01-28 09:55:30.582265: Pseudo dice [np.float32(0.9515), np.float32(0.9201)] 
2025-01-28 09:55:30.584808: Epoch time: 48.12 s 
2025-01-28 09:55:31.814434:  
2025-01-28 09:55:31.817507: Epoch 802 
2025-01-28 09:55:31.820663: Current learning rate: 0.00233 
2025-01-28 09:56:19.811568: train_loss -0.8371 
2025-01-28 09:56:19.817562: val_loss -0.7336 
2025-01-28 09:56:19.820690: Pseudo dice [np.float32(0.9522), np.float32(0.9391)] 
2025-01-28 09:56:19.823288: Epoch time: 48.0 s 
2025-01-28 09:56:21.023888:  
2025-01-28 09:56:21.027211: Epoch 803 
2025-01-28 09:56:21.029875: Current learning rate: 0.00232 
2025-01-28 09:57:08.941106: train_loss -0.8411 
2025-01-28 09:57:08.946560: val_loss -0.7921 
2025-01-28 09:57:08.949636: Pseudo dice [np.float32(0.9515), np.float32(0.926)] 
2025-01-28 09:57:08.952650: Epoch time: 47.92 s 
2025-01-28 09:57:10.140218:  
2025-01-28 09:57:10.142919: Epoch 804 
2025-01-28 09:57:10.145503: Current learning rate: 0.00231 
2025-01-28 09:57:58.084331: train_loss -0.8503 
2025-01-28 09:57:58.089959: val_loss -0.7545 
2025-01-28 09:57:58.092595: Pseudo dice [np.float32(0.9544), np.float32(0.9319)] 
2025-01-28 09:57:58.095049: Epoch time: 47.95 s 
2025-01-28 09:57:59.286034:  
2025-01-28 09:57:59.290257: Epoch 805 
2025-01-28 09:57:59.293075: Current learning rate: 0.0023 
2025-01-28 09:58:47.147472: train_loss -0.8336 
2025-01-28 09:58:47.152094: val_loss -0.749 
2025-01-28 09:58:47.155118: Pseudo dice [np.float32(0.9619), np.float32(0.8697)] 
2025-01-28 09:58:47.157836: Epoch time: 47.86 s 
2025-01-28 09:58:48.339562:  
2025-01-28 09:58:48.342354: Epoch 806 
2025-01-28 09:58:48.345021: Current learning rate: 0.00229 
2025-01-28 09:59:36.083321: train_loss -0.8359 
2025-01-28 09:59:36.090326: val_loss -0.7203 
2025-01-28 09:59:36.093302: Pseudo dice [np.float32(0.961), np.float32(0.9426)] 
2025-01-28 09:59:36.096185: Epoch time: 47.74 s 
2025-01-28 09:59:37.283564:  
2025-01-28 09:59:37.286774: Epoch 807 
2025-01-28 09:59:37.289557: Current learning rate: 0.00228 
2025-01-28 10:00:25.323547: train_loss -0.8371 
2025-01-28 10:00:25.328316: val_loss -0.754 
2025-01-28 10:00:25.330936: Pseudo dice [np.float32(0.9574), np.float32(0.9333)] 
2025-01-28 10:00:25.333739: Epoch time: 48.04 s 
2025-01-28 10:00:26.513285:  
2025-01-28 10:00:26.515944: Epoch 808 
2025-01-28 10:00:26.518610: Current learning rate: 0.00226 
2025-01-28 10:01:14.290673: train_loss -0.8306 
2025-01-28 10:01:14.296232: val_loss -0.7389 
2025-01-28 10:01:14.299123: Pseudo dice [np.float32(0.951), np.float32(0.9283)] 
2025-01-28 10:01:14.301697: Epoch time: 47.78 s 
2025-01-28 10:01:15.456758:  
2025-01-28 10:01:15.459761: Epoch 809 
2025-01-28 10:01:15.462831: Current learning rate: 0.00225 
2025-01-28 10:02:03.294553: train_loss -0.817 
2025-01-28 10:02:03.298556: val_loss -0.7382 
2025-01-28 10:02:03.301517: Pseudo dice [np.float32(0.9513), np.float32(0.9145)] 
2025-01-28 10:02:03.304049: Epoch time: 47.84 s 
2025-01-28 10:02:04.486637:  
2025-01-28 10:02:04.489371: Epoch 810 
2025-01-28 10:02:04.492160: Current learning rate: 0.00224 
2025-01-28 10:02:52.484131: train_loss -0.8123 
2025-01-28 10:02:52.491850: val_loss -0.7322 
2025-01-28 10:02:52.494950: Pseudo dice [np.float32(0.9522), np.float32(0.8666)] 
2025-01-28 10:02:52.497800: Epoch time: 48.0 s 
2025-01-28 10:02:53.684886:  
2025-01-28 10:02:53.687968: Epoch 811 
2025-01-28 10:02:53.690903: Current learning rate: 0.00223 
2025-01-28 10:03:41.515517: train_loss -0.824 
2025-01-28 10:03:41.519648: val_loss -0.7682 
2025-01-28 10:03:41.522497: Pseudo dice [np.float32(0.9546), np.float32(0.9131)] 
2025-01-28 10:03:41.525179: Epoch time: 47.83 s 
2025-01-28 10:03:43.303776:  
2025-01-28 10:03:43.306884: Epoch 812 
2025-01-28 10:03:43.309796: Current learning rate: 0.00222 
2025-01-28 10:04:31.071111: train_loss -0.8296 
2025-01-28 10:04:31.077191: val_loss -0.7538 
2025-01-28 10:04:31.080242: Pseudo dice [np.float32(0.9614), np.float32(0.9367)] 
2025-01-28 10:04:31.083062: Epoch time: 47.77 s 
2025-01-28 10:04:32.262236:  
2025-01-28 10:04:32.265656: Epoch 813 
2025-01-28 10:04:32.268325: Current learning rate: 0.00221 
2025-01-28 10:05:19.801956: train_loss -0.8474 
2025-01-28 10:05:19.806538: val_loss -0.7681 
2025-01-28 10:05:19.809494: Pseudo dice [np.float32(0.9591), np.float32(0.9263)] 
2025-01-28 10:05:19.812261: Epoch time: 47.54 s 
2025-01-28 10:05:20.995082:  
2025-01-28 10:05:20.998695: Epoch 814 
2025-01-28 10:05:21.001505: Current learning rate: 0.0022 
2025-01-28 10:06:08.694265: train_loss -0.8485 
2025-01-28 10:06:08.700351: val_loss -0.7327 
2025-01-28 10:06:08.703202: Pseudo dice [np.float32(0.9546), np.float32(0.9419)] 
2025-01-28 10:06:08.705941: Epoch time: 47.7 s 
2025-01-28 10:06:09.881986:  
2025-01-28 10:06:09.885112: Epoch 815 
2025-01-28 10:06:09.887970: Current learning rate: 0.00219 
2025-01-28 10:06:57.942525: train_loss -0.8609 
2025-01-28 10:06:57.948606: val_loss -0.7789 
2025-01-28 10:06:57.951712: Pseudo dice [np.float32(0.9549), np.float32(0.9448)] 
2025-01-28 10:06:57.954502: Epoch time: 48.06 s 
2025-01-28 10:06:59.175271:  
2025-01-28 10:06:59.178402: Epoch 816 
2025-01-28 10:06:59.181298: Current learning rate: 0.00218 
2025-01-28 10:07:47.528044: train_loss -0.8528 
2025-01-28 10:07:47.533670: val_loss -0.7466 
2025-01-28 10:07:47.536416: Pseudo dice [np.float32(0.9445), np.float32(0.9174)] 
2025-01-28 10:07:47.539008: Epoch time: 48.35 s 
2025-01-28 10:07:48.726663:  
2025-01-28 10:07:48.729791: Epoch 817 
2025-01-28 10:07:48.732937: Current learning rate: 0.00217 
2025-01-28 10:08:36.883248: train_loss -0.8324 
2025-01-28 10:08:36.887876: val_loss -0.7009 
2025-01-28 10:08:36.890960: Pseudo dice [np.float32(0.9528), np.float32(0.9227)] 
2025-01-28 10:08:36.893672: Epoch time: 48.16 s 
2025-01-28 10:08:38.093928:  
2025-01-28 10:08:38.096651: Epoch 818 
2025-01-28 10:08:38.099439: Current learning rate: 0.00216 
2025-01-28 10:09:26.330228: train_loss -0.8265 
2025-01-28 10:09:26.335657: val_loss -0.7452 
2025-01-28 10:09:26.337978: Pseudo dice [np.float32(0.9557), np.float32(0.9437)] 
2025-01-28 10:09:26.340725: Epoch time: 48.24 s 
2025-01-28 10:09:27.525740:  
2025-01-28 10:09:27.528647: Epoch 819 
2025-01-28 10:09:27.531495: Current learning rate: 0.00215 
2025-01-28 10:10:15.586550: train_loss -0.8321 
2025-01-28 10:10:15.590658: val_loss -0.773 
2025-01-28 10:10:15.593599: Pseudo dice [np.float32(0.9541), np.float32(0.9374)] 
2025-01-28 10:10:15.596405: Epoch time: 48.06 s 
2025-01-28 10:10:16.725097:  
2025-01-28 10:10:16.728086: Epoch 820 
2025-01-28 10:10:16.731319: Current learning rate: 0.00214 
2025-01-28 10:11:04.695195: train_loss -0.8415 
2025-01-28 10:11:04.701244: val_loss -0.7566 
2025-01-28 10:11:04.704044: Pseudo dice [np.float32(0.9551), np.float32(0.9312)] 
2025-01-28 10:11:04.707127: Epoch time: 47.97 s 
2025-01-28 10:11:05.872082:  
2025-01-28 10:11:05.875065: Epoch 821 
2025-01-28 10:11:05.877940: Current learning rate: 0.00213 
2025-01-28 10:11:54.310311: train_loss -0.8431 
2025-01-28 10:11:54.314826: val_loss -0.7587 
2025-01-28 10:11:54.317641: Pseudo dice [np.float32(0.9475), np.float32(0.9275)] 
2025-01-28 10:11:54.320576: Epoch time: 48.44 s 
2025-01-28 10:11:55.445928:  
2025-01-28 10:11:55.449019: Epoch 822 
2025-01-28 10:11:55.451892: Current learning rate: 0.00212 
2025-01-28 10:12:43.525350: train_loss -0.834 
2025-01-28 10:12:43.531049: val_loss -0.7169 
2025-01-28 10:12:43.533895: Pseudo dice [np.float32(0.9568), np.float32(0.9316)] 
2025-01-28 10:12:43.536498: Epoch time: 48.08 s 
2025-01-28 10:12:44.667593:  
2025-01-28 10:12:44.670460: Epoch 823 
2025-01-28 10:12:44.673244: Current learning rate: 0.0021 
2025-01-28 10:13:32.657223: train_loss -0.8339 
2025-01-28 10:13:32.661174: val_loss -0.7999 
2025-01-28 10:13:32.663884: Pseudo dice [np.float32(0.9555), np.float32(0.942)] 
2025-01-28 10:13:32.666635: Epoch time: 47.99 s 
2025-01-28 10:13:33.797225:  
2025-01-28 10:13:33.801009: Epoch 824 
2025-01-28 10:13:33.803812: Current learning rate: 0.00209 
2025-01-28 10:14:21.551887: train_loss -0.8337 
2025-01-28 10:14:21.557302: val_loss -0.7804 
2025-01-28 10:14:21.559991: Pseudo dice [np.float32(0.9614), np.float32(0.9361)] 
2025-01-28 10:14:21.562338: Epoch time: 47.76 s 
2025-01-28 10:14:22.686558:  
2025-01-28 10:14:22.689650: Epoch 825 
2025-01-28 10:14:22.692726: Current learning rate: 0.00208 
2025-01-28 10:15:10.914730: train_loss -0.8431 
2025-01-28 10:15:10.920445: val_loss -0.7949 
2025-01-28 10:15:10.923034: Pseudo dice [np.float32(0.96), np.float32(0.9407)] 
2025-01-28 10:15:10.925927: Epoch time: 48.23 s 
2025-01-28 10:15:10.928495: Yayy! New best EMA pseudo Dice: 0.9434000253677368 
2025-01-28 10:15:12.640221:  
2025-01-28 10:15:12.643256: Epoch 826 
2025-01-28 10:15:12.646241: Current learning rate: 0.00207 
2025-01-28 10:16:00.615523: train_loss -0.8563 
2025-01-28 10:16:00.621186: val_loss -0.7402 
2025-01-28 10:16:00.623958: Pseudo dice [np.float32(0.9587), np.float32(0.9441)] 
2025-01-28 10:16:00.626644: Epoch time: 47.98 s 
2025-01-28 10:16:00.629426: Yayy! New best EMA pseudo Dice: 0.9441999793052673 
2025-01-28 10:16:02.348994:  
2025-01-28 10:16:02.352141: Epoch 827 
2025-01-28 10:16:02.354789: Current learning rate: 0.00206 
2025-01-28 10:16:50.791075: train_loss -0.8468 
2025-01-28 10:16:50.795002: val_loss -0.6993 
2025-01-28 10:16:50.797538: Pseudo dice [np.float32(0.95), np.float32(0.9372)] 
2025-01-28 10:16:50.800086: Epoch time: 48.44 s 
2025-01-28 10:16:51.923826:  
2025-01-28 10:16:51.926947: Epoch 828 
2025-01-28 10:16:51.929790: Current learning rate: 0.00205 
2025-01-28 10:17:40.082451: train_loss -0.825 
2025-01-28 10:17:40.087759: val_loss -0.7622 
2025-01-28 10:17:40.090819: Pseudo dice [np.float32(0.9611), np.float32(0.9407)] 
2025-01-28 10:17:40.093252: Epoch time: 48.16 s 
2025-01-28 10:17:40.095556: Yayy! New best EMA pseudo Dice: 0.9448000192642212 
2025-01-28 10:17:41.797118:  
2025-01-28 10:17:41.800005: Epoch 829 
2025-01-28 10:17:41.802706: Current learning rate: 0.00204 
2025-01-28 10:18:29.820366: train_loss -0.8278 
2025-01-28 10:18:29.824767: val_loss -0.7359 
2025-01-28 10:18:29.827590: Pseudo dice [np.float32(0.9572), np.float32(0.9248)] 
2025-01-28 10:18:29.830213: Epoch time: 48.02 s 
2025-01-28 10:18:31.496870:  
2025-01-28 10:18:31.500114: Epoch 830 
2025-01-28 10:18:31.502954: Current learning rate: 0.00203 
2025-01-28 10:19:19.533252: train_loss -0.825 
2025-01-28 10:19:19.538252: val_loss -0.7517 
2025-01-28 10:19:19.540712: Pseudo dice [np.float32(0.9486), np.float32(0.9342)] 
2025-01-28 10:19:19.542938: Epoch time: 48.04 s 
2025-01-28 10:19:20.666014:  
2025-01-28 10:19:20.669237: Epoch 831 
2025-01-28 10:19:20.671715: Current learning rate: 0.00202 
2025-01-28 10:20:08.626057: train_loss -0.8271 
2025-01-28 10:20:08.630853: val_loss -0.7918 
2025-01-28 10:20:08.633783: Pseudo dice [np.float32(0.9574), np.float32(0.9311)] 
2025-01-28 10:20:08.636992: Epoch time: 47.96 s 
2025-01-28 10:20:09.764474:  
2025-01-28 10:20:09.767117: Epoch 832 
2025-01-28 10:20:09.769628: Current learning rate: 0.00201 
2025-01-28 10:20:57.849059: train_loss -0.8564 
2025-01-28 10:20:57.857050: val_loss -0.7597 
2025-01-28 10:20:57.859720: Pseudo dice [np.float32(0.9574), np.float32(0.9415)] 
2025-01-28 10:20:57.862434: Epoch time: 48.09 s 
2025-01-28 10:20:58.994922:  
2025-01-28 10:20:58.997893: Epoch 833 
2025-01-28 10:20:59.001109: Current learning rate: 0.002 
2025-01-28 10:21:46.819305: train_loss -0.8373 
2025-01-28 10:21:46.823352: val_loss -0.7976 
2025-01-28 10:21:46.825943: Pseudo dice [np.float32(0.9549), np.float32(0.9352)] 
2025-01-28 10:21:46.828629: Epoch time: 47.83 s 
2025-01-28 10:21:47.983286:  
2025-01-28 10:21:47.986013: Epoch 834 
2025-01-28 10:21:47.988819: Current learning rate: 0.00199 
2025-01-28 10:22:36.204843: train_loss -0.8348 
2025-01-28 10:22:36.210199: val_loss -0.7609 
2025-01-28 10:22:36.212958: Pseudo dice [np.float32(0.9571), np.float32(0.9259)] 
2025-01-28 10:22:36.215619: Epoch time: 48.22 s 
2025-01-28 10:22:37.335247:  
2025-01-28 10:22:37.338190: Epoch 835 
2025-01-28 10:22:37.341191: Current learning rate: 0.00198 
2025-01-28 10:23:25.328998: train_loss -0.8339 
2025-01-28 10:23:25.334701: val_loss -0.7745 
2025-01-28 10:23:25.337443: Pseudo dice [np.float32(0.9579), np.float32(0.9393)] 
2025-01-28 10:23:25.340200: Epoch time: 47.99 s 
2025-01-28 10:23:26.465441:  
2025-01-28 10:23:26.468519: Epoch 836 
2025-01-28 10:23:26.471311: Current learning rate: 0.00196 
2025-01-28 10:24:14.218139: train_loss -0.8384 
2025-01-28 10:24:14.223700: val_loss -0.7708 
2025-01-28 10:24:14.226364: Pseudo dice [np.float32(0.9573), np.float32(0.9351)] 
2025-01-28 10:24:14.228959: Epoch time: 47.75 s 
2025-01-28 10:24:14.231621: Yayy! New best EMA pseudo Dice: 0.9448999762535095 
2025-01-28 10:24:15.977800:  
2025-01-28 10:24:15.980956: Epoch 837 
2025-01-28 10:24:15.984067: Current learning rate: 0.00195 
2025-01-28 10:25:03.818669: train_loss -0.8307 
2025-01-28 10:25:03.823151: val_loss -0.7517 
2025-01-28 10:25:03.826200: Pseudo dice [np.float32(0.9525), np.float32(0.9199)] 
2025-01-28 10:25:03.828831: Epoch time: 47.84 s 
2025-01-28 10:25:04.979030:  
2025-01-28 10:25:04.982045: Epoch 838 
2025-01-28 10:25:04.984866: Current learning rate: 0.00194 
2025-01-28 10:25:53.130895: train_loss -0.8493 
2025-01-28 10:25:53.136297: val_loss -0.7204 
2025-01-28 10:25:53.139038: Pseudo dice [np.float32(0.9565), np.float32(0.9405)] 
2025-01-28 10:25:53.141561: Epoch time: 48.15 s 
2025-01-28 10:25:54.295651:  
2025-01-28 10:25:54.298803: Epoch 839 
2025-01-28 10:25:54.301931: Current learning rate: 0.00193 
2025-01-28 10:26:41.840509: train_loss -0.8439 
2025-01-28 10:26:41.844410: val_loss -0.7988 
2025-01-28 10:26:41.847293: Pseudo dice [np.float32(0.9578), np.float32(0.943)] 
2025-01-28 10:26:41.849943: Epoch time: 47.55 s 
2025-01-28 10:26:41.852320: Yayy! New best EMA pseudo Dice: 0.9451000094413757 
2025-01-28 10:26:43.557662:  
2025-01-28 10:26:43.560630: Epoch 840 
2025-01-28 10:26:43.563441: Current learning rate: 0.00192 
2025-01-28 10:27:31.361070: train_loss -0.8402 
2025-01-28 10:27:31.366479: val_loss -0.7429 
2025-01-28 10:27:31.369521: Pseudo dice [np.float32(0.9605), np.float32(0.9568)] 
2025-01-28 10:27:31.372226: Epoch time: 47.8 s 
2025-01-28 10:27:31.374835: Yayy! New best EMA pseudo Dice: 0.9465000033378601 
2025-01-28 10:27:33.127020:  
2025-01-28 10:27:33.130169: Epoch 841 
2025-01-28 10:27:33.133087: Current learning rate: 0.00191 
2025-01-28 10:28:20.863964: train_loss -0.8356 
2025-01-28 10:28:20.867728: val_loss -0.7851 
2025-01-28 10:28:20.870319: Pseudo dice [np.float32(0.9524), np.float32(0.935)] 
2025-01-28 10:28:20.872811: Epoch time: 47.74 s 
2025-01-28 10:28:22.021344:  
2025-01-28 10:28:22.024408: Epoch 842 
2025-01-28 10:28:22.027190: Current learning rate: 0.0019 
2025-01-28 10:29:09.882463: train_loss -0.8617 
2025-01-28 10:29:09.889745: val_loss -0.7901 
2025-01-28 10:29:09.892715: Pseudo dice [np.float32(0.9542), np.float32(0.9025)] 
2025-01-28 10:29:09.895515: Epoch time: 47.86 s 
2025-01-28 10:29:11.007430:  
2025-01-28 10:29:11.010516: Epoch 843 
2025-01-28 10:29:11.013114: Current learning rate: 0.00189 
2025-01-28 10:29:58.755771: train_loss -0.8603 
2025-01-28 10:29:58.760186: val_loss -0.7601 
2025-01-28 10:29:58.763017: Pseudo dice [np.float32(0.9594), np.float32(0.9409)] 
2025-01-28 10:29:58.765538: Epoch time: 47.75 s 
2025-01-28 10:29:59.909728:  
2025-01-28 10:29:59.912688: Epoch 844 
2025-01-28 10:29:59.915440: Current learning rate: 0.00188 
2025-01-28 10:30:47.780069: train_loss -0.8223 
2025-01-28 10:30:47.786128: val_loss -0.783 
2025-01-28 10:30:47.788736: Pseudo dice [np.float32(0.9563), np.float32(0.9083)] 
2025-01-28 10:30:47.791337: Epoch time: 47.87 s 
2025-01-28 10:30:48.905032:  
2025-01-28 10:30:48.908255: Epoch 845 
2025-01-28 10:30:48.911164: Current learning rate: 0.00187 
2025-01-28 10:31:36.759059: train_loss -0.8272 
2025-01-28 10:31:36.763299: val_loss -0.7605 
2025-01-28 10:31:36.766277: Pseudo dice [np.float32(0.9559), np.float32(0.9195)] 
2025-01-28 10:31:36.768847: Epoch time: 47.85 s 
2025-01-28 10:31:37.926097:  
2025-01-28 10:31:37.928742: Epoch 846 
2025-01-28 10:31:37.931205: Current learning rate: 0.00186 
2025-01-28 10:32:25.719281: train_loss -0.8333 
2025-01-28 10:32:25.724958: val_loss -0.7537 
2025-01-28 10:32:25.727878: Pseudo dice [np.float32(0.9542), np.float32(0.9326)] 
2025-01-28 10:32:25.730819: Epoch time: 47.79 s 
2025-01-28 10:32:26.852287:  
2025-01-28 10:32:26.855568: Epoch 847 
2025-01-28 10:32:26.858634: Current learning rate: 0.00185 
2025-01-28 10:33:14.597602: train_loss -0.8423 
2025-01-28 10:33:14.602033: val_loss -0.7781 
2025-01-28 10:33:14.604869: Pseudo dice [np.float32(0.9601), np.float32(0.9221)] 
2025-01-28 10:33:14.607459: Epoch time: 47.75 s 
2025-01-28 10:33:15.724531:  
2025-01-28 10:33:15.728090: Epoch 848 
2025-01-28 10:33:15.731190: Current learning rate: 0.00184 
2025-01-28 10:34:03.450801: train_loss -0.8255 
2025-01-28 10:34:03.457727: val_loss -0.77 
2025-01-28 10:34:03.460439: Pseudo dice [np.float32(0.9594), np.float32(0.937)] 
2025-01-28 10:34:03.463442: Epoch time: 47.73 s 
2025-01-28 10:34:05.292814:  
2025-01-28 10:34:05.295793: Epoch 849 
2025-01-28 10:34:05.298549: Current learning rate: 0.00182 
2025-01-28 10:34:53.763436: train_loss -0.8405 
2025-01-28 10:34:53.767502: val_loss -0.769 
2025-01-28 10:34:53.770421: Pseudo dice [np.float32(0.9591), np.float32(0.9283)] 
2025-01-28 10:34:53.773154: Epoch time: 48.47 s 
2025-01-28 10:34:55.482638:  
2025-01-28 10:34:55.485364: Epoch 850 
2025-01-28 10:34:55.487803: Current learning rate: 0.00181 
2025-01-28 10:35:43.419998: train_loss -0.8525 
2025-01-28 10:35:43.426329: val_loss -0.7631 
2025-01-28 10:35:43.429092: Pseudo dice [np.float32(0.9581), np.float32(0.9286)] 
2025-01-28 10:35:43.431751: Epoch time: 47.94 s 
2025-01-28 10:35:44.581246:  
2025-01-28 10:35:44.584321: Epoch 851 
2025-01-28 10:35:44.587204: Current learning rate: 0.0018 
2025-01-28 10:36:32.443129: train_loss -0.8429 
2025-01-28 10:36:32.447633: val_loss -0.7198 
2025-01-28 10:36:32.450658: Pseudo dice [np.float32(0.9571), np.float32(0.9216)] 
2025-01-28 10:36:32.453589: Epoch time: 47.86 s 
2025-01-28 10:36:33.554979:  
2025-01-28 10:36:33.558218: Epoch 852 
2025-01-28 10:36:33.561041: Current learning rate: 0.00179 
2025-01-28 10:37:21.817816: train_loss -0.8397 
2025-01-28 10:37:21.823550: val_loss -0.7623 
2025-01-28 10:37:21.826159: Pseudo dice [np.float32(0.9475), np.float32(0.9316)] 
2025-01-28 10:37:21.828806: Epoch time: 48.26 s 
2025-01-28 10:37:22.959908:  
2025-01-28 10:37:22.962850: Epoch 853 
2025-01-28 10:37:22.965753: Current learning rate: 0.00178 
2025-01-28 10:38:11.047241: train_loss -0.8397 
2025-01-28 10:38:11.050907: val_loss -0.7831 
2025-01-28 10:38:11.053487: Pseudo dice [np.float32(0.9516), np.float32(0.9227)] 
2025-01-28 10:38:11.056082: Epoch time: 48.09 s 
2025-01-28 10:38:12.156060:  
2025-01-28 10:38:12.159048: Epoch 854 
2025-01-28 10:38:12.161684: Current learning rate: 0.00177 
2025-01-28 10:38:59.936007: train_loss -0.8478 
2025-01-28 10:38:59.940959: val_loss -0.7295 
2025-01-28 10:38:59.943542: Pseudo dice [np.float32(0.9408), np.float32(0.8993)] 
2025-01-28 10:38:59.946265: Epoch time: 47.78 s 
2025-01-28 10:39:01.079177:  
2025-01-28 10:39:01.081952: Epoch 855 
2025-01-28 10:39:01.084710: Current learning rate: 0.00176 
2025-01-28 10:39:49.135189: train_loss -0.8382 
2025-01-28 10:39:49.139254: val_loss -0.7417 
2025-01-28 10:39:49.142097: Pseudo dice [np.float32(0.9598), np.float32(0.9387)] 
2025-01-28 10:39:49.144930: Epoch time: 48.06 s 
2025-01-28 10:39:50.271127:  
2025-01-28 10:39:50.273967: Epoch 856 
2025-01-28 10:39:50.276571: Current learning rate: 0.00175 
2025-01-28 10:40:38.668137: train_loss -0.8424 
2025-01-28 10:40:38.674370: val_loss -0.7475 
2025-01-28 10:40:38.677342: Pseudo dice [np.float32(0.9531), np.float32(0.9389)] 
2025-01-28 10:40:38.679991: Epoch time: 48.4 s 
2025-01-28 10:40:39.792286:  
2025-01-28 10:40:39.795431: Epoch 857 
2025-01-28 10:40:39.798286: Current learning rate: 0.00174 
2025-01-28 10:41:28.201231: train_loss -0.8429 
2025-01-28 10:41:28.205845: val_loss -0.7623 
2025-01-28 10:41:28.209020: Pseudo dice [np.float32(0.9612), np.float32(0.9327)] 
2025-01-28 10:41:28.211910: Epoch time: 48.41 s 
2025-01-28 10:41:29.344394:  
2025-01-28 10:41:29.349408: Epoch 858 
2025-01-28 10:41:29.352692: Current learning rate: 0.00173 
2025-01-28 10:42:17.500671: train_loss -0.8388 
2025-01-28 10:42:17.507781: val_loss -0.7822 
2025-01-28 10:42:17.510534: Pseudo dice [np.float32(0.959), np.float32(0.9357)] 
2025-01-28 10:42:17.513427: Epoch time: 48.16 s 
2025-01-28 10:42:18.617507:  
2025-01-28 10:42:18.620874: Epoch 859 
2025-01-28 10:42:18.623677: Current learning rate: 0.00172 
2025-01-28 10:43:07.341634: train_loss -0.8509 
2025-01-28 10:43:07.345726: val_loss -0.798 
2025-01-28 10:43:07.348588: Pseudo dice [np.float32(0.9534), np.float32(0.9364)] 
2025-01-28 10:43:07.351169: Epoch time: 48.73 s 
2025-01-28 10:43:08.455991:  
2025-01-28 10:43:08.459217: Epoch 860 
2025-01-28 10:43:08.461978: Current learning rate: 0.0017 
2025-01-28 10:43:56.613909: train_loss -0.8301 
2025-01-28 10:43:56.619562: val_loss -0.7566 
2025-01-28 10:43:56.622441: Pseudo dice [np.float32(0.9638), np.float32(0.9344)] 
2025-01-28 10:43:56.625041: Epoch time: 48.16 s 
2025-01-28 10:43:57.742031:  
2025-01-28 10:43:57.745052: Epoch 861 
2025-01-28 10:43:57.747927: Current learning rate: 0.00169 
2025-01-28 10:44:45.482494: train_loss -0.8493 
2025-01-28 10:44:45.486711: val_loss -0.7505 
2025-01-28 10:44:45.489697: Pseudo dice [np.float32(0.9618), np.float32(0.9283)] 
2025-01-28 10:44:45.492181: Epoch time: 47.74 s 
2025-01-28 10:44:46.614992:  
2025-01-28 10:44:46.617961: Epoch 862 
2025-01-28 10:44:46.621071: Current learning rate: 0.00168 
2025-01-28 10:45:34.857881: train_loss -0.8356 
2025-01-28 10:45:34.863412: val_loss -0.7729 
2025-01-28 10:45:34.866414: Pseudo dice [np.float32(0.9509), np.float32(0.9225)] 
2025-01-28 10:45:34.868853: Epoch time: 48.24 s 
2025-01-28 10:45:35.979035:  
2025-01-28 10:45:35.981969: Epoch 863 
2025-01-28 10:45:35.984981: Current learning rate: 0.00167 
2025-01-28 10:46:24.548197: train_loss -0.8396 
2025-01-28 10:46:24.552379: val_loss -0.739 
2025-01-28 10:46:24.555135: Pseudo dice [np.float32(0.9616), np.float32(0.9294)] 
2025-01-28 10:46:24.557791: Epoch time: 48.57 s 
2025-01-28 10:46:25.695492:  
2025-01-28 10:46:25.699237: Epoch 864 
2025-01-28 10:46:25.701969: Current learning rate: 0.00166 
2025-01-28 10:47:13.844399: train_loss -0.8403 
2025-01-28 10:47:13.849758: val_loss -0.7627 
2025-01-28 10:47:13.852179: Pseudo dice [np.float32(0.9545), np.float32(0.943)] 
2025-01-28 10:47:13.854656: Epoch time: 48.15 s 
2025-01-28 10:47:14.962868:  
2025-01-28 10:47:14.966674: Epoch 865 
2025-01-28 10:47:14.969629: Current learning rate: 0.00165 
2025-01-28 10:48:03.284602: train_loss -0.838 
2025-01-28 10:48:03.290868: val_loss -0.7163 
2025-01-28 10:48:03.293990: Pseudo dice [np.float32(0.9616), np.float32(0.9313)] 
2025-01-28 10:48:03.296754: Epoch time: 48.32 s 
2025-01-28 10:48:04.405869:  
2025-01-28 10:48:04.408451: Epoch 866 
2025-01-28 10:48:04.411303: Current learning rate: 0.00164 
2025-01-28 10:48:52.539522: train_loss -0.8416 
2025-01-28 10:48:52.545602: val_loss -0.7649 
2025-01-28 10:48:52.548187: Pseudo dice [np.float32(0.952), np.float32(0.9233)] 
2025-01-28 10:48:52.550788: Epoch time: 48.13 s 
2025-01-28 10:48:53.659042:  
2025-01-28 10:48:53.661810: Epoch 867 
2025-01-28 10:48:53.664458: Current learning rate: 0.00163 
2025-01-28 10:49:41.303662: train_loss -0.8388 
2025-01-28 10:49:41.308473: val_loss -0.7525 
2025-01-28 10:49:41.311082: Pseudo dice [np.float32(0.9582), np.float32(0.9277)] 
2025-01-28 10:49:41.313750: Epoch time: 47.65 s 
2025-01-28 10:49:42.431544:  
2025-01-28 10:49:42.434046: Epoch 868 
2025-01-28 10:49:42.436543: Current learning rate: 0.00162 
2025-01-28 10:50:30.710933: train_loss -0.8515 
2025-01-28 10:50:30.716216: val_loss -0.7905 
2025-01-28 10:50:30.718820: Pseudo dice [np.float32(0.9498), np.float32(0.9175)] 
2025-01-28 10:50:30.721434: Epoch time: 48.28 s 
2025-01-28 10:50:32.449080:  
2025-01-28 10:50:32.452567: Epoch 869 
2025-01-28 10:50:32.455292: Current learning rate: 0.00161 
2025-01-28 10:51:20.462010: train_loss -0.8339 
2025-01-28 10:51:20.465739: val_loss -0.7416 
2025-01-28 10:51:20.468220: Pseudo dice [np.float32(0.9546), np.float32(0.9459)] 
2025-01-28 10:51:20.470640: Epoch time: 48.01 s 
2025-01-28 10:51:21.532566:  
2025-01-28 10:51:21.535312: Epoch 870 
2025-01-28 10:51:21.537956: Current learning rate: 0.00159 
2025-01-28 10:52:09.409966: train_loss -0.8448 
2025-01-28 10:52:09.415586: val_loss -0.7665 
2025-01-28 10:52:09.418345: Pseudo dice [np.float32(0.9593), np.float32(0.94)] 
2025-01-28 10:52:09.420961: Epoch time: 47.88 s 
2025-01-28 10:52:10.527482:  
2025-01-28 10:52:10.530488: Epoch 871 
2025-01-28 10:52:10.533517: Current learning rate: 0.00158 
2025-01-28 10:52:58.569359: train_loss -0.8447 
2025-01-28 10:52:58.573420: val_loss -0.7679 
2025-01-28 10:52:58.576557: Pseudo dice [np.float32(0.9622), np.float32(0.9397)] 
2025-01-28 10:52:58.579029: Epoch time: 48.04 s 
2025-01-28 10:52:59.692891:  
2025-01-28 10:52:59.695926: Epoch 872 
2025-01-28 10:52:59.698735: Current learning rate: 0.00157 
2025-01-28 10:53:47.855497: train_loss -0.8515 
2025-01-28 10:53:47.862004: val_loss -0.7595 
2025-01-28 10:53:47.864725: Pseudo dice [np.float32(0.9578), np.float32(0.9219)] 
2025-01-28 10:53:47.867243: Epoch time: 48.16 s 
2025-01-28 10:53:48.963214:  
2025-01-28 10:53:48.966081: Epoch 873 
2025-01-28 10:53:48.968857: Current learning rate: 0.00156 
2025-01-28 10:54:36.656412: train_loss -0.8432 
2025-01-28 10:54:36.660676: val_loss -0.7759 
2025-01-28 10:54:36.663225: Pseudo dice [np.float32(0.9559), np.float32(0.9371)] 
2025-01-28 10:54:36.665632: Epoch time: 47.69 s 
2025-01-28 10:54:37.761889:  
2025-01-28 10:54:37.765263: Epoch 874 
2025-01-28 10:54:37.768251: Current learning rate: 0.00155 
2025-01-28 10:55:25.680277: train_loss -0.8456 
2025-01-28 10:55:25.685732: val_loss -0.7839 
2025-01-28 10:55:25.688262: Pseudo dice [np.float32(0.9564), np.float32(0.9277)] 
2025-01-28 10:55:25.691029: Epoch time: 47.92 s 
2025-01-28 10:55:26.789449:  
2025-01-28 10:55:26.794128: Epoch 875 
2025-01-28 10:55:26.796877: Current learning rate: 0.00154 
2025-01-28 10:56:14.696430: train_loss -0.8319 
2025-01-28 10:56:14.701252: val_loss -0.7484 
2025-01-28 10:56:14.704684: Pseudo dice [np.float32(0.9489), np.float32(0.9232)] 
2025-01-28 10:56:14.707781: Epoch time: 47.91 s 
2025-01-28 10:56:15.776448:  
2025-01-28 10:56:15.779406: Epoch 876 
2025-01-28 10:56:15.781847: Current learning rate: 0.00153 
2025-01-28 10:57:03.758631: train_loss -0.8486 
2025-01-28 10:57:03.764760: val_loss -0.7837 
2025-01-28 10:57:03.767777: Pseudo dice [np.float32(0.9541), np.float32(0.9275)] 
2025-01-28 10:57:03.770854: Epoch time: 47.98 s 
2025-01-28 10:57:04.867915:  
2025-01-28 10:57:04.870744: Epoch 877 
2025-01-28 10:57:04.873516: Current learning rate: 0.00152 
2025-01-28 10:57:52.807411: train_loss -0.862 
2025-01-28 10:57:52.811982: val_loss -0.7495 
2025-01-28 10:57:52.814914: Pseudo dice [np.float32(0.9575), np.float32(0.9196)] 
2025-01-28 10:57:52.817818: Epoch time: 47.94 s 
2025-01-28 10:57:53.913187:  
2025-01-28 10:57:53.916203: Epoch 878 
2025-01-28 10:57:53.918915: Current learning rate: 0.00151 
2025-01-28 10:58:41.498947: train_loss -0.8339 
2025-01-28 10:58:41.504293: val_loss -0.7403 
2025-01-28 10:58:41.506766: Pseudo dice [np.float32(0.9545), np.float32(0.9333)] 
2025-01-28 10:58:41.509215: Epoch time: 47.59 s 
2025-01-28 10:58:42.606548:  
2025-01-28 10:58:42.609681: Epoch 879 
2025-01-28 10:58:42.612367: Current learning rate: 0.00149 
2025-01-28 10:59:30.843580: train_loss -0.8432 
2025-01-28 10:59:30.847445: val_loss -0.7975 
2025-01-28 10:59:30.850008: Pseudo dice [np.float32(0.9553), np.float32(0.9358)] 
2025-01-28 10:59:30.852400: Epoch time: 48.24 s 
2025-01-28 10:59:31.941746:  
2025-01-28 10:59:31.944700: Epoch 880 
2025-01-28 10:59:31.947100: Current learning rate: 0.00148 
2025-01-28 11:00:19.928793: train_loss -0.8517 
2025-01-28 11:00:19.935767: val_loss -0.7361 
2025-01-28 11:00:19.938986: Pseudo dice [np.float32(0.9594), np.float32(0.9483)] 
2025-01-28 11:00:19.942360: Epoch time: 47.99 s 
2025-01-28 11:00:21.037005:  
2025-01-28 11:00:21.039708: Epoch 881 
2025-01-28 11:00:21.042434: Current learning rate: 0.00147 
2025-01-28 11:01:09.146974: train_loss -0.8475 
2025-01-28 11:01:09.152371: val_loss -0.7638 
2025-01-28 11:01:09.156177: Pseudo dice [np.float32(0.9565), np.float32(0.9315)] 
2025-01-28 11:01:09.160453: Epoch time: 48.11 s 
2025-01-28 11:01:10.269055:  
2025-01-28 11:01:10.272692: Epoch 882 
2025-01-28 11:01:10.276449: Current learning rate: 0.00146 
2025-01-28 11:01:58.881202: train_loss -0.8289 
2025-01-28 11:01:58.887345: val_loss -0.7381 
2025-01-28 11:01:58.890216: Pseudo dice [np.float32(0.9577), np.float32(0.9271)] 
2025-01-28 11:01:58.892951: Epoch time: 48.61 s 
2025-01-28 11:01:59.993801:  
2025-01-28 11:01:59.997501: Epoch 883 
2025-01-28 11:02:00.003301: Current learning rate: 0.00145 
2025-01-28 11:02:48.279775: train_loss -0.8493 
2025-01-28 11:02:48.283809: val_loss -0.7383 
2025-01-28 11:02:48.286588: Pseudo dice [np.float32(0.9613), np.float32(0.9296)] 
2025-01-28 11:02:48.289382: Epoch time: 48.29 s 
2025-01-28 11:02:49.404788:  
2025-01-28 11:02:49.407746: Epoch 884 
2025-01-28 11:02:49.410554: Current learning rate: 0.00144 
2025-01-28 11:03:37.896573: train_loss -0.8307 
2025-01-28 11:03:37.903164: val_loss -0.7239 
2025-01-28 11:03:37.906679: Pseudo dice [np.float32(0.9583), np.float32(0.9129)] 
2025-01-28 11:03:37.910192: Epoch time: 48.49 s 
2025-01-28 11:03:39.034610:  
2025-01-28 11:03:39.037815: Epoch 885 
2025-01-28 11:03:39.040835: Current learning rate: 0.00143 
2025-01-28 11:04:26.977853: train_loss -0.8465 
2025-01-28 11:04:26.982343: val_loss -0.7744 
2025-01-28 11:04:26.985423: Pseudo dice [np.float32(0.9589), np.float32(0.9457)] 
2025-01-28 11:04:26.988278: Epoch time: 47.94 s 
2025-01-28 11:04:28.092881:  
2025-01-28 11:04:28.095907: Epoch 886 
2025-01-28 11:04:28.098890: Current learning rate: 0.00142 
2025-01-28 11:05:16.345603: train_loss -0.8291 
2025-01-28 11:05:16.353245: val_loss -0.7999 
2025-01-28 11:05:16.356411: Pseudo dice [np.float32(0.9593), np.float32(0.9423)] 
2025-01-28 11:05:16.359087: Epoch time: 48.25 s 
2025-01-28 11:05:17.472346:  
2025-01-28 11:05:17.475664: Epoch 887 
2025-01-28 11:05:17.478879: Current learning rate: 0.00141 
2025-01-28 11:06:05.602857: train_loss -0.864 
2025-01-28 11:06:05.607095: val_loss -0.7618 
2025-01-28 11:06:05.609735: Pseudo dice [np.float32(0.9611), np.float32(0.9455)] 
2025-01-28 11:06:05.612039: Epoch time: 48.13 s 
2025-01-28 11:06:07.295591:  
2025-01-28 11:06:07.298326: Epoch 888 
2025-01-28 11:06:07.301049: Current learning rate: 0.00139 
2025-01-28 11:06:55.241591: train_loss -0.849 
2025-01-28 11:06:55.246646: val_loss -0.7823 
2025-01-28 11:06:55.249510: Pseudo dice [np.float32(0.9588), np.float32(0.9525)] 
2025-01-28 11:06:55.252444: Epoch time: 47.95 s 
2025-01-28 11:06:55.254874: Yayy! New best EMA pseudo Dice: 0.9466000199317932 
2025-01-28 11:06:56.900150:  
2025-01-28 11:06:56.902723: Epoch 889 
2025-01-28 11:06:56.905243: Current learning rate: 0.00138 
2025-01-28 11:07:44.780117: train_loss -0.829 
2025-01-28 11:07:44.784002: val_loss -0.7273 
2025-01-28 11:07:44.788765: Pseudo dice [np.float32(0.9581), np.float32(0.9219)] 
2025-01-28 11:07:44.791317: Epoch time: 47.88 s 
2025-01-28 11:07:45.883420:  
2025-01-28 11:07:45.885633: Epoch 890 
2025-01-28 11:07:45.887934: Current learning rate: 0.00137 
2025-01-28 11:08:34.058178: train_loss -0.8457 
2025-01-28 11:08:34.063664: val_loss -0.721 
2025-01-28 11:08:34.066461: Pseudo dice [np.float32(0.9602), np.float32(0.9317)] 
2025-01-28 11:08:34.068901: Epoch time: 48.18 s 
2025-01-28 11:08:35.182044:  
2025-01-28 11:08:35.185257: Epoch 891 
2025-01-28 11:08:35.187749: Current learning rate: 0.00136 
2025-01-28 11:09:23.360947: train_loss -0.8436 
2025-01-28 11:09:23.365695: val_loss -0.7147 
2025-01-28 11:09:23.368541: Pseudo dice [np.float32(0.9567), np.float32(0.928)] 
2025-01-28 11:09:23.371171: Epoch time: 48.18 s 
2025-01-28 11:09:24.485365:  
2025-01-28 11:09:24.488472: Epoch 892 
2025-01-28 11:09:24.492229: Current learning rate: 0.00135 
2025-01-28 11:10:12.514209: train_loss -0.8485 
2025-01-28 11:10:12.521492: val_loss -0.7674 
2025-01-28 11:10:12.524028: Pseudo dice [np.float32(0.958), np.float32(0.9377)] 
2025-01-28 11:10:12.526325: Epoch time: 48.03 s 
2025-01-28 11:10:13.631895:  
2025-01-28 11:10:13.634907: Epoch 893 
2025-01-28 11:10:13.637421: Current learning rate: 0.00134 
2025-01-28 11:11:01.581485: train_loss -0.8467 
2025-01-28 11:11:01.585447: val_loss -0.7357 
2025-01-28 11:11:01.588422: Pseudo dice [np.float32(0.9565), np.float32(0.9273)] 
2025-01-28 11:11:01.591334: Epoch time: 47.95 s 
2025-01-28 11:11:02.698445:  
2025-01-28 11:11:02.701486: Epoch 894 
2025-01-28 11:11:02.704653: Current learning rate: 0.00133 
2025-01-28 11:11:51.096665: train_loss -0.8463 
2025-01-28 11:11:51.102771: val_loss -0.7555 
2025-01-28 11:11:51.105714: Pseudo dice [np.float32(0.9599), np.float32(0.944)] 
2025-01-28 11:11:51.108507: Epoch time: 48.4 s 
2025-01-28 11:11:52.216536:  
2025-01-28 11:11:52.219491: Epoch 895 
2025-01-28 11:11:52.222448: Current learning rate: 0.00132 
2025-01-28 11:12:40.050080: train_loss -0.8549 
2025-01-28 11:12:40.054610: val_loss -0.731 
2025-01-28 11:12:40.057546: Pseudo dice [np.float32(0.9599), np.float32(0.9358)] 
2025-01-28 11:12:40.060164: Epoch time: 47.83 s 
2025-01-28 11:12:41.164072:  
2025-01-28 11:12:41.166865: Epoch 896 
2025-01-28 11:12:41.169863: Current learning rate: 0.0013 
2025-01-28 11:13:29.311776: train_loss -0.8439 
2025-01-28 11:13:29.318232: val_loss -0.7507 
2025-01-28 11:13:29.320731: Pseudo dice [np.float32(0.9594), np.float32(0.9368)] 
2025-01-28 11:13:29.323025: Epoch time: 48.15 s 
2025-01-28 11:13:30.428852:  
2025-01-28 11:13:30.432077: Epoch 897 
2025-01-28 11:13:30.434542: Current learning rate: 0.00129 
2025-01-28 11:14:18.155654: train_loss -0.8511 
2025-01-28 11:14:18.159567: val_loss -0.7846 
2025-01-28 11:14:18.162476: Pseudo dice [np.float32(0.9557), np.float32(0.9329)] 
2025-01-28 11:14:18.165109: Epoch time: 47.73 s 
2025-01-28 11:14:19.298994:  
2025-01-28 11:14:19.302336: Epoch 898 
2025-01-28 11:14:19.304964: Current learning rate: 0.00128 
2025-01-28 11:15:07.106260: train_loss -0.8543 
2025-01-28 11:15:07.111159: val_loss -0.7262 
2025-01-28 11:15:07.113754: Pseudo dice [np.float32(0.961), np.float32(0.9324)] 
2025-01-28 11:15:07.116520: Epoch time: 47.81 s 
2025-01-28 11:15:08.221624:  
2025-01-28 11:15:08.224550: Epoch 899 
2025-01-28 11:15:08.227238: Current learning rate: 0.00127 
2025-01-28 11:15:56.108087: train_loss -0.8371 
2025-01-28 11:15:56.112114: val_loss -0.7814 
2025-01-28 11:15:56.114734: Pseudo dice [np.float32(0.9582), np.float32(0.9298)] 
2025-01-28 11:15:56.117189: Epoch time: 47.89 s 
2025-01-28 11:15:57.776238:  
2025-01-28 11:15:57.778988: Epoch 900 
2025-01-28 11:15:57.781629: Current learning rate: 0.00126 
2025-01-28 11:16:45.998658: train_loss -0.8621 
2025-01-28 11:16:46.004744: val_loss -0.7621 
2025-01-28 11:16:46.007219: Pseudo dice [np.float32(0.9598), np.float32(0.9154)] 
2025-01-28 11:16:46.009614: Epoch time: 48.22 s 
2025-01-28 11:16:47.119289:  
2025-01-28 11:16:47.121821: Epoch 901 
2025-01-28 11:16:47.124349: Current learning rate: 0.00125 
2025-01-28 11:17:34.954540: train_loss -0.8472 
2025-01-28 11:17:34.958542: val_loss -0.7866 
2025-01-28 11:17:34.961093: Pseudo dice [np.float32(0.953), np.float32(0.9153)] 
2025-01-28 11:17:34.963760: Epoch time: 47.84 s 
2025-01-28 11:17:36.060155:  
2025-01-28 11:17:36.062872: Epoch 902 
2025-01-28 11:17:36.065609: Current learning rate: 0.00124 
2025-01-28 11:18:24.035479: train_loss -0.8417 
2025-01-28 11:18:24.040330: val_loss -0.7631 
2025-01-28 11:18:24.043099: Pseudo dice [np.float32(0.9579), np.float32(0.9241)] 
2025-01-28 11:18:24.045626: Epoch time: 47.98 s 
2025-01-28 11:18:25.135691:  
2025-01-28 11:18:25.138700: Epoch 903 
2025-01-28 11:18:25.141572: Current learning rate: 0.00122 
2025-01-28 11:19:13.097232: train_loss -0.835 
2025-01-28 11:19:13.101193: val_loss -0.809 
2025-01-28 11:19:13.108889: Pseudo dice [np.float32(0.9596), np.float32(0.942)] 
2025-01-28 11:19:13.111216: Epoch time: 47.96 s 
2025-01-28 11:19:14.208895:  
2025-01-28 11:19:14.211584: Epoch 904 
2025-01-28 11:19:14.214009: Current learning rate: 0.00121 
2025-01-28 11:20:02.291426: train_loss -0.8603 
2025-01-28 11:20:02.296508: val_loss -0.7602 
2025-01-28 11:20:02.299294: Pseudo dice [np.float32(0.9522), np.float32(0.9306)] 
2025-01-28 11:20:02.301920: Epoch time: 48.08 s 
2025-01-28 11:20:03.379300:  
2025-01-28 11:20:03.382072: Epoch 905 
2025-01-28 11:20:03.384653: Current learning rate: 0.0012 
2025-01-28 11:20:51.138551: train_loss -0.8605 
2025-01-28 11:20:51.142530: val_loss -0.7411 
2025-01-28 11:20:51.145334: Pseudo dice [np.float32(0.9553), np.float32(0.9391)] 
2025-01-28 11:20:51.147914: Epoch time: 47.76 s 
2025-01-28 11:20:52.242891:  
2025-01-28 11:20:52.245436: Epoch 906 
2025-01-28 11:20:52.248152: Current learning rate: 0.00119 
2025-01-28 11:21:40.450593: train_loss -0.8211 
2025-01-28 11:21:40.456211: val_loss -0.7377 
2025-01-28 11:21:40.458832: Pseudo dice [np.float32(0.9563), np.float32(0.9226)] 
2025-01-28 11:21:40.461456: Epoch time: 48.21 s 
2025-01-28 11:21:41.553521:  
2025-01-28 11:21:41.556658: Epoch 907 
2025-01-28 11:21:41.559296: Current learning rate: 0.00118 
2025-01-28 11:22:30.025068: train_loss -0.8403 
2025-01-28 11:22:30.028723: val_loss -0.7597 
2025-01-28 11:22:30.031492: Pseudo dice [np.float32(0.9567), np.float32(0.9321)] 
2025-01-28 11:22:30.033931: Epoch time: 48.47 s 
2025-01-28 11:22:31.659894:  
2025-01-28 11:22:31.662582: Epoch 908 
2025-01-28 11:22:31.665155: Current learning rate: 0.00117 
2025-01-28 11:23:19.566825: train_loss -0.8451 
2025-01-28 11:23:19.574376: val_loss -0.7297 
2025-01-28 11:23:19.577157: Pseudo dice [np.float32(0.9529), np.float32(0.9294)] 
2025-01-28 11:23:19.579768: Epoch time: 47.91 s 
2025-01-28 11:23:20.699545:  
2025-01-28 11:23:20.702378: Epoch 909 
2025-01-28 11:23:20.704902: Current learning rate: 0.00116 
2025-01-28 11:24:08.809918: train_loss -0.8679 
2025-01-28 11:24:08.813579: val_loss -0.7505 
2025-01-28 11:24:08.815934: Pseudo dice [np.float32(0.9515), np.float32(0.9365)] 
2025-01-28 11:24:08.818468: Epoch time: 48.11 s 
2025-01-28 11:24:09.932073:  
2025-01-28 11:24:09.934425: Epoch 910 
2025-01-28 11:24:09.936946: Current learning rate: 0.00115 
2025-01-28 11:24:57.930783: train_loss -0.8473 
2025-01-28 11:24:57.936321: val_loss -0.7828 
2025-01-28 11:24:57.939002: Pseudo dice [np.float32(0.9568), np.float32(0.9412)] 
2025-01-28 11:24:57.941593: Epoch time: 48.0 s 
2025-01-28 11:24:59.036335:  
2025-01-28 11:24:59.039409: Epoch 911 
2025-01-28 11:24:59.042093: Current learning rate: 0.00113 
2025-01-28 11:25:47.104252: train_loss -0.8596 
2025-01-28 11:25:47.108275: val_loss -0.7673 
2025-01-28 11:25:47.111204: Pseudo dice [np.float32(0.9565), np.float32(0.9454)] 
2025-01-28 11:25:47.113510: Epoch time: 48.07 s 
2025-01-28 11:25:48.214083:  
2025-01-28 11:25:48.217087: Epoch 912 
2025-01-28 11:25:48.219973: Current learning rate: 0.00112 
2025-01-28 11:26:36.295844: train_loss -0.855 
2025-01-28 11:26:36.301260: val_loss -0.7804 
2025-01-28 11:26:36.303944: Pseudo dice [np.float32(0.952), np.float32(0.939)] 
2025-01-28 11:26:36.306769: Epoch time: 48.08 s 
2025-01-28 11:26:37.410926:  
2025-01-28 11:26:37.413664: Epoch 913 
2025-01-28 11:26:37.416194: Current learning rate: 0.00111 
2025-01-28 11:27:25.370870: train_loss -0.8475 
2025-01-28 11:27:25.374944: val_loss -0.7455 
2025-01-28 11:27:25.377486: Pseudo dice [np.float32(0.9573), np.float32(0.9328)] 
2025-01-28 11:27:25.379869: Epoch time: 47.96 s 
2025-01-28 11:27:26.491559:  
2025-01-28 11:27:26.494352: Epoch 914 
2025-01-28 11:27:26.497171: Current learning rate: 0.0011 
2025-01-28 11:28:15.364688: train_loss -0.8557 
2025-01-28 11:28:15.370856: val_loss -0.7537 
2025-01-28 11:28:15.373141: Pseudo dice [np.float32(0.9613), np.float32(0.9417)] 
2025-01-28 11:28:15.375974: Epoch time: 48.87 s 
2025-01-28 11:28:16.485171:  
2025-01-28 11:28:16.488122: Epoch 915 
2025-01-28 11:28:16.490630: Current learning rate: 0.00109 
2025-01-28 11:29:04.576967: train_loss -0.8537 
2025-01-28 11:29:04.580878: val_loss -0.7808 
2025-01-28 11:29:04.583523: Pseudo dice [np.float32(0.9576), np.float32(0.94)] 
2025-01-28 11:29:04.586025: Epoch time: 48.09 s 
2025-01-28 11:29:05.686156:  
2025-01-28 11:29:05.689073: Epoch 916 
2025-01-28 11:29:05.691538: Current learning rate: 0.00108 
2025-01-28 11:29:54.000241: train_loss -0.8467 
2025-01-28 11:29:54.005757: val_loss -0.7563 
2025-01-28 11:29:54.008621: Pseudo dice [np.float32(0.9629), np.float32(0.9195)] 
2025-01-28 11:29:54.011239: Epoch time: 48.31 s 
2025-01-28 11:29:55.116518:  
2025-01-28 11:29:55.119307: Epoch 917 
2025-01-28 11:29:55.121999: Current learning rate: 0.00106 
2025-01-28 11:30:43.231703: train_loss -0.8621 
2025-01-28 11:30:43.235855: val_loss -0.7752 
2025-01-28 11:30:43.238596: Pseudo dice [np.float32(0.9618), np.float32(0.9309)] 
2025-01-28 11:30:43.241261: Epoch time: 48.12 s 
2025-01-28 11:30:44.349250:  
2025-01-28 11:30:44.351733: Epoch 918 
2025-01-28 11:30:44.354486: Current learning rate: 0.00105 
2025-01-28 11:31:32.458350: train_loss -0.847 
2025-01-28 11:31:32.463062: val_loss -0.7585 
2025-01-28 11:31:32.465339: Pseudo dice [np.float32(0.9603), np.float32(0.9299)] 
2025-01-28 11:31:32.467561: Epoch time: 48.11 s 
2025-01-28 11:31:33.564769:  
2025-01-28 11:31:33.567625: Epoch 919 
2025-01-28 11:31:33.570348: Current learning rate: 0.00104 
2025-01-28 11:32:21.826103: train_loss -0.8393 
2025-01-28 11:32:21.830697: val_loss -0.776 
2025-01-28 11:32:21.833498: Pseudo dice [np.float32(0.9532), np.float32(0.9357)] 
2025-01-28 11:32:21.835893: Epoch time: 48.26 s 
2025-01-28 11:32:22.943147:  
2025-01-28 11:32:22.946023: Epoch 920 
2025-01-28 11:32:22.948646: Current learning rate: 0.00103 
2025-01-28 11:33:11.096021: train_loss -0.8598 
2025-01-28 11:33:11.101124: val_loss -0.7546 
2025-01-28 11:33:11.103725: Pseudo dice [np.float32(0.9546), np.float32(0.9345)] 
2025-01-28 11:33:11.106117: Epoch time: 48.15 s 
2025-01-28 11:33:12.213833:  
2025-01-28 11:33:12.216453: Epoch 921 
2025-01-28 11:33:12.218993: Current learning rate: 0.00102 
2025-01-28 11:34:00.615938: train_loss -0.8589 
2025-01-28 11:34:00.620577: val_loss -0.768 
2025-01-28 11:34:00.623189: Pseudo dice [np.float32(0.9534), np.float32(0.9367)] 
2025-01-28 11:34:00.625732: Epoch time: 48.4 s 
2025-01-28 11:34:01.727058:  
2025-01-28 11:34:01.729708: Epoch 922 
2025-01-28 11:34:01.732474: Current learning rate: 0.00101 
2025-01-28 11:34:49.706209: train_loss -0.8507 
2025-01-28 11:34:49.712075: val_loss -0.8021 
2025-01-28 11:34:49.714651: Pseudo dice [np.float32(0.9556), np.float32(0.9388)] 
2025-01-28 11:34:49.717157: Epoch time: 47.98 s 
2025-01-28 11:34:50.820178:  
2025-01-28 11:34:50.823092: Epoch 923 
2025-01-28 11:34:50.825881: Current learning rate: 0.001 
2025-01-28 11:35:38.856050: train_loss -0.8545 
2025-01-28 11:35:38.860200: val_loss -0.7467 
2025-01-28 11:35:38.862968: Pseudo dice [np.float32(0.9615), np.float32(0.9382)] 
2025-01-28 11:35:38.865622: Epoch time: 48.04 s 
2025-01-28 11:35:39.976137:  
2025-01-28 11:35:39.979045: Epoch 924 
2025-01-28 11:35:39.981600: Current learning rate: 0.00098 
2025-01-28 11:36:27.906632: train_loss -0.8611 
2025-01-28 11:36:27.912215: val_loss -0.7961 
2025-01-28 11:36:27.915056: Pseudo dice [np.float32(0.9536), np.float32(0.9442)] 
2025-01-28 11:36:27.917645: Epoch time: 47.93 s 
2025-01-28 11:36:29.027066:  
2025-01-28 11:36:29.029902: Epoch 925 
2025-01-28 11:36:29.032474: Current learning rate: 0.00097 
2025-01-28 11:37:16.985219: train_loss -0.8397 
2025-01-28 11:37:16.989217: val_loss -0.7791 
2025-01-28 11:37:16.991904: Pseudo dice [np.float32(0.9536), np.float32(0.9419)] 
2025-01-28 11:37:16.994252: Epoch time: 47.96 s 
2025-01-28 11:37:18.091850:  
2025-01-28 11:37:18.094572: Epoch 926 
2025-01-28 11:37:18.097253: Current learning rate: 0.00096 
2025-01-28 11:38:06.672580: train_loss -0.8556 
2025-01-28 11:38:06.678636: val_loss -0.7567 
2025-01-28 11:38:06.681379: Pseudo dice [np.float32(0.9606), np.float32(0.927)] 
2025-01-28 11:38:06.683876: Epoch time: 48.58 s 
2025-01-28 11:38:08.309069:  
2025-01-28 11:38:08.311921: Epoch 927 
2025-01-28 11:38:08.314442: Current learning rate: 0.00095 
2025-01-28 11:38:56.417116: train_loss -0.8569 
2025-01-28 11:38:56.423215: val_loss -0.738 
2025-01-28 11:38:56.425784: Pseudo dice [np.float32(0.9619), np.float32(0.9284)] 
2025-01-28 11:38:56.428267: Epoch time: 48.11 s 
2025-01-28 11:38:57.530945:  
2025-01-28 11:38:57.534010: Epoch 928 
2025-01-28 11:38:57.536978: Current learning rate: 0.00094 
2025-01-28 11:39:45.978604: train_loss -0.8576 
2025-01-28 11:39:45.983324: val_loss -0.8005 
2025-01-28 11:39:45.985898: Pseudo dice [np.float32(0.9575), np.float32(0.9386)] 
2025-01-28 11:39:45.988502: Epoch time: 48.45 s 
2025-01-28 11:39:47.082745:  
2025-01-28 11:39:47.085290: Epoch 929 
2025-01-28 11:39:47.087642: Current learning rate: 0.00092 
2025-01-28 11:40:34.864311: train_loss -0.8573 
2025-01-28 11:40:34.868361: val_loss -0.7198 
2025-01-28 11:40:34.870917: Pseudo dice [np.float32(0.9577), np.float32(0.9373)] 
2025-01-28 11:40:34.873470: Epoch time: 47.78 s 
2025-01-28 11:40:35.966981:  
2025-01-28 11:40:35.969549: Epoch 930 
2025-01-28 11:40:35.972000: Current learning rate: 0.00091 
2025-01-28 11:41:24.232379: train_loss -0.8464 
2025-01-28 11:41:24.238517: val_loss -0.7705 
2025-01-28 11:41:24.241307: Pseudo dice [np.float32(0.9557), np.float32(0.9461)] 
2025-01-28 11:41:24.244014: Epoch time: 48.27 s 
2025-01-28 11:41:24.246737: Yayy! New best EMA pseudo Dice: 0.9467999935150146 
2025-01-28 11:41:25.893623:  
2025-01-28 11:41:25.896477: Epoch 931 
2025-01-28 11:41:25.899473: Current learning rate: 0.0009 
2025-01-28 11:42:13.935879: train_loss -0.8478 
2025-01-28 11:42:13.941508: val_loss -0.7615 
2025-01-28 11:42:13.944764: Pseudo dice [np.float32(0.959), np.float32(0.9331)] 
2025-01-28 11:42:13.947393: Epoch time: 48.04 s 
2025-01-28 11:42:15.027263:  
2025-01-28 11:42:15.029725: Epoch 932 
2025-01-28 11:42:15.032084: Current learning rate: 0.00089 
2025-01-28 11:43:02.978585: train_loss -0.848 
2025-01-28 11:43:02.984930: val_loss -0.7471 
2025-01-28 11:43:02.988271: Pseudo dice [np.float32(0.9633), np.float32(0.9298)] 
2025-01-28 11:43:02.990985: Epoch time: 47.95 s 
2025-01-28 11:43:04.086165:  
2025-01-28 11:43:04.089081: Epoch 933 
2025-01-28 11:43:04.091844: Current learning rate: 0.00088 
2025-01-28 11:43:51.909005: train_loss -0.8549 
2025-01-28 11:43:51.912909: val_loss -0.7801 
2025-01-28 11:43:51.915761: Pseudo dice [np.float32(0.9583), np.float32(0.9368)] 
2025-01-28 11:43:51.918321: Epoch time: 47.82 s 
2025-01-28 11:43:53.011305:  
2025-01-28 11:43:53.013901: Epoch 934 
2025-01-28 11:43:53.016517: Current learning rate: 0.00087 
2025-01-28 11:44:40.877393: train_loss -0.8504 
2025-01-28 11:44:40.882467: val_loss -0.7657 
2025-01-28 11:44:40.885196: Pseudo dice [np.float32(0.9578), np.float32(0.946)] 
2025-01-28 11:44:40.887435: Epoch time: 47.87 s 
2025-01-28 11:44:40.889561: Yayy! New best EMA pseudo Dice: 0.9473000168800354 
2025-01-28 11:44:42.524355:  
2025-01-28 11:44:42.527163: Epoch 935 
2025-01-28 11:44:42.529856: Current learning rate: 0.00085 
2025-01-28 11:45:30.585058: train_loss -0.8398 
2025-01-28 11:45:30.589374: val_loss -0.7164 
2025-01-28 11:45:30.591932: Pseudo dice [np.float32(0.9631), np.float32(0.9141)] 
2025-01-28 11:45:30.594556: Epoch time: 48.06 s 
2025-01-28 11:45:31.687660:  
2025-01-28 11:45:31.690509: Epoch 936 
2025-01-28 11:45:31.693414: Current learning rate: 0.00084 
2025-01-28 11:46:20.264207: train_loss -0.8486 
2025-01-28 11:46:20.270558: val_loss -0.7798 
2025-01-28 11:46:20.273347: Pseudo dice [np.float32(0.9558), np.float32(0.9435)] 
2025-01-28 11:46:20.276091: Epoch time: 48.58 s 
2025-01-28 11:46:21.399558:  
2025-01-28 11:46:21.402112: Epoch 937 
2025-01-28 11:46:21.405019: Current learning rate: 0.00083 
2025-01-28 11:47:09.762653: train_loss -0.8506 
2025-01-28 11:47:09.766629: val_loss -0.7234 
2025-01-28 11:47:09.769295: Pseudo dice [np.float32(0.9519), np.float32(0.9401)] 
2025-01-28 11:47:09.772028: Epoch time: 48.36 s 
2025-01-28 11:47:10.883635:  
2025-01-28 11:47:10.886280: Epoch 938 
2025-01-28 11:47:10.888986: Current learning rate: 0.00082 
2025-01-28 11:47:58.930425: train_loss -0.8495 
2025-01-28 11:47:58.936409: val_loss -0.741 
2025-01-28 11:47:58.939152: Pseudo dice [np.float32(0.9549), np.float32(0.9427)] 
2025-01-28 11:47:58.941753: Epoch time: 48.05 s 
2025-01-28 11:48:00.056402:  
2025-01-28 11:48:00.060193: Epoch 939 
2025-01-28 11:48:00.064412: Current learning rate: 0.00081 
2025-01-28 11:48:48.361470: train_loss -0.852 
2025-01-28 11:48:48.365328: val_loss -0.8012 
2025-01-28 11:48:48.368113: Pseudo dice [np.float32(0.9593), np.float32(0.9269)] 
2025-01-28 11:48:48.370618: Epoch time: 48.31 s 
2025-01-28 11:48:49.477368:  
2025-01-28 11:48:49.480008: Epoch 940 
2025-01-28 11:48:49.482650: Current learning rate: 0.00079 
2025-01-28 11:49:37.853482: train_loss -0.8617 
2025-01-28 11:49:37.859684: val_loss -0.7689 
2025-01-28 11:49:37.862699: Pseudo dice [np.float32(0.9491), np.float32(0.9287)] 
2025-01-28 11:49:37.865378: Epoch time: 48.38 s 
2025-01-28 11:49:38.961749:  
2025-01-28 11:49:38.978404: Epoch 941 
2025-01-28 11:49:38.981992: Current learning rate: 0.00078 
2025-01-28 11:50:27.217626: train_loss -0.8612 
2025-01-28 11:50:27.221640: val_loss -0.7755 
2025-01-28 11:50:27.224165: Pseudo dice [np.float32(0.9559), np.float32(0.9424)] 
2025-01-28 11:50:27.226851: Epoch time: 48.26 s 
2025-01-28 11:50:28.332004:  
2025-01-28 11:50:28.339389: Epoch 942 
2025-01-28 11:50:28.342508: Current learning rate: 0.00077 
2025-01-28 11:51:16.943416: train_loss -0.8467 
2025-01-28 11:51:16.948552: val_loss -0.782 
2025-01-28 11:51:16.951164: Pseudo dice [np.float32(0.9563), np.float32(0.9482)] 
2025-01-28 11:51:16.953741: Epoch time: 48.61 s 
2025-01-28 11:51:18.067896:  
2025-01-28 11:51:18.071234: Epoch 943 
2025-01-28 11:51:18.074705: Current learning rate: 0.00076 
2025-01-28 11:52:06.155062: train_loss -0.8553 
2025-01-28 11:52:06.159419: val_loss -0.7911 
2025-01-28 11:52:06.162091: Pseudo dice [np.float32(0.9527), np.float32(0.9372)] 
2025-01-28 11:52:06.164793: Epoch time: 48.09 s 
2025-01-28 11:52:07.274497:  
2025-01-28 11:52:07.277229: Epoch 944 
2025-01-28 11:52:07.279761: Current learning rate: 0.00075 
2025-01-28 11:52:55.398657: train_loss -0.869 
2025-01-28 11:52:55.403543: val_loss -0.765 
2025-01-28 11:52:55.406185: Pseudo dice [np.float32(0.9599), np.float32(0.9399)] 
2025-01-28 11:52:55.408715: Epoch time: 48.13 s 
2025-01-28 11:52:56.504567:  
2025-01-28 11:52:56.506999: Epoch 945 
2025-01-28 11:52:56.509184: Current learning rate: 0.00074 
2025-01-28 11:53:44.798314: train_loss -0.8634 
2025-01-28 11:53:44.803048: val_loss -0.7541 
2025-01-28 11:53:44.805713: Pseudo dice [np.float32(0.96), np.float32(0.9347)] 
2025-01-28 11:53:44.808211: Epoch time: 48.29 s 
2025-01-28 11:53:45.894003:  
2025-01-28 11:53:45.897189: Epoch 946 
2025-01-28 11:53:45.899864: Current learning rate: 0.00072 
2025-01-28 11:54:33.825346: train_loss -0.8466 
2025-01-28 11:54:33.831073: val_loss -0.8012 
2025-01-28 11:54:33.833844: Pseudo dice [np.float32(0.9513), np.float32(0.9363)] 
2025-01-28 11:54:33.836521: Epoch time: 47.93 s 
2025-01-28 11:54:35.525876:  
2025-01-28 11:54:35.528869: Epoch 947 
2025-01-28 11:54:35.531645: Current learning rate: 0.00071 
2025-01-28 11:55:23.566319: train_loss -0.8489 
2025-01-28 11:55:23.570035: val_loss -0.741 
2025-01-28 11:55:23.572625: Pseudo dice [np.float32(0.9564), np.float32(0.9436)] 
2025-01-28 11:55:23.574950: Epoch time: 48.04 s 
2025-01-28 11:55:24.688344:  
2025-01-28 11:55:24.691142: Epoch 948 
2025-01-28 11:55:24.693901: Current learning rate: 0.0007 
2025-01-28 11:56:13.021650: train_loss -0.8463 
2025-01-28 11:56:13.027238: val_loss -0.7718 
2025-01-28 11:56:13.031060: Pseudo dice [np.float32(0.9554), np.float32(0.9409)] 
2025-01-28 11:56:13.033715: Epoch time: 48.33 s 
2025-01-28 11:56:14.129760:  
2025-01-28 11:56:14.132579: Epoch 949 
2025-01-28 11:56:14.135504: Current learning rate: 0.00069 
2025-01-28 11:57:02.041996: train_loss -0.8414 
2025-01-28 11:57:02.047210: val_loss -0.775 
2025-01-28 11:57:02.050614: Pseudo dice [np.float32(0.9526), np.float32(0.9376)] 
2025-01-28 11:57:02.053176: Epoch time: 47.91 s 
2025-01-28 11:57:03.705901:  
2025-01-28 11:57:03.708632: Epoch 950 
2025-01-28 11:57:03.711174: Current learning rate: 0.00067 
2025-01-28 11:57:51.479189: train_loss -0.851 
2025-01-28 11:57:51.485601: val_loss -0.7454 
2025-01-28 11:57:51.488626: Pseudo dice [np.float32(0.9561), np.float32(0.9285)] 
2025-01-28 11:57:51.491178: Epoch time: 47.77 s 
2025-01-28 11:57:52.576732:  
2025-01-28 11:57:52.579357: Epoch 951 
2025-01-28 11:57:52.581801: Current learning rate: 0.00066 
2025-01-28 11:58:40.589263: train_loss -0.8622 
2025-01-28 11:58:40.592747: val_loss -0.7707 
2025-01-28 11:58:40.595490: Pseudo dice [np.float32(0.9579), np.float32(0.9374)] 
2025-01-28 11:58:40.598038: Epoch time: 48.01 s 
2025-01-28 11:58:41.692671:  
2025-01-28 11:58:41.695446: Epoch 952 
2025-01-28 11:58:41.698356: Current learning rate: 0.00065 
2025-01-28 11:59:29.510020: train_loss -0.8462 
2025-01-28 11:59:29.515011: val_loss -0.7688 
2025-01-28 11:59:29.517429: Pseudo dice [np.float32(0.9594), np.float32(0.9464)] 
2025-01-28 11:59:29.519896: Epoch time: 47.82 s 
2025-01-28 11:59:30.614571:  
2025-01-28 11:59:30.618939: Epoch 953 
2025-01-28 11:59:30.621693: Current learning rate: 0.00064 
2025-01-28 12:00:18.572043: train_loss -0.85 
2025-01-28 12:00:18.576035: val_loss -0.7487 
2025-01-28 12:00:18.578529: Pseudo dice [np.float32(0.9593), np.float32(0.9503)] 
2025-01-28 12:00:18.580821: Epoch time: 47.96 s 
2025-01-28 12:00:18.583508: Yayy! New best EMA pseudo Dice: 0.9478999972343445 
2025-01-28 12:00:20.312232:  
2025-01-28 12:00:20.314807: Epoch 954 
2025-01-28 12:00:20.317599: Current learning rate: 0.00063 
2025-01-28 12:01:08.372562: train_loss -0.853 
2025-01-28 12:01:08.378228: val_loss -0.7695 
2025-01-28 12:01:08.381353: Pseudo dice [np.float32(0.9549), np.float32(0.939)] 
2025-01-28 12:01:08.383969: Epoch time: 48.06 s 
2025-01-28 12:01:09.499054:  
2025-01-28 12:01:09.502257: Epoch 955 
2025-01-28 12:01:09.505116: Current learning rate: 0.00061 
2025-01-28 12:01:57.296750: train_loss -0.8415 
2025-01-28 12:01:57.302310: val_loss -0.7326 
2025-01-28 12:01:57.304991: Pseudo dice [np.float32(0.9576), np.float32(0.9381)] 
2025-01-28 12:01:57.308017: Epoch time: 47.8 s 
2025-01-28 12:01:58.423399:  
2025-01-28 12:01:58.425992: Epoch 956 
2025-01-28 12:01:58.428386: Current learning rate: 0.0006 
2025-01-28 12:02:46.226034: train_loss -0.8528 
2025-01-28 12:02:46.231370: val_loss -0.765 
2025-01-28 12:02:46.233877: Pseudo dice [np.float32(0.9597), np.float32(0.9416)] 
2025-01-28 12:02:46.236418: Epoch time: 47.8 s 
2025-01-28 12:02:46.238659: Yayy! New best EMA pseudo Dice: 0.9480999708175659 
2025-01-28 12:02:47.937560:  
2025-01-28 12:02:47.940064: Epoch 957 
2025-01-28 12:02:47.942604: Current learning rate: 0.00059 
2025-01-28 12:03:35.956770: train_loss -0.8359 
2025-01-28 12:03:35.961227: val_loss -0.7548 
2025-01-28 12:03:35.964091: Pseudo dice [np.float32(0.9576), np.float32(0.9385)] 
2025-01-28 12:03:35.966736: Epoch time: 48.02 s 
2025-01-28 12:03:37.073619:  
2025-01-28 12:03:37.076473: Epoch 958 
2025-01-28 12:03:37.078986: Current learning rate: 0.00058 
2025-01-28 12:04:25.425822: train_loss -0.8495 
2025-01-28 12:04:25.431261: val_loss -0.7635 
2025-01-28 12:04:25.433776: Pseudo dice [np.float32(0.9586), np.float32(0.9259)] 
2025-01-28 12:04:25.436333: Epoch time: 48.35 s 
2025-01-28 12:04:26.565005:  
2025-01-28 12:04:26.567855: Epoch 959 
2025-01-28 12:04:26.570472: Current learning rate: 0.00056 
2025-01-28 12:05:14.887531: train_loss -0.8547 
2025-01-28 12:05:14.891475: val_loss -0.7594 
2025-01-28 12:05:14.894540: Pseudo dice [np.float32(0.9542), np.float32(0.9238)] 
2025-01-28 12:05:14.897525: Epoch time: 48.32 s 
2025-01-28 12:05:16.045229:  
2025-01-28 12:05:16.048080: Epoch 960 
2025-01-28 12:05:16.050889: Current learning rate: 0.00055 
2025-01-28 12:06:03.888732: train_loss -0.8359 
2025-01-28 12:06:03.896514: val_loss -0.726 
2025-01-28 12:06:03.905893: Pseudo dice [np.float32(0.9648), np.float32(0.9259)] 
2025-01-28 12:06:03.909067: Epoch time: 47.84 s 
2025-01-28 12:06:05.041570:  
2025-01-28 12:06:05.044677: Epoch 961 
2025-01-28 12:06:05.047184: Current learning rate: 0.00054 
2025-01-28 12:06:53.190392: train_loss -0.836 
2025-01-28 12:06:53.194589: val_loss -0.7495 
2025-01-28 12:06:53.197097: Pseudo dice [np.float32(0.9597), np.float32(0.9339)] 
2025-01-28 12:06:53.199619: Epoch time: 48.15 s 
2025-01-28 12:06:54.336602:  
2025-01-28 12:06:54.339467: Epoch 962 
2025-01-28 12:06:54.342245: Current learning rate: 0.00053 
2025-01-28 12:07:42.555143: train_loss -0.8507 
2025-01-28 12:07:42.560694: val_loss -0.7318 
2025-01-28 12:07:42.563435: Pseudo dice [np.float32(0.9605), np.float32(0.9278)] 
2025-01-28 12:07:42.565866: Epoch time: 48.22 s 
2025-01-28 12:07:43.682658:  
2025-01-28 12:07:43.685612: Epoch 963 
2025-01-28 12:07:43.688145: Current learning rate: 0.00051 
2025-01-28 12:08:31.710921: train_loss -0.8513 
2025-01-28 12:08:31.714956: val_loss -0.7911 
2025-01-28 12:08:31.717498: Pseudo dice [np.float32(0.9629), np.float32(0.9428)] 
2025-01-28 12:08:31.719878: Epoch time: 48.03 s 
2025-01-28 12:08:32.846655:  
2025-01-28 12:08:32.849241: Epoch 964 
2025-01-28 12:08:32.851844: Current learning rate: 0.0005 
2025-01-28 12:09:21.043096: train_loss -0.8515 
2025-01-28 12:09:21.049623: val_loss -0.7731 
2025-01-28 12:09:21.052363: Pseudo dice [np.float32(0.9548), np.float32(0.9296)] 
2025-01-28 12:09:21.055230: Epoch time: 48.2 s 
2025-01-28 12:09:22.177497:  
2025-01-28 12:09:22.180668: Epoch 965 
2025-01-28 12:09:22.183580: Current learning rate: 0.00049 
2025-01-28 12:10:10.464541: train_loss -0.8674 
2025-01-28 12:10:10.468378: val_loss -0.7795 
2025-01-28 12:10:10.470857: Pseudo dice [np.float32(0.9558), np.float32(0.9134)] 
2025-01-28 12:10:10.473353: Epoch time: 48.29 s 
2025-01-28 12:10:12.215155:  
2025-01-28 12:10:12.218161: Epoch 966 
2025-01-28 12:10:12.221020: Current learning rate: 0.00048 
2025-01-28 12:11:00.062884: train_loss -0.8553 
2025-01-28 12:11:00.068403: val_loss -0.8081 
2025-01-28 12:11:00.071136: Pseudo dice [np.float32(0.9534), np.float32(0.9379)] 
2025-01-28 12:11:00.073747: Epoch time: 47.85 s 
2025-01-28 12:11:01.193712:  
2025-01-28 12:11:01.196422: Epoch 967 
2025-01-28 12:11:01.198978: Current learning rate: 0.00046 
2025-01-28 12:11:49.432326: train_loss -0.8375 
2025-01-28 12:11:49.436231: val_loss -0.789 
2025-01-28 12:11:49.438841: Pseudo dice [np.float32(0.9585), np.float32(0.9161)] 
2025-01-28 12:11:49.441439: Epoch time: 48.24 s 
2025-01-28 12:11:50.564112:  
2025-01-28 12:11:50.566897: Epoch 968 
2025-01-28 12:11:50.569512: Current learning rate: 0.00045 
2025-01-28 12:12:38.501668: train_loss -0.8595 
2025-01-28 12:12:38.506401: val_loss -0.7677 
2025-01-28 12:12:38.508882: Pseudo dice [np.float32(0.9613), np.float32(0.9465)] 
2025-01-28 12:12:38.511203: Epoch time: 47.94 s 
2025-01-28 12:12:39.632712:  
2025-01-28 12:12:39.635124: Epoch 969 
2025-01-28 12:12:39.637362: Current learning rate: 0.00044 
2025-01-28 12:13:27.931168: train_loss -0.8552 
2025-01-28 12:13:27.935135: val_loss -0.7548 
2025-01-28 12:13:27.937831: Pseudo dice [np.float32(0.9607), np.float32(0.9444)] 
2025-01-28 12:13:27.940319: Epoch time: 48.3 s 
2025-01-28 12:13:29.070708:  
2025-01-28 12:13:29.073755: Epoch 970 
2025-01-28 12:13:29.076741: Current learning rate: 0.00043 
2025-01-28 12:14:16.776591: train_loss -0.8595 
2025-01-28 12:14:16.782304: val_loss -0.7982 
2025-01-28 12:14:16.785694: Pseudo dice [np.float32(0.9534), np.float32(0.9432)] 
2025-01-28 12:14:16.788410: Epoch time: 47.71 s 
2025-01-28 12:14:17.915968:  
2025-01-28 12:14:17.918751: Epoch 971 
2025-01-28 12:14:17.921637: Current learning rate: 0.00041 
2025-01-28 12:15:06.127478: train_loss -0.839 
2025-01-28 12:15:06.131549: val_loss -0.8205 
2025-01-28 12:15:06.134273: Pseudo dice [np.float32(0.9513), np.float32(0.939)] 
2025-01-28 12:15:06.136995: Epoch time: 48.21 s 
2025-01-28 12:15:07.262951:  
2025-01-28 12:15:07.266928: Epoch 972 
2025-01-28 12:15:07.269822: Current learning rate: 0.0004 
2025-01-28 12:15:55.186090: train_loss -0.8538 
2025-01-28 12:15:55.191322: val_loss -0.7323 
2025-01-28 12:15:55.194125: Pseudo dice [np.float32(0.9558), np.float32(0.9465)] 
2025-01-28 12:15:55.196872: Epoch time: 47.92 s 
2025-01-28 12:15:56.327372:  
2025-01-28 12:15:56.330002: Epoch 973 
2025-01-28 12:15:56.332513: Current learning rate: 0.00039 
2025-01-28 12:16:44.541418: train_loss -0.8614 
2025-01-28 12:16:44.545151: val_loss -0.7506 
2025-01-28 12:16:44.547684: Pseudo dice [np.float32(0.9618), np.float32(0.9368)] 
2025-01-28 12:16:44.550083: Epoch time: 48.21 s 
2025-01-28 12:16:45.677349:  
2025-01-28 12:16:45.680309: Epoch 974 
2025-01-28 12:16:45.683085: Current learning rate: 0.00037 
2025-01-28 12:17:33.800453: train_loss -0.8581 
2025-01-28 12:17:33.806088: val_loss -0.7674 
2025-01-28 12:17:33.808825: Pseudo dice [np.float32(0.9482), np.float32(0.9357)] 
2025-01-28 12:17:33.811616: Epoch time: 48.12 s 
2025-01-28 12:17:34.974542:  
2025-01-28 12:17:34.977813: Epoch 975 
2025-01-28 12:17:34.981092: Current learning rate: 0.00036 
2025-01-28 12:18:22.941726: train_loss -0.8496 
2025-01-28 12:18:22.946689: val_loss -0.7907 
2025-01-28 12:18:22.949361: Pseudo dice [np.float32(0.9587), np.float32(0.9491)] 
2025-01-28 12:18:22.952098: Epoch time: 47.97 s 
2025-01-28 12:18:24.082096:  
2025-01-28 12:18:24.085048: Epoch 976 
2025-01-28 12:18:24.087950: Current learning rate: 0.00035 
2025-01-28 12:19:11.947248: train_loss -0.8519 
2025-01-28 12:19:11.952319: val_loss -0.7508 
2025-01-28 12:19:11.954988: Pseudo dice [np.float32(0.9527), np.float32(0.9383)] 
2025-01-28 12:19:11.957231: Epoch time: 47.87 s 
2025-01-28 12:19:13.103178:  
2025-01-28 12:19:13.106213: Epoch 977 
2025-01-28 12:19:13.109245: Current learning rate: 0.00034 
2025-01-28 12:20:01.333368: train_loss -0.8638 
2025-01-28 12:20:01.337436: val_loss -0.7693 
2025-01-28 12:20:01.340339: Pseudo dice [np.float32(0.959), np.float32(0.9451)] 
2025-01-28 12:20:01.343243: Epoch time: 48.23 s 
2025-01-28 12:20:02.468270:  
2025-01-28 12:20:02.471338: Epoch 978 
2025-01-28 12:20:02.474162: Current learning rate: 0.00032 
2025-01-28 12:20:50.686642: train_loss -0.8419 
2025-01-28 12:20:50.692308: val_loss -0.7748 
2025-01-28 12:20:50.695307: Pseudo dice [np.float32(0.9525), np.float32(0.9299)] 
2025-01-28 12:20:50.698112: Epoch time: 48.22 s 
2025-01-28 12:20:51.823646:  
2025-01-28 12:20:51.826415: Epoch 979 
2025-01-28 12:20:51.829050: Current learning rate: 0.00031 
2025-01-28 12:21:39.765182: train_loss -0.8507 
2025-01-28 12:21:39.768942: val_loss -0.7546 
2025-01-28 12:21:39.771726: Pseudo dice [np.float32(0.9608), np.float32(0.9386)] 
2025-01-28 12:21:39.774498: Epoch time: 47.94 s 
2025-01-28 12:21:40.904303:  
2025-01-28 12:21:40.907141: Epoch 980 
2025-01-28 12:21:40.909780: Current learning rate: 0.0003 
2025-01-28 12:22:28.929271: train_loss -0.8446 
2025-01-28 12:22:28.934755: val_loss -0.746 
2025-01-28 12:22:28.937251: Pseudo dice [np.float32(0.9579), np.float32(0.9349)] 
2025-01-28 12:22:28.939488: Epoch time: 48.03 s 
2025-01-28 12:22:30.059717:  
2025-01-28 12:22:30.062536: Epoch 981 
2025-01-28 12:22:30.065263: Current learning rate: 0.00028 
2025-01-28 12:23:17.960341: train_loss -0.8536 
2025-01-28 12:23:17.965741: val_loss -0.7448 
2025-01-28 12:23:17.968277: Pseudo dice [np.float32(0.9591), np.float32(0.942)] 
2025-01-28 12:23:17.970764: Epoch time: 47.9 s 
2025-01-28 12:23:19.093795:  
2025-01-28 12:23:19.096667: Epoch 982 
2025-01-28 12:23:19.099316: Current learning rate: 0.00027 
2025-01-28 12:24:07.380311: train_loss -0.8436 
2025-01-28 12:24:07.385571: val_loss -0.7685 
2025-01-28 12:24:07.388114: Pseudo dice [np.float32(0.9527), np.float32(0.9438)] 
2025-01-28 12:24:07.390474: Epoch time: 48.29 s 
2025-01-28 12:24:08.514359:  
2025-01-28 12:24:08.517117: Epoch 983 
2025-01-28 12:24:08.519942: Current learning rate: 0.00026 
2025-01-28 12:24:56.631217: train_loss -0.8644 
2025-01-28 12:24:56.635155: val_loss -0.8068 
2025-01-28 12:24:56.637806: Pseudo dice [np.float32(0.959), np.float32(0.9344)] 
2025-01-28 12:24:56.640359: Epoch time: 48.12 s 
2025-01-28 12:24:57.772995:  
2025-01-28 12:24:57.775980: Epoch 984 
2025-01-28 12:24:57.778416: Current learning rate: 0.00024 
2025-01-28 12:25:46.004798: train_loss -0.8453 
2025-01-28 12:25:46.010330: val_loss -0.7746 
2025-01-28 12:25:46.012936: Pseudo dice [np.float32(0.962), np.float32(0.9263)] 
2025-01-28 12:25:46.015708: Epoch time: 48.23 s 
2025-01-28 12:25:47.768203:  
2025-01-28 12:25:47.770903: Epoch 985 
2025-01-28 12:25:47.773403: Current learning rate: 0.00023 
2025-01-28 12:26:36.004319: train_loss -0.8419 
2025-01-28 12:26:36.010678: val_loss -0.7768 
2025-01-28 12:26:36.013310: Pseudo dice [np.float32(0.9584), np.float32(0.923)] 
2025-01-28 12:26:36.015898: Epoch time: 48.24 s 
2025-01-28 12:26:37.162486:  
2025-01-28 12:26:37.165368: Epoch 986 
2025-01-28 12:26:37.168616: Current learning rate: 0.00021 
2025-01-28 12:27:25.237496: train_loss -0.8598 
2025-01-28 12:27:25.243556: val_loss -0.7609 
2025-01-28 12:27:25.246398: Pseudo dice [np.float32(0.9558), np.float32(0.9447)] 
2025-01-28 12:27:25.249141: Epoch time: 48.08 s 
2025-01-28 12:27:26.396884:  
2025-01-28 12:27:26.399758: Epoch 987 
2025-01-28 12:27:26.402495: Current learning rate: 0.0002 
2025-01-28 12:28:14.496403: train_loss -0.8653 
2025-01-28 12:28:14.500319: val_loss -0.7453 
2025-01-28 12:28:14.503351: Pseudo dice [np.float32(0.9583), np.float32(0.9354)] 
2025-01-28 12:28:14.506473: Epoch time: 48.1 s 
2025-01-28 12:28:15.629750:  
2025-01-28 12:28:15.632594: Epoch 988 
2025-01-28 12:28:15.635341: Current learning rate: 0.00019 
2025-01-28 12:29:03.575544: train_loss -0.8611 
2025-01-28 12:29:03.581184: val_loss -0.8119 
2025-01-28 12:29:03.584121: Pseudo dice [np.float32(0.9511), np.float32(0.9378)] 
2025-01-28 12:29:03.586702: Epoch time: 47.95 s 
2025-01-28 12:29:04.738574:  
2025-01-28 12:29:04.741827: Epoch 989 
2025-01-28 12:29:04.744857: Current learning rate: 0.00017 
2025-01-28 12:29:53.023023: train_loss -0.8383 
2025-01-28 12:29:53.027577: val_loss -0.7277 
2025-01-28 12:29:53.030359: Pseudo dice [np.float32(0.9626), np.float32(0.9484)] 
2025-01-28 12:29:53.033174: Epoch time: 48.29 s 
2025-01-28 12:29:54.191771:  
2025-01-28 12:29:54.195011: Epoch 990 
2025-01-28 12:29:54.197746: Current learning rate: 0.00016 
2025-01-28 12:30:42.231216: train_loss -0.8503 
2025-01-28 12:30:42.236611: val_loss -0.7518 
2025-01-28 12:30:42.239222: Pseudo dice [np.float32(0.9615), np.float32(0.9319)] 
2025-01-28 12:30:42.241847: Epoch time: 48.04 s 
2025-01-28 12:30:43.373998:  
2025-01-28 12:30:43.376687: Epoch 991 
2025-01-28 12:30:43.379269: Current learning rate: 0.00014 
2025-01-28 12:31:31.433262: train_loss -0.8614 
2025-01-28 12:31:31.437310: val_loss -0.7911 
2025-01-28 12:31:31.440105: Pseudo dice [np.float32(0.953), np.float32(0.9362)] 
2025-01-28 12:31:31.442940: Epoch time: 48.06 s 
2025-01-28 12:31:32.592525:  
2025-01-28 12:31:32.596827: Epoch 992 
2025-01-28 12:31:32.599214: Current learning rate: 0.00013 
2025-01-28 12:32:20.456199: train_loss -0.8555 
2025-01-28 12:32:20.462878: val_loss -0.768 
2025-01-28 12:32:20.465486: Pseudo dice [np.float32(0.9577), np.float32(0.9455)] 
2025-01-28 12:32:20.468016: Epoch time: 47.86 s 
2025-01-28 12:32:21.615421:  
2025-01-28 12:32:21.618266: Epoch 993 
2025-01-28 12:32:21.620841: Current learning rate: 0.00011 
2025-01-28 12:33:09.297722: train_loss -0.8577 
2025-01-28 12:33:09.301794: val_loss -0.7759 
2025-01-28 12:33:09.304714: Pseudo dice [np.float32(0.9531), np.float32(0.9362)] 
2025-01-28 12:33:09.307469: Epoch time: 47.68 s 
2025-01-28 12:33:10.460500:  
2025-01-28 12:33:10.463463: Epoch 994 
2025-01-28 12:33:10.466234: Current learning rate: 0.0001 
2025-01-28 12:33:58.595262: train_loss -0.8551 
2025-01-28 12:33:58.599892: val_loss -0.7416 
2025-01-28 12:33:58.602420: Pseudo dice [np.float32(0.9562), np.float32(0.9497)] 
2025-01-28 12:33:58.605234: Epoch time: 48.14 s 
2025-01-28 12:33:59.770911:  
2025-01-28 12:33:59.773707: Epoch 995 
2025-01-28 12:33:59.777401: Current learning rate: 8e-05 
2025-01-28 12:34:47.751313: train_loss -0.832 
2025-01-28 12:34:47.755499: val_loss -0.7976 
2025-01-28 12:34:47.758205: Pseudo dice [np.float32(0.9572), np.float32(0.9341)] 
2025-01-28 12:34:47.761167: Epoch time: 47.99 s 
2025-01-28 12:34:48.875310:  
2025-01-28 12:34:48.878196: Epoch 996 
2025-01-28 12:34:48.881071: Current learning rate: 7e-05 
2025-01-28 12:35:36.724840: train_loss -0.8563 
2025-01-28 12:35:36.729967: val_loss -0.7716 
2025-01-28 12:35:36.732794: Pseudo dice [np.float32(0.9576), np.float32(0.9326)] 
2025-01-28 12:35:36.735446: Epoch time: 47.85 s 
2025-01-28 12:35:37.853441:  
2025-01-28 12:35:37.856529: Epoch 997 
2025-01-28 12:35:37.859499: Current learning rate: 5e-05 
2025-01-28 12:36:26.382017: train_loss -0.8543 
2025-01-28 12:36:26.388320: val_loss -0.7661 
2025-01-28 12:36:26.391138: Pseudo dice [np.float32(0.9522), np.float32(0.9318)] 
2025-01-28 12:36:26.393552: Epoch time: 48.53 s 
2025-01-28 12:36:27.544474:  
2025-01-28 12:36:27.547382: Epoch 998 
2025-01-28 12:36:27.549859: Current learning rate: 4e-05 
2025-01-28 12:37:15.344371: train_loss -0.8544 
2025-01-28 12:37:15.349350: val_loss -0.7681 
2025-01-28 12:37:15.352029: Pseudo dice [np.float32(0.9576), np.float32(0.9466)] 
2025-01-28 12:37:15.354902: Epoch time: 47.8 s 
2025-01-28 12:37:16.473633:  
2025-01-28 12:37:16.476719: Epoch 999 
2025-01-28 12:37:16.479661: Current learning rate: 2e-05 
2025-01-28 12:38:04.493455: train_loss -0.8565 
2025-01-28 12:38:04.497597: val_loss -0.7497 
2025-01-28 12:38:04.500324: Pseudo dice [np.float32(0.9593), np.float32(0.9492)] 
2025-01-28 12:38:04.502677: Epoch time: 48.02 s 
2025-01-28 12:38:06.246503: Training done. 
2025-01-28 12:38:06.379297: Using splits from existing split file: /srv/scratch/z5362216/kits19/nnUNet_db/nnUNet_preprocessed/Dataset001_Kits19/splits_final.json 
2025-01-28 12:38:06.383117: The split file contains 5 splits. 
2025-01-28 12:38:06.385575: Desired fold for training: 3 
2025-01-28 12:38:06.388081: This split has 80 training and 20 validation cases. 
2025-01-28 12:38:06.430937: predicting imaging_003 
2025-01-28 12:38:06.439921: imaging_003, shape torch.Size([1, 136, 221, 221]), rank 0 
2025-01-28 12:38:28.166535: predicting imaging_005 
2025-01-28 12:38:28.176675: imaging_005, shape torch.Size([1, 210, 252, 252]), rank 0 
2025-01-28 12:38:34.293965: predicting imaging_015 
2025-01-28 12:38:34.304535: imaging_015, shape torch.Size([1, 113, 182, 182]), rank 0 
2025-01-28 12:38:35.350693: predicting imaging_023 
2025-01-28 12:38:35.359784: imaging_023, shape torch.Size([1, 162, 202, 202]), rank 0 
2025-01-28 12:38:39.452852: predicting imaging_031 
2025-01-28 12:38:39.461365: imaging_031, shape torch.Size([1, 177, 134, 134]), rank 0 
2025-01-28 12:38:59.835658: predicting imaging_044 
2025-01-28 12:38:59.844681: imaging_044, shape torch.Size([1, 254, 249, 249]), rank 0 
2025-01-28 12:39:15.229435: predicting imaging_053 
2025-01-28 12:39:15.242418: imaging_053, shape torch.Size([1, 139, 200, 200]), rank 0 
2025-01-28 12:39:19.911592: predicting imaging_055 
2025-01-28 12:39:19.923663: imaging_055, shape torch.Size([1, 254, 227, 227]), rank 0 
2025-01-28 12:39:26.337934: predicting imaging_056 
2025-01-28 12:39:26.352279: imaging_056, shape torch.Size([1, 227, 202, 202]), rank 0 
2025-01-28 12:39:35.629581: predicting imaging_061 
2025-01-28 12:39:35.653165: imaging_061, shape torch.Size([1, 73, 252, 252]), rank 0 
2025-01-28 12:40:00.755276: predicting imaging_064 
2025-01-28 12:40:00.766742: imaging_064, shape torch.Size([1, 134, 181, 181]), rank 0 
