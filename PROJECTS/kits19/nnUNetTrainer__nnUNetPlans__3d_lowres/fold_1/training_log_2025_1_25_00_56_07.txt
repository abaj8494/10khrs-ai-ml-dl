
#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################
 
2025-01-25 00:56:08.826695: Using torch.compile... 
2025-01-25 00:56:14.006559: do_dummy_2d_data_aug: False 
2025-01-25 00:56:14.181278: Using splits from existing split file: /srv/scratch/z5362216/kits19/nnUNet_db/nnUNet_preprocessed/Dataset001_Kits19/splits_final.json 
2025-01-25 00:56:14.221835: The split file contains 5 splits. 
2025-01-25 00:56:14.226650: Desired fold for training: 1 
2025-01-25 00:56:14.229458: This split has 80 training and 20 validation cases. 

This is the configuration used by this training:
Configuration name: 3d_lowres
 {'data_identifier': 'nnUNetPlans_3d_lowres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 2, 'patch_size': [128, 128, 128], 'median_image_size_in_voxels': [200, 205, 205], 'spacing': [1.9849520718478983, 1.9849270710444444, 1.9849270710444444], 'normalization_schemes': ['CTNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': False, 'next_stage': '3d_cascade_fullres'} 
 
These are the global plan.json settings:
 {'dataset_name': 'Dataset001_Kits19', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [3.0, 0.7939453125, 0.7939453125], 'original_median_shape_after_transp': [104, 512, 512], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [2, 0, 1], 'transpose_backward': [1, 2, 0], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 2553.0, 'mean': 104.46720886230469, 'median': 104.0, 'min': -277.0, 'percentile_00_5': -73.0, 'percentile_99_5': 292.0, 'std': 74.68063354492188}}} 
 
2025-01-25 00:56:17.201760: unpacking dataset... 
2025-01-25 00:56:26.945071: unpacking done... 
2025-01-25 00:56:27.044048: 
printing the network instead:
 
2025-01-25 00:56:27.046928: OptimizedModule(
  (_orig_mod): PlainConvUNet(
    (encoder): PlainConvEncoder(
      (stages): Sequential(
        (0): Sequential(
          (0): StackedConvBlocks(
            (convs): Sequential(
              (0): ConvDropoutNormReLU(
                (conv): Conv3d(1, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(1, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (1): ConvDropoutNormReLU(
                (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
            )
          )
        )
        (1): Sequential(
          (0): StackedConvBlocks(
            (convs): Sequential(
              (0): ConvDropoutNormReLU(
                (conv): Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                  (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (1): ConvDropoutNormReLU(
                (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
            )
          )
        )
        (2): Sequential(
          (0): StackedConvBlocks(
            (convs): Sequential(
              (0): ConvDropoutNormReLU(
                (conv): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                  (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (1): ConvDropoutNormReLU(
                (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
            )
          )
        )
        (3): Sequential(
          (0): StackedConvBlocks(
            (convs): Sequential(
              (0): ConvDropoutNormReLU(
                (conv): Conv3d(128, 256, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                (norm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(128, 256, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                  (1): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (1): ConvDropoutNormReLU(
                (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
            )
          )
        )
        (4): Sequential(
          (0): StackedConvBlocks(
            (convs): Sequential(
              (0): ConvDropoutNormReLU(
                (conv): Conv3d(256, 320, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                (norm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(256, 320, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                  (1): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (1): ConvDropoutNormReLU(
                (conv): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
            )
          )
        )
        (5): Sequential(
          (0): StackedConvBlocks(
            (convs): Sequential(
              (0): ConvDropoutNormReLU(
                (conv): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                (norm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                  (1): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (1): ConvDropoutNormReLU(
                (conv): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
            )
          )
        )
      )
    )
    (decoder): UNetDecoder(
      (encoder): PlainConvEncoder(
        (stages): Sequential(
          (0): Sequential(
            (0): StackedConvBlocks(
              (convs): Sequential(
                (0): ConvDropoutNormReLU(
                  (conv): Conv3d(1, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                  (all_modules): Sequential(
                    (0): Conv3d(1, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                    (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                    (2): LeakyReLU(negative_slope=0.01, inplace=True)
                  )
                )
                (1): ConvDropoutNormReLU(
                  (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                  (all_modules): Sequential(
                    (0): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                    (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                    (2): LeakyReLU(negative_slope=0.01, inplace=True)
                  )
                )
              )
            )
          )
          (1): Sequential(
            (0): StackedConvBlocks(
              (convs): Sequential(
                (0): ConvDropoutNormReLU(
                  (conv): Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                  (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                  (all_modules): Sequential(
                    (0): Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                    (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                    (2): LeakyReLU(negative_slope=0.01, inplace=True)
                  )
                )
                (1): ConvDropoutNormReLU(
                  (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                  (all_modules): Sequential(
                    (0): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                    (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                    (2): LeakyReLU(negative_slope=0.01, inplace=True)
                  )
                )
              )
            )
          )
          (2): Sequential(
            (0): StackedConvBlocks(
              (convs): Sequential(
                (0): ConvDropoutNormReLU(
                  (conv): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                  (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                  (all_modules): Sequential(
                    (0): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                    (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                    (2): LeakyReLU(negative_slope=0.01, inplace=True)
                  )
                )
                (1): ConvDropoutNormReLU(
                  (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                  (all_modules): Sequential(
                    (0): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                    (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                    (2): LeakyReLU(negative_slope=0.01, inplace=True)
                  )
                )
              )
            )
          )
          (3): Sequential(
            (0): StackedConvBlocks(
              (convs): Sequential(
                (0): ConvDropoutNormReLU(
                  (conv): Conv3d(128, 256, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                  (norm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                  (all_modules): Sequential(
                    (0): Conv3d(128, 256, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                    (1): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                    (2): LeakyReLU(negative_slope=0.01, inplace=True)
                  )
                )
                (1): ConvDropoutNormReLU(
                  (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (norm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                  (all_modules): Sequential(
                    (0): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                    (1): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                    (2): LeakyReLU(negative_slope=0.01, inplace=True)
                  )
                )
              )
            )
          )
          (4): Sequential(
            (0): StackedConvBlocks(
              (convs): Sequential(
                (0): ConvDropoutNormReLU(
                  (conv): Conv3d(256, 320, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                  (norm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                  (all_modules): Sequential(
                    (0): Conv3d(256, 320, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                    (1): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                    (2): LeakyReLU(negative_slope=0.01, inplace=True)
                  )
                )
                (1): ConvDropoutNormReLU(
                  (conv): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (norm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                  (all_modules): Sequential(
                    (0): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                    (1): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                    (2): LeakyReLU(negative_slope=0.01, inplace=True)
                  )
                )
              )
            )
          )
          (5): Sequential(
            (0): StackedConvBlocks(
              (convs): Sequential(
                (0): ConvDropoutNormReLU(
                  (conv): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                  (norm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                  (all_modules): Sequential(
                    (0): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                    (1): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                    (2): LeakyReLU(negative_slope=0.01, inplace=True)
                  )
                )
                (1): ConvDropoutNormReLU(
                  (conv): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (norm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                  (all_modules): Sequential(
                    (0): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                    (1): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                    (2): LeakyReLU(negative_slope=0.01, inplace=True)
                  )
                )
              )
            )
          )
        )
      )
      (stages): ModuleList(
        (0): StackedConvBlocks(
          (convs): Sequential(
            (0): ConvDropoutNormReLU(
              (conv): Conv3d(640, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
              (norm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(640, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (1): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (1): ConvDropoutNormReLU(
              (conv): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
              (norm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (1): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
          )
        )
        (1): StackedConvBlocks(
          (convs): Sequential(
            (0): ConvDropoutNormReLU(
              (conv): Conv3d(512, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
              (norm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(512, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (1): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (1): ConvDropoutNormReLU(
              (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
              (norm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (1): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
          )
        )
        (2): StackedConvBlocks(
          (convs): Sequential(
            (0): ConvDropoutNormReLU(
              (conv): Conv3d(256, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
              (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(256, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (1): ConvDropoutNormReLU(
              (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
              (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
          )
        )
        (3): StackedConvBlocks(
          (convs): Sequential(
            (0): ConvDropoutNormReLU(
              (conv): Conv3d(128, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
              (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(128, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (1): ConvDropoutNormReLU(
              (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
              (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
          )
        )
        (4): StackedConvBlocks(
          (convs): Sequential(
            (0): ConvDropoutNormReLU(
              (conv): Conv3d(64, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
              (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(64, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (1): ConvDropoutNormReLU(
              (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
              (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
          )
        )
      )
      (transpconvs): ModuleList(
        (0): ConvTranspose3d(320, 320, kernel_size=(2, 2, 2), stride=(2, 2, 2))
        (1): ConvTranspose3d(320, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2))
        (2): ConvTranspose3d(256, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2))
        (3): ConvTranspose3d(128, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2))
        (4): ConvTranspose3d(64, 32, kernel_size=(2, 2, 2), stride=(2, 2, 2))
      )
      (seg_layers): ModuleList(
        (0): Conv3d(320, 3, kernel_size=(1, 1, 1), stride=(1, 1, 1))
        (1): Conv3d(256, 3, kernel_size=(1, 1, 1), stride=(1, 1, 1))
        (2): Conv3d(128, 3, kernel_size=(1, 1, 1), stride=(1, 1, 1))
        (3): Conv3d(64, 3, kernel_size=(1, 1, 1), stride=(1, 1, 1))
        (4): Conv3d(32, 3, kernel_size=(1, 1, 1), stride=(1, 1, 1))
      )
    )
  )
) 
2025-01-25 00:56:27.055449: 
 
2025-01-25 00:56:27.058243: Unable to plot network architecture: nnUNet_compile is enabled! 
2025-01-25 00:56:27.157512:  
2025-01-25 00:56:27.160630: Epoch 800 
2025-01-25 00:56:27.163866: Current learning rate: 0.00235 
2025-01-25 00:58:46.505971: train_loss -0.8335 
2025-01-25 00:58:46.513416: val_loss -0.7706 
2025-01-25 00:58:46.520722: Pseudo dice [np.float32(0.9529), np.float32(0.924)] 
2025-01-25 00:58:46.523541: Epoch time: 139.35 s 
2025-01-25 00:58:48.789989:  
2025-01-25 00:58:48.792798: Epoch 801 
2025-01-25 00:58:48.795743: Current learning rate: 0.00234 
2025-01-25 00:59:37.159230: train_loss -0.8564 
2025-01-25 00:59:37.162559: val_loss -0.7895 
2025-01-25 00:59:37.165135: Pseudo dice [np.float32(0.9563), np.float32(0.9137)] 
2025-01-25 00:59:37.167661: Epoch time: 48.37 s 
2025-01-25 00:59:38.450986:  
2025-01-25 00:59:38.454147: Epoch 802 
2025-01-25 00:59:38.457403: Current learning rate: 0.00233 
2025-01-25 01:00:27.568702: train_loss -0.8494 
2025-01-25 01:00:27.575810: val_loss -0.7984 
2025-01-25 01:00:27.578621: Pseudo dice [np.float32(0.9493), np.float32(0.9357)] 
2025-01-25 01:00:27.581129: Epoch time: 49.12 s 
2025-01-25 01:00:28.897182:  
2025-01-25 01:00:28.900717: Epoch 803 
2025-01-25 01:00:28.904172: Current learning rate: 0.00232 
2025-01-25 01:01:18.086144: train_loss -0.8299 
2025-01-25 01:01:18.090361: val_loss -0.7599 
2025-01-25 01:01:18.093702: Pseudo dice [np.float32(0.9542), np.float32(0.9062)] 
2025-01-25 01:01:18.096704: Epoch time: 49.19 s 
2025-01-25 01:01:19.426241:  
2025-01-25 01:01:19.429360: Epoch 804 
2025-01-25 01:01:19.432155: Current learning rate: 0.00231 
2025-01-25 01:02:08.516462: train_loss -0.841 
2025-01-25 01:02:08.523907: val_loss -0.7727 
2025-01-25 01:02:08.526889: Pseudo dice [np.float32(0.95), np.float32(0.901)] 
2025-01-25 01:02:08.529675: Epoch time: 49.09 s 
2025-01-25 01:02:09.825403:  
2025-01-25 01:02:09.828763: Epoch 805 
2025-01-25 01:02:09.831705: Current learning rate: 0.0023 
2025-01-25 01:02:58.253887: train_loss -0.8337 
2025-01-25 01:02:58.257909: val_loss -0.7731 
2025-01-25 01:02:58.260820: Pseudo dice [np.float32(0.9456), np.float32(0.8647)] 
2025-01-25 01:02:58.264009: Epoch time: 48.43 s 
2025-01-25 01:02:59.559658:  
2025-01-25 01:02:59.562928: Epoch 806 
2025-01-25 01:02:59.565766: Current learning rate: 0.00229 
2025-01-25 01:03:48.177460: train_loss -0.8412 
2025-01-25 01:03:48.185752: val_loss -0.7056 
2025-01-25 01:03:48.189023: Pseudo dice [np.float32(0.9425), np.float32(0.866)] 
2025-01-25 01:03:48.192116: Epoch time: 48.62 s 
2025-01-25 01:03:49.477584:  
2025-01-25 01:03:49.480942: Epoch 807 
2025-01-25 01:03:49.483772: Current learning rate: 0.00228 
2025-01-25 01:04:37.883620: train_loss -0.8417 
2025-01-25 01:04:37.887127: val_loss -0.715 
2025-01-25 01:04:37.889891: Pseudo dice [np.float32(0.9475), np.float32(0.9147)] 
2025-01-25 01:04:37.892634: Epoch time: 48.41 s 
2025-01-25 01:04:39.178042:  
2025-01-25 01:04:39.181629: Epoch 808 
2025-01-25 01:04:39.184664: Current learning rate: 0.00226 
2025-01-25 01:05:28.070560: train_loss -0.8369 
2025-01-25 01:05:28.077796: val_loss -0.7901 
2025-01-25 01:05:28.080484: Pseudo dice [np.float32(0.9546), np.float32(0.9321)] 
2025-01-25 01:05:28.082853: Epoch time: 48.89 s 
2025-01-25 01:05:29.370379:  
2025-01-25 01:05:29.373275: Epoch 809 
2025-01-25 01:05:29.375960: Current learning rate: 0.00225 
2025-01-25 01:06:18.290833: train_loss -0.8421 
2025-01-25 01:06:18.294351: val_loss -0.7761 
2025-01-25 01:06:18.297356: Pseudo dice [np.float32(0.9443), np.float32(0.929)] 
2025-01-25 01:06:18.300013: Epoch time: 48.92 s 
2025-01-25 01:06:19.584873:  
2025-01-25 01:06:19.587731: Epoch 810 
2025-01-25 01:06:19.590830: Current learning rate: 0.00224 
2025-01-25 01:07:08.690336: train_loss -0.8331 
2025-01-25 01:07:08.697112: val_loss -0.794 
2025-01-25 01:07:08.699935: Pseudo dice [np.float32(0.9459), np.float32(0.9253)] 
2025-01-25 01:07:08.702851: Epoch time: 49.11 s 
2025-01-25 01:07:09.982002:  
2025-01-25 01:07:09.984455: Epoch 811 
2025-01-25 01:07:09.986783: Current learning rate: 0.00223 
2025-01-25 01:07:58.929981: train_loss -0.8403 
2025-01-25 01:07:58.933099: val_loss -0.7777 
2025-01-25 01:07:58.935634: Pseudo dice [np.float32(0.9494), np.float32(0.9296)] 
2025-01-25 01:07:58.937909: Epoch time: 48.95 s 
2025-01-25 01:08:00.214457:  
2025-01-25 01:08:00.216853: Epoch 812 
2025-01-25 01:08:00.219290: Current learning rate: 0.00222 
2025-01-25 01:08:49.262592: train_loss -0.829 
2025-01-25 01:08:49.269334: val_loss -0.794 
2025-01-25 01:08:49.272376: Pseudo dice [np.float32(0.9426), np.float32(0.9319)] 
2025-01-25 01:08:49.275231: Epoch time: 49.05 s 
2025-01-25 01:08:50.569167:  
2025-01-25 01:08:50.571790: Epoch 813 
2025-01-25 01:08:50.574633: Current learning rate: 0.00221 
2025-01-25 01:09:39.297959: train_loss -0.8426 
2025-01-25 01:09:39.301381: val_loss -0.7417 
2025-01-25 01:09:39.304008: Pseudo dice [np.float32(0.9531), np.float32(0.9318)] 
2025-01-25 01:09:39.306529: Epoch time: 48.73 s 
2025-01-25 01:09:40.596152:  
2025-01-25 01:09:40.599368: Epoch 814 
2025-01-25 01:09:40.602316: Current learning rate: 0.0022 
2025-01-25 01:10:29.394764: train_loss -0.8506 
2025-01-25 01:10:29.401019: val_loss -0.7721 
2025-01-25 01:10:29.403481: Pseudo dice [np.float32(0.9541), np.float32(0.9377)] 
2025-01-25 01:10:29.405909: Epoch time: 48.8 s 
2025-01-25 01:10:30.688652:  
2025-01-25 01:10:30.691225: Epoch 815 
2025-01-25 01:10:30.693930: Current learning rate: 0.00219 
2025-01-25 01:11:19.746596: train_loss -0.8456 
2025-01-25 01:11:19.750282: val_loss -0.735 
2025-01-25 01:11:19.753049: Pseudo dice [np.float32(0.943), np.float32(0.9251)] 
2025-01-25 01:11:19.756091: Epoch time: 49.06 s 
2025-01-25 01:11:21.040702:  
2025-01-25 01:11:21.044048: Epoch 816 
2025-01-25 01:11:21.047312: Current learning rate: 0.00218 
2025-01-25 01:12:10.148184: train_loss -0.8325 
2025-01-25 01:12:10.154871: val_loss -0.7537 
2025-01-25 01:12:10.157480: Pseudo dice [np.float32(0.955), np.float32(0.9341)] 
2025-01-25 01:12:10.160296: Epoch time: 49.11 s 
2025-01-25 01:12:11.449277:  
2025-01-25 01:12:11.452479: Epoch 817 
2025-01-25 01:12:11.455232: Current learning rate: 0.00217 
2025-01-25 01:13:00.176280: train_loss -0.8392 
2025-01-25 01:13:00.180053: val_loss -0.7757 
2025-01-25 01:13:00.183063: Pseudo dice [np.float32(0.9526), np.float32(0.9125)] 
2025-01-25 01:13:00.185591: Epoch time: 48.73 s 
2025-01-25 01:13:02.316059:  
2025-01-25 01:13:02.319228: Epoch 818 
2025-01-25 01:13:02.321970: Current learning rate: 0.00216 
2025-01-25 01:13:51.133477: train_loss -0.8377 
2025-01-25 01:13:51.141573: val_loss -0.7703 
2025-01-25 01:13:51.143999: Pseudo dice [np.float32(0.9599), np.float32(0.9407)] 
2025-01-25 01:13:51.146684: Epoch time: 48.82 s 
2025-01-25 01:13:52.427709:  
2025-01-25 01:13:52.430761: Epoch 819 
2025-01-25 01:13:52.433523: Current learning rate: 0.00215 
2025-01-25 01:14:41.500933: train_loss -0.8535 
2025-01-25 01:14:41.504060: val_loss -0.7327 
2025-01-25 01:14:41.506408: Pseudo dice [np.float32(0.9539), np.float32(0.9151)] 
2025-01-25 01:14:41.508772: Epoch time: 49.07 s 
2025-01-25 01:14:42.726200:  
2025-01-25 01:14:42.729216: Epoch 820 
2025-01-25 01:14:42.732192: Current learning rate: 0.00214 
2025-01-25 01:15:31.756878: train_loss -0.851 
2025-01-25 01:15:31.762782: val_loss -0.801 
2025-01-25 01:15:31.765270: Pseudo dice [np.float32(0.9561), np.float32(0.9351)] 
2025-01-25 01:15:31.767730: Epoch time: 49.03 s 
2025-01-25 01:15:32.990515:  
2025-01-25 01:15:32.993631: Epoch 821 
2025-01-25 01:15:32.996594: Current learning rate: 0.00213 
2025-01-25 01:16:22.438426: train_loss -0.8441 
2025-01-25 01:16:22.441737: val_loss -0.7766 
2025-01-25 01:16:22.444220: Pseudo dice [np.float32(0.956), np.float32(0.9419)] 
2025-01-25 01:16:22.446903: Epoch time: 49.45 s 
2025-01-25 01:16:22.449473: Yayy! New best EMA pseudo Dice: 0.9377999901771545 
2025-01-25 01:16:24.194969:  
2025-01-25 01:16:24.197660: Epoch 822 
2025-01-25 01:16:24.199993: Current learning rate: 0.00212 
2025-01-25 01:17:13.044811: train_loss -0.8227 
2025-01-25 01:17:13.062773: val_loss -0.757 
2025-01-25 01:17:13.066312: Pseudo dice [np.float32(0.9341), np.float32(0.9163)] 
2025-01-25 01:17:13.069301: Epoch time: 48.85 s 
2025-01-25 01:17:14.300449:  
2025-01-25 01:17:14.303678: Epoch 823 
2025-01-25 01:17:14.306366: Current learning rate: 0.0021 
2025-01-25 01:18:03.365652: train_loss -0.8445 
2025-01-25 01:18:03.370118: val_loss -0.7945 
2025-01-25 01:18:03.373129: Pseudo dice [np.float32(0.9531), np.float32(0.9358)] 
2025-01-25 01:18:03.375996: Epoch time: 49.07 s 
2025-01-25 01:18:04.597518:  
2025-01-25 01:18:04.600099: Epoch 824 
2025-01-25 01:18:04.602685: Current learning rate: 0.00209 
2025-01-25 01:18:53.535084: train_loss -0.842 
2025-01-25 01:18:53.542207: val_loss -0.772 
2025-01-25 01:18:53.544989: Pseudo dice [np.float32(0.9516), np.float32(0.9108)] 
2025-01-25 01:18:53.547510: Epoch time: 48.94 s 
2025-01-25 01:18:54.772746:  
2025-01-25 01:18:54.775884: Epoch 825 
2025-01-25 01:18:54.779616: Current learning rate: 0.00208 
2025-01-25 01:19:44.065462: train_loss -0.8466 
2025-01-25 01:19:44.069626: val_loss -0.7596 
2025-01-25 01:19:44.072664: Pseudo dice [np.float32(0.9544), np.float32(0.931)] 
2025-01-25 01:19:44.075341: Epoch time: 49.29 s 
2025-01-25 01:19:45.306301:  
2025-01-25 01:19:45.309407: Epoch 826 
2025-01-25 01:19:45.312304: Current learning rate: 0.00207 
2025-01-25 01:20:33.984009: train_loss -0.8354 
2025-01-25 01:20:33.989299: val_loss -0.7382 
2025-01-25 01:20:33.992172: Pseudo dice [np.float32(0.9459), np.float32(0.906)] 
2025-01-25 01:20:33.994924: Epoch time: 48.68 s 
2025-01-25 01:20:35.215549:  
2025-01-25 01:20:35.218809: Epoch 827 
2025-01-25 01:20:35.222147: Current learning rate: 0.00206 
2025-01-25 01:21:23.952255: train_loss -0.8402 
2025-01-25 01:21:23.957224: val_loss -0.7718 
2025-01-25 01:21:23.960826: Pseudo dice [np.float32(0.9489), np.float32(0.9034)] 
2025-01-25 01:21:23.964300: Epoch time: 48.74 s 
2025-01-25 01:21:25.189547:  
2025-01-25 01:21:25.193221: Epoch 828 
2025-01-25 01:21:25.196581: Current learning rate: 0.00205 
2025-01-25 01:22:14.331786: train_loss -0.8238 
2025-01-25 01:22:14.337368: val_loss -0.7633 
2025-01-25 01:22:14.340040: Pseudo dice [np.float32(0.9395), np.float32(0.9216)] 
2025-01-25 01:22:14.342588: Epoch time: 49.14 s 
2025-01-25 01:22:15.564780:  
2025-01-25 01:22:15.567776: Epoch 829 
2025-01-25 01:22:15.570630: Current learning rate: 0.00204 
2025-01-25 01:23:04.390070: train_loss -0.8347 
2025-01-25 01:23:04.394489: val_loss -0.796 
2025-01-25 01:23:04.397519: Pseudo dice [np.float32(0.9445), np.float32(0.9258)] 
2025-01-25 01:23:04.400324: Epoch time: 48.83 s 
2025-01-25 01:23:05.629721:  
2025-01-25 01:23:05.632284: Epoch 830 
2025-01-25 01:23:05.635066: Current learning rate: 0.00203 
2025-01-25 01:23:54.059048: train_loss -0.8274 
2025-01-25 01:23:54.065161: val_loss -0.765 
2025-01-25 01:23:54.067923: Pseudo dice [np.float32(0.9552), np.float32(0.924)] 
2025-01-25 01:23:54.071182: Epoch time: 48.43 s 
2025-01-25 01:23:55.296237:  
2025-01-25 01:23:55.299262: Epoch 831 
2025-01-25 01:23:55.301889: Current learning rate: 0.00202 
2025-01-25 01:24:44.111128: train_loss -0.8403 
2025-01-25 01:24:44.115923: val_loss -0.76 
2025-01-25 01:24:44.119026: Pseudo dice [np.float32(0.9496), np.float32(0.9381)] 
2025-01-25 01:24:44.122006: Epoch time: 48.82 s 
2025-01-25 01:24:45.358689:  
2025-01-25 01:24:45.363873: Epoch 832 
2025-01-25 01:24:45.366688: Current learning rate: 0.00201 
2025-01-25 01:25:34.124258: train_loss -0.8602 
2025-01-25 01:25:34.129890: val_loss -0.7412 
2025-01-25 01:25:34.132707: Pseudo dice [np.float32(0.9519), np.float32(0.899)] 
2025-01-25 01:25:34.135815: Epoch time: 48.77 s 
2025-01-25 01:25:35.362888:  
2025-01-25 01:25:35.365497: Epoch 833 
2025-01-25 01:25:35.368172: Current learning rate: 0.002 
2025-01-25 01:26:24.455128: train_loss -0.8456 
2025-01-25 01:26:24.459882: val_loss -0.7948 
2025-01-25 01:26:24.462904: Pseudo dice [np.float32(0.9432), np.float32(0.9189)] 
2025-01-25 01:26:24.465648: Epoch time: 49.09 s 
2025-01-25 01:26:25.693473:  
2025-01-25 01:26:25.696383: Epoch 834 
2025-01-25 01:26:25.699290: Current learning rate: 0.00199 
2025-01-25 01:27:14.899852: train_loss -0.8286 
2025-01-25 01:27:14.905058: val_loss -0.7166 
2025-01-25 01:27:14.907830: Pseudo dice [np.float32(0.9532), np.float32(0.921)] 
2025-01-25 01:27:14.910507: Epoch time: 49.21 s 
2025-01-25 01:27:16.122478:  
2025-01-25 01:27:16.126118: Epoch 835 
2025-01-25 01:27:16.129021: Current learning rate: 0.00198 
2025-01-25 01:28:04.979711: train_loss -0.8449 
2025-01-25 01:28:04.984619: val_loss -0.8115 
2025-01-25 01:28:04.987868: Pseudo dice [np.float32(0.951), np.float32(0.9274)] 
2025-01-25 01:28:04.990794: Epoch time: 48.86 s 
2025-01-25 01:28:06.851209:  
2025-01-25 01:28:06.853871: Epoch 836 
2025-01-25 01:28:06.856335: Current learning rate: 0.00196 
2025-01-25 01:28:55.878114: train_loss -0.8443 
2025-01-25 01:28:55.883206: val_loss -0.7662 
2025-01-25 01:28:55.885998: Pseudo dice [np.float32(0.946), np.float32(0.9141)] 
2025-01-25 01:28:55.888382: Epoch time: 49.03 s 
2025-01-25 01:28:57.103487:  
2025-01-25 01:28:57.106772: Epoch 837 
2025-01-25 01:28:57.109546: Current learning rate: 0.00195 
2025-01-25 01:29:46.199360: train_loss -0.8494 
2025-01-25 01:29:46.203932: val_loss -0.7656 
2025-01-25 01:29:46.206813: Pseudo dice [np.float32(0.9263), np.float32(0.92)] 
2025-01-25 01:29:46.209878: Epoch time: 49.1 s 
2025-01-25 01:29:47.435409:  
2025-01-25 01:29:47.437994: Epoch 838 
2025-01-25 01:29:47.440633: Current learning rate: 0.00194 
2025-01-25 01:30:35.917924: train_loss -0.8482 
2025-01-25 01:30:35.923625: val_loss -0.7311 
2025-01-25 01:30:35.926155: Pseudo dice [np.float32(0.9571), np.float32(0.9353)] 
2025-01-25 01:30:35.928939: Epoch time: 48.48 s 
2025-01-25 01:30:37.148679:  
2025-01-25 01:30:37.151716: Epoch 839 
2025-01-25 01:30:37.154629: Current learning rate: 0.00193 
2025-01-25 01:31:25.449740: train_loss -0.8452 
2025-01-25 01:31:25.454131: val_loss -0.7192 
2025-01-25 01:31:25.457521: Pseudo dice [np.float32(0.9437), np.float32(0.9308)] 
2025-01-25 01:31:25.460200: Epoch time: 48.3 s 
2025-01-25 01:31:26.688366:  
2025-01-25 01:31:26.691305: Epoch 840 
2025-01-25 01:31:26.694389: Current learning rate: 0.00192 
2025-01-25 01:32:16.136788: train_loss -0.848 
2025-01-25 01:32:16.143716: val_loss -0.7807 
2025-01-25 01:32:16.146702: Pseudo dice [np.float32(0.9448), np.float32(0.9161)] 
2025-01-25 01:32:16.149006: Epoch time: 49.45 s 
2025-01-25 01:32:17.371752:  
2025-01-25 01:32:17.374501: Epoch 841 
2025-01-25 01:32:17.377399: Current learning rate: 0.00191 
2025-01-25 01:33:05.912406: train_loss -0.8271 
2025-01-25 01:33:05.916789: val_loss -0.7115 
2025-01-25 01:33:05.919953: Pseudo dice [np.float32(0.9412), np.float32(0.8799)] 
2025-01-25 01:33:05.922799: Epoch time: 48.54 s 
2025-01-25 01:33:07.148849:  
2025-01-25 01:33:07.152084: Epoch 842 
2025-01-25 01:33:07.154888: Current learning rate: 0.0019 
2025-01-25 01:33:55.482267: train_loss -0.8289 
2025-01-25 01:33:55.487539: val_loss -0.703 
2025-01-25 01:33:55.490104: Pseudo dice [np.float32(0.9423), np.float32(0.804)] 
2025-01-25 01:33:55.492552: Epoch time: 48.33 s 
2025-01-25 01:33:56.723703:  
2025-01-25 01:33:56.727303: Epoch 843 
2025-01-25 01:33:56.730223: Current learning rate: 0.00189 
2025-01-25 01:34:45.731277: train_loss -0.841 
2025-01-25 01:34:45.735610: val_loss -0.768 
2025-01-25 01:34:45.740263: Pseudo dice [np.float32(0.9392), np.float32(0.9113)] 
2025-01-25 01:34:45.742980: Epoch time: 49.01 s 
2025-01-25 01:34:46.962528:  
2025-01-25 01:34:46.965058: Epoch 844 
2025-01-25 01:34:46.967551: Current learning rate: 0.00188 
2025-01-25 01:35:35.390621: train_loss -0.8574 
2025-01-25 01:35:35.396776: val_loss -0.7543 
2025-01-25 01:35:35.399094: Pseudo dice [np.float32(0.9501), np.float32(0.9224)] 
2025-01-25 01:35:35.401887: Epoch time: 48.43 s 
2025-01-25 01:35:36.625651:  
2025-01-25 01:35:36.628830: Epoch 845 
2025-01-25 01:35:36.631607: Current learning rate: 0.00187 
2025-01-25 01:36:25.799737: train_loss -0.8438 
2025-01-25 01:36:25.804014: val_loss -0.7364 
2025-01-25 01:36:25.806530: Pseudo dice [np.float32(0.9456), np.float32(0.9091)] 
2025-01-25 01:36:25.808986: Epoch time: 49.18 s 
2025-01-25 01:36:27.040598:  
2025-01-25 01:36:27.043523: Epoch 846 
2025-01-25 01:36:27.046577: Current learning rate: 0.00186 
2025-01-25 01:37:15.982815: train_loss -0.8439 
2025-01-25 01:37:15.988169: val_loss -0.7686 
2025-01-25 01:37:15.991359: Pseudo dice [np.float32(0.9522), np.float32(0.9254)] 
2025-01-25 01:37:15.993903: Epoch time: 48.94 s 
2025-01-25 01:37:17.208007:  
2025-01-25 01:37:17.211488: Epoch 847 
2025-01-25 01:37:17.214256: Current learning rate: 0.00185 
2025-01-25 01:38:05.961461: train_loss -0.8408 
2025-01-25 01:38:05.965778: val_loss -0.7667 
2025-01-25 01:38:05.968740: Pseudo dice [np.float32(0.9523), np.float32(0.9236)] 
2025-01-25 01:38:05.971618: Epoch time: 48.75 s 
2025-01-25 01:38:07.190675:  
2025-01-25 01:38:07.193221: Epoch 848 
2025-01-25 01:38:07.195915: Current learning rate: 0.00184 
2025-01-25 01:38:55.689146: train_loss -0.8443 
2025-01-25 01:38:55.694068: val_loss -0.7422 
2025-01-25 01:38:55.696625: Pseudo dice [np.float32(0.9391), np.float32(0.8813)] 
2025-01-25 01:38:55.698950: Epoch time: 48.5 s 
2025-01-25 01:38:56.917006:  
2025-01-25 01:38:56.919958: Epoch 849 
2025-01-25 01:38:56.922801: Current learning rate: 0.00182 
2025-01-25 01:39:46.063831: train_loss -0.8262 
2025-01-25 01:39:46.070854: val_loss -0.7866 
2025-01-25 01:39:46.074000: Pseudo dice [np.float32(0.9475), np.float32(0.9319)] 
2025-01-25 01:39:46.076937: Epoch time: 49.15 s 
2025-01-25 01:39:47.864620:  
2025-01-25 01:39:47.867618: Epoch 850 
2025-01-25 01:39:47.870292: Current learning rate: 0.00181 
2025-01-25 01:40:37.005625: train_loss -0.8411 
2025-01-25 01:40:37.010584: val_loss -0.7209 
2025-01-25 01:40:37.013400: Pseudo dice [np.float32(0.9539), np.float32(0.918)] 
2025-01-25 01:40:37.016096: Epoch time: 49.14 s 
2025-01-25 01:40:38.218867:  
2025-01-25 01:40:38.221916: Epoch 851 
2025-01-25 01:40:38.224812: Current learning rate: 0.0018 
2025-01-25 01:41:26.985087: train_loss -0.8412 
2025-01-25 01:41:26.989730: val_loss -0.7576 
2025-01-25 01:41:26.992806: Pseudo dice [np.float32(0.9289), np.float32(0.8709)] 
2025-01-25 01:41:26.995656: Epoch time: 48.77 s 
2025-01-25 01:41:28.206290:  
2025-01-25 01:41:28.208940: Epoch 852 
2025-01-25 01:41:28.211587: Current learning rate: 0.00179 
2025-01-25 01:42:16.911496: train_loss -0.845 
2025-01-25 01:42:16.916610: val_loss -0.7652 
2025-01-25 01:42:16.919267: Pseudo dice [np.float32(0.9558), np.float32(0.8951)] 
2025-01-25 01:42:16.922000: Epoch time: 48.71 s 
2025-01-25 01:42:18.132376:  
2025-01-25 01:42:18.135304: Epoch 853 
2025-01-25 01:42:18.138022: Current learning rate: 0.00178 
2025-01-25 01:43:07.147239: train_loss -0.8431 
2025-01-25 01:43:07.151582: val_loss -0.7908 
2025-01-25 01:43:07.154404: Pseudo dice [np.float32(0.9572), np.float32(0.926)] 
2025-01-25 01:43:07.157030: Epoch time: 49.02 s 
2025-01-25 01:43:08.361112:  
2025-01-25 01:43:08.364037: Epoch 854 
2025-01-25 01:43:08.367238: Current learning rate: 0.00177 
2025-01-25 01:43:57.261750: train_loss -0.8376 
2025-01-25 01:43:57.267184: val_loss -0.7615 
2025-01-25 01:43:57.269910: Pseudo dice [np.float32(0.951), np.float32(0.9083)] 
2025-01-25 01:43:57.272782: Epoch time: 48.9 s 
2025-01-25 01:43:59.148156:  
2025-01-25 01:43:59.150952: Epoch 855 
2025-01-25 01:43:59.153684: Current learning rate: 0.00176 
2025-01-25 01:44:48.011775: train_loss -0.8371 
2025-01-25 01:44:48.015229: val_loss -0.8052 
2025-01-25 01:44:48.017475: Pseudo dice [np.float32(0.9491), np.float32(0.9065)] 
2025-01-25 01:44:48.019627: Epoch time: 48.86 s 
2025-01-25 01:44:49.224108:  
2025-01-25 01:44:49.227288: Epoch 856 
2025-01-25 01:44:49.229918: Current learning rate: 0.00175 
2025-01-25 01:45:38.004576: train_loss -0.8412 
2025-01-25 01:45:38.010019: val_loss -0.7581 
2025-01-25 01:45:38.012635: Pseudo dice [np.float32(0.9471), np.float32(0.9024)] 
2025-01-25 01:45:38.015005: Epoch time: 48.78 s 
2025-01-25 01:45:39.229599:  
2025-01-25 01:45:39.232024: Epoch 857 
2025-01-25 01:45:39.234653: Current learning rate: 0.00174 
2025-01-25 01:46:27.842567: train_loss -0.8514 
2025-01-25 01:46:27.846585: val_loss -0.7686 
2025-01-25 01:46:27.849243: Pseudo dice [np.float32(0.9493), np.float32(0.8996)] 
2025-01-25 01:46:27.852055: Epoch time: 48.61 s 
2025-01-25 01:46:29.059317:  
2025-01-25 01:46:29.062468: Epoch 858 
2025-01-25 01:46:29.065265: Current learning rate: 0.00173 
2025-01-25 01:47:17.728293: train_loss -0.8469 
2025-01-25 01:47:17.733391: val_loss -0.7959 
2025-01-25 01:47:17.736263: Pseudo dice [np.float32(0.9601), np.float32(0.9424)] 
2025-01-25 01:47:17.738512: Epoch time: 48.67 s 
2025-01-25 01:47:18.942803:  
2025-01-25 01:47:18.945547: Epoch 859 
2025-01-25 01:47:18.948097: Current learning rate: 0.00172 
2025-01-25 01:48:07.482980: train_loss -0.8353 
2025-01-25 01:48:07.487234: val_loss -0.7581 
2025-01-25 01:48:07.489913: Pseudo dice [np.float32(0.9561), np.float32(0.9146)] 
2025-01-25 01:48:07.492257: Epoch time: 48.54 s 
2025-01-25 01:48:08.706132:  
2025-01-25 01:48:08.708986: Epoch 860 
2025-01-25 01:48:08.711973: Current learning rate: 0.0017 
2025-01-25 01:48:57.892773: train_loss -0.8421 
2025-01-25 01:48:57.898364: val_loss -0.764 
2025-01-25 01:48:57.901345: Pseudo dice [np.float32(0.9431), np.float32(0.9137)] 
2025-01-25 01:48:57.903773: Epoch time: 49.19 s 
2025-01-25 01:48:59.109083:  
2025-01-25 01:48:59.112253: Epoch 861 
2025-01-25 01:48:59.114956: Current learning rate: 0.00169 
2025-01-25 01:49:47.963495: train_loss -0.8488 
2025-01-25 01:49:47.968515: val_loss -0.7593 
2025-01-25 01:49:47.971558: Pseudo dice [np.float32(0.9617), np.float32(0.9047)] 
2025-01-25 01:49:47.974258: Epoch time: 48.86 s 
2025-01-25 01:49:49.185395:  
2025-01-25 01:49:49.188864: Epoch 862 
2025-01-25 01:49:49.191815: Current learning rate: 0.00168 
2025-01-25 01:50:37.981419: train_loss -0.8407 
2025-01-25 01:50:37.986870: val_loss -0.7849 
2025-01-25 01:50:37.990224: Pseudo dice [np.float32(0.9565), np.float32(0.9271)] 
2025-01-25 01:50:37.992990: Epoch time: 48.8 s 
2025-01-25 01:50:39.203327:  
2025-01-25 01:50:39.205941: Epoch 863 
2025-01-25 01:50:39.208370: Current learning rate: 0.00167 
2025-01-25 01:51:28.429015: train_loss -0.8543 
2025-01-25 01:51:28.433571: val_loss -0.7699 
2025-01-25 01:51:28.436346: Pseudo dice [np.float32(0.9586), np.float32(0.9205)] 
2025-01-25 01:51:28.439179: Epoch time: 49.23 s 
2025-01-25 01:51:29.651965:  
2025-01-25 01:51:29.654902: Epoch 864 
2025-01-25 01:51:29.657595: Current learning rate: 0.00166 
2025-01-25 01:52:19.074147: train_loss -0.8504 
2025-01-25 01:52:19.079841: val_loss -0.7895 
2025-01-25 01:52:19.082315: Pseudo dice [np.float32(0.9559), np.float32(0.9365)] 
2025-01-25 01:52:19.084698: Epoch time: 49.42 s 
2025-01-25 01:52:20.292462:  
2025-01-25 01:52:20.295140: Epoch 865 
2025-01-25 01:52:20.297530: Current learning rate: 0.00165 
2025-01-25 01:53:08.772007: train_loss -0.8502 
2025-01-25 01:53:08.776024: val_loss -0.761 
2025-01-25 01:53:08.778664: Pseudo dice [np.float32(0.938), np.float32(0.9086)] 
2025-01-25 01:53:08.781177: Epoch time: 48.48 s 
2025-01-25 01:53:09.997855:  
2025-01-25 01:53:10.000847: Epoch 866 
2025-01-25 01:53:10.003866: Current learning rate: 0.00164 
2025-01-25 01:53:58.942037: train_loss -0.8464 
2025-01-25 01:53:58.947566: val_loss -0.7758 
2025-01-25 01:53:58.950428: Pseudo dice [np.float32(0.946), np.float32(0.9071)] 
2025-01-25 01:53:58.953037: Epoch time: 48.95 s 
2025-01-25 01:54:00.169676:  
2025-01-25 01:54:00.172890: Epoch 867 
2025-01-25 01:54:00.176342: Current learning rate: 0.00163 
2025-01-25 01:54:48.881511: train_loss -0.8445 
2025-01-25 01:54:48.885615: val_loss -0.7641 
2025-01-25 01:54:48.888340: Pseudo dice [np.float32(0.95), np.float32(0.9162)] 
2025-01-25 01:54:48.890885: Epoch time: 48.71 s 
2025-01-25 01:54:50.106149:  
2025-01-25 01:54:50.108842: Epoch 868 
2025-01-25 01:54:50.111449: Current learning rate: 0.00162 
2025-01-25 01:55:38.734459: train_loss -0.8331 
2025-01-25 01:55:38.742072: val_loss -0.7435 
2025-01-25 01:55:38.744979: Pseudo dice [np.float32(0.9472), np.float32(0.91)] 
2025-01-25 01:55:38.747859: Epoch time: 48.63 s 
2025-01-25 01:55:39.965780:  
2025-01-25 01:55:39.968914: Epoch 869 
2025-01-25 01:55:39.971271: Current learning rate: 0.00161 
2025-01-25 01:56:28.698647: train_loss -0.8663 
2025-01-25 01:56:28.702876: val_loss -0.7478 
2025-01-25 01:56:28.705617: Pseudo dice [np.float32(0.9444), np.float32(0.8915)] 
2025-01-25 01:56:28.708113: Epoch time: 48.73 s 
2025-01-25 01:56:29.923504:  
2025-01-25 01:56:29.926376: Epoch 870 
2025-01-25 01:56:29.929093: Current learning rate: 0.00159 
2025-01-25 01:57:18.731263: train_loss -0.8449 
2025-01-25 01:57:18.736644: val_loss -0.7427 
2025-01-25 01:57:18.739393: Pseudo dice [np.float32(0.9551), np.float32(0.9052)] 
2025-01-25 01:57:18.741818: Epoch time: 48.81 s 
2025-01-25 01:57:19.944774:  
2025-01-25 01:57:19.948106: Epoch 871 
2025-01-25 01:57:19.951035: Current learning rate: 0.00158 
2025-01-25 01:58:08.204617: train_loss -0.8456 
2025-01-25 01:58:08.208359: val_loss -0.7287 
2025-01-25 01:58:08.210957: Pseudo dice [np.float32(0.9504), np.float32(0.9185)] 
2025-01-25 01:58:08.213560: Epoch time: 48.26 s 
2025-01-25 01:58:09.420832:  
2025-01-25 01:58:09.423951: Epoch 872 
2025-01-25 01:58:09.426803: Current learning rate: 0.00157 
2025-01-25 01:58:57.957105: train_loss -0.8443 
2025-01-25 01:58:57.962222: val_loss -0.7374 
2025-01-25 01:58:57.964578: Pseudo dice [np.float32(0.9542), np.float32(0.9176)] 
2025-01-25 01:58:57.967027: Epoch time: 48.54 s 
2025-01-25 01:58:59.175108:  
2025-01-25 01:58:59.178386: Epoch 873 
2025-01-25 01:58:59.181581: Current learning rate: 0.00156 
2025-01-25 01:59:47.920957: train_loss -0.8514 
2025-01-25 01:59:47.924990: val_loss -0.7292 
2025-01-25 01:59:47.927903: Pseudo dice [np.float32(0.9504), np.float32(0.9347)] 
2025-01-25 01:59:47.930795: Epoch time: 48.75 s 
2025-01-25 01:59:49.794264:  
2025-01-25 01:59:49.797101: Epoch 874 
2025-01-25 01:59:49.799880: Current learning rate: 0.00155 
2025-01-25 02:00:38.503474: train_loss -0.8468 
2025-01-25 02:00:38.509249: val_loss -0.7517 
2025-01-25 02:00:38.511873: Pseudo dice [np.float32(0.9515), np.float32(0.9261)] 
2025-01-25 02:00:38.514126: Epoch time: 48.71 s 
2025-01-25 02:00:39.725228:  
2025-01-25 02:00:39.728503: Epoch 875 
2025-01-25 02:00:39.731187: Current learning rate: 0.00154 
2025-01-25 02:01:28.323645: train_loss -0.8372 
2025-01-25 02:01:28.328161: val_loss -0.7565 
2025-01-25 02:01:28.331160: Pseudo dice [np.float32(0.9602), np.float32(0.9215)] 
2025-01-25 02:01:28.333900: Epoch time: 48.6 s 
2025-01-25 02:01:29.549686:  
2025-01-25 02:01:29.552533: Epoch 876 
2025-01-25 02:01:29.555133: Current learning rate: 0.00153 
2025-01-25 02:02:18.243596: train_loss -0.8451 
2025-01-25 02:02:18.249256: val_loss -0.7697 
2025-01-25 02:02:18.251966: Pseudo dice [np.float32(0.9482), np.float32(0.9248)] 
2025-01-25 02:02:18.254316: Epoch time: 48.7 s 
2025-01-25 02:02:19.462489:  
2025-01-25 02:02:19.465561: Epoch 877 
2025-01-25 02:02:19.469144: Current learning rate: 0.00152 
2025-01-25 02:03:08.119803: train_loss -0.8522 
2025-01-25 02:03:08.124126: val_loss -0.7469 
2025-01-25 02:03:08.127862: Pseudo dice [np.float32(0.9533), np.float32(0.8846)] 
2025-01-25 02:03:08.130409: Epoch time: 48.66 s 
2025-01-25 02:03:09.353683:  
2025-01-25 02:03:09.356614: Epoch 878 
2025-01-25 02:03:09.359177: Current learning rate: 0.00151 
2025-01-25 02:03:58.625443: train_loss -0.8333 
2025-01-25 02:03:58.631353: val_loss -0.7617 
2025-01-25 02:03:58.634020: Pseudo dice [np.float32(0.9541), np.float32(0.9309)] 
2025-01-25 02:03:58.636643: Epoch time: 49.27 s 
2025-01-25 02:03:59.857029:  
2025-01-25 02:03:59.860015: Epoch 879 
2025-01-25 02:03:59.862956: Current learning rate: 0.00149 
2025-01-25 02:04:48.557406: train_loss -0.8459 
2025-01-25 02:04:48.561727: val_loss -0.758 
2025-01-25 02:04:48.564383: Pseudo dice [np.float32(0.9564), np.float32(0.917)] 
2025-01-25 02:04:48.566942: Epoch time: 48.7 s 
2025-01-25 02:04:49.817723:  
2025-01-25 02:04:49.820408: Epoch 880 
2025-01-25 02:04:49.823081: Current learning rate: 0.00148 
2025-01-25 02:05:38.387089: train_loss -0.8421 
2025-01-25 02:05:38.393031: val_loss -0.7843 
2025-01-25 02:05:38.395809: Pseudo dice [np.float32(0.9563), np.float32(0.9292)] 
2025-01-25 02:05:38.398751: Epoch time: 48.57 s 
2025-01-25 02:05:39.618584:  
2025-01-25 02:05:39.621799: Epoch 881 
2025-01-25 02:05:39.624927: Current learning rate: 0.00147 
2025-01-25 02:06:28.538053: train_loss -0.8385 
2025-01-25 02:06:28.542796: val_loss -0.756 
2025-01-25 02:06:28.545561: Pseudo dice [np.float32(0.9538), np.float32(0.918)] 
2025-01-25 02:06:28.548355: Epoch time: 48.92 s 
2025-01-25 02:06:29.774340:  
2025-01-25 02:06:29.777218: Epoch 882 
2025-01-25 02:06:29.779755: Current learning rate: 0.00146 
2025-01-25 02:07:18.577687: train_loss -0.8439 
2025-01-25 02:07:18.583589: val_loss -0.7617 
2025-01-25 02:07:18.586690: Pseudo dice [np.float32(0.9465), np.float32(0.9077)] 
2025-01-25 02:07:18.589381: Epoch time: 48.8 s 
2025-01-25 02:07:19.806647:  
2025-01-25 02:07:19.809930: Epoch 883 
2025-01-25 02:07:19.813464: Current learning rate: 0.00145 
2025-01-25 02:08:08.686326: train_loss -0.8529 
2025-01-25 02:08:08.690812: val_loss -0.7845 
2025-01-25 02:08:08.693580: Pseudo dice [np.float32(0.9529), np.float32(0.9253)] 
2025-01-25 02:08:08.696243: Epoch time: 48.88 s 
2025-01-25 02:08:09.907587:  
2025-01-25 02:08:09.910332: Epoch 884 
2025-01-25 02:08:09.912809: Current learning rate: 0.00144 
2025-01-25 02:08:59.126994: train_loss -0.8475 
2025-01-25 02:08:59.132506: val_loss -0.7723 
2025-01-25 02:08:59.135365: Pseudo dice [np.float32(0.9564), np.float32(0.9134)] 
2025-01-25 02:08:59.137901: Epoch time: 49.22 s 
2025-01-25 02:09:00.353733:  
2025-01-25 02:09:00.357012: Epoch 885 
2025-01-25 02:09:00.359915: Current learning rate: 0.00143 
2025-01-25 02:09:49.417730: train_loss -0.8444 
2025-01-25 02:09:49.421870: val_loss -0.7975 
2025-01-25 02:09:49.424820: Pseudo dice [np.float32(0.9596), np.float32(0.927)] 
2025-01-25 02:09:49.427447: Epoch time: 49.07 s 
2025-01-25 02:09:50.640804:  
2025-01-25 02:09:50.643574: Epoch 886 
2025-01-25 02:09:50.646393: Current learning rate: 0.00142 
2025-01-25 02:10:39.358639: train_loss -0.8388 
2025-01-25 02:10:39.364982: val_loss -0.7779 
2025-01-25 02:10:39.368076: Pseudo dice [np.float32(0.9511), np.float32(0.9053)] 
2025-01-25 02:10:39.371058: Epoch time: 48.72 s 
2025-01-25 02:10:40.578637:  
2025-01-25 02:10:40.581424: Epoch 887 
2025-01-25 02:10:40.584257: Current learning rate: 0.00141 
2025-01-25 02:11:29.683813: train_loss -0.8434 
2025-01-25 02:11:29.688213: val_loss -0.7539 
2025-01-25 02:11:29.690900: Pseudo dice [np.float32(0.9541), np.float32(0.8491)] 
2025-01-25 02:11:29.693906: Epoch time: 49.11 s 
2025-01-25 02:11:30.909464:  
2025-01-25 02:11:30.912406: Epoch 888 
2025-01-25 02:11:30.915209: Current learning rate: 0.00139 
2025-01-25 02:12:20.001729: train_loss -0.8407 
2025-01-25 02:12:20.007049: val_loss -0.7484 
2025-01-25 02:12:20.009278: Pseudo dice [np.float32(0.9565), np.float32(0.9186)] 
2025-01-25 02:12:20.012056: Epoch time: 49.09 s 
2025-01-25 02:12:21.217302:  
2025-01-25 02:12:21.220732: Epoch 889 
2025-01-25 02:12:21.224047: Current learning rate: 0.00138 
2025-01-25 02:13:10.011938: train_loss -0.8451 
2025-01-25 02:13:10.017865: val_loss -0.7551 
2025-01-25 02:13:10.020226: Pseudo dice [np.float32(0.9526), np.float32(0.8871)] 
2025-01-25 02:13:10.022793: Epoch time: 48.8 s 
2025-01-25 02:13:11.239627:  
2025-01-25 02:13:11.242514: Epoch 890 
2025-01-25 02:13:11.245185: Current learning rate: 0.00137 
2025-01-25 02:14:00.310534: train_loss -0.832 
2025-01-25 02:14:00.315838: val_loss -0.796 
2025-01-25 02:14:00.318571: Pseudo dice [np.float32(0.9576), np.float32(0.9254)] 
2025-01-25 02:14:00.321367: Epoch time: 49.07 s 
2025-01-25 02:14:01.535232:  
2025-01-25 02:14:01.538039: Epoch 891 
2025-01-25 02:14:01.540489: Current learning rate: 0.00136 
2025-01-25 02:14:49.949420: train_loss -0.8508 
2025-01-25 02:14:49.953598: val_loss -0.7661 
2025-01-25 02:14:49.956110: Pseudo dice [np.float32(0.955), np.float32(0.9199)] 
2025-01-25 02:14:49.958941: Epoch time: 48.42 s 
2025-01-25 02:14:51.173224:  
2025-01-25 02:14:51.176404: Epoch 892 
2025-01-25 02:14:51.179286: Current learning rate: 0.00135 
2025-01-25 02:15:39.777020: train_loss -0.8576 
2025-01-25 02:15:39.782729: val_loss -0.7734 
2025-01-25 02:15:39.785260: Pseudo dice [np.float32(0.95), np.float32(0.8961)] 
2025-01-25 02:15:39.787890: Epoch time: 48.6 s 
2025-01-25 02:15:40.993787:  
2025-01-25 02:15:40.996333: Epoch 893 
2025-01-25 02:15:40.998768: Current learning rate: 0.00134 
2025-01-25 02:16:29.621849: train_loss -0.8557 
2025-01-25 02:16:29.626175: val_loss -0.7563 
2025-01-25 02:16:29.628990: Pseudo dice [np.float32(0.9472), np.float32(0.8871)] 
2025-01-25 02:16:29.631998: Epoch time: 48.63 s 
2025-01-25 02:16:31.513746:  
2025-01-25 02:16:31.516546: Epoch 894 
2025-01-25 02:16:31.519086: Current learning rate: 0.00133 
2025-01-25 02:17:20.097821: train_loss -0.8486 
2025-01-25 02:17:20.107100: val_loss -0.7712 
2025-01-25 02:17:20.109781: Pseudo dice [np.float32(0.9499), np.float32(0.9118)] 
2025-01-25 02:17:20.112530: Epoch time: 48.59 s 
2025-01-25 02:17:21.326023:  
2025-01-25 02:17:21.328882: Epoch 895 
2025-01-25 02:17:21.331566: Current learning rate: 0.00132 
2025-01-25 02:18:10.423166: train_loss -0.8475 
2025-01-25 02:18:10.427023: val_loss -0.7294 
2025-01-25 02:18:10.429727: Pseudo dice [np.float32(0.942), np.float32(0.9068)] 
2025-01-25 02:18:10.432404: Epoch time: 49.1 s 
2025-01-25 02:18:11.640589:  
2025-01-25 02:18:11.644824: Epoch 896 
2025-01-25 02:18:11.647641: Current learning rate: 0.0013 
2025-01-25 02:19:00.304944: train_loss -0.8473 
2025-01-25 02:19:00.310429: val_loss -0.7951 
2025-01-25 02:19:00.313105: Pseudo dice [np.float32(0.9558), np.float32(0.9223)] 
2025-01-25 02:19:00.315754: Epoch time: 48.67 s 
2025-01-25 02:19:01.522630:  
2025-01-25 02:19:01.525548: Epoch 897 
2025-01-25 02:19:01.528822: Current learning rate: 0.00129 
2025-01-25 02:19:50.184048: train_loss -0.8718 
2025-01-25 02:19:50.189562: val_loss -0.7462 
2025-01-25 02:19:50.191813: Pseudo dice [np.float32(0.9477), np.float32(0.9086)] 
2025-01-25 02:19:50.194267: Epoch time: 48.66 s 
2025-01-25 02:19:51.393800:  
2025-01-25 02:19:51.396939: Epoch 898 
2025-01-25 02:19:51.399930: Current learning rate: 0.00128 
2025-01-25 02:20:39.764086: train_loss -0.8404 
2025-01-25 02:20:39.770330: val_loss -0.7133 
2025-01-25 02:20:39.773365: Pseudo dice [np.float32(0.9339), np.float32(0.8442)] 
2025-01-25 02:20:39.776378: Epoch time: 48.37 s 
2025-01-25 02:20:40.992171:  
2025-01-25 02:20:40.994855: Epoch 899 
2025-01-25 02:20:40.997468: Current learning rate: 0.00127 
2025-01-25 02:21:30.584923: train_loss -0.8349 
2025-01-25 02:21:30.591488: val_loss -0.7591 
2025-01-25 02:21:30.594163: Pseudo dice [np.float32(0.9513), np.float32(0.9051)] 
2025-01-25 02:21:30.596689: Epoch time: 49.59 s 
2025-01-25 02:21:32.372505:  
2025-01-25 02:21:32.375226: Epoch 900 
2025-01-25 02:21:32.378228: Current learning rate: 0.00126 
2025-01-25 02:22:20.817772: train_loss -0.848 
2025-01-25 02:22:20.823522: val_loss -0.7234 
2025-01-25 02:22:20.826605: Pseudo dice [np.float32(0.9468), np.float32(0.8974)] 
2025-01-25 02:22:20.829473: Epoch time: 48.45 s 
2025-01-25 02:22:22.035976:  
2025-01-25 02:22:22.038758: Epoch 901 
2025-01-25 02:22:22.041333: Current learning rate: 0.00125 
2025-01-25 02:23:10.679035: train_loss -0.8299 
2025-01-25 02:23:10.684934: val_loss -0.7672 
2025-01-25 02:23:10.687651: Pseudo dice [np.float32(0.9467), np.float32(0.9232)] 
2025-01-25 02:23:10.690355: Epoch time: 48.64 s 
2025-01-25 02:23:11.903531:  
2025-01-25 02:23:11.908062: Epoch 902 
2025-01-25 02:23:11.910805: Current learning rate: 0.00124 
2025-01-25 02:24:01.363759: train_loss -0.8516 
2025-01-25 02:24:01.369830: val_loss -0.7756 
2025-01-25 02:24:01.372865: Pseudo dice [np.float32(0.9586), np.float32(0.9266)] 
2025-01-25 02:24:01.375557: Epoch time: 49.46 s 
2025-01-25 02:24:02.586691:  
2025-01-25 02:24:02.589775: Epoch 903 
2025-01-25 02:24:02.592718: Current learning rate: 0.00122 
2025-01-25 02:24:51.283178: train_loss -0.8611 
2025-01-25 02:24:51.287181: val_loss -0.771 
2025-01-25 02:24:51.289738: Pseudo dice [np.float32(0.9536), np.float32(0.8963)] 
2025-01-25 02:24:51.292257: Epoch time: 48.7 s 
2025-01-25 02:24:52.498870:  
2025-01-25 02:24:52.502204: Epoch 904 
2025-01-25 02:24:52.505144: Current learning rate: 0.00121 
2025-01-25 02:25:41.149053: train_loss -0.8389 
2025-01-25 02:25:41.154751: val_loss -0.7709 
2025-01-25 02:25:41.157498: Pseudo dice [np.float32(0.9534), np.float32(0.9133)] 
2025-01-25 02:25:41.160026: Epoch time: 48.65 s 
2025-01-25 02:25:42.372703:  
2025-01-25 02:25:42.375823: Epoch 905 
2025-01-25 02:25:42.378784: Current learning rate: 0.0012 
2025-01-25 02:26:30.912609: train_loss -0.8516 
2025-01-25 02:26:30.916907: val_loss -0.7612 
2025-01-25 02:26:30.919866: Pseudo dice [np.float32(0.9479), np.float32(0.9332)] 
2025-01-25 02:26:30.922623: Epoch time: 48.54 s 
2025-01-25 02:26:32.134766:  
2025-01-25 02:26:32.137629: Epoch 906 
2025-01-25 02:26:32.140509: Current learning rate: 0.00119 
2025-01-25 02:27:20.998988: train_loss -0.8459 
2025-01-25 02:27:21.004316: val_loss -0.7982 
2025-01-25 02:27:21.007370: Pseudo dice [np.float32(0.9519), np.float32(0.9157)] 
2025-01-25 02:27:21.009892: Epoch time: 48.87 s 
2025-01-25 02:27:22.229288:  
2025-01-25 02:27:22.232488: Epoch 907 
2025-01-25 02:27:22.235488: Current learning rate: 0.00118 
2025-01-25 02:28:11.032876: train_loss -0.8407 
2025-01-25 02:28:11.036844: val_loss -0.8218 
2025-01-25 02:28:11.039551: Pseudo dice [np.float32(0.9455), np.float32(0.9012)] 
2025-01-25 02:28:11.041833: Epoch time: 48.81 s 
2025-01-25 02:28:12.256554:  
2025-01-25 02:28:12.259779: Epoch 908 
2025-01-25 02:28:12.262644: Current learning rate: 0.00117 
2025-01-25 02:29:01.121901: train_loss -0.8508 
2025-01-25 02:29:01.126936: val_loss -0.7859 
2025-01-25 02:29:01.129483: Pseudo dice [np.float32(0.9447), np.float32(0.9291)] 
2025-01-25 02:29:01.132095: Epoch time: 48.87 s 
2025-01-25 02:29:02.342334:  
2025-01-25 02:29:02.345466: Epoch 909 
2025-01-25 02:29:02.348172: Current learning rate: 0.00116 
2025-01-25 02:29:50.589247: train_loss -0.8503 
2025-01-25 02:29:50.593073: val_loss -0.7663 
2025-01-25 02:29:50.595721: Pseudo dice [np.float32(0.9559), np.float32(0.8908)] 
2025-01-25 02:29:50.598350: Epoch time: 48.25 s 
2025-01-25 02:29:51.806447:  
2025-01-25 02:29:51.809956: Epoch 910 
2025-01-25 02:29:51.812752: Current learning rate: 0.00115 
2025-01-25 02:30:40.210027: train_loss -0.852 
2025-01-25 02:30:40.216126: val_loss -0.7468 
2025-01-25 02:30:40.219364: Pseudo dice [np.float32(0.9482), np.float32(0.9136)] 
2025-01-25 02:30:40.222235: Epoch time: 48.4 s 
2025-01-25 02:30:41.428889:  
2025-01-25 02:30:41.431592: Epoch 911 
2025-01-25 02:30:41.434125: Current learning rate: 0.00113 
2025-01-25 02:31:30.446413: train_loss -0.8476 
2025-01-25 02:31:30.450735: val_loss -0.7899 
2025-01-25 02:31:30.453742: Pseudo dice [np.float32(0.9577), np.float32(0.9252)] 
2025-01-25 02:31:30.456500: Epoch time: 49.02 s 
2025-01-25 02:31:31.673100:  
2025-01-25 02:31:31.675837: Epoch 912 
2025-01-25 02:31:31.678554: Current learning rate: 0.00112 
2025-01-25 02:32:20.134477: train_loss -0.8453 
2025-01-25 02:32:20.140576: val_loss -0.806 
2025-01-25 02:32:20.143337: Pseudo dice [np.float32(0.9512), np.float32(0.9366)] 
2025-01-25 02:32:20.146192: Epoch time: 48.46 s 
2025-01-25 02:32:22.000430:  
2025-01-25 02:32:22.003304: Epoch 913 
2025-01-25 02:32:22.006212: Current learning rate: 0.00111 
2025-01-25 02:33:10.647504: train_loss -0.8576 
2025-01-25 02:33:10.651657: val_loss -0.7692 
2025-01-25 02:33:10.653939: Pseudo dice [np.float32(0.9562), np.float32(0.9264)] 
2025-01-25 02:33:10.656490: Epoch time: 48.65 s 
2025-01-25 02:33:11.863983:  
2025-01-25 02:33:11.866969: Epoch 914 
2025-01-25 02:33:11.869729: Current learning rate: 0.0011 
2025-01-25 02:34:00.539519: train_loss -0.8418 
2025-01-25 02:34:00.552682: val_loss -0.7327 
2025-01-25 02:34:00.555810: Pseudo dice [np.float32(0.9521), np.float32(0.9271)] 
2025-01-25 02:34:00.558945: Epoch time: 48.68 s 
2025-01-25 02:34:01.769471:  
2025-01-25 02:34:01.772037: Epoch 915 
2025-01-25 02:34:01.774557: Current learning rate: 0.00109 
2025-01-25 02:34:50.793297: train_loss -0.8535 
2025-01-25 02:34:50.797678: val_loss -0.7893 
2025-01-25 02:34:50.800657: Pseudo dice [np.float32(0.9554), np.float32(0.9267)] 
2025-01-25 02:34:50.803265: Epoch time: 49.03 s 
2025-01-25 02:34:52.009518:  
2025-01-25 02:34:52.012174: Epoch 916 
2025-01-25 02:34:52.014736: Current learning rate: 0.00108 
2025-01-25 02:35:40.998353: train_loss -0.8566 
2025-01-25 02:35:41.004652: val_loss -0.7917 
2025-01-25 02:35:41.007462: Pseudo dice [np.float32(0.9529), np.float32(0.8971)] 
2025-01-25 02:35:41.010160: Epoch time: 48.99 s 
2025-01-25 02:35:42.217279:  
2025-01-25 02:35:42.219855: Epoch 917 
2025-01-25 02:35:42.222434: Current learning rate: 0.00106 
2025-01-25 02:36:30.329496: train_loss -0.8477 
2025-01-25 02:36:30.333375: val_loss -0.7921 
2025-01-25 02:36:30.336290: Pseudo dice [np.float32(0.9553), np.float32(0.9006)] 
2025-01-25 02:36:30.340419: Epoch time: 48.11 s 
2025-01-25 02:36:31.551407:  
2025-01-25 02:36:31.554540: Epoch 918 
2025-01-25 02:36:31.557679: Current learning rate: 0.00105 
2025-01-25 02:37:19.994786: train_loss -0.8572 
2025-01-25 02:37:20.000398: val_loss -0.8105 
2025-01-25 02:37:20.003062: Pseudo dice [np.float32(0.9578), np.float32(0.9203)] 
2025-01-25 02:37:20.005533: Epoch time: 48.44 s 
2025-01-25 02:37:21.217369:  
2025-01-25 02:37:21.220476: Epoch 919 
2025-01-25 02:37:21.223341: Current learning rate: 0.00104 
2025-01-25 02:38:09.818798: train_loss -0.8534 
2025-01-25 02:38:09.825677: val_loss -0.7305 
2025-01-25 02:38:09.829024: Pseudo dice [np.float32(0.9532), np.float32(0.8966)] 
2025-01-25 02:38:09.831877: Epoch time: 48.6 s 
2025-01-25 02:38:11.051264:  
2025-01-25 02:38:11.054269: Epoch 920 
2025-01-25 02:38:11.057173: Current learning rate: 0.00103 
2025-01-25 02:38:59.941059: train_loss -0.8437 
2025-01-25 02:38:59.946504: val_loss -0.7953 
2025-01-25 02:38:59.949186: Pseudo dice [np.float32(0.9517), np.float32(0.9367)] 
2025-01-25 02:38:59.951655: Epoch time: 48.89 s 
2025-01-25 02:39:01.164500:  
2025-01-25 02:39:01.167751: Epoch 921 
2025-01-25 02:39:01.170952: Current learning rate: 0.00102 
2025-01-25 02:39:49.707201: train_loss -0.8515 
2025-01-25 02:39:49.711329: val_loss -0.7868 
2025-01-25 02:39:49.713933: Pseudo dice [np.float32(0.9525), np.float32(0.9323)] 
2025-01-25 02:39:49.716507: Epoch time: 48.54 s 
2025-01-25 02:39:50.987650:  
2025-01-25 02:39:50.990342: Epoch 922 
2025-01-25 02:39:50.993044: Current learning rate: 0.00101 
2025-01-25 02:40:39.748308: train_loss -0.8505 
2025-01-25 02:40:39.754555: val_loss -0.7888 
2025-01-25 02:40:39.757868: Pseudo dice [np.float32(0.9543), np.float32(0.9211)] 
2025-01-25 02:40:39.760549: Epoch time: 48.76 s 
2025-01-25 02:40:40.990198:  
2025-01-25 02:40:40.993226: Epoch 923 
2025-01-25 02:40:40.996373: Current learning rate: 0.001 
2025-01-25 02:41:30.116085: train_loss -0.8514 
2025-01-25 02:41:30.120882: val_loss -0.7821 
2025-01-25 02:41:30.124056: Pseudo dice [np.float32(0.9507), np.float32(0.9321)] 
2025-01-25 02:41:30.127012: Epoch time: 49.13 s 
2025-01-25 02:41:31.343454:  
2025-01-25 02:41:31.346192: Epoch 924 
2025-01-25 02:41:31.348757: Current learning rate: 0.00098 
2025-01-25 02:42:19.929148: train_loss -0.8481 
2025-01-25 02:42:19.935645: val_loss -0.7957 
2025-01-25 02:42:19.938600: Pseudo dice [np.float32(0.959), np.float32(0.8983)] 
2025-01-25 02:42:19.941520: Epoch time: 48.59 s 
2025-01-25 02:42:21.155492:  
2025-01-25 02:42:21.158011: Epoch 925 
2025-01-25 02:42:21.160657: Current learning rate: 0.00097 
2025-01-25 02:43:09.459030: train_loss -0.837 
2025-01-25 02:43:09.463797: val_loss -0.7848 
2025-01-25 02:43:09.466884: Pseudo dice [np.float32(0.9608), np.float32(0.9332)] 
2025-01-25 02:43:09.469636: Epoch time: 48.3 s 
2025-01-25 02:43:10.688405:  
2025-01-25 02:43:10.691298: Epoch 926 
2025-01-25 02:43:10.694107: Current learning rate: 0.00096 
2025-01-25 02:43:59.292877: train_loss -0.8369 
2025-01-25 02:43:59.298191: val_loss -0.7914 
2025-01-25 02:43:59.300713: Pseudo dice [np.float32(0.9504), np.float32(0.9429)] 
2025-01-25 02:43:59.303208: Epoch time: 48.61 s 
2025-01-25 02:44:00.514138:  
2025-01-25 02:44:00.517087: Epoch 927 
2025-01-25 02:44:00.519990: Current learning rate: 0.00095 
2025-01-25 02:44:49.348520: train_loss -0.8567 
2025-01-25 02:44:49.352879: val_loss -0.8185 
2025-01-25 02:44:49.355743: Pseudo dice [np.float32(0.9544), np.float32(0.9411)] 
2025-01-25 02:44:49.358372: Epoch time: 48.84 s 
2025-01-25 02:44:49.361082: Yayy! New best EMA pseudo Dice: 0.9383000135421753 
2025-01-25 02:44:51.126541:  
2025-01-25 02:44:51.129289: Epoch 928 
2025-01-25 02:44:51.131930: Current learning rate: 0.00094 
2025-01-25 02:45:39.809611: train_loss -0.8459 
2025-01-25 02:45:39.814944: val_loss -0.7736 
2025-01-25 02:45:39.817473: Pseudo dice [np.float32(0.9498), np.float32(0.9362)] 
2025-01-25 02:45:39.820131: Epoch time: 48.68 s 
2025-01-25 02:45:39.822545: Yayy! New best EMA pseudo Dice: 0.9387000203132629 
2025-01-25 02:45:41.629706:  
2025-01-25 02:45:41.632743: Epoch 929 
2025-01-25 02:45:41.635582: Current learning rate: 0.00092 
2025-01-25 02:46:30.988628: train_loss -0.8295 
2025-01-25 02:46:30.992390: val_loss -0.7573 
2025-01-25 02:46:30.994971: Pseudo dice [np.float32(0.9571), np.float32(0.9323)] 
2025-01-25 02:46:30.997411: Epoch time: 49.36 s 
2025-01-25 02:46:30.999819: Yayy! New best EMA pseudo Dice: 0.939300000667572 
2025-01-25 02:46:32.869707:  
2025-01-25 02:46:32.872144: Epoch 930 
2025-01-25 02:46:32.874782: Current learning rate: 0.00091 
2025-01-25 02:47:22.063031: train_loss -0.8668 
2025-01-25 02:47:22.069042: val_loss -0.7586 
2025-01-25 02:47:22.072062: Pseudo dice [np.float32(0.954), np.float32(0.9345)] 
2025-01-25 02:47:22.074857: Epoch time: 49.19 s 
2025-01-25 02:47:22.077985: Yayy! New best EMA pseudo Dice: 0.9398000240325928 
2025-01-25 02:47:23.885601:  
2025-01-25 02:47:23.888883: Epoch 931 
2025-01-25 02:47:23.892076: Current learning rate: 0.0009 
2025-01-25 02:48:12.504611: train_loss -0.8392 
2025-01-25 02:48:12.508816: val_loss -0.7731 
2025-01-25 02:48:12.511660: Pseudo dice [np.float32(0.9525), np.float32(0.9193)] 
2025-01-25 02:48:12.514263: Epoch time: 48.62 s 
2025-01-25 02:48:14.394883:  
2025-01-25 02:48:14.397705: Epoch 932 
2025-01-25 02:48:14.400577: Current learning rate: 0.00089 
2025-01-25 02:49:03.408976: train_loss -0.8457 
2025-01-25 02:49:03.414912: val_loss -0.8117 
2025-01-25 02:49:03.418322: Pseudo dice [np.float32(0.9551), np.float32(0.9284)] 
2025-01-25 02:49:03.420937: Epoch time: 49.02 s 
2025-01-25 02:49:04.664801:  
2025-01-25 02:49:04.667688: Epoch 933 
2025-01-25 02:49:04.670383: Current learning rate: 0.00088 
2025-01-25 02:49:53.571016: train_loss -0.836 
2025-01-25 02:49:53.575301: val_loss -0.7796 
2025-01-25 02:49:53.577749: Pseudo dice [np.float32(0.9523), np.float32(0.9169)] 
2025-01-25 02:49:53.580154: Epoch time: 48.91 s 
2025-01-25 02:49:54.789987:  
2025-01-25 02:49:54.792970: Epoch 934 
2025-01-25 02:49:54.795785: Current learning rate: 0.00087 
2025-01-25 02:50:43.516907: train_loss -0.8556 
2025-01-25 02:50:43.522484: val_loss -0.7936 
2025-01-25 02:50:43.525264: Pseudo dice [np.float32(0.9561), np.float32(0.9237)] 
2025-01-25 02:50:43.527858: Epoch time: 48.73 s 
2025-01-25 02:50:44.747994:  
2025-01-25 02:50:44.750833: Epoch 935 
2025-01-25 02:50:44.753685: Current learning rate: 0.00085 
2025-01-25 02:51:33.444405: train_loss -0.8602 
2025-01-25 02:51:33.448563: val_loss -0.7817 
2025-01-25 02:51:33.451411: Pseudo dice [np.float32(0.9559), np.float32(0.9266)] 
2025-01-25 02:51:33.453703: Epoch time: 48.7 s 
2025-01-25 02:51:34.666155:  
2025-01-25 02:51:34.669339: Epoch 936 
2025-01-25 02:51:34.672249: Current learning rate: 0.00084 
2025-01-25 02:52:23.069914: train_loss -0.8579 
2025-01-25 02:52:23.075650: val_loss -0.8065 
2025-01-25 02:52:23.078757: Pseudo dice [np.float32(0.9555), np.float32(0.9317)] 
2025-01-25 02:52:23.081621: Epoch time: 48.41 s 
2025-01-25 02:52:23.084613: Yayy! New best EMA pseudo Dice: 0.9398000240325928 
2025-01-25 02:52:24.891004:  
2025-01-25 02:52:24.893816: Epoch 937 
2025-01-25 02:52:24.896600: Current learning rate: 0.00083 
2025-01-25 02:53:14.225183: train_loss -0.8533 
2025-01-25 02:53:14.229273: val_loss -0.7896 
2025-01-25 02:53:14.231879: Pseudo dice [np.float32(0.9551), np.float32(0.9274)] 
2025-01-25 02:53:14.234928: Epoch time: 49.34 s 
2025-01-25 02:53:14.237585: Yayy! New best EMA pseudo Dice: 0.9399999976158142 
2025-01-25 02:53:16.055978:  
2025-01-25 02:53:16.058956: Epoch 938 
2025-01-25 02:53:16.061632: Current learning rate: 0.00082 
2025-01-25 02:54:05.438146: train_loss -0.8528 
2025-01-25 02:54:05.444173: val_loss -0.8053 
2025-01-25 02:54:05.447052: Pseudo dice [np.float32(0.9561), np.float32(0.9205)] 
2025-01-25 02:54:05.450116: Epoch time: 49.38 s 
2025-01-25 02:54:06.656613:  
2025-01-25 02:54:06.659194: Epoch 939 
2025-01-25 02:54:06.661597: Current learning rate: 0.00081 
2025-01-25 02:54:55.867183: train_loss -0.8549 
2025-01-25 02:54:55.871622: val_loss -0.7667 
2025-01-25 02:54:55.874721: Pseudo dice [np.float32(0.9535), np.float32(0.9123)] 
2025-01-25 02:54:55.877484: Epoch time: 49.21 s 
2025-01-25 02:54:57.086023:  
2025-01-25 02:54:57.088959: Epoch 940 
2025-01-25 02:54:57.092013: Current learning rate: 0.00079 
2025-01-25 02:55:45.360723: train_loss -0.8549 
2025-01-25 02:55:45.366285: val_loss -0.7543 
2025-01-25 02:55:45.368957: Pseudo dice [np.float32(0.9433), np.float32(0.9179)] 
2025-01-25 02:55:45.371704: Epoch time: 48.28 s 
2025-01-25 02:55:46.578229:  
2025-01-25 02:55:46.581383: Epoch 941 
2025-01-25 02:55:46.584461: Current learning rate: 0.00078 
2025-01-25 02:56:35.141243: train_loss -0.8556 
2025-01-25 02:56:35.149114: val_loss -0.767 
2025-01-25 02:56:35.152927: Pseudo dice [np.float32(0.9555), np.float32(0.9216)] 
2025-01-25 02:56:35.156752: Epoch time: 48.56 s 
2025-01-25 02:56:36.365033:  
2025-01-25 02:56:36.367849: Epoch 942 
2025-01-25 02:56:36.370587: Current learning rate: 0.00077 
2025-01-25 02:57:25.120054: train_loss -0.8532 
2025-01-25 02:57:25.125860: val_loss -0.7918 
2025-01-25 02:57:25.128975: Pseudo dice [np.float32(0.9543), np.float32(0.9231)] 
2025-01-25 02:57:25.131583: Epoch time: 48.76 s 
2025-01-25 02:57:26.352398:  
2025-01-25 02:57:26.355517: Epoch 943 
2025-01-25 02:57:26.358390: Current learning rate: 0.00076 
2025-01-25 02:58:14.844864: train_loss -0.8606 
2025-01-25 02:58:14.849650: val_loss -0.7584 
2025-01-25 02:58:14.852411: Pseudo dice [np.float32(0.9503), np.float32(0.9224)] 
2025-01-25 02:58:14.855591: Epoch time: 48.49 s 
2025-01-25 02:58:16.076801:  
2025-01-25 02:58:16.079665: Epoch 944 
2025-01-25 02:58:16.082677: Current learning rate: 0.00075 
2025-01-25 02:59:05.039353: train_loss -0.8459 
2025-01-25 02:59:05.045496: val_loss -0.7635 
2025-01-25 02:59:05.048270: Pseudo dice [np.float32(0.9582), np.float32(0.9084)] 
2025-01-25 02:59:05.050966: Epoch time: 48.96 s 
2025-01-25 02:59:06.254716:  
2025-01-25 02:59:06.257576: Epoch 945 
2025-01-25 02:59:06.260494: Current learning rate: 0.00074 
2025-01-25 02:59:54.904957: train_loss -0.8566 
2025-01-25 02:59:54.911771: val_loss -0.7751 
2025-01-25 02:59:54.914808: Pseudo dice [np.float32(0.95), np.float32(0.9241)] 
2025-01-25 02:59:54.917825: Epoch time: 48.65 s 
2025-01-25 02:59:56.121447:  
2025-01-25 02:59:56.124871: Epoch 946 
2025-01-25 02:59:56.127840: Current learning rate: 0.00072 
2025-01-25 03:00:44.656102: train_loss -0.8504 
2025-01-25 03:00:44.662745: val_loss -0.7681 
2025-01-25 03:00:44.665925: Pseudo dice [np.float32(0.952), np.float32(0.8923)] 
2025-01-25 03:00:44.668888: Epoch time: 48.54 s 
2025-01-25 03:00:45.877470:  
2025-01-25 03:00:45.880843: Epoch 947 
2025-01-25 03:00:45.884310: Current learning rate: 0.00071 
2025-01-25 03:01:34.164448: train_loss -0.8603 
2025-01-25 03:01:34.169279: val_loss -0.73 
2025-01-25 03:01:34.172306: Pseudo dice [np.float32(0.9616), np.float32(0.8966)] 
2025-01-25 03:01:34.174686: Epoch time: 48.29 s 
2025-01-25 03:01:35.373389:  
2025-01-25 03:01:35.376833: Epoch 948 
2025-01-25 03:01:35.379836: Current learning rate: 0.0007 
2025-01-25 03:02:24.615803: train_loss -0.8523 
2025-01-25 03:02:24.622145: val_loss -0.7644 
2025-01-25 03:02:24.624831: Pseudo dice [np.float32(0.9485), np.float32(0.9225)] 
2025-01-25 03:02:24.627459: Epoch time: 49.24 s 
2025-01-25 03:02:25.871444:  
2025-01-25 03:02:25.874627: Epoch 949 
2025-01-25 03:02:25.878930: Current learning rate: 0.00069 
2025-01-25 03:03:14.784006: train_loss -0.852 
2025-01-25 03:03:14.788863: val_loss -0.728 
2025-01-25 03:03:14.791932: Pseudo dice [np.float32(0.9602), np.float32(0.9165)] 
2025-01-25 03:03:14.794978: Epoch time: 48.91 s 
2025-01-25 03:03:17.392096:  
2025-01-25 03:03:17.394918: Epoch 950 
2025-01-25 03:03:17.397638: Current learning rate: 0.00067 
2025-01-25 03:04:06.524505: train_loss -0.8465 
2025-01-25 03:04:06.529632: val_loss -0.8102 
2025-01-25 03:04:06.532559: Pseudo dice [np.float32(0.9561), np.float32(0.9169)] 
2025-01-25 03:04:06.535149: Epoch time: 49.13 s 
2025-01-25 03:04:07.733696:  
2025-01-25 03:04:07.736820: Epoch 951 
2025-01-25 03:04:07.739520: Current learning rate: 0.00066 
2025-01-25 03:04:56.332035: train_loss -0.8519 
2025-01-25 03:04:56.336788: val_loss -0.7692 
2025-01-25 03:04:56.339703: Pseudo dice [np.float32(0.9575), np.float32(0.9109)] 
2025-01-25 03:04:56.342779: Epoch time: 48.6 s 
2025-01-25 03:04:57.539858:  
2025-01-25 03:04:57.542665: Epoch 952 
2025-01-25 03:04:57.545235: Current learning rate: 0.00065 
2025-01-25 03:05:46.484185: train_loss -0.8407 
2025-01-25 03:05:46.489457: val_loss -0.762 
2025-01-25 03:05:46.491924: Pseudo dice [np.float32(0.9532), np.float32(0.9193)] 
2025-01-25 03:05:46.494491: Epoch time: 48.95 s 
2025-01-25 03:05:47.694224:  
2025-01-25 03:05:47.697035: Epoch 953 
2025-01-25 03:05:47.699857: Current learning rate: 0.00064 
2025-01-25 03:06:36.100373: train_loss -0.8612 
2025-01-25 03:06:36.104649: val_loss -0.7665 
2025-01-25 03:06:36.107627: Pseudo dice [np.float32(0.9568), np.float32(0.918)] 
2025-01-25 03:06:36.110401: Epoch time: 48.41 s 
2025-01-25 03:06:37.382335:  
2025-01-25 03:06:37.386223: Epoch 954 
2025-01-25 03:06:37.389352: Current learning rate: 0.00063 
2025-01-25 03:07:25.960053: train_loss -0.8571 
2025-01-25 03:07:25.966085: val_loss -0.7305 
2025-01-25 03:07:25.969131: Pseudo dice [np.float32(0.9383), np.float32(0.9107)] 
2025-01-25 03:07:25.971709: Epoch time: 48.58 s 
2025-01-25 03:07:27.181241:  
2025-01-25 03:07:27.184037: Epoch 955 
2025-01-25 03:07:27.186824: Current learning rate: 0.00061 
2025-01-25 03:08:15.738710: train_loss -0.8517 
2025-01-25 03:08:15.743242: val_loss -0.7375 
2025-01-25 03:08:15.746151: Pseudo dice [np.float32(0.9565), np.float32(0.9146)] 
2025-01-25 03:08:15.749252: Epoch time: 48.56 s 
2025-01-25 03:08:16.954036:  
2025-01-25 03:08:16.957379: Epoch 956 
2025-01-25 03:08:16.960595: Current learning rate: 0.0006 
2025-01-25 03:09:06.491508: train_loss -0.8555 
2025-01-25 03:09:06.496948: val_loss -0.7773 
2025-01-25 03:09:06.499832: Pseudo dice [np.float32(0.9527), np.float32(0.9342)] 
2025-01-25 03:09:06.502392: Epoch time: 49.54 s 
2025-01-25 03:09:07.710559:  
2025-01-25 03:09:07.713827: Epoch 957 
2025-01-25 03:09:07.717217: Current learning rate: 0.00059 
2025-01-25 03:09:56.659411: train_loss -0.8465 
2025-01-25 03:09:56.663733: val_loss -0.7984 
2025-01-25 03:09:56.666695: Pseudo dice [np.float32(0.9541), np.float32(0.9306)] 
2025-01-25 03:09:56.669816: Epoch time: 48.95 s 
2025-01-25 03:09:57.903832:  
2025-01-25 03:09:57.906675: Epoch 958 
2025-01-25 03:09:57.909339: Current learning rate: 0.00058 
2025-01-25 03:10:46.483591: train_loss -0.8401 
2025-01-25 03:10:46.489130: val_loss -0.8069 
2025-01-25 03:10:46.492040: Pseudo dice [np.float32(0.9576), np.float32(0.9375)] 
2025-01-25 03:10:46.494882: Epoch time: 48.58 s 
2025-01-25 03:10:47.727611:  
2025-01-25 03:10:47.731197: Epoch 959 
2025-01-25 03:10:47.734411: Current learning rate: 0.00056 
2025-01-25 03:11:36.177174: train_loss -0.859 
2025-01-25 03:11:36.181993: val_loss -0.723 
2025-01-25 03:11:36.185329: Pseudo dice [np.float32(0.9559), np.float32(0.932)] 
2025-01-25 03:11:36.188271: Epoch time: 48.45 s 
2025-01-25 03:11:37.424256:  
2025-01-25 03:11:37.427241: Epoch 960 
2025-01-25 03:11:37.430338: Current learning rate: 0.00055 
2025-01-25 03:12:26.169611: train_loss -0.8494 
2025-01-25 03:12:26.177452: val_loss -0.7856 
2025-01-25 03:12:26.180570: Pseudo dice [np.float32(0.9548), np.float32(0.9382)] 
2025-01-25 03:12:26.183566: Epoch time: 48.75 s 
2025-01-25 03:12:27.391008:  
2025-01-25 03:12:27.393976: Epoch 961 
2025-01-25 03:12:27.397066: Current learning rate: 0.00054 
2025-01-25 03:13:15.929216: train_loss -0.8477 
2025-01-25 03:13:15.933838: val_loss -0.7788 
2025-01-25 03:13:15.936727: Pseudo dice [np.float32(0.9609), np.float32(0.9203)] 
2025-01-25 03:13:15.939647: Epoch time: 48.54 s 
2025-01-25 03:13:17.148905:  
2025-01-25 03:13:17.152106: Epoch 962 
2025-01-25 03:13:17.155214: Current learning rate: 0.00053 
2025-01-25 03:14:05.687186: train_loss -0.861 
2025-01-25 03:14:05.693468: val_loss -0.7187 
2025-01-25 03:14:05.696343: Pseudo dice [np.float32(0.9603), np.float32(0.9377)] 
2025-01-25 03:14:05.698972: Epoch time: 48.54 s 
2025-01-25 03:14:05.701817: Yayy! New best EMA pseudo Dice: 0.9401000142097473 
2025-01-25 03:14:07.538890:  
2025-01-25 03:14:07.542146: Epoch 963 
2025-01-25 03:14:07.545612: Current learning rate: 0.00051 
2025-01-25 03:14:55.980275: train_loss -0.858 
2025-01-25 03:14:55.985332: val_loss -0.7728 
2025-01-25 03:14:55.988512: Pseudo dice [np.float32(0.9448), np.float32(0.9279)] 
2025-01-25 03:14:55.991295: Epoch time: 48.44 s 
2025-01-25 03:14:57.236559:  
2025-01-25 03:14:57.239601: Epoch 964 
2025-01-25 03:14:57.242262: Current learning rate: 0.0005 
2025-01-25 03:15:45.969411: train_loss -0.8443 
2025-01-25 03:15:45.974622: val_loss -0.7833 
2025-01-25 03:15:45.977170: Pseudo dice [np.float32(0.9581), np.float32(0.9176)] 
2025-01-25 03:15:45.979794: Epoch time: 48.73 s 
2025-01-25 03:15:47.219383:  
2025-01-25 03:15:47.222768: Epoch 965 
2025-01-25 03:15:47.225716: Current learning rate: 0.00049 
2025-01-25 03:16:35.720310: train_loss -0.8509 
2025-01-25 03:16:35.724500: val_loss -0.807 
2025-01-25 03:16:35.727238: Pseudo dice [np.float32(0.9607), np.float32(0.9169)] 
2025-01-25 03:16:35.732490: Epoch time: 48.5 s 
2025-01-25 03:16:36.973216:  
2025-01-25 03:16:36.976467: Epoch 966 
2025-01-25 03:16:36.979211: Current learning rate: 0.00048 
2025-01-25 03:17:25.558575: train_loss -0.8589 
2025-01-25 03:17:25.564599: val_loss -0.7726 
2025-01-25 03:17:25.567639: Pseudo dice [np.float32(0.958), np.float32(0.9294)] 
2025-01-25 03:17:25.570195: Epoch time: 48.59 s 
2025-01-25 03:17:26.817759:  
2025-01-25 03:17:26.820948: Epoch 967 
2025-01-25 03:17:26.823828: Current learning rate: 0.00046 
2025-01-25 03:18:16.042789: train_loss -0.8682 
2025-01-25 03:18:16.047575: val_loss -0.8115 
2025-01-25 03:18:16.050691: Pseudo dice [np.float32(0.959), np.float32(0.9314)] 
2025-01-25 03:18:16.053526: Epoch time: 49.23 s 
2025-01-25 03:18:16.056550: Yayy! New best EMA pseudo Dice: 0.9404000043869019 
2025-01-25 03:18:17.926899:  
2025-01-25 03:18:17.929515: Epoch 968 
2025-01-25 03:18:17.932086: Current learning rate: 0.00045 
2025-01-25 03:19:06.796650: train_loss -0.8591 
2025-01-25 03:19:06.804261: val_loss -0.7658 
2025-01-25 03:19:06.807190: Pseudo dice [np.float32(0.9591), np.float32(0.8924)] 
2025-01-25 03:19:06.810040: Epoch time: 48.87 s 
2025-01-25 03:19:08.779463:  
2025-01-25 03:19:08.782918: Epoch 969 
2025-01-25 03:19:08.786155: Current learning rate: 0.00044 
2025-01-25 03:19:57.475723: train_loss -0.8662 
2025-01-25 03:19:57.480840: val_loss -0.8106 
2025-01-25 03:19:57.483942: Pseudo dice [np.float32(0.9517), np.float32(0.9262)] 
2025-01-25 03:19:57.486501: Epoch time: 48.7 s 
2025-01-25 03:19:58.735679:  
2025-01-25 03:19:58.738691: Epoch 970 
2025-01-25 03:19:58.741627: Current learning rate: 0.00043 
2025-01-25 03:20:47.505495: train_loss -0.8585 
2025-01-25 03:20:47.511701: val_loss -0.7712 
2025-01-25 03:20:47.514203: Pseudo dice [np.float32(0.9548), np.float32(0.9222)] 
2025-01-25 03:20:47.516835: Epoch time: 48.77 s 
2025-01-25 03:20:48.724077:  
2025-01-25 03:20:48.727465: Epoch 971 
2025-01-25 03:20:48.731450: Current learning rate: 0.00041 
2025-01-25 03:21:37.241750: train_loss -0.8668 
2025-01-25 03:21:37.246825: val_loss -0.8055 
2025-01-25 03:21:37.249835: Pseudo dice [np.float32(0.9519), np.float32(0.9245)] 
2025-01-25 03:21:37.252838: Epoch time: 48.52 s 
2025-01-25 03:21:38.505260:  
2025-01-25 03:21:38.508914: Epoch 972 
2025-01-25 03:21:38.511976: Current learning rate: 0.0004 
2025-01-25 03:22:26.868710: train_loss -0.855 
2025-01-25 03:22:26.873851: val_loss -0.7769 
2025-01-25 03:22:26.876426: Pseudo dice [np.float32(0.9492), np.float32(0.9286)] 
2025-01-25 03:22:26.879169: Epoch time: 48.36 s 
2025-01-25 03:22:28.090206:  
2025-01-25 03:22:28.093032: Epoch 973 
2025-01-25 03:22:28.095941: Current learning rate: 0.00039 
2025-01-25 03:23:16.824265: train_loss -0.8581 
2025-01-25 03:23:16.828780: val_loss -0.8056 
2025-01-25 03:23:16.831386: Pseudo dice [np.float32(0.953), np.float32(0.9389)] 
2025-01-25 03:23:16.834130: Epoch time: 48.74 s 
2025-01-25 03:23:18.050424:  
2025-01-25 03:23:18.053226: Epoch 974 
2025-01-25 03:23:18.055705: Current learning rate: 0.00037 
2025-01-25 03:24:06.646753: train_loss -0.8563 
2025-01-25 03:24:06.652715: val_loss -0.8176 
2025-01-25 03:24:06.655704: Pseudo dice [np.float32(0.9532), np.float32(0.9384)] 
2025-01-25 03:24:06.658554: Epoch time: 48.6 s 
2025-01-25 03:24:07.907263:  
2025-01-25 03:24:07.910352: Epoch 975 
2025-01-25 03:24:07.913364: Current learning rate: 0.00036 
2025-01-25 03:24:56.625841: train_loss -0.8508 
2025-01-25 03:24:56.629880: val_loss -0.8151 
2025-01-25 03:24:56.632632: Pseudo dice [np.float32(0.9577), np.float32(0.9274)] 
2025-01-25 03:24:56.635216: Epoch time: 48.72 s 
2025-01-25 03:24:57.854412:  
2025-01-25 03:24:57.857418: Epoch 976 
2025-01-25 03:24:57.860349: Current learning rate: 0.00035 
2025-01-25 03:25:46.387511: train_loss -0.8483 
2025-01-25 03:25:46.393523: val_loss -0.7812 
2025-01-25 03:25:46.396999: Pseudo dice [np.float32(0.9454), np.float32(0.915)] 
2025-01-25 03:25:46.400292: Epoch time: 48.53 s 
2025-01-25 03:25:47.640671:  
2025-01-25 03:25:47.643719: Epoch 977 
2025-01-25 03:25:47.646682: Current learning rate: 0.00034 
2025-01-25 03:26:36.375628: train_loss -0.8697 
2025-01-25 03:26:36.379621: val_loss -0.7908 
2025-01-25 03:26:36.382536: Pseudo dice [np.float32(0.9493), np.float32(0.9311)] 
2025-01-25 03:26:36.385312: Epoch time: 48.74 s 
2025-01-25 03:26:37.599123:  
2025-01-25 03:26:37.602337: Epoch 978 
2025-01-25 03:26:37.605126: Current learning rate: 0.00032 
2025-01-25 03:27:26.775822: train_loss -0.8555 
2025-01-25 03:27:26.781111: val_loss -0.8134 
2025-01-25 03:27:26.784075: Pseudo dice [np.float32(0.9558), np.float32(0.9196)] 
2025-01-25 03:27:26.786816: Epoch time: 49.18 s 
2025-01-25 03:27:28.035971:  
2025-01-25 03:27:28.038903: Epoch 979 
2025-01-25 03:27:28.041669: Current learning rate: 0.00031 
2025-01-25 03:28:16.922689: train_loss -0.8606 
2025-01-25 03:28:16.927651: val_loss -0.7767 
2025-01-25 03:28:16.930935: Pseudo dice [np.float32(0.9551), np.float32(0.9108)] 
2025-01-25 03:28:16.933771: Epoch time: 48.89 s 
2025-01-25 03:28:18.148584:  
2025-01-25 03:28:18.151155: Epoch 980 
2025-01-25 03:28:18.153627: Current learning rate: 0.0003 
2025-01-25 03:29:06.890333: train_loss -0.8533 
2025-01-25 03:29:06.896162: val_loss -0.7913 
2025-01-25 03:29:06.899189: Pseudo dice [np.float32(0.9536), np.float32(0.9293)] 
2025-01-25 03:29:06.901833: Epoch time: 48.74 s 
2025-01-25 03:29:08.159345:  
2025-01-25 03:29:08.162354: Epoch 981 
2025-01-25 03:29:08.165678: Current learning rate: 0.00028 
2025-01-25 03:29:56.822163: train_loss -0.8665 
2025-01-25 03:29:56.826093: val_loss -0.7474 
2025-01-25 03:29:56.828745: Pseudo dice [np.float32(0.9606), np.float32(0.9122)] 
2025-01-25 03:29:56.831428: Epoch time: 48.66 s 
2025-01-25 03:29:58.045880:  
2025-01-25 03:29:58.048758: Epoch 982 
2025-01-25 03:29:58.051237: Current learning rate: 0.00027 
2025-01-25 03:30:47.278410: train_loss -0.8573 
2025-01-25 03:30:47.283930: val_loss -0.784 
2025-01-25 03:30:47.286945: Pseudo dice [np.float32(0.9597), np.float32(0.9129)] 
2025-01-25 03:30:47.289781: Epoch time: 49.23 s 
2025-01-25 03:30:48.497991:  
2025-01-25 03:30:48.501154: Epoch 983 
2025-01-25 03:30:48.505730: Current learning rate: 0.00026 
2025-01-25 03:31:37.106744: train_loss -0.8319 
2025-01-25 03:31:37.113269: val_loss -0.7593 
2025-01-25 03:31:37.116132: Pseudo dice [np.float32(0.9628), np.float32(0.912)] 
2025-01-25 03:31:37.119008: Epoch time: 48.61 s 
2025-01-25 03:31:38.389970:  
2025-01-25 03:31:38.393000: Epoch 984 
2025-01-25 03:31:38.395673: Current learning rate: 0.00024 
2025-01-25 03:32:27.096673: train_loss -0.8516 
2025-01-25 03:32:27.103047: val_loss -0.8268 
2025-01-25 03:32:27.106320: Pseudo dice [np.float32(0.9495), np.float32(0.9288)] 
2025-01-25 03:32:27.109314: Epoch time: 48.71 s 
2025-01-25 03:32:28.322225:  
2025-01-25 03:32:28.324871: Epoch 985 
2025-01-25 03:32:28.327330: Current learning rate: 0.00023 
2025-01-25 03:33:16.939803: train_loss -0.8601 
2025-01-25 03:33:16.945766: val_loss -0.7657 
2025-01-25 03:33:16.948997: Pseudo dice [np.float32(0.953), np.float32(0.9214)] 
2025-01-25 03:33:16.952342: Epoch time: 48.62 s 
2025-01-25 03:33:18.443637:  
2025-01-25 03:33:18.446471: Epoch 986 
2025-01-25 03:33:18.449283: Current learning rate: 0.00021 
2025-01-25 03:34:07.077404: train_loss -0.8468 
2025-01-25 03:34:07.084924: val_loss -0.7817 
2025-01-25 03:34:07.087687: Pseudo dice [np.float32(0.9449), np.float32(0.9309)] 
2025-01-25 03:34:07.090327: Epoch time: 48.63 s 
2025-01-25 03:34:08.310585:  
2025-01-25 03:34:08.313200: Epoch 987 
2025-01-25 03:34:08.315811: Current learning rate: 0.0002 
2025-01-25 03:34:57.039640: train_loss -0.8661 
2025-01-25 03:34:57.044235: val_loss -0.7673 
2025-01-25 03:34:57.047321: Pseudo dice [np.float32(0.9589), np.float32(0.9303)] 
2025-01-25 03:34:57.050371: Epoch time: 48.73 s 
2025-01-25 03:34:58.885616:  
2025-01-25 03:34:58.888296: Epoch 988 
2025-01-25 03:34:58.891236: Current learning rate: 0.00019 
2025-01-25 03:35:47.798152: train_loss -0.8577 
2025-01-25 03:35:47.803816: val_loss -0.7916 
2025-01-25 03:35:47.806512: Pseudo dice [np.float32(0.9532), np.float32(0.9338)] 
2025-01-25 03:35:47.809055: Epoch time: 48.91 s 
2025-01-25 03:35:49.027654:  
2025-01-25 03:35:49.030895: Epoch 989 
2025-01-25 03:35:49.034016: Current learning rate: 0.00017 
2025-01-25 03:36:37.723100: train_loss -0.8475 
2025-01-25 03:36:37.727560: val_loss -0.8007 
2025-01-25 03:36:37.730390: Pseudo dice [np.float32(0.9554), np.float32(0.919)] 
2025-01-25 03:36:37.732980: Epoch time: 48.7 s 
2025-01-25 03:36:38.985101:  
2025-01-25 03:36:38.988063: Epoch 990 
2025-01-25 03:36:38.991057: Current learning rate: 0.00016 
2025-01-25 03:37:28.154428: train_loss -0.8559 
2025-01-25 03:37:28.160389: val_loss -0.7695 
2025-01-25 03:37:28.162731: Pseudo dice [np.float32(0.9451), np.float32(0.926)] 
2025-01-25 03:37:28.165236: Epoch time: 49.17 s 
2025-01-25 03:37:29.385819:  
2025-01-25 03:37:29.389077: Epoch 991 
2025-01-25 03:37:29.392259: Current learning rate: 0.00014 
2025-01-25 03:38:18.216606: train_loss -0.8677 
2025-01-25 03:38:18.221235: val_loss -0.8059 
2025-01-25 03:38:18.225820: Pseudo dice [np.float32(0.9607), np.float32(0.9227)] 
2025-01-25 03:38:18.228698: Epoch time: 48.83 s 
2025-01-25 03:38:19.487703:  
2025-01-25 03:38:19.491207: Epoch 992 
2025-01-25 03:38:19.494370: Current learning rate: 0.00013 
2025-01-25 03:39:08.126205: train_loss -0.84 
2025-01-25 03:39:08.131841: val_loss -0.7573 
2025-01-25 03:39:08.134291: Pseudo dice [np.float32(0.9558), np.float32(0.9266)] 
2025-01-25 03:39:08.136869: Epoch time: 48.64 s 
2025-01-25 03:39:09.391752:  
2025-01-25 03:39:09.394233: Epoch 993 
2025-01-25 03:39:09.396921: Current learning rate: 0.00011 
2025-01-25 03:39:57.990637: train_loss -0.8701 
2025-01-25 03:39:57.995289: val_loss -0.7646 
2025-01-25 03:39:57.998033: Pseudo dice [np.float32(0.9531), np.float32(0.9271)] 
2025-01-25 03:39:58.000600: Epoch time: 48.6 s 
2025-01-25 03:39:59.218596:  
2025-01-25 03:39:59.221709: Epoch 994 
2025-01-25 03:39:59.224791: Current learning rate: 0.0001 
2025-01-25 03:40:47.839505: train_loss -0.8563 
2025-01-25 03:40:47.846210: val_loss -0.7971 
2025-01-25 03:40:47.849477: Pseudo dice [np.float32(0.9541), np.float32(0.9346)] 
2025-01-25 03:40:47.852390: Epoch time: 48.62 s 
2025-01-25 03:40:49.098744:  
2025-01-25 03:40:49.102013: Epoch 995 
2025-01-25 03:40:49.104929: Current learning rate: 8e-05 
2025-01-25 03:41:38.642125: train_loss -0.8433 
2025-01-25 03:41:38.646563: val_loss -0.7694 
2025-01-25 03:41:38.649217: Pseudo dice [np.float32(0.9593), np.float32(0.9261)] 
2025-01-25 03:41:38.652326: Epoch time: 49.54 s 
2025-01-25 03:41:39.868578:  
2025-01-25 03:41:39.871595: Epoch 996 
2025-01-25 03:41:39.874676: Current learning rate: 7e-05 
2025-01-25 03:42:28.867047: train_loss -0.8645 
2025-01-25 03:42:28.872308: val_loss -0.8147 
2025-01-25 03:42:28.875010: Pseudo dice [np.float32(0.9574), np.float32(0.9147)] 
2025-01-25 03:42:28.878074: Epoch time: 49.0 s 
2025-01-25 03:42:30.138509:  
2025-01-25 03:42:30.141456: Epoch 997 
2025-01-25 03:42:30.144566: Current learning rate: 5e-05 
2025-01-25 03:43:19.213838: train_loss -0.8401 
2025-01-25 03:43:19.219822: val_loss -0.79 
2025-01-25 03:43:19.222416: Pseudo dice [np.float32(0.9561), np.float32(0.9323)] 
2025-01-25 03:43:19.225165: Epoch time: 49.08 s 
2025-01-25 03:43:20.435527:  
2025-01-25 03:43:20.438870: Epoch 998 
2025-01-25 03:43:20.441620: Current learning rate: 4e-05 
2025-01-25 03:44:09.073902: train_loss -0.8592 
2025-01-25 03:44:09.080269: val_loss -0.7484 
2025-01-25 03:44:09.083143: Pseudo dice [np.float32(0.9484), np.float32(0.9337)] 
2025-01-25 03:44:09.086063: Epoch time: 48.64 s 
2025-01-25 03:44:10.306814:  
2025-01-25 03:44:10.309731: Epoch 999 
2025-01-25 03:44:10.312384: Current learning rate: 2e-05 
2025-01-25 03:44:59.070222: train_loss -0.8601 
2025-01-25 03:44:59.074405: val_loss -0.8135 
2025-01-25 03:44:59.078843: Pseudo dice [np.float32(0.9516), np.float32(0.9326)] 
2025-01-25 03:44:59.081645: Epoch time: 48.76 s 
2025-01-25 03:44:59.084054: Yayy! New best EMA pseudo Dice: 0.940500020980835 
2025-01-25 03:45:01.478278: Training done. 
2025-01-25 03:45:01.595705: Using splits from existing split file: /srv/scratch/z5362216/kits19/nnUNet_db/nnUNet_preprocessed/Dataset001_Kits19/splits_final.json 
2025-01-25 03:45:01.599538: The split file contains 5 splits. 
2025-01-25 03:45:01.602090: Desired fold for training: 1 
2025-01-25 03:45:01.604548: This split has 80 training and 20 validation cases. 
2025-01-25 03:45:01.684170: predicting imaging_000 
2025-01-25 03:45:01.692918: imaging_000, shape torch.Size([1, 154, 237, 237]), rank 0 
2025-01-25 03:45:26.365462: predicting imaging_002 
2025-01-25 03:45:26.377777: imaging_002, shape torch.Size([1, 131, 242, 242]), rank 0 
2025-01-25 03:45:32.597583: predicting imaging_006 
2025-01-25 03:45:32.606995: imaging_006, shape torch.Size([1, 237, 191, 191]), rank 0 
2025-01-25 03:45:38.130644: predicting imaging_008 
2025-01-25 03:45:38.158639: imaging_008, shape torch.Size([1, 343, 211, 211]), rank 0 
2025-01-25 03:45:58.308109: predicting imaging_017 
2025-01-25 03:45:58.340330: imaging_017, shape torch.Size([1, 244, 171, 171]), rank 0 
2025-01-25 03:46:06.751458: predicting imaging_026 
2025-01-25 03:46:06.767349: imaging_026, shape torch.Size([1, 380, 226, 226]), rank 0 
2025-01-25 03:46:30.328543: predicting imaging_030 
2025-01-25 03:46:30.373060: imaging_030, shape torch.Size([1, 96, 188, 188]), rank 0 
2025-01-25 03:46:37.000557: predicting imaging_037 
2025-01-25 03:46:37.028700: imaging_037, shape torch.Size([1, 244, 242, 242]), rank 0 
2025-01-25 03:46:47.659553: predicting imaging_039 
2025-01-25 03:46:47.675535: imaging_039, shape torch.Size([1, 227, 181, 181]), rank 0 
2025-01-25 03:46:52.606410: predicting imaging_045 
2025-01-25 03:46:52.624704: imaging_045, shape torch.Size([1, 94, 191, 191]), rank 0 
2025-01-25 03:46:56.252592: predicting imaging_046 
2025-01-25 03:46:56.289543: imaging_046, shape torch.Size([1, 240, 252, 252]), rank 0 
2025-01-25 03:47:12.834325: predicting imaging_047 
2025-01-25 03:47:12.849684: imaging_047, shape torch.Size([1, 358, 221, 221]), rank 0 
2025-01-25 03:47:31.603550: predicting imaging_050 
2025-01-25 03:47:31.619177: imaging_050, shape torch.Size([1, 121, 178, 178]), rank 0 
2025-01-25 03:47:34.019599: predicting imaging_052 
2025-01-25 03:47:34.040645: imaging_052, shape torch.Size([1, 170, 247, 247]), rank 0 
2025-01-25 03:47:44.314659: predicting imaging_054 
2025-01-25 03:47:44.344225: imaging_054, shape torch.Size([1, 262, 252, 252]), rank 0 
2025-01-25 03:48:02.650094: predicting imaging_057 
2025-01-25 03:48:02.677559: imaging_057, shape torch.Size([1, 121, 212, 212]), rank 0 
2025-01-25 03:48:08.111615: predicting imaging_071 
2025-01-25 03:48:08.126873: imaging_071, shape torch.Size([1, 154, 181, 181]), rank 0 
2025-01-25 03:48:13.095635: predicting imaging_076 
2025-01-25 03:48:13.110271: imaging_076, shape torch.Size([1, 166, 175, 175]), rank 0 
2025-01-25 03:48:25.927565: predicting imaging_087 
2025-01-25 03:48:25.943907: imaging_087, shape torch.Size([1, 123, 178, 178]), rank 0 
2025-01-25 03:48:35.521610: predicting imaging_092 
2025-01-25 03:48:35.534729: imaging_092, shape torch.Size([1, 247, 181, 181]), rank 0 
2025-01-25 03:49:53.026581: Validation complete 
2025-01-25 03:49:53.029409: Mean Validation Dice:  0.8929642457019602 
