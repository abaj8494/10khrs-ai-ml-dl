
#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################
 
2024-11-11 17:19:12.481750: do_dummy_2d_data_aug: False 
2024-11-11 17:19:12.571881: Using splits from existing split file: /srv/scratch/z5362216/kits19/nnUNet_db/nnUNet_preprocessed/Dataset001_Kits19/splits_final.json 
2024-11-11 17:19:12.574861: The split file contains 5 splits. 
2024-11-11 17:19:12.577323: Desired fold for training: 4 
2024-11-11 17:19:12.579695: This split has 80 training and 20 validation cases. 
2024-11-11 17:19:15.689774: Using torch.compile... 

This is the configuration used by this training:
Configuration name: 2d
 {'data_identifier': 'nnUNetPlans_2d', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 12, 'patch_size': [512, 512], 'median_image_size_in_voxels': [512.0, 512.0], 'spacing': [0.7939453125, 0.7939453125], 'normalization_schemes': ['CTNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 8, 'features_per_stage': [32, 64, 128, 256, 512, 512, 512, 512], 'conv_op': 'torch.nn.modules.conv.Conv2d', 'kernel_sizes': [[3, 3], [3, 3], [3, 3], [3, 3], [3, 3], [3, 3], [3, 3], [3, 3]], 'strides': [[1, 1], [2, 2], [2, 2], [2, 2], [2, 2], [2, 2], [2, 2], [2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm2d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True} 
 
These are the global plan.json settings:
 {'dataset_name': 'Dataset001_Kits19', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [3.0, 0.7939453125, 0.7939453125], 'original_median_shape_after_transp': [104, 512, 512], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [2, 0, 1], 'transpose_backward': [1, 2, 0], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 2553.0, 'mean': 104.46720886230469, 'median': 104.0, 'min': -277.0, 'percentile_00_5': -73.0, 'percentile_99_5': 292.0, 'std': 74.68063354492188}}} 
 
2024-11-11 17:19:16.556424: unpacking dataset... 
2024-11-11 17:19:22.223676: unpacking done... 
2024-11-11 17:19:22.244944: 
printing the network instead:
 
2024-11-11 17:19:22.247498: OptimizedModule(
  (_orig_mod): PlainConvUNet(
    (encoder): PlainConvEncoder(
      (stages): Sequential(
        (0): Sequential(
          (0): StackedConvBlocks(
            (convs): Sequential(
              (0): ConvDropoutNormReLU(
                (conv): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (norm): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  (1): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (1): ConvDropoutNormReLU(
                (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (norm): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  (1): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
            )
          )
        )
        (1): Sequential(
          (0): StackedConvBlocks(
            (convs): Sequential(
              (0): ConvDropoutNormReLU(
                (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (norm): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                  (1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (1): ConvDropoutNormReLU(
                (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (norm): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  (1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
            )
          )
        )
        (2): Sequential(
          (0): StackedConvBlocks(
            (convs): Sequential(
              (0): ConvDropoutNormReLU(
                (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (norm): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                  (1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (1): ConvDropoutNormReLU(
                (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (norm): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  (1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
            )
          )
        )
        (3): Sequential(
          (0): StackedConvBlocks(
            (convs): Sequential(
              (0): ConvDropoutNormReLU(
                (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (norm): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                  (1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (1): ConvDropoutNormReLU(
                (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (norm): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  (1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
            )
          )
        )
        (4): Sequential(
          (0): StackedConvBlocks(
            (convs): Sequential(
              (0): ConvDropoutNormReLU(
                (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (norm): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                  (1): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (1): ConvDropoutNormReLU(
                (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (norm): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  (1): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
            )
          )
        )
        (5): Sequential(
          (0): StackedConvBlocks(
            (convs): Sequential(
              (0): ConvDropoutNormReLU(
                (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (norm): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                  (1): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (1): ConvDropoutNormReLU(
                (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (norm): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  (1): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
            )
          )
        )
        (6): Sequential(
          (0): StackedConvBlocks(
            (convs): Sequential(
              (0): ConvDropoutNormReLU(
                (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (norm): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                  (1): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (1): ConvDropoutNormReLU(
                (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (norm): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  (1): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
            )
          )
        )
        (7): Sequential(
          (0): StackedConvBlocks(
            (convs): Sequential(
              (0): ConvDropoutNormReLU(
                (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (norm): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                  (1): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (1): ConvDropoutNormReLU(
                (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (norm): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  (1): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
            )
          )
        )
      )
    )
    (decoder): UNetDecoder(
      (encoder): PlainConvEncoder(
        (stages): Sequential(
          (0): Sequential(
            (0): StackedConvBlocks(
              (convs): Sequential(
                (0): ConvDropoutNormReLU(
                  (conv): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  (norm): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                  (all_modules): Sequential(
                    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                    (1): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                    (2): LeakyReLU(negative_slope=0.01, inplace=True)
                  )
                )
                (1): ConvDropoutNormReLU(
                  (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  (norm): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                  (all_modules): Sequential(
                    (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                    (1): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                    (2): LeakyReLU(negative_slope=0.01, inplace=True)
                  )
                )
              )
            )
          )
          (1): Sequential(
            (0): StackedConvBlocks(
              (convs): Sequential(
                (0): ConvDropoutNormReLU(
                  (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                  (norm): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                  (all_modules): Sequential(
                    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                    (1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                    (2): LeakyReLU(negative_slope=0.01, inplace=True)
                  )
                )
                (1): ConvDropoutNormReLU(
                  (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  (norm): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                  (all_modules): Sequential(
                    (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                    (1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                    (2): LeakyReLU(negative_slope=0.01, inplace=True)
                  )
                )
              )
            )
          )
          (2): Sequential(
            (0): StackedConvBlocks(
              (convs): Sequential(
                (0): ConvDropoutNormReLU(
                  (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                  (norm): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                  (all_modules): Sequential(
                    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                    (1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                    (2): LeakyReLU(negative_slope=0.01, inplace=True)
                  )
                )
                (1): ConvDropoutNormReLU(
                  (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  (norm): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                  (all_modules): Sequential(
                    (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                    (1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                    (2): LeakyReLU(negative_slope=0.01, inplace=True)
                  )
                )
              )
            )
          )
          (3): Sequential(
            (0): StackedConvBlocks(
              (convs): Sequential(
                (0): ConvDropoutNormReLU(
                  (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                  (norm): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                  (all_modules): Sequential(
                    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                    (1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                    (2): LeakyReLU(negative_slope=0.01, inplace=True)
                  )
                )
                (1): ConvDropoutNormReLU(
                  (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  (norm): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                  (all_modules): Sequential(
                    (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                    (1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                    (2): LeakyReLU(negative_slope=0.01, inplace=True)
                  )
                )
              )
            )
          )
          (4): Sequential(
            (0): StackedConvBlocks(
              (convs): Sequential(
                (0): ConvDropoutNormReLU(
                  (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                  (norm): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                  (all_modules): Sequential(
                    (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                    (1): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                    (2): LeakyReLU(negative_slope=0.01, inplace=True)
                  )
                )
                (1): ConvDropoutNormReLU(
                  (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  (norm): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                  (all_modules): Sequential(
                    (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                    (1): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                    (2): LeakyReLU(negative_slope=0.01, inplace=True)
                  )
                )
              )
            )
          )
          (5): Sequential(
            (0): StackedConvBlocks(
              (convs): Sequential(
                (0): ConvDropoutNormReLU(
                  (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                  (norm): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                  (all_modules): Sequential(
                    (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                    (1): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                    (2): LeakyReLU(negative_slope=0.01, inplace=True)
                  )
                )
                (1): ConvDropoutNormReLU(
                  (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  (norm): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                  (all_modules): Sequential(
                    (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                    (1): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                    (2): LeakyReLU(negative_slope=0.01, inplace=True)
                  )
                )
              )
            )
          )
          (6): Sequential(
            (0): StackedConvBlocks(
              (convs): Sequential(
                (0): ConvDropoutNormReLU(
                  (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                  (norm): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                  (all_modules): Sequential(
                    (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                    (1): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                    (2): LeakyReLU(negative_slope=0.01, inplace=True)
                  )
                )
                (1): ConvDropoutNormReLU(
                  (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  (norm): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                  (all_modules): Sequential(
                    (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                    (1): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                    (2): LeakyReLU(negative_slope=0.01, inplace=True)
                  )
                )
              )
            )
          )
          (7): Sequential(
            (0): StackedConvBlocks(
              (convs): Sequential(
                (0): ConvDropoutNormReLU(
                  (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                  (norm): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                  (all_modules): Sequential(
                    (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                    (1): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                    (2): LeakyReLU(negative_slope=0.01, inplace=True)
                  )
                )
                (1): ConvDropoutNormReLU(
                  (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  (norm): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                  (all_modules): Sequential(
                    (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                    (1): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                    (2): LeakyReLU(negative_slope=0.01, inplace=True)
                  )
                )
              )
            )
          )
        )
      )
      (stages): ModuleList(
        (0-2): 3 x StackedConvBlocks(
          (convs): Sequential(
            (0): ConvDropoutNormReLU(
              (conv): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (norm): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (1): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (1): ConvDropoutNormReLU(
              (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (norm): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (1): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
          )
        )
        (3): StackedConvBlocks(
          (convs): Sequential(
            (0): ConvDropoutNormReLU(
              (conv): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (norm): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (1): ConvDropoutNormReLU(
              (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (norm): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
          )
        )
        (4): StackedConvBlocks(
          (convs): Sequential(
            (0): ConvDropoutNormReLU(
              (conv): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (norm): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (1): ConvDropoutNormReLU(
              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (norm): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
          )
        )
        (5): StackedConvBlocks(
          (convs): Sequential(
            (0): ConvDropoutNormReLU(
              (conv): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (norm): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (1): ConvDropoutNormReLU(
              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (norm): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
          )
        )
        (6): StackedConvBlocks(
          (convs): Sequential(
            (0): ConvDropoutNormReLU(
              (conv): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (norm): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (1): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (1): ConvDropoutNormReLU(
              (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (norm): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (1): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
          )
        )
      )
      (transpconvs): ModuleList(
        (0-2): 3 x ConvTranspose2d(512, 512, kernel_size=(2, 2), stride=(2, 2))
        (3): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))
        (4): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))
        (5): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))
        (6): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))
      )
      (seg_layers): ModuleList(
        (0-2): 3 x Conv2d(512, 3, kernel_size=(1, 1), stride=(1, 1))
        (3): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))
        (4): Conv2d(128, 3, kernel_size=(1, 1), stride=(1, 1))
        (5): Conv2d(64, 3, kernel_size=(1, 1), stride=(1, 1))
        (6): Conv2d(32, 3, kernel_size=(1, 1), stride=(1, 1))
      )
    )
  )
) 
2024-11-11 17:19:22.256076: 
 
2024-11-11 17:19:22.258344: Unable to plot network architecture: nnUNet_compile is enabled! 
2024-11-11 17:19:22.271168:  
2024-11-11 17:19:22.273871: Epoch 0 
2024-11-11 17:19:22.276268: Current learning rate: 0.01 
2024-11-11 17:20:22.390236: train_loss -0.0478 
2024-11-11 17:20:22.395714: val_loss -0.3158 
2024-11-11 17:20:22.398261: Pseudo dice [np.float32(0.7232), np.float32(0.0)] 
2024-11-11 17:20:22.400824: Epoch time: 60.12 s 
2024-11-11 17:20:22.403230: Yayy! New best EMA pseudo Dice: 0.36160001158714294 
2024-11-11 17:20:24.201965:  
2024-11-11 17:20:24.204864: Epoch 1 
2024-11-11 17:20:24.208323: Current learning rate: 0.00999 
2024-11-11 17:21:03.736827: train_loss -0.4169 
2024-11-11 17:21:03.740744: val_loss -0.5119 
2024-11-11 17:21:03.743237: Pseudo dice [np.float32(0.8064), np.float32(0.3437)] 
2024-11-11 17:21:03.745698: Epoch time: 39.54 s 
2024-11-11 17:21:03.748106: Yayy! New best EMA pseudo Dice: 0.3828999996185303 
2024-11-11 17:21:06.023324:  
2024-11-11 17:21:06.025532: Epoch 2 
2024-11-11 17:21:06.027769: Current learning rate: 0.00998 
2024-11-11 17:21:45.640011: train_loss -0.5478 
2024-11-11 17:21:45.645495: val_loss -0.5378 
2024-11-11 17:21:45.647983: Pseudo dice [np.float32(0.8602), np.float32(0.3946)] 
2024-11-11 17:21:45.650187: Epoch time: 39.62 s 
2024-11-11 17:21:45.652390: Yayy! New best EMA pseudo Dice: 0.4074000120162964 
2024-11-11 17:21:47.595432:  
2024-11-11 17:21:47.599307: Epoch 3 
2024-11-11 17:21:47.601835: Current learning rate: 0.00997 
2024-11-11 17:22:27.216779: train_loss -0.594 
2024-11-11 17:22:27.219718: val_loss -0.6155 
2024-11-11 17:22:27.222116: Pseudo dice [np.float32(0.8928), np.float32(0.4845)] 
2024-11-11 17:22:27.224437: Epoch time: 39.62 s 
2024-11-11 17:22:27.226786: Yayy! New best EMA pseudo Dice: 0.43549999594688416 
2024-11-11 17:22:29.179670:  
2024-11-11 17:22:29.182013: Epoch 4 
2024-11-11 17:22:29.184634: Current learning rate: 0.00996 
2024-11-11 17:23:08.773129: train_loss -0.6281 
2024-11-11 17:23:08.778509: val_loss -0.6155 
2024-11-11 17:23:08.780864: Pseudo dice [np.float32(0.905), np.float32(0.4667)] 
2024-11-11 17:23:08.783041: Epoch time: 39.59 s 
2024-11-11 17:23:08.785443: Yayy! New best EMA pseudo Dice: 0.46050000190734863 
2024-11-11 17:23:10.789057:  
2024-11-11 17:23:10.791598: Epoch 5 
2024-11-11 17:23:10.794061: Current learning rate: 0.00995 
2024-11-11 17:23:50.376133: train_loss -0.6753 
2024-11-11 17:23:50.379220: val_loss -0.6302 
2024-11-11 17:23:50.381842: Pseudo dice [np.float32(0.903), np.float32(0.5056)] 
2024-11-11 17:23:50.384254: Epoch time: 39.59 s 
2024-11-11 17:23:50.386811: Yayy! New best EMA pseudo Dice: 0.48489999771118164 
2024-11-11 17:23:52.310300:  
2024-11-11 17:23:52.314140: Epoch 6 
2024-11-11 17:23:52.317086: Current learning rate: 0.00995 
2024-11-11 17:24:31.882285: train_loss -0.6609 
2024-11-11 17:24:31.891661: val_loss -0.6223 
2024-11-11 17:24:31.894220: Pseudo dice [np.float32(0.8865), np.float32(0.5179)] 
2024-11-11 17:24:31.896513: Epoch time: 39.57 s 
2024-11-11 17:24:31.898901: Yayy! New best EMA pseudo Dice: 0.506600022315979 
2024-11-11 17:24:33.913211:  
2024-11-11 17:24:33.915521: Epoch 7 
2024-11-11 17:24:33.917669: Current learning rate: 0.00994 
2024-11-11 17:25:13.500437: train_loss -0.675 
2024-11-11 17:25:13.503343: val_loss -0.6438 
2024-11-11 17:25:13.505682: Pseudo dice [np.float32(0.8883), np.float32(0.5277)] 
2024-11-11 17:25:13.508017: Epoch time: 39.59 s 
2024-11-11 17:25:13.510512: Yayy! New best EMA pseudo Dice: 0.5267999768257141 
2024-11-11 17:25:15.466360:  
2024-11-11 17:25:15.469088: Epoch 8 
2024-11-11 17:25:15.471870: Current learning rate: 0.00993 
2024-11-11 17:25:55.049731: train_loss -0.6702 
2024-11-11 17:25:55.055084: val_loss -0.6915 
2024-11-11 17:25:55.057969: Pseudo dice [np.float32(0.9199), np.float32(0.6193)] 
2024-11-11 17:25:55.060263: Epoch time: 39.58 s 
2024-11-11 17:25:55.062380: Yayy! New best EMA pseudo Dice: 0.5511000156402588 
2024-11-11 17:25:57.009870:  
2024-11-11 17:25:57.012848: Epoch 9 
2024-11-11 17:25:57.015121: Current learning rate: 0.00992 
2024-11-11 17:26:36.574783: train_loss -0.692 
2024-11-11 17:26:36.577775: val_loss -0.6235 
2024-11-11 17:26:36.580878: Pseudo dice [np.float32(0.8879), np.float32(0.5072)] 
2024-11-11 17:26:36.583273: Epoch time: 39.57 s 
2024-11-11 17:26:36.585622: Yayy! New best EMA pseudo Dice: 0.5656999945640564 
2024-11-11 17:26:38.557606:  
2024-11-11 17:26:38.560434: Epoch 10 
2024-11-11 17:26:38.563129: Current learning rate: 0.00991 
2024-11-11 17:27:18.138349: train_loss -0.7138 
2024-11-11 17:27:18.143932: val_loss -0.6635 
2024-11-11 17:27:18.146454: Pseudo dice [np.float32(0.9231), np.float32(0.5246)] 
2024-11-11 17:27:18.149114: Epoch time: 39.58 s 
2024-11-11 17:27:18.151470: Yayy! New best EMA pseudo Dice: 0.5814999938011169 
2024-11-11 17:27:20.132771:  
2024-11-11 17:27:20.135347: Epoch 11 
2024-11-11 17:27:20.137821: Current learning rate: 0.0099 
2024-11-11 17:27:59.697173: train_loss -0.7299 
2024-11-11 17:27:59.700144: val_loss -0.69 
2024-11-11 17:27:59.702507: Pseudo dice [np.float32(0.9293), np.float32(0.6342)] 
2024-11-11 17:27:59.704877: Epoch time: 39.57 s 
2024-11-11 17:27:59.707386: Yayy! New best EMA pseudo Dice: 0.6014999747276306 
2024-11-11 17:28:01.678290:  
2024-11-11 17:28:01.680946: Epoch 12 
2024-11-11 17:28:01.683379: Current learning rate: 0.00989 
2024-11-11 17:28:41.273889: train_loss -0.737 
2024-11-11 17:28:41.279895: val_loss -0.6934 
2024-11-11 17:28:41.282670: Pseudo dice [np.float32(0.9276), np.float32(0.5701)] 
2024-11-11 17:28:41.285419: Epoch time: 39.6 s 
2024-11-11 17:28:41.287789: Yayy! New best EMA pseudo Dice: 0.6162999868392944 
2024-11-11 17:28:43.196295:  
2024-11-11 17:28:43.198717: Epoch 13 
2024-11-11 17:28:43.201389: Current learning rate: 0.00988 
2024-11-11 17:29:22.788840: train_loss -0.7413 
2024-11-11 17:29:22.791908: val_loss -0.6956 
2024-11-11 17:29:22.794543: Pseudo dice [np.float32(0.9239), np.float32(0.6193)] 
2024-11-11 17:29:22.796871: Epoch time: 39.59 s 
2024-11-11 17:29:22.799123: Yayy! New best EMA pseudo Dice: 0.6317999958992004 
2024-11-11 17:29:25.041961:  
2024-11-11 17:29:25.044897: Epoch 14 
2024-11-11 17:29:25.047261: Current learning rate: 0.00987 
2024-11-11 17:30:04.670340: train_loss -0.7256 
2024-11-11 17:30:04.675860: val_loss -0.6816 
2024-11-11 17:30:04.678242: Pseudo dice [np.float32(0.9272), np.float32(0.6197)] 
2024-11-11 17:30:04.680405: Epoch time: 39.63 s 
2024-11-11 17:30:04.682782: Yayy! New best EMA pseudo Dice: 0.6460000276565552 
2024-11-11 17:30:06.635449:  
2024-11-11 17:30:06.637822: Epoch 15 
2024-11-11 17:30:06.640304: Current learning rate: 0.00986 
2024-11-11 17:30:46.230500: train_loss -0.7598 
2024-11-11 17:30:46.233151: val_loss -0.7533 
2024-11-11 17:30:46.235365: Pseudo dice [np.float32(0.9332), np.float32(0.6896)] 
2024-11-11 17:30:46.237568: Epoch time: 39.6 s 
2024-11-11 17:30:46.239885: Yayy! New best EMA pseudo Dice: 0.6625000238418579 
2024-11-11 17:30:48.164887:  
2024-11-11 17:30:48.167777: Epoch 16 
2024-11-11 17:30:48.170130: Current learning rate: 0.00986 
2024-11-11 17:31:27.769868: train_loss -0.7788 
2024-11-11 17:31:27.779103: val_loss -0.7642 
2024-11-11 17:31:27.781394: Pseudo dice [np.float32(0.9334), np.float32(0.7423)] 
2024-11-11 17:31:27.783590: Epoch time: 39.61 s 
2024-11-11 17:31:27.785867: Yayy! New best EMA pseudo Dice: 0.6801000237464905 
2024-11-11 17:31:29.766859:  
2024-11-11 17:31:29.769331: Epoch 17 
2024-11-11 17:31:29.771712: Current learning rate: 0.00985 
2024-11-11 17:32:09.375896: train_loss -0.7888 
2024-11-11 17:32:09.378783: val_loss -0.7432 
2024-11-11 17:32:09.381241: Pseudo dice [np.float32(0.9308), np.float32(0.7265)] 
2024-11-11 17:32:09.383741: Epoch time: 39.61 s 
2024-11-11 17:32:09.386004: Yayy! New best EMA pseudo Dice: 0.6948999762535095 
2024-11-11 17:32:11.332670:  
2024-11-11 17:32:11.335554: Epoch 18 
2024-11-11 17:32:11.337978: Current learning rate: 0.00984 
2024-11-11 17:32:50.933511: train_loss -0.8021 
2024-11-11 17:32:50.939050: val_loss -0.7493 
2024-11-11 17:32:50.941796: Pseudo dice [np.float32(0.9396), np.float32(0.7254)] 
2024-11-11 17:32:50.944176: Epoch time: 39.6 s 
2024-11-11 17:32:50.946335: Yayy! New best EMA pseudo Dice: 0.7087000012397766 
2024-11-11 17:32:52.962192:  
2024-11-11 17:32:52.964883: Epoch 19 
2024-11-11 17:32:52.967314: Current learning rate: 0.00983 
2024-11-11 17:33:32.566939: train_loss -0.7911 
2024-11-11 17:33:32.570025: val_loss -0.7194 
2024-11-11 17:33:32.572324: Pseudo dice [np.float32(0.9281), np.float32(0.6766)] 
2024-11-11 17:33:32.575889: Epoch time: 39.61 s 
2024-11-11 17:33:32.578234: Yayy! New best EMA pseudo Dice: 0.7179999947547913 
2024-11-11 17:33:34.550611:  
2024-11-11 17:33:34.553025: Epoch 20 
2024-11-11 17:33:34.555472: Current learning rate: 0.00982 
2024-11-11 17:34:14.187629: train_loss -0.7385 
2024-11-11 17:34:14.192759: val_loss -0.6706 
2024-11-11 17:34:14.195101: Pseudo dice [np.float32(0.9318), np.float32(0.563)] 
2024-11-11 17:34:14.197460: Epoch time: 39.64 s 
2024-11-11 17:34:14.199785: Yayy! New best EMA pseudo Dice: 0.7210000157356262 
2024-11-11 17:34:16.213641:  
2024-11-11 17:34:16.216217: Epoch 21 
2024-11-11 17:34:16.218799: Current learning rate: 0.00981 
2024-11-11 17:34:55.839061: train_loss -0.7657 
2024-11-11 17:34:55.850815: val_loss -0.7321 
2024-11-11 17:34:55.853305: Pseudo dice [np.float32(0.9329), np.float32(0.6706)] 
2024-11-11 17:34:55.855733: Epoch time: 39.63 s 
2024-11-11 17:34:55.857883: Yayy! New best EMA pseudo Dice: 0.7290999889373779 
2024-11-11 17:34:57.787725:  
2024-11-11 17:34:57.790421: Epoch 22 
2024-11-11 17:34:57.793200: Current learning rate: 0.0098 
2024-11-11 17:35:37.409972: train_loss -0.7777 
2024-11-11 17:35:37.415554: val_loss -0.6927 
2024-11-11 17:35:37.418197: Pseudo dice [np.float32(0.9383), np.float32(0.6299)] 
2024-11-11 17:35:37.420868: Epoch time: 39.62 s 
2024-11-11 17:35:37.423302: Yayy! New best EMA pseudo Dice: 0.7346000075340271 
2024-11-11 17:35:39.314433:  
2024-11-11 17:35:39.316773: Epoch 23 
2024-11-11 17:35:39.319159: Current learning rate: 0.00979 
2024-11-11 17:36:18.925894: train_loss -0.7676 
2024-11-11 17:36:18.928783: val_loss -0.665 
2024-11-11 17:36:18.931132: Pseudo dice [np.float32(0.9249), np.float32(0.6045)] 
2024-11-11 17:36:18.933619: Epoch time: 39.61 s 
2024-11-11 17:36:18.935906: Yayy! New best EMA pseudo Dice: 0.7376000285148621 
2024-11-11 17:36:20.870326:  
2024-11-11 17:36:20.873135: Epoch 24 
2024-11-11 17:36:20.875645: Current learning rate: 0.00978 
2024-11-11 17:37:00.495944: train_loss -0.7937 
2024-11-11 17:37:00.501550: val_loss -0.7244 
2024-11-11 17:37:00.503855: Pseudo dice [np.float32(0.9248), np.float32(0.7278)] 
2024-11-11 17:37:00.506079: Epoch time: 39.63 s 
2024-11-11 17:37:00.508149: Yayy! New best EMA pseudo Dice: 0.746399998664856 
2024-11-11 17:37:02.455471:  
2024-11-11 17:37:02.457844: Epoch 25 
2024-11-11 17:37:02.460358: Current learning rate: 0.00977 
2024-11-11 17:37:42.076290: train_loss -0.8113 
2024-11-11 17:37:42.079148: val_loss -0.7308 
2024-11-11 17:37:42.081660: Pseudo dice [np.float32(0.9371), np.float32(0.6677)] 
2024-11-11 17:37:42.084072: Epoch time: 39.62 s 
2024-11-11 17:37:42.086448: Yayy! New best EMA pseudo Dice: 0.7519999742507935 
2024-11-11 17:37:44.316083:  
2024-11-11 17:37:44.318726: Epoch 26 
2024-11-11 17:37:44.321036: Current learning rate: 0.00977 
2024-11-11 17:38:23.946191: train_loss -0.8144 
2024-11-11 17:38:23.953211: val_loss -0.7699 
2024-11-11 17:38:23.955350: Pseudo dice [np.float32(0.9456), np.float32(0.7228)] 
2024-11-11 17:38:23.957612: Epoch time: 39.63 s 
2024-11-11 17:38:23.959793: Yayy! New best EMA pseudo Dice: 0.7602999806404114 
2024-11-11 17:38:25.848136:  
2024-11-11 17:38:25.850835: Epoch 27 
2024-11-11 17:38:25.853292: Current learning rate: 0.00976 
2024-11-11 17:39:05.484985: train_loss -0.8032 
2024-11-11 17:39:05.487867: val_loss -0.7809 
2024-11-11 17:39:05.490332: Pseudo dice [np.float32(0.9427), np.float32(0.737)] 
2024-11-11 17:39:05.492645: Epoch time: 39.64 s 
2024-11-11 17:39:05.495113: Yayy! New best EMA pseudo Dice: 0.7681999802589417 
2024-11-11 17:39:07.446000:  
2024-11-11 17:39:07.448725: Epoch 28 
2024-11-11 17:39:07.451204: Current learning rate: 0.00975 
2024-11-11 17:39:47.086147: train_loss -0.8395 
2024-11-11 17:39:47.091469: val_loss -0.7497 
2024-11-11 17:39:47.093745: Pseudo dice [np.float32(0.9514), np.float32(0.7292)] 
2024-11-11 17:39:47.096139: Epoch time: 39.64 s 
2024-11-11 17:39:47.098308: Yayy! New best EMA pseudo Dice: 0.7753999829292297 
2024-11-11 17:39:48.983573:  
2024-11-11 17:39:48.986428: Epoch 29 
2024-11-11 17:39:48.988730: Current learning rate: 0.00974 
2024-11-11 17:40:28.646321: train_loss -0.8224 
2024-11-11 17:40:28.649310: val_loss -0.7505 
2024-11-11 17:40:28.651561: Pseudo dice [np.float32(0.9489), np.float32(0.7386)] 
2024-11-11 17:40:28.653941: Epoch time: 39.66 s 
2024-11-11 17:40:28.656202: Yayy! New best EMA pseudo Dice: 0.7822999954223633 
2024-11-11 17:40:30.579820:  
2024-11-11 17:40:30.582321: Epoch 30 
2024-11-11 17:40:30.584758: Current learning rate: 0.00973 
2024-11-11 17:41:10.247576: train_loss -0.8412 
2024-11-11 17:41:10.252973: val_loss -0.7285 
2024-11-11 17:41:10.255471: Pseudo dice [np.float32(0.9435), np.float32(0.681)] 
2024-11-11 17:41:10.257753: Epoch time: 39.67 s 
2024-11-11 17:41:10.260032: Yayy! New best EMA pseudo Dice: 0.7853000164031982 
2024-11-11 17:41:12.172309:  
2024-11-11 17:41:12.175451: Epoch 31 
2024-11-11 17:41:12.177925: Current learning rate: 0.00972 
2024-11-11 17:41:51.843538: train_loss -0.8098 
2024-11-11 17:41:51.846689: val_loss -0.7493 
2024-11-11 17:41:51.848998: Pseudo dice [np.float32(0.9423), np.float32(0.6829)] 
2024-11-11 17:41:51.851410: Epoch time: 39.67 s 
2024-11-11 17:41:51.853644: Yayy! New best EMA pseudo Dice: 0.7879999876022339 
2024-11-11 17:41:53.777852:  
2024-11-11 17:41:53.780303: Epoch 32 
2024-11-11 17:41:53.782592: Current learning rate: 0.00971 
2024-11-11 17:42:33.452968: train_loss -0.797 
2024-11-11 17:42:33.458232: val_loss -0.7575 
2024-11-11 17:42:33.460452: Pseudo dice [np.float32(0.9356), np.float32(0.6933)] 
2024-11-11 17:42:33.462746: Epoch time: 39.68 s 
2024-11-11 17:42:33.464863: Yayy! New best EMA pseudo Dice: 0.7906000018119812 
2024-11-11 17:42:35.431101:  
2024-11-11 17:42:35.434050: Epoch 33 
2024-11-11 17:42:35.436846: Current learning rate: 0.0097 
2024-11-11 17:43:15.088214: train_loss -0.7595 
2024-11-11 17:43:15.090991: val_loss -0.7149 
2024-11-11 17:43:15.093668: Pseudo dice [np.float32(0.9021), np.float32(0.6546)] 
2024-11-11 17:43:15.095984: Epoch time: 39.66 s 
2024-11-11 17:43:16.227582:  
2024-11-11 17:43:16.229840: Epoch 34 
2024-11-11 17:43:16.232732: Current learning rate: 0.00969 
2024-11-11 17:43:55.896788: train_loss -0.7672 
2024-11-11 17:43:55.901899: val_loss -0.6846 
2024-11-11 17:43:55.904309: Pseudo dice [np.float32(0.9252), np.float32(0.5918)] 
2024-11-11 17:43:55.906638: Epoch time: 39.67 s 
2024-11-11 17:43:57.051834:  
2024-11-11 17:43:57.054145: Epoch 35 
2024-11-11 17:43:57.056613: Current learning rate: 0.00968 
2024-11-11 17:44:36.710457: train_loss -0.8051 
2024-11-11 17:44:36.713147: val_loss -0.7424 
2024-11-11 17:44:36.715352: Pseudo dice [np.float32(0.9443), np.float32(0.7141)] 
2024-11-11 17:44:36.717518: Epoch time: 39.66 s 
2024-11-11 17:44:37.867670:  
2024-11-11 17:44:37.870065: Epoch 36 
2024-11-11 17:44:37.872311: Current learning rate: 0.00968 
2024-11-11 17:45:17.553940: train_loss -0.785 
2024-11-11 17:45:17.566925: val_loss -0.7788 
2024-11-11 17:45:17.569393: Pseudo dice [np.float32(0.9429), np.float32(0.7423)] 
2024-11-11 17:45:17.571685: Epoch time: 39.69 s 
2024-11-11 17:45:17.573900: Yayy! New best EMA pseudo Dice: 0.795799970626831 
2024-11-11 17:45:19.549133:  
2024-11-11 17:45:19.551969: Epoch 37 
2024-11-11 17:45:19.554636: Current learning rate: 0.00967 
2024-11-11 17:45:59.218633: train_loss -0.8161 
2024-11-11 17:45:59.221583: val_loss -0.7655 
2024-11-11 17:45:59.224063: Pseudo dice [np.float32(0.9464), np.float32(0.7279)] 
2024-11-11 17:45:59.226439: Epoch time: 39.67 s 
2024-11-11 17:45:59.228912: Yayy! New best EMA pseudo Dice: 0.7998999953269958 
2024-11-11 17:46:01.607789:  
2024-11-11 17:46:01.610546: Epoch 38 
2024-11-11 17:46:01.612919: Current learning rate: 0.00966 
2024-11-11 17:46:41.302237: train_loss -0.838 
2024-11-11 17:46:41.307788: val_loss -0.7439 
2024-11-11 17:46:41.310270: Pseudo dice [np.float32(0.9431), np.float32(0.7018)] 
2024-11-11 17:46:41.312840: Epoch time: 39.7 s 
2024-11-11 17:46:41.315281: Yayy! New best EMA pseudo Dice: 0.8022000193595886 
2024-11-11 17:46:43.305523:  
2024-11-11 17:46:43.308102: Epoch 39 
2024-11-11 17:46:43.310453: Current learning rate: 0.00965 
2024-11-11 17:47:22.983614: train_loss -0.831 
2024-11-11 17:47:22.986624: val_loss -0.7635 
2024-11-11 17:47:22.988955: Pseudo dice [np.float32(0.9468), np.float32(0.7382)] 
2024-11-11 17:47:22.991187: Epoch time: 39.68 s 
2024-11-11 17:47:22.993542: Yayy! New best EMA pseudo Dice: 0.8062000274658203 
2024-11-11 17:47:24.951669:  
2024-11-11 17:47:24.954436: Epoch 40 
2024-11-11 17:47:24.956852: Current learning rate: 0.00964 
2024-11-11 17:48:04.635697: train_loss -0.8198 
2024-11-11 17:48:04.643708: val_loss -0.7807 
2024-11-11 17:48:04.645828: Pseudo dice [np.float32(0.9341), np.float32(0.7546)] 
2024-11-11 17:48:04.647937: Epoch time: 39.69 s 
2024-11-11 17:48:04.650140: Yayy! New best EMA pseudo Dice: 0.8100000023841858 
2024-11-11 17:48:06.624815:  
2024-11-11 17:48:06.627382: Epoch 41 
2024-11-11 17:48:06.630287: Current learning rate: 0.00963 
2024-11-11 17:48:46.289457: train_loss -0.8272 
2024-11-11 17:48:46.292361: val_loss -0.7361 
2024-11-11 17:48:46.294812: Pseudo dice [np.float32(0.9477), np.float32(0.6983)] 
2024-11-11 17:48:46.297268: Epoch time: 39.67 s 
2024-11-11 17:48:46.299397: Yayy! New best EMA pseudo Dice: 0.8112999796867371 
2024-11-11 17:48:48.245405:  
2024-11-11 17:48:48.247710: Epoch 42 
2024-11-11 17:48:48.250036: Current learning rate: 0.00962 
2024-11-11 17:49:27.937371: train_loss -0.8352 
2024-11-11 17:49:27.943016: val_loss -0.7591 
2024-11-11 17:49:27.945644: Pseudo dice [np.float32(0.9468), np.float32(0.7471)] 
2024-11-11 17:49:27.948035: Epoch time: 39.69 s 
2024-11-11 17:49:27.950330: Yayy! New best EMA pseudo Dice: 0.8148999810218811 
2024-11-11 17:49:29.850590:  
2024-11-11 17:49:29.853201: Epoch 43 
2024-11-11 17:49:29.855694: Current learning rate: 0.00961 
2024-11-11 17:50:09.510665: train_loss -0.8291 
2024-11-11 17:50:09.514130: val_loss -0.7173 
2024-11-11 17:50:09.516635: Pseudo dice [np.float32(0.9423), np.float32(0.6683)] 
2024-11-11 17:50:09.518920: Epoch time: 39.66 s 
2024-11-11 17:50:10.633979:  
2024-11-11 17:50:10.636904: Epoch 44 
2024-11-11 17:50:10.639596: Current learning rate: 0.0096 
2024-11-11 17:50:50.317996: train_loss -0.8284 
2024-11-11 17:50:50.329205: val_loss -0.818 
2024-11-11 17:50:50.331860: Pseudo dice [np.float32(0.9472), np.float32(0.7934)] 
2024-11-11 17:50:50.334208: Epoch time: 39.69 s 
2024-11-11 17:50:50.336501: Yayy! New best EMA pseudo Dice: 0.819599986076355 
2024-11-11 17:50:52.218276:  
2024-11-11 17:50:52.221093: Epoch 45 
2024-11-11 17:50:52.223632: Current learning rate: 0.00959 
2024-11-11 17:51:31.890013: train_loss -0.8325 
2024-11-11 17:51:31.892902: val_loss -0.7622 
2024-11-11 17:51:31.895800: Pseudo dice [np.float32(0.9472), np.float32(0.7357)] 
2024-11-11 17:51:31.898168: Epoch time: 39.67 s 
2024-11-11 17:51:31.900690: Yayy! New best EMA pseudo Dice: 0.8217999935150146 
2024-11-11 17:51:33.857853:  
2024-11-11 17:51:33.860643: Epoch 46 
2024-11-11 17:51:33.863154: Current learning rate: 0.00959 
2024-11-11 17:52:13.523402: train_loss -0.8248 
2024-11-11 17:52:13.529044: val_loss -0.7658 
2024-11-11 17:52:13.531338: Pseudo dice [np.float32(0.9464), np.float32(0.6986)] 
2024-11-11 17:52:13.533523: Epoch time: 39.67 s 
2024-11-11 17:52:13.535966: Yayy! New best EMA pseudo Dice: 0.8217999935150146 
2024-11-11 17:52:15.457419:  
2024-11-11 17:52:15.459988: Epoch 47 
2024-11-11 17:52:15.462422: Current learning rate: 0.00958 
2024-11-11 17:52:55.119913: train_loss -0.8265 
2024-11-11 17:52:55.127018: val_loss -0.7346 
2024-11-11 17:52:55.129592: Pseudo dice [np.float32(0.9419), np.float32(0.716)] 
2024-11-11 17:52:55.132032: Epoch time: 39.66 s 
2024-11-11 17:52:55.134152: Yayy! New best EMA pseudo Dice: 0.8226000070571899 
2024-11-11 17:52:57.056195:  
2024-11-11 17:52:57.058697: Epoch 48 
2024-11-11 17:52:57.060904: Current learning rate: 0.00957 
2024-11-11 17:53:36.709154: train_loss -0.8434 
2024-11-11 17:53:36.714513: val_loss -0.7076 
2024-11-11 17:53:36.716836: Pseudo dice [np.float32(0.9356), np.float32(0.5891)] 
2024-11-11 17:53:36.719150: Epoch time: 39.65 s 
2024-11-11 17:53:37.831622:  
2024-11-11 17:53:37.834140: Epoch 49 
2024-11-11 17:53:37.836778: Current learning rate: 0.00956 
2024-11-11 17:54:17.523324: train_loss -0.81 
2024-11-11 17:54:17.526387: val_loss -0.7774 
2024-11-11 17:54:17.528719: Pseudo dice [np.float32(0.9409), np.float32(0.7227)] 
2024-11-11 17:54:17.531021: Epoch time: 39.69 s 
2024-11-11 17:54:19.831959:  
2024-11-11 17:54:19.834545: Epoch 50 
2024-11-11 17:54:19.836807: Current learning rate: 0.00955 
2024-11-11 17:54:59.518143: train_loss -0.8283 
2024-11-11 17:54:59.522810: val_loss -0.7465 
2024-11-11 17:54:59.525455: Pseudo dice [np.float32(0.9402), np.float32(0.6855)] 
2024-11-11 17:54:59.527987: Epoch time: 39.69 s 
2024-11-11 17:55:00.643048:  
2024-11-11 17:55:00.646106: Epoch 51 
2024-11-11 17:55:00.648566: Current learning rate: 0.00954 
2024-11-11 17:55:40.345757: train_loss -0.8169 
2024-11-11 17:55:40.353893: val_loss -0.7506 
2024-11-11 17:55:40.356659: Pseudo dice [np.float32(0.9283), np.float32(0.6838)] 
2024-11-11 17:55:40.359112: Epoch time: 39.7 s 
2024-11-11 17:55:41.470162:  
2024-11-11 17:55:41.472648: Epoch 52 
2024-11-11 17:55:41.475162: Current learning rate: 0.00953 
2024-11-11 17:56:21.154081: train_loss -0.8228 
2024-11-11 17:56:21.159263: val_loss -0.7637 
2024-11-11 17:56:21.161643: Pseudo dice [np.float32(0.9423), np.float32(0.7359)] 
2024-11-11 17:56:21.164119: Epoch time: 39.69 s 
2024-11-11 17:56:22.279917:  
2024-11-11 17:56:22.282437: Epoch 53 
2024-11-11 17:56:22.284933: Current learning rate: 0.00952 
2024-11-11 17:57:01.963802: train_loss -0.8447 
2024-11-11 17:57:01.967242: val_loss -0.7682 
2024-11-11 17:57:01.969664: Pseudo dice [np.float32(0.9438), np.float32(0.752)] 
2024-11-11 17:57:01.971876: Epoch time: 39.69 s 
2024-11-11 17:57:03.095631:  
2024-11-11 17:57:03.098260: Epoch 54 
2024-11-11 17:57:03.100728: Current learning rate: 0.00951 
2024-11-11 17:57:42.792938: train_loss -0.8266 
2024-11-11 17:57:42.797741: val_loss -0.793 
2024-11-11 17:57:42.800201: Pseudo dice [np.float32(0.9483), np.float32(0.7669)] 
2024-11-11 17:57:42.802841: Epoch time: 39.7 s 
2024-11-11 17:57:42.805187: Yayy! New best EMA pseudo Dice: 0.8252000212669373 
2024-11-11 17:57:44.734790:  
2024-11-11 17:57:44.737454: Epoch 55 
2024-11-11 17:57:44.739970: Current learning rate: 0.0095 
2024-11-11 17:58:24.397885: train_loss -0.836 
2024-11-11 17:58:24.405879: val_loss -0.7796 
2024-11-11 17:58:24.408207: Pseudo dice [np.float32(0.9457), np.float32(0.717)] 
2024-11-11 17:58:24.410681: Epoch time: 39.66 s 
2024-11-11 17:58:24.412914: Yayy! New best EMA pseudo Dice: 0.8258000016212463 
2024-11-11 17:58:26.393821:  
2024-11-11 17:58:26.396062: Epoch 56 
2024-11-11 17:58:26.398273: Current learning rate: 0.00949 
2024-11-11 17:59:06.057668: train_loss -0.8364 
2024-11-11 17:59:06.062430: val_loss -0.7489 
2024-11-11 17:59:06.064762: Pseudo dice [np.float32(0.9392), np.float32(0.696)] 
2024-11-11 17:59:06.067029: Epoch time: 39.67 s 
2024-11-11 17:59:07.196770:  
2024-11-11 17:59:07.199308: Epoch 57 
2024-11-11 17:59:07.201640: Current learning rate: 0.00949 
2024-11-11 17:59:46.854510: train_loss -0.8498 
2024-11-11 17:59:46.857936: val_loss -0.7969 
2024-11-11 17:59:46.860158: Pseudo dice [np.float32(0.948), np.float32(0.7625)] 
2024-11-11 17:59:46.862474: Epoch time: 39.66 s 
2024-11-11 17:59:46.865637: Yayy! New best EMA pseudo Dice: 0.828000009059906 
2024-11-11 17:59:48.803137:  
2024-11-11 17:59:48.805840: Epoch 58 
2024-11-11 17:59:48.808251: Current learning rate: 0.00948 
2024-11-11 18:00:28.459788: train_loss -0.8343 
2024-11-11 18:00:28.464553: val_loss -0.7696 
2024-11-11 18:00:28.467035: Pseudo dice [np.float32(0.9337), np.float32(0.7537)] 
2024-11-11 18:00:28.469576: Epoch time: 39.66 s 
2024-11-11 18:00:28.471999: Yayy! New best EMA pseudo Dice: 0.8295999765396118 
2024-11-11 18:00:30.568054:  
2024-11-11 18:00:30.570541: Epoch 59 
2024-11-11 18:00:30.573063: Current learning rate: 0.00947 
2024-11-11 18:01:10.237588: train_loss -0.8464 
2024-11-11 18:01:10.254634: val_loss -0.7669 
2024-11-11 18:01:10.257169: Pseudo dice [np.float32(0.9351), np.float32(0.6975)] 
2024-11-11 18:01:10.259739: Epoch time: 39.67 s 
2024-11-11 18:01:11.392231:  
2024-11-11 18:01:11.394953: Epoch 60 
2024-11-11 18:01:11.397257: Current learning rate: 0.00946 
2024-11-11 18:01:51.076519: train_loss -0.8222 
2024-11-11 18:01:51.081437: val_loss -0.7777 
2024-11-11 18:01:51.083836: Pseudo dice [np.float32(0.9399), np.float32(0.7467)] 
2024-11-11 18:01:51.086295: Epoch time: 39.69 s 
2024-11-11 18:01:51.088593: Yayy! New best EMA pseudo Dice: 0.829800009727478 
2024-11-11 18:01:53.010193:  
2024-11-11 18:01:53.012584: Epoch 61 
2024-11-11 18:01:53.014938: Current learning rate: 0.00945 
2024-11-11 18:02:32.679190: train_loss -0.8262 
2024-11-11 18:02:32.682601: val_loss -0.7661 
2024-11-11 18:02:32.684749: Pseudo dice [np.float32(0.9435), np.float32(0.7492)] 
2024-11-11 18:02:32.686957: Epoch time: 39.67 s 
2024-11-11 18:02:32.689164: Yayy! New best EMA pseudo Dice: 0.8313999772071838 
2024-11-11 18:02:34.605957:  
2024-11-11 18:02:34.608740: Epoch 62 
2024-11-11 18:02:34.611463: Current learning rate: 0.00944 
2024-11-11 18:03:14.285147: train_loss -0.8498 
2024-11-11 18:03:14.290044: val_loss -0.7861 
2024-11-11 18:03:14.292280: Pseudo dice [np.float32(0.9391), np.float32(0.7523)] 
2024-11-11 18:03:14.294635: Epoch time: 39.68 s 
2024-11-11 18:03:14.297059: Yayy! New best EMA pseudo Dice: 0.8328999876976013 
2024-11-11 18:03:16.694751:  
2024-11-11 18:03:16.697349: Epoch 63 
2024-11-11 18:03:16.700477: Current learning rate: 0.00943 
2024-11-11 18:03:56.369236: train_loss -0.8097 
2024-11-11 18:03:56.372905: val_loss -0.7885 
2024-11-11 18:03:56.375276: Pseudo dice [np.float32(0.9378), np.float32(0.7255)] 
2024-11-11 18:03:56.377863: Epoch time: 39.68 s 
2024-11-11 18:03:57.508749:  
2024-11-11 18:03:57.511177: Epoch 64 
2024-11-11 18:03:57.513742: Current learning rate: 0.00942 
2024-11-11 18:04:37.195817: train_loss -0.8533 
2024-11-11 18:04:37.203763: val_loss -0.7766 
2024-11-11 18:04:37.206099: Pseudo dice [np.float32(0.948), np.float32(0.7541)] 
2024-11-11 18:04:37.211189: Epoch time: 39.69 s 
2024-11-11 18:04:37.213537: Yayy! New best EMA pseudo Dice: 0.8345999717712402 
2024-11-11 18:04:39.169785:  
2024-11-11 18:04:39.172449: Epoch 65 
2024-11-11 18:04:39.174946: Current learning rate: 0.00941 
2024-11-11 18:05:18.840125: train_loss -0.8407 
2024-11-11 18:05:18.843313: val_loss -0.7392 
2024-11-11 18:05:18.845590: Pseudo dice [np.float32(0.9368), np.float32(0.6709)] 
2024-11-11 18:05:18.847751: Epoch time: 39.67 s 
2024-11-11 18:05:19.988652:  
2024-11-11 18:05:19.991427: Epoch 66 
2024-11-11 18:05:19.993923: Current learning rate: 0.0094 
2024-11-11 18:05:59.668017: train_loss -0.8322 
2024-11-11 18:05:59.673146: val_loss -0.7566 
2024-11-11 18:05:59.675523: Pseudo dice [np.float32(0.9383), np.float32(0.7119)] 
2024-11-11 18:05:59.677927: Epoch time: 39.68 s 
2024-11-11 18:06:00.820419:  
2024-11-11 18:06:00.822932: Epoch 67 
2024-11-11 18:06:00.825443: Current learning rate: 0.00939 
2024-11-11 18:06:40.514810: train_loss -0.8425 
2024-11-11 18:06:40.518450: val_loss -0.7433 
2024-11-11 18:06:40.520792: Pseudo dice [np.float32(0.9499), np.float32(0.6477)] 
2024-11-11 18:06:40.523081: Epoch time: 39.7 s 
2024-11-11 18:06:41.674391:  
2024-11-11 18:06:41.676969: Epoch 68 
2024-11-11 18:06:41.679376: Current learning rate: 0.00939 
2024-11-11 18:07:21.374774: train_loss -0.8373 
2024-11-11 18:07:21.379904: val_loss -0.7445 
2024-11-11 18:07:21.382238: Pseudo dice [np.float32(0.9409), np.float32(0.7034)] 
2024-11-11 18:07:21.384683: Epoch time: 39.7 s 
2024-11-11 18:07:22.538371:  
2024-11-11 18:07:22.540819: Epoch 69 
2024-11-11 18:07:22.543201: Current learning rate: 0.00938 
2024-11-11 18:08:02.220669: train_loss -0.8338 
2024-11-11 18:08:02.229607: val_loss -0.7336 
2024-11-11 18:08:02.232157: Pseudo dice [np.float32(0.9463), np.float32(0.6591)] 
2024-11-11 18:08:02.234575: Epoch time: 39.68 s 
2024-11-11 18:08:03.393289:  
2024-11-11 18:08:03.395835: Epoch 70 
2024-11-11 18:08:03.398339: Current learning rate: 0.00937 
2024-11-11 18:08:43.055067: train_loss -0.8491 
2024-11-11 18:08:43.060145: val_loss -0.7614 
2024-11-11 18:08:43.062534: Pseudo dice [np.float32(0.9466), np.float32(0.722)] 
2024-11-11 18:08:43.064952: Epoch time: 39.66 s 
2024-11-11 18:08:44.219394:  
2024-11-11 18:08:44.222093: Epoch 71 
2024-11-11 18:08:44.224815: Current learning rate: 0.00936 
2024-11-11 18:09:23.890919: train_loss -0.8255 
2024-11-11 18:09:23.894797: val_loss -0.7706 
2024-11-11 18:09:23.897248: Pseudo dice [np.float32(0.9501), np.float32(0.7587)] 
2024-11-11 18:09:23.899779: Epoch time: 39.67 s 
2024-11-11 18:09:25.056268:  
2024-11-11 18:09:25.058807: Epoch 72 
2024-11-11 18:09:25.061365: Current learning rate: 0.00935 
2024-11-11 18:10:04.735998: train_loss -0.8555 
2024-11-11 18:10:04.741150: val_loss -0.8058 
2024-11-11 18:10:04.743516: Pseudo dice [np.float32(0.9405), np.float32(0.7855)] 
2024-11-11 18:10:04.745957: Epoch time: 39.68 s 
2024-11-11 18:10:05.898607:  
2024-11-11 18:10:05.900886: Epoch 73 
2024-11-11 18:10:05.903365: Current learning rate: 0.00934 
2024-11-11 18:10:45.554771: train_loss -0.8664 
2024-11-11 18:10:45.558363: val_loss -0.7856 
2024-11-11 18:10:45.560804: Pseudo dice [np.float32(0.9458), np.float32(0.7181)] 
2024-11-11 18:10:45.563032: Epoch time: 39.66 s 
2024-11-11 18:10:46.707054:  
2024-11-11 18:10:46.709542: Epoch 74 
2024-11-11 18:10:46.712083: Current learning rate: 0.00933 
2024-11-11 18:11:26.387925: train_loss -0.8732 
2024-11-11 18:11:26.393381: val_loss -0.7563 
2024-11-11 18:11:26.395752: Pseudo dice [np.float32(0.9471), np.float32(0.7096)] 
2024-11-11 18:11:26.398213: Epoch time: 39.68 s 
2024-11-11 18:11:27.899344:  
2024-11-11 18:11:27.901723: Epoch 75 
2024-11-11 18:11:27.904006: Current learning rate: 0.00932 
2024-11-11 18:12:07.609427: train_loss -0.8297 
2024-11-11 18:12:07.613517: val_loss -0.7871 
2024-11-11 18:12:07.616210: Pseudo dice [np.float32(0.9451), np.float32(0.7838)] 
2024-11-11 18:12:07.618436: Epoch time: 39.71 s 
2024-11-11 18:12:07.620555: Yayy! New best EMA pseudo Dice: 0.8349000215530396 
2024-11-11 18:12:09.575432:  
2024-11-11 18:12:09.577847: Epoch 76 
2024-11-11 18:12:09.580352: Current learning rate: 0.00931 
2024-11-11 18:12:49.276109: train_loss -0.8625 
2024-11-11 18:12:49.281167: val_loss -0.8054 
2024-11-11 18:12:49.283424: Pseudo dice [np.float32(0.949), np.float32(0.7741)] 
2024-11-11 18:12:49.285923: Epoch time: 39.7 s 
2024-11-11 18:12:49.288111: Yayy! New best EMA pseudo Dice: 0.8374999761581421 
2024-11-11 18:12:51.201364:  
2024-11-11 18:12:51.204013: Epoch 77 
2024-11-11 18:12:51.206336: Current learning rate: 0.0093 
2024-11-11 18:13:30.887966: train_loss -0.8707 
2024-11-11 18:13:30.891514: val_loss -0.7731 
2024-11-11 18:13:30.893806: Pseudo dice [np.float32(0.9513), np.float32(0.6971)] 
2024-11-11 18:13:30.896046: Epoch time: 39.69 s 
2024-11-11 18:13:32.058972:  
2024-11-11 18:13:32.061433: Epoch 78 
2024-11-11 18:13:32.063444: Current learning rate: 0.0093 
2024-11-11 18:14:11.756895: train_loss -0.8672 
2024-11-11 18:14:11.761663: val_loss -0.7512 
2024-11-11 18:14:11.764157: Pseudo dice [np.float32(0.9449), np.float32(0.7074)] 
2024-11-11 18:14:11.766366: Epoch time: 39.7 s 
2024-11-11 18:14:12.925571:  
2024-11-11 18:14:12.928230: Epoch 79 
2024-11-11 18:14:12.930834: Current learning rate: 0.00929 
2024-11-11 18:14:52.618913: train_loss -0.8714 
2024-11-11 18:14:52.625232: val_loss -0.7912 
2024-11-11 18:14:52.627321: Pseudo dice [np.float32(0.9517), np.float32(0.7805)] 
2024-11-11 18:14:52.629417: Epoch time: 39.69 s 
2024-11-11 18:14:52.631680: Yayy! New best EMA pseudo Dice: 0.8382999897003174 
2024-11-11 18:14:54.581499:  
2024-11-11 18:14:54.584291: Epoch 80 
2024-11-11 18:14:54.586694: Current learning rate: 0.00928 
2024-11-11 18:15:34.250798: train_loss -0.8554 
2024-11-11 18:15:34.255334: val_loss -0.806 
2024-11-11 18:15:34.257616: Pseudo dice [np.float32(0.9489), np.float32(0.7997)] 
2024-11-11 18:15:34.259850: Epoch time: 39.67 s 
2024-11-11 18:15:34.262012: Yayy! New best EMA pseudo Dice: 0.8418999910354614 
2024-11-11 18:15:36.227682:  
2024-11-11 18:15:36.230365: Epoch 81 
2024-11-11 18:15:36.232876: Current learning rate: 0.00927 
2024-11-11 18:16:15.908368: train_loss -0.8732 
2024-11-11 18:16:15.912094: val_loss -0.7696 
2024-11-11 18:16:15.914364: Pseudo dice [np.float32(0.9462), np.float32(0.7271)] 
2024-11-11 18:16:15.916648: Epoch time: 39.68 s 
2024-11-11 18:16:17.076539:  
2024-11-11 18:16:17.079218: Epoch 82 
2024-11-11 18:16:17.081609: Current learning rate: 0.00926 
2024-11-11 18:16:56.744149: train_loss -0.8554 
2024-11-11 18:16:56.754192: val_loss -0.7714 
2024-11-11 18:16:56.756762: Pseudo dice [np.float32(0.9414), np.float32(0.7562)] 
2024-11-11 18:16:56.759275: Epoch time: 39.67 s 
2024-11-11 18:16:56.761416: Yayy! New best EMA pseudo Dice: 0.8421000242233276 
2024-11-11 18:16:58.739015:  
2024-11-11 18:16:58.741724: Epoch 83 
2024-11-11 18:16:58.744307: Current learning rate: 0.00925 
2024-11-11 18:17:38.413913: train_loss -0.8747 
2024-11-11 18:17:38.417405: val_loss -0.806 
2024-11-11 18:17:38.419741: Pseudo dice [np.float32(0.9524), np.float32(0.7564)] 
2024-11-11 18:17:38.422202: Epoch time: 39.68 s 
2024-11-11 18:17:38.424840: Yayy! New best EMA pseudo Dice: 0.8432999849319458 
2024-11-11 18:17:40.409872:  
2024-11-11 18:17:40.412449: Epoch 84 
2024-11-11 18:17:40.414789: Current learning rate: 0.00924 
2024-11-11 18:18:20.072101: train_loss -0.8709 
2024-11-11 18:18:20.081255: val_loss -0.7635 
2024-11-11 18:18:20.083624: Pseudo dice [np.float32(0.9511), np.float32(0.7213)] 
2024-11-11 18:18:20.085895: Epoch time: 39.66 s 
2024-11-11 18:18:21.181468:  
2024-11-11 18:18:21.183928: Epoch 85 
2024-11-11 18:18:21.186546: Current learning rate: 0.00923 
2024-11-11 18:19:00.854173: train_loss -0.8742 
2024-11-11 18:19:00.857990: val_loss -0.7181 
2024-11-11 18:19:00.860494: Pseudo dice [np.float32(0.9439), np.float32(0.6819)] 
2024-11-11 18:19:00.863037: Epoch time: 39.67 s 
2024-11-11 18:19:01.962158:  
2024-11-11 18:19:01.964695: Epoch 86 
2024-11-11 18:19:01.967065: Current learning rate: 0.00922 
2024-11-11 18:19:41.637689: train_loss -0.8581 
2024-11-11 18:19:41.642753: val_loss -0.8013 
2024-11-11 18:19:41.645212: Pseudo dice [np.float32(0.9477), np.float32(0.7553)] 
2024-11-11 18:19:41.647388: Epoch time: 39.68 s 
2024-11-11 18:19:43.102709:  
2024-11-11 18:19:43.105146: Epoch 87 
2024-11-11 18:19:43.107596: Current learning rate: 0.00921 
2024-11-11 18:20:22.805787: train_loss -0.8619 
2024-11-11 18:20:22.812071: val_loss -0.7808 
2024-11-11 18:20:22.814694: Pseudo dice [np.float32(0.9443), np.float32(0.7202)] 
2024-11-11 18:20:22.817024: Epoch time: 39.7 s 
2024-11-11 18:20:23.917723:  
2024-11-11 18:20:23.920210: Epoch 88 
2024-11-11 18:20:23.922586: Current learning rate: 0.0092 
2024-11-11 18:21:03.629709: train_loss -0.8636 
2024-11-11 18:21:03.634645: val_loss -0.7678 
2024-11-11 18:21:03.636765: Pseudo dice [np.float32(0.9423), np.float32(0.7361)] 
2024-11-11 18:21:03.640299: Epoch time: 39.71 s 
2024-11-11 18:21:04.746099:  
2024-11-11 18:21:04.748559: Epoch 89 
2024-11-11 18:21:04.750581: Current learning rate: 0.0092 
2024-11-11 18:21:44.464433: train_loss -0.8654 
2024-11-11 18:21:44.467897: val_loss -0.7844 
2024-11-11 18:21:44.470252: Pseudo dice [np.float32(0.9508), np.float32(0.7249)] 
2024-11-11 18:21:44.472539: Epoch time: 39.72 s 
2024-11-11 18:21:45.590241:  
2024-11-11 18:21:45.593045: Epoch 90 
2024-11-11 18:21:45.595566: Current learning rate: 0.00919 
2024-11-11 18:22:25.306048: train_loss -0.8621 
2024-11-11 18:22:25.312826: val_loss -0.8259 
2024-11-11 18:22:25.315220: Pseudo dice [np.float32(0.9479), np.float32(0.7748)] 
2024-11-11 18:22:25.317475: Epoch time: 39.72 s 
2024-11-11 18:22:26.419492:  
2024-11-11 18:22:26.422066: Epoch 91 
2024-11-11 18:22:26.424643: Current learning rate: 0.00918 
2024-11-11 18:23:06.109336: train_loss -0.8607 
2024-11-11 18:23:06.112994: val_loss -0.7965 
2024-11-11 18:23:06.115574: Pseudo dice [np.float32(0.9362), np.float32(0.7776)] 
2024-11-11 18:23:06.118124: Epoch time: 39.69 s 
2024-11-11 18:23:06.120930: Yayy! New best EMA pseudo Dice: 0.8434000015258789 
2024-11-11 18:23:08.081174:  
2024-11-11 18:23:08.084729: Epoch 92 
2024-11-11 18:23:08.087121: Current learning rate: 0.00917 
2024-11-11 18:23:47.756434: train_loss -0.8528 
2024-11-11 18:23:47.761655: val_loss -0.7884 
2024-11-11 18:23:47.764415: Pseudo dice [np.float32(0.9468), np.float32(0.757)] 
2024-11-11 18:23:47.766723: Epoch time: 39.68 s 
2024-11-11 18:23:47.769007: Yayy! New best EMA pseudo Dice: 0.8442000150680542 
2024-11-11 18:23:49.625130:  
2024-11-11 18:23:49.627660: Epoch 93 
2024-11-11 18:23:49.629862: Current learning rate: 0.00916 
2024-11-11 18:24:29.306028: train_loss -0.8771 
2024-11-11 18:24:29.315737: val_loss -0.7632 
2024-11-11 18:24:29.318155: Pseudo dice [np.float32(0.9473), np.float32(0.7229)] 
2024-11-11 18:24:29.320587: Epoch time: 39.68 s 
2024-11-11 18:24:30.410455:  
2024-11-11 18:24:30.412890: Epoch 94 
2024-11-11 18:24:30.415516: Current learning rate: 0.00915 
2024-11-11 18:25:10.113750: train_loss -0.8564 
2024-11-11 18:25:10.118496: val_loss -0.7 
2024-11-11 18:25:10.121123: Pseudo dice [np.float32(0.9341), np.float32(0.6352)] 
2024-11-11 18:25:10.123430: Epoch time: 39.7 s 
2024-11-11 18:25:11.214623:  
2024-11-11 18:25:11.217149: Epoch 95 
2024-11-11 18:25:11.219621: Current learning rate: 0.00914 
2024-11-11 18:25:50.902927: train_loss -0.8525 
2024-11-11 18:25:50.906625: val_loss -0.7851 
2024-11-11 18:25:50.909153: Pseudo dice [np.float32(0.9489), np.float32(0.7537)] 
2024-11-11 18:25:50.911453: Epoch time: 39.69 s 
2024-11-11 18:25:51.996011:  
2024-11-11 18:25:51.998686: Epoch 96 
2024-11-11 18:25:52.001066: Current learning rate: 0.00913 
2024-11-11 18:26:31.660416: train_loss -0.8649 
2024-11-11 18:26:31.665540: val_loss -0.7566 
2024-11-11 18:26:31.668060: Pseudo dice [np.float32(0.9514), np.float32(0.7308)] 
2024-11-11 18:26:31.670407: Epoch time: 39.67 s 
2024-11-11 18:26:32.772039:  
2024-11-11 18:26:32.774533: Epoch 97 
2024-11-11 18:26:32.777080: Current learning rate: 0.00912 
2024-11-11 18:27:12.451084: train_loss -0.8714 
2024-11-11 18:27:12.454424: val_loss -0.8007 
2024-11-11 18:27:12.456598: Pseudo dice [np.float32(0.9505), np.float32(0.763)] 
2024-11-11 18:27:12.458649: Epoch time: 39.68 s 
2024-11-11 18:27:13.564003:  
2024-11-11 18:27:13.566345: Epoch 98 
2024-11-11 18:27:13.568870: Current learning rate: 0.00911 
2024-11-11 18:27:53.118642: train_loss -0.8764 
2024-11-11 18:27:53.130343: val_loss -0.8274 
2024-11-11 18:27:53.133095: Pseudo dice [np.float32(0.9509), np.float32(0.8155)] 
2024-11-11 18:27:53.135836: Epoch time: 39.56 s 
2024-11-11 18:27:53.138148: Yayy! New best EMA pseudo Dice: 0.8450999855995178 
2024-11-11 18:27:55.036098:  
2024-11-11 18:27:55.038486: Epoch 99 
2024-11-11 18:27:55.040705: Current learning rate: 0.0091 
2024-11-11 18:28:34.616713: train_loss -0.8774 
2024-11-11 18:28:34.620568: val_loss -0.8021 
2024-11-11 18:28:34.622848: Pseudo dice [np.float32(0.9553), np.float32(0.7747)] 
2024-11-11 18:28:34.625722: Epoch time: 39.58 s 
2024-11-11 18:28:35.384651: Yayy! New best EMA pseudo Dice: 0.847100019454956 
2024-11-11 18:28:37.607407:  
2024-11-11 18:28:37.610162: Epoch 100 
2024-11-11 18:28:37.612538: Current learning rate: 0.0091 
2024-11-11 18:29:17.212288: train_loss -0.8503 
2024-11-11 18:29:17.216644: val_loss -0.8013 
2024-11-11 18:29:17.218746: Pseudo dice [np.float32(0.9486), np.float32(0.776)] 
2024-11-11 18:29:17.220906: Epoch time: 39.61 s 
2024-11-11 18:29:17.223137: Yayy! New best EMA pseudo Dice: 0.8485999703407288 
2024-11-11 18:29:19.121329:  
2024-11-11 18:29:19.123733: Epoch 101 
2024-11-11 18:29:19.126201: Current learning rate: 0.00909 
2024-11-11 18:29:58.729687: train_loss -0.8381 
2024-11-11 18:29:58.735404: val_loss -0.7325 
2024-11-11 18:29:58.737418: Pseudo dice [np.float32(0.9442), np.float32(0.6849)] 
2024-11-11 18:29:58.739601: Epoch time: 39.61 s 
2024-11-11 18:29:59.847930:  
2024-11-11 18:29:59.850362: Epoch 102 
2024-11-11 18:29:59.852751: Current learning rate: 0.00908 
2024-11-11 18:30:39.457466: train_loss -0.8601 
2024-11-11 18:30:39.461811: val_loss -0.7816 
2024-11-11 18:30:39.464117: Pseudo dice [np.float32(0.9403), np.float32(0.7508)] 
2024-11-11 18:30:39.466220: Epoch time: 39.61 s 
2024-11-11 18:30:40.572876:  
2024-11-11 18:30:40.575313: Epoch 103 
2024-11-11 18:30:40.577600: Current learning rate: 0.00907 
2024-11-11 18:31:20.196916: train_loss -0.8501 
2024-11-11 18:31:20.200561: val_loss -0.7677 
2024-11-11 18:31:20.203027: Pseudo dice [np.float32(0.9527), np.float32(0.7389)] 
2024-11-11 18:31:20.205111: Epoch time: 39.63 s 
2024-11-11 18:31:21.312017:  
2024-11-11 18:31:21.314581: Epoch 104 
2024-11-11 18:31:21.317008: Current learning rate: 0.00906 
2024-11-11 18:32:00.941454: train_loss -0.8763 
2024-11-11 18:32:00.950545: val_loss -0.7704 
2024-11-11 18:32:00.953523: Pseudo dice [np.float32(0.9544), np.float32(0.7429)] 
2024-11-11 18:32:00.956007: Epoch time: 39.63 s 
2024-11-11 18:32:02.069034:  
2024-11-11 18:32:02.071455: Epoch 105 
2024-11-11 18:32:02.073987: Current learning rate: 0.00905 
2024-11-11 18:32:41.698106: train_loss -0.8519 
2024-11-11 18:32:41.702044: val_loss -0.7538 
2024-11-11 18:32:41.704479: Pseudo dice [np.float32(0.9401), np.float32(0.674)] 
2024-11-11 18:32:41.706890: Epoch time: 39.63 s 
2024-11-11 18:32:42.815293:  
2024-11-11 18:32:42.817853: Epoch 106 
2024-11-11 18:32:42.820301: Current learning rate: 0.00904 
2024-11-11 18:33:22.446808: train_loss -0.8698 
2024-11-11 18:33:22.451523: val_loss -0.7333 
2024-11-11 18:33:22.453864: Pseudo dice [np.float32(0.9386), np.float32(0.6541)] 
2024-11-11 18:33:22.456220: Epoch time: 39.63 s 
2024-11-11 18:33:24.385572:  
2024-11-11 18:33:24.388141: Epoch 107 
2024-11-11 18:33:24.390709: Current learning rate: 0.00903 
2024-11-11 18:34:04.005278: train_loss -0.8646 
2024-11-11 18:34:04.008944: val_loss -0.8194 
2024-11-11 18:34:04.011140: Pseudo dice [np.float32(0.9459), np.float32(0.7807)] 
2024-11-11 18:34:04.013318: Epoch time: 39.62 s 
2024-11-11 18:34:05.126512:  
2024-11-11 18:34:05.129066: Epoch 108 
2024-11-11 18:34:05.131516: Current learning rate: 0.00902 
2024-11-11 18:34:44.759868: train_loss -0.8551 
2024-11-11 18:34:44.768115: val_loss -0.7356 
2024-11-11 18:34:44.770550: Pseudo dice [np.float32(0.9448), np.float32(0.6219)] 
2024-11-11 18:34:44.772848: Epoch time: 39.63 s 
2024-11-11 18:34:45.885652:  
2024-11-11 18:34:45.887879: Epoch 109 
2024-11-11 18:34:45.890275: Current learning rate: 0.00901 
2024-11-11 18:35:25.513504: train_loss -0.8514 
2024-11-11 18:35:25.516987: val_loss -0.7169 
2024-11-11 18:35:25.519206: Pseudo dice [np.float32(0.9382), np.float32(0.6722)] 
2024-11-11 18:35:25.521465: Epoch time: 39.63 s 
2024-11-11 18:35:26.630596:  
2024-11-11 18:35:26.633187: Epoch 110 
2024-11-11 18:35:26.635741: Current learning rate: 0.009 
2024-11-11 18:36:06.258360: train_loss -0.8585 
2024-11-11 18:36:06.263122: val_loss -0.8164 
2024-11-11 18:36:06.266205: Pseudo dice [np.float32(0.9504), np.float32(0.7888)] 
2024-11-11 18:36:06.268438: Epoch time: 39.63 s 
2024-11-11 18:36:07.383147:  
2024-11-11 18:36:07.385733: Epoch 111 
2024-11-11 18:36:07.388176: Current learning rate: 0.009 
2024-11-11 18:36:47.005394: train_loss -0.8664 
2024-11-11 18:36:47.014034: val_loss -0.8092 
2024-11-11 18:36:47.016321: Pseudo dice [np.float32(0.9533), np.float32(0.7584)] 
2024-11-11 18:36:47.018553: Epoch time: 39.62 s 
2024-11-11 18:36:48.125989:  
2024-11-11 18:36:48.128411: Epoch 112 
2024-11-11 18:36:48.130786: Current learning rate: 0.00899 
2024-11-11 18:37:27.736981: train_loss -0.8667 
2024-11-11 18:37:27.742008: val_loss -0.7402 
2024-11-11 18:37:27.744408: Pseudo dice [np.float32(0.9321), np.float32(0.7191)] 
2024-11-11 18:37:27.746854: Epoch time: 39.61 s 
2024-11-11 18:37:29.200565:  
2024-11-11 18:37:29.202980: Epoch 113 
2024-11-11 18:37:29.205257: Current learning rate: 0.00898 
2024-11-11 18:38:08.842775: train_loss -0.8726 
2024-11-11 18:38:08.851185: val_loss -0.795 
2024-11-11 18:38:08.853688: Pseudo dice [np.float32(0.9525), np.float32(0.7724)] 
2024-11-11 18:38:08.855787: Epoch time: 39.64 s 
2024-11-11 18:38:09.968443:  
2024-11-11 18:38:09.970665: Epoch 114 
2024-11-11 18:38:09.973128: Current learning rate: 0.00897 
2024-11-11 18:38:49.595932: train_loss -0.876 
2024-11-11 18:38:49.600900: val_loss -0.8048 
2024-11-11 18:38:49.603321: Pseudo dice [np.float32(0.945), np.float32(0.75)] 
2024-11-11 18:38:49.605575: Epoch time: 39.63 s 
2024-11-11 18:38:50.723474:  
2024-11-11 18:38:50.726493: Epoch 115 
2024-11-11 18:38:50.728819: Current learning rate: 0.00896 
2024-11-11 18:39:30.374972: train_loss -0.858 
2024-11-11 18:39:30.382224: val_loss -0.7485 
2024-11-11 18:39:30.384654: Pseudo dice [np.float32(0.9534), np.float32(0.7097)] 
2024-11-11 18:39:30.387353: Epoch time: 39.65 s 
2024-11-11 18:39:31.516009:  
2024-11-11 18:39:31.518464: Epoch 116 
2024-11-11 18:39:31.521130: Current learning rate: 0.00895 
2024-11-11 18:40:11.164007: train_loss -0.8352 
2024-11-11 18:40:11.171939: val_loss -0.7652 
2024-11-11 18:40:11.174238: Pseudo dice [np.float32(0.9439), np.float32(0.7299)] 
2024-11-11 18:40:11.176598: Epoch time: 39.65 s 
2024-11-11 18:40:12.308520:  
2024-11-11 18:40:12.311084: Epoch 117 
2024-11-11 18:40:12.313595: Current learning rate: 0.00894 
2024-11-11 18:40:51.945108: train_loss -0.865 
2024-11-11 18:40:51.948639: val_loss -0.7801 
2024-11-11 18:40:51.951380: Pseudo dice [np.float32(0.942), np.float32(0.7436)] 
2024-11-11 18:40:51.953752: Epoch time: 39.64 s 
2024-11-11 18:40:53.082791:  
2024-11-11 18:40:53.085420: Epoch 118 
2024-11-11 18:40:53.088006: Current learning rate: 0.00893 
2024-11-11 18:41:32.716819: train_loss -0.8827 
2024-11-11 18:41:32.722023: val_loss -0.7944 
2024-11-11 18:41:32.724345: Pseudo dice [np.float32(0.9521), np.float32(0.7659)] 
2024-11-11 18:41:32.726854: Epoch time: 39.64 s 
2024-11-11 18:41:33.856670:  
2024-11-11 18:41:33.859056: Epoch 119 
2024-11-11 18:41:33.861428: Current learning rate: 0.00892 
2024-11-11 18:42:13.476812: train_loss -0.8845 
2024-11-11 18:42:13.480268: val_loss -0.7914 
2024-11-11 18:42:13.482761: Pseudo dice [np.float32(0.9507), np.float32(0.7626)] 
2024-11-11 18:42:13.485002: Epoch time: 39.62 s 
2024-11-11 18:42:14.614060:  
2024-11-11 18:42:14.618107: Epoch 120 
2024-11-11 18:42:14.620363: Current learning rate: 0.00891 
2024-11-11 18:42:54.264758: train_loss -0.8925 
2024-11-11 18:42:54.273168: val_loss -0.8018 
2024-11-11 18:42:54.275543: Pseudo dice [np.float32(0.9501), np.float32(0.7552)] 
2024-11-11 18:42:54.277640: Epoch time: 39.65 s 
2024-11-11 18:42:55.404570:  
2024-11-11 18:42:55.406852: Epoch 121 
2024-11-11 18:42:55.408924: Current learning rate: 0.0089 
2024-11-11 18:43:35.032261: train_loss -0.8678 
2024-11-11 18:43:35.039297: val_loss -0.741 
2024-11-11 18:43:35.041703: Pseudo dice [np.float32(0.9401), np.float32(0.6722)] 
2024-11-11 18:43:35.044002: Epoch time: 39.63 s 
2024-11-11 18:43:36.176008:  
2024-11-11 18:43:36.178584: Epoch 122 
2024-11-11 18:43:36.180809: Current learning rate: 0.00889 
2024-11-11 18:44:15.826405: train_loss -0.8557 
2024-11-11 18:44:15.831017: val_loss -0.7707 
2024-11-11 18:44:15.833600: Pseudo dice [np.float32(0.944), np.float32(0.7251)] 
2024-11-11 18:44:15.836124: Epoch time: 39.65 s 
2024-11-11 18:44:16.966889:  
2024-11-11 18:44:16.969625: Epoch 123 
2024-11-11 18:44:16.972146: Current learning rate: 0.00889 
2024-11-11 18:44:56.611572: train_loss -0.8682 
2024-11-11 18:44:56.615062: val_loss -0.8074 
2024-11-11 18:44:56.617737: Pseudo dice [np.float32(0.9514), np.float32(0.7704)] 
2024-11-11 18:44:56.620153: Epoch time: 39.65 s 
2024-11-11 18:44:57.749129:  
2024-11-11 18:44:57.751440: Epoch 124 
2024-11-11 18:44:57.753713: Current learning rate: 0.00888 
2024-11-11 18:45:37.400728: train_loss -0.854 
2024-11-11 18:45:37.405455: val_loss -0.7806 
2024-11-11 18:45:37.407788: Pseudo dice [np.float32(0.942), np.float32(0.7196)] 
2024-11-11 18:45:37.410094: Epoch time: 39.65 s 
2024-11-11 18:45:38.534508:  
2024-11-11 18:45:38.537069: Epoch 125 
2024-11-11 18:45:38.539420: Current learning rate: 0.00887 
2024-11-11 18:46:18.179615: train_loss -0.8833 
2024-11-11 18:46:18.187833: val_loss -0.8014 
2024-11-11 18:46:18.190159: Pseudo dice [np.float32(0.9527), np.float32(0.7891)] 
2024-11-11 18:46:18.192316: Epoch time: 39.65 s 
2024-11-11 18:46:19.673995:  
2024-11-11 18:46:19.676685: Epoch 126 
2024-11-11 18:46:19.679199: Current learning rate: 0.00886 
2024-11-11 18:46:59.315686: train_loss -0.8897 
2024-11-11 18:46:59.320713: val_loss -0.7656 
2024-11-11 18:46:59.323120: Pseudo dice [np.float32(0.9538), np.float32(0.681)] 
2024-11-11 18:46:59.325508: Epoch time: 39.64 s 
2024-11-11 18:47:00.454539:  
2024-11-11 18:47:00.457154: Epoch 127 
2024-11-11 18:47:00.459630: Current learning rate: 0.00885 
2024-11-11 18:47:40.114076: train_loss -0.878 
2024-11-11 18:47:40.121081: val_loss -0.7741 
2024-11-11 18:47:40.123368: Pseudo dice [np.float32(0.9413), np.float32(0.6987)] 
2024-11-11 18:47:40.125522: Epoch time: 39.66 s 
2024-11-11 18:47:41.263182:  
2024-11-11 18:47:41.265984: Epoch 128 
2024-11-11 18:47:41.268447: Current learning rate: 0.00884 
2024-11-11 18:48:20.927489: train_loss -0.8731 
2024-11-11 18:48:20.932353: val_loss -0.8373 
2024-11-11 18:48:20.934988: Pseudo dice [np.float32(0.9533), np.float32(0.7973)] 
2024-11-11 18:48:20.937199: Epoch time: 39.67 s 
2024-11-11 18:48:22.073740:  
2024-11-11 18:48:22.076910: Epoch 129 
2024-11-11 18:48:22.079734: Current learning rate: 0.00883 
2024-11-11 18:49:01.735809: train_loss -0.8831 
2024-11-11 18:49:01.745533: val_loss -0.8204 
2024-11-11 18:49:01.748080: Pseudo dice [np.float32(0.9568), np.float32(0.7865)] 
2024-11-11 18:49:01.750376: Epoch time: 39.66 s 
2024-11-11 18:49:02.882893:  
2024-11-11 18:49:02.885621: Epoch 130 
2024-11-11 18:49:02.888331: Current learning rate: 0.00882 
2024-11-11 18:49:42.564718: train_loss -0.8736 
2024-11-11 18:49:42.570003: val_loss -0.807 
2024-11-11 18:49:42.572694: Pseudo dice [np.float32(0.9511), np.float32(0.7252)] 
2024-11-11 18:49:42.575247: Epoch time: 39.68 s 
2024-11-11 18:49:43.712843:  
2024-11-11 18:49:43.715289: Epoch 131 
2024-11-11 18:49:43.717540: Current learning rate: 0.00881 
2024-11-11 18:50:23.365720: train_loss -0.8796 
2024-11-11 18:50:23.370138: val_loss -0.8123 
2024-11-11 18:50:23.372471: Pseudo dice [np.float32(0.9503), np.float32(0.7915)] 
2024-11-11 18:50:23.374573: Epoch time: 39.65 s 
2024-11-11 18:50:24.507401:  
2024-11-11 18:50:24.509733: Epoch 132 
2024-11-11 18:50:24.511980: Current learning rate: 0.0088 
2024-11-11 18:51:04.174576: train_loss -0.8787 
2024-11-11 18:51:04.179377: val_loss -0.8045 
2024-11-11 18:51:04.181661: Pseudo dice [np.float32(0.955), np.float32(0.7586)] 
2024-11-11 18:51:04.183987: Epoch time: 39.67 s 
2024-11-11 18:51:05.314914:  
2024-11-11 18:51:05.317268: Epoch 133 
2024-11-11 18:51:05.319483: Current learning rate: 0.00879 
2024-11-11 18:51:45.005096: train_loss -0.8924 
2024-11-11 18:51:45.008541: val_loss -0.8445 
2024-11-11 18:51:45.010722: Pseudo dice [np.float32(0.9571), np.float32(0.8184)] 
2024-11-11 18:51:45.012887: Epoch time: 39.69 s 
2024-11-11 18:51:45.015162: Yayy! New best EMA pseudo Dice: 0.8521999716758728 
2024-11-11 18:51:46.945116:  
2024-11-11 18:51:46.947653: Epoch 134 
2024-11-11 18:51:46.950091: Current learning rate: 0.00879 
2024-11-11 18:52:26.612512: train_loss -0.9065 
2024-11-11 18:52:26.621369: val_loss -0.7888 
2024-11-11 18:52:26.623871: Pseudo dice [np.float32(0.9574), np.float32(0.7893)] 
2024-11-11 18:52:26.626230: Epoch time: 39.67 s 
2024-11-11 18:52:26.628475: Yayy! New best EMA pseudo Dice: 0.8543000221252441 
2024-11-11 18:52:28.554991:  
2024-11-11 18:52:28.557451: Epoch 135 
2024-11-11 18:52:28.559861: Current learning rate: 0.00878 
2024-11-11 18:53:08.208486: train_loss -0.8885 
2024-11-11 18:53:08.212075: val_loss -0.8149 
2024-11-11 18:53:08.214424: Pseudo dice [np.float32(0.9471), np.float32(0.7681)] 
2024-11-11 18:53:08.216733: Epoch time: 39.65 s 
2024-11-11 18:53:08.223755: Yayy! New best EMA pseudo Dice: 0.8546000123023987 
2024-11-11 18:53:10.233680:  
2024-11-11 18:53:10.236075: Epoch 136 
2024-11-11 18:53:10.238698: Current learning rate: 0.00877 
2024-11-11 18:53:49.909784: train_loss -0.869 
2024-11-11 18:53:49.914415: val_loss -0.7966 
2024-11-11 18:53:49.916705: Pseudo dice [np.float32(0.9474), np.float32(0.7716)] 
2024-11-11 18:53:49.919004: Epoch time: 39.68 s 
2024-11-11 18:53:49.921422: Yayy! New best EMA pseudo Dice: 0.8550999760627747 
2024-11-11 18:53:51.845026:  
2024-11-11 18:53:51.847574: Epoch 137 
2024-11-11 18:53:51.851127: Current learning rate: 0.00876 
2024-11-11 18:54:31.541342: train_loss -0.8795 
2024-11-11 18:54:31.550997: val_loss -0.7698 
2024-11-11 18:54:31.553716: Pseudo dice [np.float32(0.9492), np.float32(0.6973)] 
2024-11-11 18:54:31.555883: Epoch time: 39.7 s 
2024-11-11 18:54:33.048316:  
2024-11-11 18:54:33.050885: Epoch 138 
2024-11-11 18:54:33.053463: Current learning rate: 0.00875 
2024-11-11 18:55:12.730466: train_loss -0.8381 
2024-11-11 18:55:12.735381: val_loss -0.7471 
2024-11-11 18:55:12.737810: Pseudo dice [np.float32(0.9519), np.float32(0.7176)] 
2024-11-11 18:55:12.740175: Epoch time: 39.68 s 
2024-11-11 18:55:13.892869:  
2024-11-11 18:55:13.895227: Epoch 139 
2024-11-11 18:55:13.897829: Current learning rate: 0.00874 
2024-11-11 18:55:53.593834: train_loss -0.8722 
2024-11-11 18:55:53.597322: val_loss -0.8107 
2024-11-11 18:55:53.599664: Pseudo dice [np.float32(0.9556), np.float32(0.7699)] 
2024-11-11 18:55:53.602304: Epoch time: 39.7 s 
2024-11-11 18:55:54.749289:  
2024-11-11 18:55:54.751903: Epoch 140 
2024-11-11 18:55:54.754378: Current learning rate: 0.00873 
2024-11-11 18:56:34.438981: train_loss -0.8825 
2024-11-11 18:56:34.448221: val_loss -0.7713 
2024-11-11 18:56:34.450750: Pseudo dice [np.float32(0.946), np.float32(0.7519)] 
2024-11-11 18:56:34.453126: Epoch time: 39.69 s 
2024-11-11 18:56:35.602084:  
2024-11-11 18:56:35.604352: Epoch 141 
2024-11-11 18:56:35.606664: Current learning rate: 0.00872 
2024-11-11 18:57:15.272987: train_loss -0.8923 
2024-11-11 18:57:15.276522: val_loss -0.7843 
2024-11-11 18:57:15.278883: Pseudo dice [np.float32(0.9522), np.float32(0.7765)] 
2024-11-11 18:57:15.280971: Epoch time: 39.67 s 
2024-11-11 18:57:16.427941:  
2024-11-11 18:57:16.430173: Epoch 142 
2024-11-11 18:57:16.432445: Current learning rate: 0.00871 
2024-11-11 18:57:56.076815: train_loss -0.8935 
2024-11-11 18:57:56.085355: val_loss -0.7771 
2024-11-11 18:57:56.087910: Pseudo dice [np.float32(0.9563), np.float32(0.6991)] 
2024-11-11 18:57:56.090271: Epoch time: 39.65 s 
2024-11-11 18:57:57.238851:  
2024-11-11 18:57:57.241241: Epoch 143 
2024-11-11 18:57:57.243678: Current learning rate: 0.0087 
2024-11-11 18:58:36.900876: train_loss -0.8741 
2024-11-11 18:58:36.909092: val_loss -0.7627 
2024-11-11 18:58:36.911264: Pseudo dice [np.float32(0.9474), np.float32(0.7232)] 
2024-11-11 18:58:36.913672: Epoch time: 39.66 s 
2024-11-11 18:58:38.063571:  
2024-11-11 18:58:38.065841: Epoch 144 
2024-11-11 18:58:38.068309: Current learning rate: 0.00869 
2024-11-11 18:59:17.726940: train_loss -0.8865 
2024-11-11 18:59:17.731841: val_loss -0.7788 
2024-11-11 18:59:17.734361: Pseudo dice [np.float32(0.9482), np.float32(0.7563)] 
2024-11-11 18:59:17.736847: Epoch time: 39.66 s 
2024-11-11 18:59:18.892907:  
2024-11-11 18:59:18.895333: Epoch 145 
2024-11-11 18:59:18.897582: Current learning rate: 0.00868 
2024-11-11 18:59:58.549410: train_loss -0.8866 
2024-11-11 18:59:58.553444: val_loss -0.7619 
2024-11-11 18:59:58.555496: Pseudo dice [np.float32(0.9417), np.float32(0.7175)] 
2024-11-11 18:59:58.557816: Epoch time: 39.66 s 
2024-11-11 18:59:59.703035:  
2024-11-11 18:59:59.706486: Epoch 146 
2024-11-11 18:59:59.708931: Current learning rate: 0.00868 
2024-11-11 19:00:39.370967: train_loss -0.8866 
2024-11-11 19:00:39.377128: val_loss -0.7817 
2024-11-11 19:00:39.379638: Pseudo dice [np.float32(0.9496), np.float32(0.7613)] 
2024-11-11 19:00:39.381824: Epoch time: 39.67 s 
2024-11-11 19:00:40.532746:  
2024-11-11 19:00:40.535352: Epoch 147 
2024-11-11 19:00:40.537941: Current learning rate: 0.00867 
2024-11-11 19:01:20.202008: train_loss -0.8777 
2024-11-11 19:01:20.205682: val_loss -0.781 
2024-11-11 19:01:20.208194: Pseudo dice [np.float32(0.9561), np.float32(0.767)] 
2024-11-11 19:01:20.210635: Epoch time: 39.67 s 
2024-11-11 19:01:21.361948:  
2024-11-11 19:01:21.364513: Epoch 148 
2024-11-11 19:01:21.367392: Current learning rate: 0.00866 
2024-11-11 19:02:01.010447: train_loss -0.8788 
2024-11-11 19:02:01.015371: val_loss -0.79 
2024-11-11 19:02:01.017826: Pseudo dice [np.float32(0.9545), np.float32(0.7948)] 
2024-11-11 19:02:01.020036: Epoch time: 39.65 s 
2024-11-11 19:02:02.177068:  
2024-11-11 19:02:02.179387: Epoch 149 
2024-11-11 19:02:02.181791: Current learning rate: 0.00865 
2024-11-11 19:02:41.859805: train_loss -0.8887 
2024-11-11 19:02:41.863084: val_loss -0.7599 
2024-11-11 19:02:41.865431: Pseudo dice [np.float32(0.9507), np.float32(0.714)] 
2024-11-11 19:02:41.867673: Epoch time: 39.68 s 
2024-11-11 19:02:43.796191:  
2024-11-11 19:02:43.798607: Epoch 150 
2024-11-11 19:02:43.800801: Current learning rate: 0.00864 
2024-11-11 19:03:23.462013: train_loss -0.8918 
2024-11-11 19:03:23.470228: val_loss -0.7947 
2024-11-11 19:03:23.472710: Pseudo dice [np.float32(0.947), np.float32(0.7279)] 
2024-11-11 19:03:23.475121: Epoch time: 39.67 s 
2024-11-11 19:03:24.977084:  
2024-11-11 19:03:24.980640: Epoch 151 
2024-11-11 19:03:24.983086: Current learning rate: 0.00863 
2024-11-11 19:04:04.667972: train_loss -0.8868 
2024-11-11 19:04:04.671386: val_loss -0.851 
2024-11-11 19:04:04.673717: Pseudo dice [np.float32(0.9548), np.float32(0.8263)] 
2024-11-11 19:04:04.676004: Epoch time: 39.69 s 
2024-11-11 19:04:05.834243:  
2024-11-11 19:04:05.837291: Epoch 152 
2024-11-11 19:04:05.839695: Current learning rate: 0.00862 
2024-11-11 19:04:45.521292: train_loss -0.8957 
2024-11-11 19:04:45.526259: val_loss -0.7736 
2024-11-11 19:04:45.528834: Pseudo dice [np.float32(0.9467), np.float32(0.6877)] 
2024-11-11 19:04:45.531145: Epoch time: 39.69 s 
2024-11-11 19:04:46.681864:  
2024-11-11 19:04:46.684399: Epoch 153 
2024-11-11 19:04:46.686674: Current learning rate: 0.00861 
2024-11-11 19:05:26.372267: train_loss -0.8894 
2024-11-11 19:05:26.375845: val_loss -0.7917 
2024-11-11 19:05:26.378421: Pseudo dice [np.float32(0.9435), np.float32(0.777)] 
2024-11-11 19:05:26.380695: Epoch time: 39.69 s 
2024-11-11 19:05:27.550430:  
2024-11-11 19:05:27.552948: Epoch 154 
2024-11-11 19:05:27.555380: Current learning rate: 0.0086 
2024-11-11 19:06:07.228190: train_loss -0.8922 
2024-11-11 19:06:07.232655: val_loss -0.7472 
2024-11-11 19:06:07.234905: Pseudo dice [np.float32(0.9441), np.float32(0.663)] 
2024-11-11 19:06:07.236776: Epoch time: 39.68 s 
2024-11-11 19:06:08.405570:  
2024-11-11 19:06:08.408284: Epoch 155 
2024-11-11 19:06:08.410818: Current learning rate: 0.00859 
2024-11-11 19:06:48.073842: train_loss -0.8711 
2024-11-11 19:06:48.082000: val_loss -0.7936 
2024-11-11 19:06:48.084008: Pseudo dice [np.float32(0.9398), np.float32(0.7565)] 
2024-11-11 19:06:48.086282: Epoch time: 39.67 s 
2024-11-11 19:06:49.257474:  
2024-11-11 19:06:49.259729: Epoch 156 
2024-11-11 19:06:49.262094: Current learning rate: 0.00858 
2024-11-11 19:07:28.918314: train_loss -0.8801 
2024-11-11 19:07:28.922980: val_loss -0.7429 
2024-11-11 19:07:28.925551: Pseudo dice [np.float32(0.9417), np.float32(0.7133)] 
2024-11-11 19:07:28.943573: Epoch time: 39.66 s 
2024-11-11 19:07:30.110757:  
2024-11-11 19:07:30.113298: Epoch 157 
2024-11-11 19:07:30.115915: Current learning rate: 0.00858 
2024-11-11 19:08:09.764708: train_loss -0.8751 
2024-11-11 19:08:09.768406: val_loss -0.8112 
2024-11-11 19:08:09.770673: Pseudo dice [np.float32(0.9544), np.float32(0.7773)] 
2024-11-11 19:08:09.772841: Epoch time: 39.66 s 
2024-11-11 19:08:10.941953:  
2024-11-11 19:08:10.944409: Epoch 158 
2024-11-11 19:08:10.946839: Current learning rate: 0.00857 
2024-11-11 19:08:50.608510: train_loss -0.8801 
2024-11-11 19:08:50.613361: val_loss -0.793 
2024-11-11 19:08:50.616235: Pseudo dice [np.float32(0.9499), np.float32(0.7838)] 
2024-11-11 19:08:50.618600: Epoch time: 39.67 s 
2024-11-11 19:08:51.788131:  
2024-11-11 19:08:51.790692: Epoch 159 
2024-11-11 19:08:51.792991: Current learning rate: 0.00856 
2024-11-11 19:09:31.435748: train_loss -0.8912 
2024-11-11 19:09:31.439375: val_loss -0.7801 
2024-11-11 19:09:31.441780: Pseudo dice [np.float32(0.9532), np.float32(0.7628)] 
2024-11-11 19:09:31.444086: Epoch time: 39.65 s 
2024-11-11 19:09:32.615891:  
2024-11-11 19:09:32.618299: Epoch 160 
2024-11-11 19:09:32.620770: Current learning rate: 0.00855 
2024-11-11 19:10:12.283293: train_loss -0.8863 
2024-11-11 19:10:12.288312: val_loss -0.7427 
2024-11-11 19:10:12.290752: Pseudo dice [np.float32(0.9353), np.float32(0.7113)] 
2024-11-11 19:10:12.293143: Epoch time: 39.67 s 
2024-11-11 19:10:13.464237:  
2024-11-11 19:10:13.466654: Epoch 161 
2024-11-11 19:10:13.469104: Current learning rate: 0.00854 
2024-11-11 19:10:53.108635: train_loss -0.8819 
2024-11-11 19:10:53.112455: val_loss -0.8165 
2024-11-11 19:10:53.114908: Pseudo dice [np.float32(0.9523), np.float32(0.8204)] 
2024-11-11 19:10:53.117607: Epoch time: 39.65 s 
2024-11-11 19:10:54.286061:  
2024-11-11 19:10:54.288693: Epoch 162 
2024-11-11 19:10:54.291085: Current learning rate: 0.00853 
2024-11-11 19:11:33.927746: train_loss -0.8964 
2024-11-11 19:11:33.932829: val_loss -0.8381 
2024-11-11 19:11:33.935302: Pseudo dice [np.float32(0.9489), np.float32(0.8216)] 
2024-11-11 19:11:33.937587: Epoch time: 39.64 s 
2024-11-11 19:11:35.447614:  
2024-11-11 19:11:35.450391: Epoch 163 
2024-11-11 19:11:35.452656: Current learning rate: 0.00852 
2024-11-11 19:12:15.101465: train_loss -0.8927 
2024-11-11 19:12:15.104948: val_loss -0.7883 
2024-11-11 19:12:15.107547: Pseudo dice [np.float32(0.9521), np.float32(0.812)] 
2024-11-11 19:12:15.109852: Epoch time: 39.66 s 
2024-11-11 19:12:15.112186: Yayy! New best EMA pseudo Dice: 0.8568999767303467 
2024-11-11 19:12:17.066797:  
2024-11-11 19:12:17.069151: Epoch 164 
2024-11-11 19:12:17.071743: Current learning rate: 0.00851 
2024-11-11 19:12:56.717274: train_loss -0.8991 
2024-11-11 19:12:56.722202: val_loss -0.8044 
2024-11-11 19:12:56.725008: Pseudo dice [np.float32(0.9504), np.float32(0.8027)] 
2024-11-11 19:12:56.727895: Epoch time: 39.65 s 
2024-11-11 19:12:56.730978: Yayy! New best EMA pseudo Dice: 0.8589000105857849 
2024-11-11 19:12:58.628713:  
2024-11-11 19:12:58.631461: Epoch 165 
2024-11-11 19:12:58.633888: Current learning rate: 0.0085 
2024-11-11 19:13:38.279969: train_loss -0.9013 
2024-11-11 19:13:38.283562: val_loss -0.8265 
2024-11-11 19:13:38.285853: Pseudo dice [np.float32(0.9606), np.float32(0.8424)] 
2024-11-11 19:13:38.287999: Epoch time: 39.65 s 
2024-11-11 19:13:38.290166: Yayy! New best EMA pseudo Dice: 0.863099992275238 
2024-11-11 19:13:40.237439:  
2024-11-11 19:13:40.239826: Epoch 166 
2024-11-11 19:13:40.242041: Current learning rate: 0.00849 
2024-11-11 19:14:19.871334: train_loss -0.8919 
2024-11-11 19:14:19.878987: val_loss -0.8152 
2024-11-11 19:14:19.881253: Pseudo dice [np.float32(0.9581), np.float32(0.8081)] 
2024-11-11 19:14:19.883490: Epoch time: 39.64 s 
2024-11-11 19:14:19.886649: Yayy! New best EMA pseudo Dice: 0.8651000261306763 
2024-11-11 19:14:21.847298:  
2024-11-11 19:14:21.849962: Epoch 167 
2024-11-11 19:14:21.852447: Current learning rate: 0.00848 
2024-11-11 19:15:01.513732: train_loss -0.8909 
2024-11-11 19:15:01.517199: val_loss -0.807 
2024-11-11 19:15:01.519686: Pseudo dice [np.float32(0.9507), np.float32(0.7999)] 
2024-11-11 19:15:01.522055: Epoch time: 39.67 s 
2024-11-11 19:15:01.524769: Yayy! New best EMA pseudo Dice: 0.866100013256073 
2024-11-11 19:15:03.490433:  
2024-11-11 19:15:03.493614: Epoch 168 
2024-11-11 19:15:03.495808: Current learning rate: 0.00847 
2024-11-11 19:15:43.130140: train_loss -0.8503 
2024-11-11 19:15:43.134865: val_loss -0.7917 
2024-11-11 19:15:43.137367: Pseudo dice [np.float32(0.9427), np.float32(0.7376)] 
2024-11-11 19:15:43.139737: Epoch time: 39.64 s 
2024-11-11 19:15:44.301571:  
2024-11-11 19:15:44.304279: Epoch 169 
2024-11-11 19:15:44.306658: Current learning rate: 0.00847 
2024-11-11 19:16:23.970162: train_loss -0.8541 
2024-11-11 19:16:23.982208: val_loss -0.7457 
2024-11-11 19:16:23.984593: Pseudo dice [np.float32(0.9435), np.float32(0.6524)] 
2024-11-11 19:16:23.986910: Epoch time: 39.67 s 
2024-11-11 19:16:25.148646:  
2024-11-11 19:16:25.151024: Epoch 170 
2024-11-11 19:16:25.153521: Current learning rate: 0.00846 
2024-11-11 19:17:04.810539: train_loss -0.8581 
2024-11-11 19:17:04.815564: val_loss -0.7638 
2024-11-11 19:17:04.817907: Pseudo dice [np.float32(0.9583), np.float32(0.6684)] 
2024-11-11 19:17:04.820179: Epoch time: 39.66 s 
2024-11-11 19:17:05.979725:  
2024-11-11 19:17:05.982864: Epoch 171 
2024-11-11 19:17:05.985462: Current learning rate: 0.00845 
2024-11-11 19:17:45.647747: train_loss -0.8756 
2024-11-11 19:17:45.651172: val_loss -0.81 
2024-11-11 19:17:45.653558: Pseudo dice [np.float32(0.9453), np.float32(0.7705)] 
2024-11-11 19:17:45.656158: Epoch time: 39.67 s 
2024-11-11 19:17:46.823560:  
2024-11-11 19:17:46.826025: Epoch 172 
2024-11-11 19:17:46.828499: Current learning rate: 0.00844 
2024-11-11 19:18:26.517431: train_loss -0.8765 
2024-11-11 19:18:26.526903: val_loss -0.8597 
2024-11-11 19:18:26.529479: Pseudo dice [np.float32(0.9582), np.float32(0.835)] 
2024-11-11 19:18:26.531937: Epoch time: 39.7 s 
2024-11-11 19:18:27.687845:  
2024-11-11 19:18:27.690554: Epoch 173 
2024-11-11 19:18:27.692999: Current learning rate: 0.00843 
2024-11-11 19:19:07.354299: train_loss -0.889 
2024-11-11 19:19:07.357940: val_loss -0.7923 
2024-11-11 19:19:07.360236: Pseudo dice [np.float32(0.9559), np.float32(0.7482)] 
2024-11-11 19:19:07.362603: Epoch time: 39.67 s 
2024-11-11 19:19:08.520751:  
2024-11-11 19:19:08.523313: Epoch 174 
2024-11-11 19:19:08.525872: Current learning rate: 0.00842 
2024-11-11 19:19:48.210350: train_loss -0.8772 
2024-11-11 19:19:48.215281: val_loss -0.7496 
2024-11-11 19:19:48.217791: Pseudo dice [np.float32(0.9505), np.float32(0.6873)] 
2024-11-11 19:19:48.220116: Epoch time: 39.69 s 
2024-11-11 19:19:49.723468:  
2024-11-11 19:19:49.726086: Epoch 175 
2024-11-11 19:19:49.728739: Current learning rate: 0.00841 
2024-11-11 19:20:29.404833: train_loss -0.8866 
2024-11-11 19:20:29.408439: val_loss -0.7663 
2024-11-11 19:20:29.411031: Pseudo dice [np.float32(0.9513), np.float32(0.7056)] 
2024-11-11 19:20:29.413190: Epoch time: 39.68 s 
2024-11-11 19:20:30.574285:  
2024-11-11 19:20:30.576706: Epoch 176 
2024-11-11 19:20:30.579159: Current learning rate: 0.0084 
2024-11-11 19:21:10.265676: train_loss -0.8707 
2024-11-11 19:21:10.270095: val_loss -0.7979 
2024-11-11 19:21:10.272499: Pseudo dice [np.float32(0.9483), np.float32(0.7612)] 
2024-11-11 19:21:10.275659: Epoch time: 39.69 s 
2024-11-11 19:21:11.437108:  
2024-11-11 19:21:11.439626: Epoch 177 
2024-11-11 19:21:11.442136: Current learning rate: 0.00839 
2024-11-11 19:21:51.139583: train_loss -0.8921 
2024-11-11 19:21:51.143184: val_loss -0.7903 
2024-11-11 19:21:51.145603: Pseudo dice [np.float32(0.9559), np.float32(0.7887)] 
2024-11-11 19:21:51.147750: Epoch time: 39.7 s 
2024-11-11 19:21:52.305440:  
2024-11-11 19:21:52.308016: Epoch 178 
2024-11-11 19:21:52.310440: Current learning rate: 0.00838 
2024-11-11 19:22:31.986196: train_loss -0.8953 
2024-11-11 19:22:31.996905: val_loss -0.8566 
2024-11-11 19:22:31.999174: Pseudo dice [np.float32(0.9623), np.float32(0.8405)] 
2024-11-11 19:22:32.001291: Epoch time: 39.68 s 
2024-11-11 19:22:33.160905:  
2024-11-11 19:22:33.163406: Epoch 179 
2024-11-11 19:22:33.165906: Current learning rate: 0.00837 
2024-11-11 19:23:12.850658: train_loss -0.8976 
2024-11-11 19:23:12.853863: val_loss -0.7864 
2024-11-11 19:23:12.856088: Pseudo dice [np.float32(0.9483), np.float32(0.7625)] 
2024-11-11 19:23:12.858409: Epoch time: 39.69 s 
2024-11-11 19:23:14.022343:  
2024-11-11 19:23:14.024717: Epoch 180 
2024-11-11 19:23:14.027040: Current learning rate: 0.00836 
2024-11-11 19:23:53.722185: train_loss -0.8798 
2024-11-11 19:23:53.729753: val_loss -0.7985 
2024-11-11 19:23:53.732276: Pseudo dice [np.float32(0.9553), np.float32(0.7701)] 
2024-11-11 19:23:53.734657: Epoch time: 39.7 s 
2024-11-11 19:23:54.893640:  
2024-11-11 19:23:54.896375: Epoch 181 
2024-11-11 19:23:54.899187: Current learning rate: 0.00836 
2024-11-11 19:24:34.600969: train_loss -0.891 
2024-11-11 19:24:34.604543: val_loss -0.8032 
2024-11-11 19:24:34.607133: Pseudo dice [np.float32(0.9531), np.float32(0.7799)] 
2024-11-11 19:24:34.609559: Epoch time: 39.71 s 
2024-11-11 19:24:35.765590:  
2024-11-11 19:24:35.768171: Epoch 182 
2024-11-11 19:24:35.770533: Current learning rate: 0.00835 
2024-11-11 19:25:15.461130: train_loss -0.8711 
2024-11-11 19:25:15.465817: val_loss -0.7979 
2024-11-11 19:25:15.468193: Pseudo dice [np.float32(0.9351), np.float32(0.7911)] 
2024-11-11 19:25:15.470489: Epoch time: 39.7 s 
2024-11-11 19:25:16.622806:  
2024-11-11 19:25:16.626608: Epoch 183 
2024-11-11 19:25:16.629041: Current learning rate: 0.00834 
2024-11-11 19:25:56.328038: train_loss -0.8889 
2024-11-11 19:25:56.340243: val_loss -0.7981 
2024-11-11 19:25:56.342706: Pseudo dice [np.float32(0.952), np.float32(0.7774)] 
2024-11-11 19:25:56.344920: Epoch time: 39.71 s 
2024-11-11 19:25:57.492702:  
2024-11-11 19:25:57.495217: Epoch 184 
2024-11-11 19:25:57.497682: Current learning rate: 0.00833 
2024-11-11 19:26:37.187132: train_loss -0.8961 
2024-11-11 19:26:37.193445: val_loss -0.8199 
2024-11-11 19:26:37.199732: Pseudo dice [np.float32(0.9563), np.float32(0.8286)] 
2024-11-11 19:26:37.202812: Epoch time: 39.7 s 
2024-11-11 19:26:38.363888:  
2024-11-11 19:26:38.366457: Epoch 185 
2024-11-11 19:26:38.368906: Current learning rate: 0.00832 
2024-11-11 19:27:18.046767: train_loss -0.9054 
2024-11-11 19:27:18.050164: val_loss -0.8138 
2024-11-11 19:27:18.052299: Pseudo dice [np.float32(0.9516), np.float32(0.771)] 
2024-11-11 19:27:18.054824: Epoch time: 39.68 s 
2024-11-11 19:27:19.214515:  
2024-11-11 19:27:19.216958: Epoch 186 
2024-11-11 19:27:19.219552: Current learning rate: 0.00831 
2024-11-11 19:27:58.877581: train_loss -0.9029 
2024-11-11 19:27:58.882321: val_loss -0.7396 
2024-11-11 19:27:58.884778: Pseudo dice [np.float32(0.9286), np.float32(0.6456)] 
2024-11-11 19:27:58.887239: Epoch time: 39.66 s 
2024-11-11 19:28:00.389590:  
2024-11-11 19:28:00.392234: Epoch 187 
2024-11-11 19:28:00.394613: Current learning rate: 0.0083 
2024-11-11 19:28:40.082412: train_loss -0.8896 
2024-11-11 19:28:40.085953: val_loss -0.7799 
2024-11-11 19:28:40.088182: Pseudo dice [np.float32(0.9468), np.float32(0.7395)] 
2024-11-11 19:28:40.090770: Epoch time: 39.69 s 
2024-11-11 19:28:41.246380:  
2024-11-11 19:28:41.248883: Epoch 188 
2024-11-11 19:28:41.251317: Current learning rate: 0.00829 
2024-11-11 19:29:20.917483: train_loss -0.8971 
2024-11-11 19:29:20.922339: val_loss -0.8269 
2024-11-11 19:29:20.924831: Pseudo dice [np.float32(0.9524), np.float32(0.8036)] 
2024-11-11 19:29:20.927237: Epoch time: 39.67 s 
2024-11-11 19:29:22.083300:  
2024-11-11 19:29:22.086273: Epoch 189 
2024-11-11 19:29:22.088692: Current learning rate: 0.00828 
2024-11-11 19:30:01.760779: train_loss -0.909 
2024-11-11 19:30:01.764242: val_loss -0.8554 
2024-11-11 19:30:01.766587: Pseudo dice [np.float32(0.9598), np.float32(0.8499)] 
2024-11-11 19:30:01.768993: Epoch time: 39.68 s 
2024-11-11 19:30:02.925910:  
2024-11-11 19:30:02.928534: Epoch 190 
2024-11-11 19:30:02.931178: Current learning rate: 0.00827 
2024-11-11 19:30:42.598971: train_loss -0.8972 
2024-11-11 19:30:42.603549: val_loss -0.7856 
2024-11-11 19:30:42.605847: Pseudo dice [np.float32(0.9588), np.float32(0.742)] 
2024-11-11 19:30:42.607994: Epoch time: 39.67 s 
2024-11-11 19:30:43.765610:  
2024-11-11 19:30:43.768281: Epoch 191 
2024-11-11 19:30:43.770808: Current learning rate: 0.00826 
2024-11-11 19:31:23.443764: train_loss -0.9072 
2024-11-11 19:31:23.447088: val_loss -0.8138 
2024-11-11 19:31:23.449361: Pseudo dice [np.float32(0.9517), np.float32(0.7827)] 
2024-11-11 19:31:23.451671: Epoch time: 39.68 s 
2024-11-11 19:31:24.618097:  
2024-11-11 19:31:24.620589: Epoch 192 
2024-11-11 19:31:24.622892: Current learning rate: 0.00825 
2024-11-11 19:32:04.302373: train_loss -0.9063 
2024-11-11 19:32:04.307141: val_loss -0.8338 
2024-11-11 19:32:04.309477: Pseudo dice [np.float32(0.9552), np.float32(0.8001)] 
2024-11-11 19:32:04.311813: Epoch time: 39.69 s 
2024-11-11 19:32:05.482059:  
2024-11-11 19:32:05.485072: Epoch 193 
2024-11-11 19:32:05.487678: Current learning rate: 0.00824 
2024-11-11 19:32:45.154906: train_loss -0.9075 
2024-11-11 19:32:45.158047: val_loss -0.8459 
2024-11-11 19:32:45.160264: Pseudo dice [np.float32(0.9619), np.float32(0.8143)] 
2024-11-11 19:32:45.162568: Epoch time: 39.67 s 
2024-11-11 19:32:46.323996:  
2024-11-11 19:32:46.326691: Epoch 194 
2024-11-11 19:32:46.329044: Current learning rate: 0.00824 
2024-11-11 19:33:25.997968: train_loss -0.8973 
2024-11-11 19:33:26.009919: val_loss -0.7399 
2024-11-11 19:33:26.012496: Pseudo dice [np.float32(0.9444), np.float32(0.6972)] 
2024-11-11 19:33:26.015088: Epoch time: 39.68 s 
2024-11-11 19:33:27.185030:  
2024-11-11 19:33:27.187390: Epoch 195 
2024-11-11 19:33:27.189684: Current learning rate: 0.00823 
2024-11-11 19:34:06.863734: train_loss -0.8842 
2024-11-11 19:34:06.867105: val_loss -0.7606 
2024-11-11 19:34:06.869330: Pseudo dice [np.float32(0.9445), np.float32(0.6861)] 
2024-11-11 19:34:06.871572: Epoch time: 39.68 s 
2024-11-11 19:34:08.039187:  
2024-11-11 19:34:08.041582: Epoch 196 
2024-11-11 19:34:08.043791: Current learning rate: 0.00822 
2024-11-11 19:34:47.698679: train_loss -0.8933 
2024-11-11 19:34:47.703120: val_loss -0.8043 
2024-11-11 19:34:47.705451: Pseudo dice [np.float32(0.9431), np.float32(0.7419)] 
2024-11-11 19:34:47.707646: Epoch time: 39.66 s 
2024-11-11 19:34:48.877825:  
2024-11-11 19:34:48.880236: Epoch 197 
2024-11-11 19:34:48.882667: Current learning rate: 0.00821 
2024-11-11 19:35:28.552608: train_loss -0.8752 
2024-11-11 19:35:28.556792: val_loss -0.788 
2024-11-11 19:35:28.559106: Pseudo dice [np.float32(0.945), np.float32(0.7624)] 
2024-11-11 19:35:28.561452: Epoch time: 39.68 s 
2024-11-11 19:35:29.757992:  
2024-11-11 19:35:29.760577: Epoch 198 
2024-11-11 19:35:29.763074: Current learning rate: 0.0082 
2024-11-11 19:36:09.423363: train_loss -0.8916 
2024-11-11 19:36:09.428123: val_loss -0.8159 
2024-11-11 19:36:09.430637: Pseudo dice [np.float32(0.9574), np.float32(0.7786)] 
2024-11-11 19:36:09.432885: Epoch time: 39.67 s 
2024-11-11 19:36:10.599018:  
2024-11-11 19:36:10.601593: Epoch 199 
2024-11-11 19:36:10.604165: Current learning rate: 0.00819 
2024-11-11 19:36:50.596050: train_loss -0.8875 
2024-11-11 19:36:50.599804: val_loss -0.8054 
2024-11-11 19:36:50.602473: Pseudo dice [np.float32(0.9508), np.float32(0.7439)] 
2024-11-11 19:36:50.604678: Epoch time: 40.0 s 
2024-11-11 19:36:52.523978:  
2024-11-11 19:36:52.526518: Epoch 200 
2024-11-11 19:36:52.528925: Current learning rate: 0.00818 
2024-11-11 19:37:32.189677: train_loss -0.8881 
2024-11-11 19:37:32.200187: val_loss -0.7616 
2024-11-11 19:37:32.202814: Pseudo dice [np.float32(0.9519), np.float32(0.7456)] 
2024-11-11 19:37:32.205153: Epoch time: 39.67 s 
2024-11-11 19:37:33.376967:  
2024-11-11 19:37:33.379474: Epoch 201 
2024-11-11 19:37:33.381818: Current learning rate: 0.00817 
2024-11-11 19:38:13.042970: train_loss -0.8781 
2024-11-11 19:38:13.046791: val_loss -0.764 
2024-11-11 19:38:13.049314: Pseudo dice [np.float32(0.9471), np.float32(0.7082)] 
2024-11-11 19:38:13.051570: Epoch time: 39.67 s 
2024-11-11 19:38:14.223310:  
2024-11-11 19:38:14.226064: Epoch 202 
2024-11-11 19:38:14.228495: Current learning rate: 0.00816 
2024-11-11 19:38:53.898691: train_loss -0.8788 
2024-11-11 19:38:53.904841: val_loss -0.7813 
2024-11-11 19:38:53.907248: Pseudo dice [np.float32(0.9481), np.float32(0.7535)] 
2024-11-11 19:38:53.909555: Epoch time: 39.68 s 
2024-11-11 19:38:55.076005:  
2024-11-11 19:38:55.078405: Epoch 203 
2024-11-11 19:38:55.080900: Current learning rate: 0.00815 
2024-11-11 19:39:34.748371: train_loss -0.8915 
2024-11-11 19:39:34.754474: val_loss -0.7962 
2024-11-11 19:39:34.756717: Pseudo dice [np.float32(0.9536), np.float32(0.7924)] 
2024-11-11 19:39:34.758996: Epoch time: 39.67 s 
2024-11-11 19:39:35.931512:  
2024-11-11 19:39:35.933964: Epoch 204 
2024-11-11 19:39:35.936025: Current learning rate: 0.00814 
2024-11-11 19:40:15.623360: train_loss -0.8933 
2024-11-11 19:40:15.628230: val_loss -0.8238 
2024-11-11 19:40:15.630537: Pseudo dice [np.float32(0.9557), np.float32(0.8092)] 
2024-11-11 19:40:15.633694: Epoch time: 39.69 s 
2024-11-11 19:40:16.807579:  
2024-11-11 19:40:16.811603: Epoch 205 
2024-11-11 19:40:16.813974: Current learning rate: 0.00813 
2024-11-11 19:40:56.496897: train_loss -0.9028 
2024-11-11 19:40:56.500694: val_loss -0.7439 
2024-11-11 19:40:56.503326: Pseudo dice [np.float32(0.9502), np.float32(0.741)] 
2024-11-11 19:40:56.505778: Epoch time: 39.69 s 
2024-11-11 19:40:57.606385:  
2024-11-11 19:40:57.609112: Epoch 206 
2024-11-11 19:40:57.611513: Current learning rate: 0.00813 
2024-11-11 19:41:37.296978: train_loss -0.8962 
2024-11-11 19:41:37.306432: val_loss -0.8073 
2024-11-11 19:41:37.308838: Pseudo dice [np.float32(0.9532), np.float32(0.7812)] 
2024-11-11 19:41:37.311445: Epoch time: 39.69 s 
2024-11-11 19:41:38.419613:  
2024-11-11 19:41:38.422163: Epoch 207 
2024-11-11 19:41:38.424698: Current learning rate: 0.00812 
2024-11-11 19:42:17.995047: train_loss -0.895 
2024-11-11 19:42:17.998307: val_loss -0.7524 
2024-11-11 19:42:18.000653: Pseudo dice [np.float32(0.9564), np.float32(0.7702)] 
2024-11-11 19:42:18.002876: Epoch time: 39.58 s 
2024-11-11 19:42:19.110941:  
2024-11-11 19:42:19.113319: Epoch 208 
2024-11-11 19:42:19.115639: Current learning rate: 0.00811 
2024-11-11 19:42:58.670849: train_loss -0.8468 
2024-11-11 19:42:58.675735: val_loss -0.7529 
2024-11-11 19:42:58.678308: Pseudo dice [np.float32(0.9426), np.float32(0.7342)] 
2024-11-11 19:42:58.680763: Epoch time: 39.56 s 
2024-11-11 19:42:59.789743:  
2024-11-11 19:42:59.792297: Epoch 209 
2024-11-11 19:42:59.794883: Current learning rate: 0.0081 
2024-11-11 19:43:39.362338: train_loss -0.8645 
2024-11-11 19:43:39.372207: val_loss -0.7949 
2024-11-11 19:43:39.374470: Pseudo dice [np.float32(0.94), np.float32(0.761)] 
2024-11-11 19:43:39.376817: Epoch time: 39.57 s 
2024-11-11 19:43:40.482607:  
2024-11-11 19:43:40.485524: Epoch 210 
2024-11-11 19:43:40.488127: Current learning rate: 0.00809 
2024-11-11 19:44:20.044910: train_loss -0.8893 
2024-11-11 19:44:20.049812: val_loss -0.8098 
2024-11-11 19:44:20.052217: Pseudo dice [np.float32(0.9533), np.float32(0.7917)] 
2024-11-11 19:44:20.054509: Epoch time: 39.56 s 
2024-11-11 19:44:21.168480:  
2024-11-11 19:44:21.171271: Epoch 211 
2024-11-11 19:44:21.174109: Current learning rate: 0.00808 
2024-11-11 19:45:00.729157: train_loss -0.8821 
2024-11-11 19:45:00.732907: val_loss -0.8552 
2024-11-11 19:45:00.735467: Pseudo dice [np.float32(0.9575), np.float32(0.8522)] 
2024-11-11 19:45:00.737973: Epoch time: 39.56 s 
2024-11-11 19:45:02.179825:  
2024-11-11 19:45:02.182417: Epoch 212 
2024-11-11 19:45:02.184978: Current learning rate: 0.00807 
2024-11-11 19:45:41.754642: train_loss -0.8702 
2024-11-11 19:45:41.759365: val_loss -0.8268 
2024-11-11 19:45:41.761743: Pseudo dice [np.float32(0.9519), np.float32(0.7955)] 
2024-11-11 19:45:41.764048: Epoch time: 39.58 s 
2024-11-11 19:45:42.877404:  
2024-11-11 19:45:42.879904: Epoch 213 
2024-11-11 19:45:42.882332: Current learning rate: 0.00806 
2024-11-11 19:46:22.471553: train_loss -0.877 
2024-11-11 19:46:22.474880: val_loss -0.7291 
2024-11-11 19:46:22.477380: Pseudo dice [np.float32(0.9525), np.float32(0.5633)] 
2024-11-11 19:46:22.479663: Epoch time: 39.6 s 
2024-11-11 19:46:23.585995:  
2024-11-11 19:46:23.588286: Epoch 214 
2024-11-11 19:46:23.590717: Current learning rate: 0.00805 
2024-11-11 19:47:03.183692: train_loss -0.8725 
2024-11-11 19:47:03.188621: val_loss -0.8078 
2024-11-11 19:47:03.191016: Pseudo dice [np.float32(0.9502), np.float32(0.8061)] 
2024-11-11 19:47:03.193326: Epoch time: 39.6 s 
2024-11-11 19:47:04.304510:  
2024-11-11 19:47:04.307194: Epoch 215 
2024-11-11 19:47:04.309524: Current learning rate: 0.00804 
2024-11-11 19:47:43.897593: train_loss -0.8806 
2024-11-11 19:47:43.901288: val_loss -0.8191 
2024-11-11 19:47:43.903803: Pseudo dice [np.float32(0.9442), np.float32(0.806)] 
2024-11-11 19:47:43.906191: Epoch time: 39.59 s 
2024-11-11 19:47:45.017132:  
2024-11-11 19:47:45.020032: Epoch 216 
2024-11-11 19:47:45.022552: Current learning rate: 0.00803 
2024-11-11 19:48:24.625442: train_loss -0.8892 
2024-11-11 19:48:24.635185: val_loss -0.8242 
2024-11-11 19:48:24.637588: Pseudo dice [np.float32(0.9561), np.float32(0.7868)] 
2024-11-11 19:48:24.639944: Epoch time: 39.61 s 
2024-11-11 19:48:25.755801:  
2024-11-11 19:48:25.758378: Epoch 217 
2024-11-11 19:48:25.760893: Current learning rate: 0.00802 
2024-11-11 19:49:05.353884: train_loss -0.9027 
2024-11-11 19:49:05.357599: val_loss -0.7749 
2024-11-11 19:49:05.359945: Pseudo dice [np.float32(0.9486), np.float32(0.727)] 
2024-11-11 19:49:05.362273: Epoch time: 39.6 s 
2024-11-11 19:49:06.471803:  
2024-11-11 19:49:06.474760: Epoch 218 
2024-11-11 19:49:06.477653: Current learning rate: 0.00801 
2024-11-11 19:49:46.083449: train_loss -0.91 
2024-11-11 19:49:46.088133: val_loss -0.8137 
2024-11-11 19:49:46.090649: Pseudo dice [np.float32(0.9576), np.float32(0.8173)] 
2024-11-11 19:49:46.093222: Epoch time: 39.61 s 
2024-11-11 19:49:47.204334:  
2024-11-11 19:49:47.207038: Epoch 219 
2024-11-11 19:49:47.209600: Current learning rate: 0.00801 
2024-11-11 19:50:26.825877: train_loss -0.9108 
2024-11-11 19:50:26.829862: val_loss -0.8463 
2024-11-11 19:50:26.832262: Pseudo dice [np.float32(0.9601), np.float32(0.8269)] 
2024-11-11 19:50:26.835002: Epoch time: 39.62 s 
2024-11-11 19:50:27.949351:  
2024-11-11 19:50:27.952158: Epoch 220 
2024-11-11 19:50:27.954732: Current learning rate: 0.008 
2024-11-11 19:51:07.569478: train_loss -0.8941 
2024-11-11 19:51:07.578152: val_loss -0.8195 
2024-11-11 19:51:07.580614: Pseudo dice [np.float32(0.9546), np.float32(0.8261)] 
2024-11-11 19:51:07.582968: Epoch time: 39.62 s 
2024-11-11 19:51:08.694911:  
2024-11-11 19:51:08.710630: Epoch 221 
2024-11-11 19:51:08.713428: Current learning rate: 0.00799 
2024-11-11 19:51:48.326091: train_loss -0.8864 
2024-11-11 19:51:48.329493: val_loss -0.736 
2024-11-11 19:51:48.331980: Pseudo dice [np.float32(0.9427), np.float32(0.7058)] 
2024-11-11 19:51:48.334388: Epoch time: 39.63 s 
2024-11-11 19:51:49.448636:  
2024-11-11 19:51:49.451155: Epoch 222 
2024-11-11 19:51:49.453627: Current learning rate: 0.00798 
2024-11-11 19:52:29.075851: train_loss -0.8891 
2024-11-11 19:52:29.080989: val_loss -0.795 
2024-11-11 19:52:29.083310: Pseudo dice [np.float32(0.9514), np.float32(0.756)] 
2024-11-11 19:52:29.085849: Epoch time: 39.63 s 
2024-11-11 19:52:30.194180:  
2024-11-11 19:52:30.196734: Epoch 223 
2024-11-11 19:52:30.199325: Current learning rate: 0.00797 
2024-11-11 19:53:09.835643: train_loss -0.8599 
2024-11-11 19:53:09.843385: val_loss -0.781 
2024-11-11 19:53:09.845772: Pseudo dice [np.float32(0.9498), np.float32(0.7185)] 
2024-11-11 19:53:09.847937: Epoch time: 39.64 s 
2024-11-11 19:53:10.958551:  
2024-11-11 19:53:10.960934: Epoch 224 
2024-11-11 19:53:10.963216: Current learning rate: 0.00796 
2024-11-11 19:53:50.597237: train_loss -0.8988 
2024-11-11 19:53:50.601580: val_loss -0.7978 
2024-11-11 19:53:50.603672: Pseudo dice [np.float32(0.9556), np.float32(0.7756)] 
2024-11-11 19:53:50.605923: Epoch time: 39.64 s 
2024-11-11 19:53:52.055917:  
2024-11-11 19:53:52.058591: Epoch 225 
2024-11-11 19:53:52.060953: Current learning rate: 0.00795 
2024-11-11 19:54:31.722748: train_loss -0.8958 
2024-11-11 19:54:31.726131: val_loss -0.8524 
2024-11-11 19:54:31.728580: Pseudo dice [np.float32(0.9588), np.float32(0.8338)] 
2024-11-11 19:54:31.731003: Epoch time: 39.67 s 
2024-11-11 19:54:32.830684:  
2024-11-11 19:54:32.833247: Epoch 226 
2024-11-11 19:54:32.835646: Current learning rate: 0.00794 
2024-11-11 19:55:12.477792: train_loss -0.9138 
2024-11-11 19:55:12.482260: val_loss -0.8051 
2024-11-11 19:55:12.484642: Pseudo dice [np.float32(0.9556), np.float32(0.7721)] 
2024-11-11 19:55:12.487044: Epoch time: 39.65 s 
2024-11-11 19:55:13.584193:  
2024-11-11 19:55:13.586469: Epoch 227 
2024-11-11 19:55:13.588797: Current learning rate: 0.00793 
2024-11-11 19:55:53.220162: train_loss -0.8804 
2024-11-11 19:55:53.224117: val_loss -0.7681 
2024-11-11 19:55:53.226510: Pseudo dice [np.float32(0.9547), np.float32(0.7485)] 
2024-11-11 19:55:53.228917: Epoch time: 39.64 s 
2024-11-11 19:55:54.328484:  
2024-11-11 19:55:54.331198: Epoch 228 
2024-11-11 19:55:54.333722: Current learning rate: 0.00792 
2024-11-11 19:56:33.992307: train_loss -0.8983 
2024-11-11 19:56:33.997442: val_loss -0.8199 
2024-11-11 19:56:33.999788: Pseudo dice [np.float32(0.9542), np.float32(0.7892)] 
2024-11-11 19:56:34.002665: Epoch time: 39.66 s 
2024-11-11 19:56:35.096861:  
2024-11-11 19:56:35.099300: Epoch 229 
2024-11-11 19:56:35.101771: Current learning rate: 0.00791 
2024-11-11 19:57:14.734228: train_loss -0.9024 
2024-11-11 19:57:14.737710: val_loss -0.7681 
2024-11-11 19:57:14.740235: Pseudo dice [np.float32(0.9506), np.float32(0.7037)] 
2024-11-11 19:57:14.742632: Epoch time: 39.64 s 
2024-11-11 19:57:15.840871:  
2024-11-11 19:57:15.843271: Epoch 230 
2024-11-11 19:57:15.845659: Current learning rate: 0.0079 
2024-11-11 19:57:55.485741: train_loss -0.9178 
2024-11-11 19:57:55.490540: val_loss -0.8525 
2024-11-11 19:57:55.492984: Pseudo dice [np.float32(0.9578), np.float32(0.8434)] 
2024-11-11 19:57:55.495501: Epoch time: 39.65 s 
2024-11-11 19:57:56.602249:  
2024-11-11 19:57:56.604821: Epoch 231 
2024-11-11 19:57:56.607269: Current learning rate: 0.00789 
2024-11-11 19:58:36.253630: train_loss -0.9031 
2024-11-11 19:58:36.261918: val_loss -0.7518 
2024-11-11 19:58:36.264441: Pseudo dice [np.float32(0.9397), np.float32(0.692)] 
2024-11-11 19:58:36.266839: Epoch time: 39.65 s 
2024-11-11 19:58:37.362431:  
2024-11-11 19:58:37.364851: Epoch 232 
2024-11-11 19:58:37.367223: Current learning rate: 0.00789 
2024-11-11 19:59:16.951643: train_loss -0.8909 
2024-11-11 19:59:16.956532: val_loss -0.7838 
2024-11-11 19:59:16.958937: Pseudo dice [np.float32(0.9366), np.float32(0.7369)] 
2024-11-11 19:59:16.961384: Epoch time: 39.59 s 
2024-11-11 19:59:18.059999:  
2024-11-11 19:59:18.062696: Epoch 233 
2024-11-11 19:59:18.065066: Current learning rate: 0.00788 
2024-11-11 19:59:57.650220: train_loss -0.9004 
2024-11-11 19:59:57.653808: val_loss -0.7938 
2024-11-11 19:59:57.656064: Pseudo dice [np.float32(0.9493), np.float32(0.7409)] 
2024-11-11 19:59:57.658487: Epoch time: 39.59 s 
2024-11-11 19:59:58.753535:  
2024-11-11 19:59:58.755854: Epoch 234 
2024-11-11 19:59:58.758320: Current learning rate: 0.00787 
2024-11-11 20:00:38.338176: train_loss -0.8911 
2024-11-11 20:00:38.342714: val_loss -0.764 
2024-11-11 20:00:38.344918: Pseudo dice [np.float32(0.9465), np.float32(0.7212)] 
2024-11-11 20:00:38.347192: Epoch time: 39.59 s 
2024-11-11 20:00:39.443675:  
2024-11-11 20:00:39.446353: Epoch 235 
2024-11-11 20:00:39.448942: Current learning rate: 0.00786 
2024-11-11 20:01:19.056654: train_loss -0.9028 
2024-11-11 20:01:19.060834: val_loss -0.712 
2024-11-11 20:01:19.063184: Pseudo dice [np.float32(0.9355), np.float32(0.6082)] 
2024-11-11 20:01:19.065408: Epoch time: 39.61 s 
2024-11-11 20:01:20.161465:  
2024-11-11 20:01:20.163920: Epoch 236 
2024-11-11 20:01:20.166489: Current learning rate: 0.00785 
2024-11-11 20:01:59.755952: train_loss -0.8997 
2024-11-11 20:01:59.760892: val_loss -0.7599 
2024-11-11 20:01:59.763155: Pseudo dice [np.float32(0.9554), np.float32(0.7501)] 
2024-11-11 20:01:59.765541: Epoch time: 39.6 s 
2024-11-11 20:02:00.866518:  
2024-11-11 20:02:00.868916: Epoch 237 
2024-11-11 20:02:00.871359: Current learning rate: 0.00784 
2024-11-11 20:02:40.453260: train_loss -0.8913 
2024-11-11 20:02:40.456783: val_loss -0.7856 
2024-11-11 20:02:40.459172: Pseudo dice [np.float32(0.9462), np.float32(0.7001)] 
2024-11-11 20:02:40.461647: Epoch time: 39.59 s 
2024-11-11 20:02:41.898037:  
2024-11-11 20:02:41.900585: Epoch 238 
2024-11-11 20:02:41.902942: Current learning rate: 0.00783 
2024-11-11 20:03:21.501555: train_loss -0.8925 
2024-11-11 20:03:21.506123: val_loss -0.7785 
2024-11-11 20:03:21.508455: Pseudo dice [np.float32(0.9536), np.float32(0.7648)] 
2024-11-11 20:03:21.510735: Epoch time: 39.6 s 
2024-11-11 20:03:22.605872:  
2024-11-11 20:03:22.608446: Epoch 239 
2024-11-11 20:03:22.610950: Current learning rate: 0.00782 
2024-11-11 20:04:02.208497: train_loss -0.8924 
2024-11-11 20:04:02.216244: val_loss -0.8022 
2024-11-11 20:04:02.218747: Pseudo dice [np.float32(0.9522), np.float32(0.769)] 
2024-11-11 20:04:02.221107: Epoch time: 39.6 s 
2024-11-11 20:04:03.335076:  
2024-11-11 20:04:03.337424: Epoch 240 
2024-11-11 20:04:03.339678: Current learning rate: 0.00781 
2024-11-11 20:04:42.928202: train_loss -0.9033 
2024-11-11 20:04:42.933057: val_loss -0.8354 
2024-11-11 20:04:42.935487: Pseudo dice [np.float32(0.9575), np.float32(0.8195)] 
2024-11-11 20:04:42.937748: Epoch time: 39.59 s 
2024-11-11 20:04:44.050009:  
2024-11-11 20:04:44.052465: Epoch 241 
2024-11-11 20:04:44.054718: Current learning rate: 0.0078 
2024-11-11 20:05:23.638910: train_loss -0.9002 
2024-11-11 20:05:23.642470: val_loss -0.8101 
2024-11-11 20:05:23.644970: Pseudo dice [np.float32(0.9546), np.float32(0.7756)] 
2024-11-11 20:05:23.647502: Epoch time: 39.59 s 
2024-11-11 20:05:24.761602:  
2024-11-11 20:05:24.764026: Epoch 242 
2024-11-11 20:05:24.767092: Current learning rate: 0.00779 
2024-11-11 20:06:04.363550: train_loss -0.9197 
2024-11-11 20:06:04.368480: val_loss -0.7778 
2024-11-11 20:06:04.370786: Pseudo dice [np.float32(0.9528), np.float32(0.746)] 
2024-11-11 20:06:04.373311: Epoch time: 39.6 s 
2024-11-11 20:06:05.482662:  
2024-11-11 20:06:05.485162: Epoch 243 
2024-11-11 20:06:05.487617: Current learning rate: 0.00778 
2024-11-11 20:06:45.074386: train_loss -0.8926 
2024-11-11 20:06:45.077760: val_loss -0.8023 
2024-11-11 20:06:45.080005: Pseudo dice [np.float32(0.9481), np.float32(0.7814)] 
2024-11-11 20:06:45.082252: Epoch time: 39.59 s 
2024-11-11 20:06:46.198757:  
2024-11-11 20:06:46.201298: Epoch 244 
2024-11-11 20:06:46.203625: Current learning rate: 0.00777 
2024-11-11 20:07:25.826152: train_loss -0.8868 
2024-11-11 20:07:25.831167: val_loss -0.8107 
2024-11-11 20:07:25.833595: Pseudo dice [np.float32(0.9483), np.float32(0.778)] 
2024-11-11 20:07:25.836807: Epoch time: 39.63 s 
2024-11-11 20:07:26.952880:  
2024-11-11 20:07:26.955244: Epoch 245 
2024-11-11 20:07:26.957626: Current learning rate: 0.00777 
2024-11-11 20:08:06.586994: train_loss -0.8924 
2024-11-11 20:08:06.590162: val_loss -0.7973 
2024-11-11 20:08:06.592362: Pseudo dice [np.float32(0.9588), np.float32(0.7622)] 
2024-11-11 20:08:06.594497: Epoch time: 39.64 s 
2024-11-11 20:08:07.711511:  
2024-11-11 20:08:07.713860: Epoch 246 
2024-11-11 20:08:07.716006: Current learning rate: 0.00776 
2024-11-11 20:08:47.313221: train_loss -0.889 
2024-11-11 20:08:47.318012: val_loss -0.7793 
2024-11-11 20:08:47.320320: Pseudo dice [np.float32(0.9555), np.float32(0.7309)] 
2024-11-11 20:08:47.322673: Epoch time: 39.6 s 
2024-11-11 20:08:48.438518:  
2024-11-11 20:08:48.440837: Epoch 247 
2024-11-11 20:08:48.443320: Current learning rate: 0.00775 
2024-11-11 20:09:28.078322: train_loss -0.8867 
2024-11-11 20:09:28.083423: val_loss -0.8209 
2024-11-11 20:09:28.086547: Pseudo dice [np.float32(0.9477), np.float32(0.8072)] 
2024-11-11 20:09:28.089435: Epoch time: 39.64 s 
2024-11-11 20:09:29.201818:  
2024-11-11 20:09:29.204726: Epoch 248 
2024-11-11 20:09:29.207712: Current learning rate: 0.00774 
2024-11-11 20:10:08.808194: train_loss -0.892 
2024-11-11 20:10:08.817133: val_loss -0.8353 
2024-11-11 20:10:08.819502: Pseudo dice [np.float32(0.9558), np.float32(0.8295)] 
2024-11-11 20:10:08.821874: Epoch time: 39.61 s 
2024-11-11 20:10:09.934124:  
2024-11-11 20:10:09.936497: Epoch 249 
2024-11-11 20:10:09.938705: Current learning rate: 0.00773 
2024-11-11 20:10:49.547672: train_loss -0.9027 
2024-11-11 20:10:49.551071: val_loss -0.8255 
2024-11-11 20:10:49.553518: Pseudo dice [np.float32(0.9554), np.float32(0.7864)] 
2024-11-11 20:10:49.555891: Epoch time: 39.61 s 
2024-11-11 20:10:51.466877:  
2024-11-11 20:10:51.469484: Epoch 250 
2024-11-11 20:10:51.472100: Current learning rate: 0.00772 
2024-11-11 20:11:31.072544: train_loss -0.9001 
2024-11-11 20:11:31.077250: val_loss -0.8033 
2024-11-11 20:11:31.079556: Pseudo dice [np.float32(0.9519), np.float32(0.7457)] 
2024-11-11 20:11:31.082045: Epoch time: 39.61 s 
2024-11-11 20:11:32.194270:  
2024-11-11 20:11:32.196724: Epoch 251 
2024-11-11 20:11:32.199054: Current learning rate: 0.00771 
2024-11-11 20:12:12.044365: train_loss -0.9037 
2024-11-11 20:12:12.047828: val_loss -0.78 
2024-11-11 20:12:12.050209: Pseudo dice [np.float32(0.953), np.float32(0.7166)] 
2024-11-11 20:12:12.052597: Epoch time: 39.85 s 
2024-11-11 20:12:13.166903:  
2024-11-11 20:12:13.169426: Epoch 252 
2024-11-11 20:12:13.171647: Current learning rate: 0.0077 
2024-11-11 20:12:52.793372: train_loss -0.8956 
2024-11-11 20:12:52.800782: val_loss -0.8433 
2024-11-11 20:12:52.803141: Pseudo dice [np.float32(0.9586), np.float32(0.8301)] 
2024-11-11 20:12:52.805436: Epoch time: 39.63 s 
2024-11-11 20:12:53.916982:  
2024-11-11 20:12:53.919911: Epoch 253 
2024-11-11 20:12:53.922449: Current learning rate: 0.00769 
2024-11-11 20:13:33.544546: train_loss -0.9072 
2024-11-11 20:13:33.547898: val_loss -0.8153 
2024-11-11 20:13:33.550099: Pseudo dice [np.float32(0.9555), np.float32(0.7677)] 
2024-11-11 20:13:33.552505: Epoch time: 39.63 s 
2024-11-11 20:13:34.663322:  
2024-11-11 20:13:34.666090: Epoch 254 
2024-11-11 20:13:34.668516: Current learning rate: 0.00768 
2024-11-11 20:14:14.304677: train_loss -0.9194 
2024-11-11 20:14:14.310681: val_loss -0.7553 
2024-11-11 20:14:14.313069: Pseudo dice [np.float32(0.9521), np.float32(0.7243)] 
2024-11-11 20:14:14.315299: Epoch time: 39.64 s 
2024-11-11 20:14:15.429235:  
2024-11-11 20:14:15.431938: Epoch 255 
2024-11-11 20:14:15.434223: Current learning rate: 0.00767 
2024-11-11 20:14:55.097129: train_loss -0.8872 
2024-11-11 20:14:55.100759: val_loss -0.8043 
2024-11-11 20:14:55.103184: Pseudo dice [np.float32(0.9525), np.float32(0.7808)] 
2024-11-11 20:14:55.105621: Epoch time: 39.67 s 
2024-11-11 20:14:56.220831:  
2024-11-11 20:14:56.223343: Epoch 256 
2024-11-11 20:14:56.225745: Current learning rate: 0.00766 
2024-11-11 20:15:35.872449: train_loss -0.8812 
2024-11-11 20:15:35.877327: val_loss -0.7449 
2024-11-11 20:15:35.879619: Pseudo dice [np.float32(0.9466), np.float32(0.6721)] 
2024-11-11 20:15:35.881899: Epoch time: 39.65 s 
2024-11-11 20:15:37.000602:  
2024-11-11 20:15:37.003176: Epoch 257 
2024-11-11 20:15:37.005632: Current learning rate: 0.00765 
2024-11-11 20:16:16.630745: train_loss -0.9035 
2024-11-11 20:16:16.634850: val_loss -0.8483 
2024-11-11 20:16:16.637323: Pseudo dice [np.float32(0.9583), np.float32(0.8576)] 
2024-11-11 20:16:16.639653: Epoch time: 39.63 s 
2024-11-11 20:16:17.752380:  
2024-11-11 20:16:17.756180: Epoch 258 
2024-11-11 20:16:17.758634: Current learning rate: 0.00764 
2024-11-11 20:16:57.394608: train_loss -0.9087 
2024-11-11 20:16:57.399784: val_loss -0.8337 
2024-11-11 20:16:57.402251: Pseudo dice [np.float32(0.9615), np.float32(0.8307)] 
2024-11-11 20:16:57.404597: Epoch time: 39.64 s 
2024-11-11 20:16:58.519162:  
2024-11-11 20:16:58.521760: Epoch 259 
2024-11-11 20:16:58.524311: Current learning rate: 0.00764 
2024-11-11 20:17:38.164664: train_loss -0.9207 
2024-11-11 20:17:38.167985: val_loss -0.8413 
2024-11-11 20:17:38.170186: Pseudo dice [np.float32(0.9564), np.float32(0.7987)] 
2024-11-11 20:17:38.172532: Epoch time: 39.65 s 
2024-11-11 20:17:39.287973:  
2024-11-11 20:17:39.290591: Epoch 260 
2024-11-11 20:17:39.293027: Current learning rate: 0.00763 
2024-11-11 20:18:18.928957: train_loss -0.9158 
2024-11-11 20:18:18.964053: val_loss -0.8558 
2024-11-11 20:18:18.966418: Pseudo dice [np.float32(0.9637), np.float32(0.8455)] 
2024-11-11 20:18:18.968609: Epoch time: 39.64 s 
2024-11-11 20:18:18.971054: Yayy! New best EMA pseudo Dice: 0.8687999844551086 
2024-11-11 20:18:20.891301:  
2024-11-11 20:18:20.894017: Epoch 261 
2024-11-11 20:18:20.896424: Current learning rate: 0.00762 
2024-11-11 20:19:00.546688: train_loss -0.8842 
2024-11-11 20:19:00.550153: val_loss -0.7815 
2024-11-11 20:19:00.552597: Pseudo dice [np.float32(0.9508), np.float32(0.7612)] 
2024-11-11 20:19:00.555034: Epoch time: 39.66 s 
2024-11-11 20:19:01.677449:  
2024-11-11 20:19:01.680372: Epoch 262 
2024-11-11 20:19:01.682977: Current learning rate: 0.00761 
2024-11-11 20:19:41.349875: train_loss -0.8694 
2024-11-11 20:19:41.354875: val_loss -0.7442 
2024-11-11 20:19:41.357173: Pseudo dice [np.float32(0.9419), np.float32(0.6801)] 
2024-11-11 20:19:41.359440: Epoch time: 39.67 s 
2024-11-11 20:19:42.478077:  
2024-11-11 20:19:42.480711: Epoch 263 
2024-11-11 20:19:42.483461: Current learning rate: 0.0076 
2024-11-11 20:20:22.135972: train_loss -0.8904 
2024-11-11 20:20:22.142637: val_loss -0.8463 
2024-11-11 20:20:22.144958: Pseudo dice [np.float32(0.9597), np.float32(0.8326)] 
2024-11-11 20:20:22.147206: Epoch time: 39.66 s 
2024-11-11 20:20:23.597754:  
2024-11-11 20:20:23.600464: Epoch 264 
2024-11-11 20:20:23.602763: Current learning rate: 0.00759 
2024-11-11 20:21:03.280578: train_loss -0.8984 
2024-11-11 20:21:03.285416: val_loss -0.7334 
2024-11-11 20:21:03.287977: Pseudo dice [np.float32(0.9543), np.float32(0.7118)] 
2024-11-11 20:21:03.290476: Epoch time: 39.68 s 
2024-11-11 20:21:04.411353:  
2024-11-11 20:21:04.413821: Epoch 265 
2024-11-11 20:21:04.416272: Current learning rate: 0.00758 
2024-11-11 20:21:44.087136: train_loss -0.9016 
2024-11-11 20:21:44.090789: val_loss -0.8493 
2024-11-11 20:21:44.092997: Pseudo dice [np.float32(0.9564), np.float32(0.828)] 
2024-11-11 20:21:44.095297: Epoch time: 39.68 s 
2024-11-11 20:21:45.213551:  
2024-11-11 20:21:45.216575: Epoch 266 
2024-11-11 20:21:45.218971: Current learning rate: 0.00757 
2024-11-11 20:22:24.904634: train_loss -0.8954 
2024-11-11 20:22:24.909630: val_loss -0.7998 
2024-11-11 20:22:24.912174: Pseudo dice [np.float32(0.9525), np.float32(0.7683)] 
2024-11-11 20:22:24.914873: Epoch time: 39.69 s 
2024-11-11 20:22:26.031909:  
2024-11-11 20:22:26.034401: Epoch 267 
2024-11-11 20:22:26.036750: Current learning rate: 0.00756 
2024-11-11 20:23:05.717302: train_loss -0.8905 
2024-11-11 20:23:05.721729: val_loss -0.7826 
2024-11-11 20:23:05.724346: Pseudo dice [np.float32(0.9555), np.float32(0.7415)] 
2024-11-11 20:23:05.726585: Epoch time: 39.69 s 
2024-11-11 20:23:06.845789:  
2024-11-11 20:23:06.848244: Epoch 268 
2024-11-11 20:23:06.850549: Current learning rate: 0.00755 
2024-11-11 20:23:46.529725: train_loss -0.8779 
2024-11-11 20:23:46.539462: val_loss -0.8271 
2024-11-11 20:23:46.541821: Pseudo dice [np.float32(0.957), np.float32(0.8008)] 
2024-11-11 20:23:46.544234: Epoch time: 39.69 s 
2024-11-11 20:23:47.664775:  
2024-11-11 20:23:47.667457: Epoch 269 
2024-11-11 20:23:47.670203: Current learning rate: 0.00754 
2024-11-11 20:24:27.326167: train_loss -0.8951 
2024-11-11 20:24:27.329594: val_loss -0.7949 
2024-11-11 20:24:27.331858: Pseudo dice [np.float32(0.945), np.float32(0.7533)] 
2024-11-11 20:24:27.334265: Epoch time: 39.66 s 
2024-11-11 20:24:28.456324:  
2024-11-11 20:24:28.458727: Epoch 270 
2024-11-11 20:24:28.461181: Current learning rate: 0.00753 
2024-11-11 20:25:08.130847: train_loss -0.8931 
2024-11-11 20:25:08.135411: val_loss -0.7935 
2024-11-11 20:25:08.137820: Pseudo dice [np.float32(0.9505), np.float32(0.7536)] 
2024-11-11 20:25:08.140068: Epoch time: 39.68 s 
2024-11-11 20:25:09.256260:  
2024-11-11 20:25:09.258687: Epoch 271 
2024-11-11 20:25:09.261204: Current learning rate: 0.00752 
2024-11-11 20:25:48.910908: train_loss -0.9135 
2024-11-11 20:25:48.914295: val_loss -0.8423 
2024-11-11 20:25:48.916636: Pseudo dice [np.float32(0.9555), np.float32(0.8136)] 
2024-11-11 20:25:48.919014: Epoch time: 39.66 s 
2024-11-11 20:25:50.042146:  
2024-11-11 20:25:50.045271: Epoch 272 
2024-11-11 20:25:50.047809: Current learning rate: 0.00751 
2024-11-11 20:26:29.683337: train_loss -0.9146 
2024-11-11 20:26:29.688068: val_loss -0.8106 
2024-11-11 20:26:29.690382: Pseudo dice [np.float32(0.9539), np.float32(0.7653)] 
2024-11-11 20:26:29.692722: Epoch time: 39.64 s 
2024-11-11 20:26:30.815109:  
2024-11-11 20:26:30.817576: Epoch 273 
2024-11-11 20:26:30.819939: Current learning rate: 0.00751 
2024-11-11 20:27:10.483992: train_loss -0.9053 
2024-11-11 20:27:10.491714: val_loss -0.8436 
2024-11-11 20:27:10.493930: Pseudo dice [np.float32(0.9568), np.float32(0.8379)] 
2024-11-11 20:27:10.496279: Epoch time: 39.67 s 
2024-11-11 20:27:11.617282:  
2024-11-11 20:27:11.619821: Epoch 274 
2024-11-11 20:27:11.622278: Current learning rate: 0.0075 
2024-11-11 20:27:51.286493: train_loss -0.895 
2024-11-11 20:27:51.291338: val_loss -0.7915 
2024-11-11 20:27:51.293907: Pseudo dice [np.float32(0.9522), np.float32(0.7376)] 
2024-11-11 20:27:51.296230: Epoch time: 39.67 s 
2024-11-11 20:27:52.414248:  
2024-11-11 20:27:52.417190: Epoch 275 
2024-11-11 20:27:52.419803: Current learning rate: 0.00749 
2024-11-11 20:28:32.075010: train_loss -0.8942 
2024-11-11 20:28:32.078825: val_loss -0.7495 
2024-11-11 20:28:32.081231: Pseudo dice [np.float32(0.9424), np.float32(0.7196)] 
2024-11-11 20:28:32.083782: Epoch time: 39.66 s 
2024-11-11 20:28:33.201158:  
2024-11-11 20:28:33.203554: Epoch 276 
2024-11-11 20:28:33.205980: Current learning rate: 0.00748 
2024-11-11 20:29:12.846158: train_loss -0.8942 
2024-11-11 20:29:12.851279: val_loss -0.7562 
2024-11-11 20:29:12.853714: Pseudo dice [np.float32(0.9483), np.float32(0.7335)] 
2024-11-11 20:29:12.856051: Epoch time: 39.65 s 
2024-11-11 20:29:14.339271:  
2024-11-11 20:29:14.342094: Epoch 277 
2024-11-11 20:29:14.344495: Current learning rate: 0.00747 
2024-11-11 20:29:54.036562: train_loss -0.9044 
2024-11-11 20:29:54.039903: val_loss -0.815 
2024-11-11 20:29:54.042194: Pseudo dice [np.float32(0.9569), np.float32(0.7802)] 
2024-11-11 20:29:54.044396: Epoch time: 39.7 s 
2024-11-11 20:29:55.165326:  
2024-11-11 20:29:55.167891: Epoch 278 
2024-11-11 20:29:55.170441: Current learning rate: 0.00746 
2024-11-11 20:30:34.834388: train_loss -0.9147 
2024-11-11 20:30:34.843278: val_loss -0.8107 
2024-11-11 20:30:34.845563: Pseudo dice [np.float32(0.9505), np.float32(0.7626)] 
2024-11-11 20:30:34.847962: Epoch time: 39.67 s 
2024-11-11 20:30:35.968441:  
2024-11-11 20:30:35.971256: Epoch 279 
2024-11-11 20:30:35.973763: Current learning rate: 0.00745 
2024-11-11 20:31:15.627745: train_loss -0.9065 
2024-11-11 20:31:15.631458: val_loss -0.7805 
2024-11-11 20:31:15.633870: Pseudo dice [np.float32(0.9497), np.float32(0.7485)] 
2024-11-11 20:31:15.636380: Epoch time: 39.66 s 
2024-11-11 20:31:16.754407:  
2024-11-11 20:31:16.757080: Epoch 280 
2024-11-11 20:31:16.759622: Current learning rate: 0.00744 
2024-11-11 20:31:56.420193: train_loss -0.913 
2024-11-11 20:31:56.424923: val_loss -0.807 
2024-11-11 20:31:56.427333: Pseudo dice [np.float32(0.9568), np.float32(0.7506)] 
2024-11-11 20:31:56.429573: Epoch time: 39.67 s 
2024-11-11 20:31:57.548679:  
2024-11-11 20:31:57.551018: Epoch 281 
2024-11-11 20:31:57.553317: Current learning rate: 0.00743 
2024-11-11 20:32:37.226926: train_loss -0.9088 
2024-11-11 20:32:37.230343: val_loss -0.828 
2024-11-11 20:32:37.232578: Pseudo dice [np.float32(0.9538), np.float32(0.8047)] 
2024-11-11 20:32:37.234613: Epoch time: 39.68 s 
2024-11-11 20:32:38.355311:  
2024-11-11 20:32:38.358002: Epoch 282 
2024-11-11 20:32:38.360533: Current learning rate: 0.00742 
2024-11-11 20:33:18.055627: train_loss -0.9024 
2024-11-11 20:33:18.060233: val_loss -0.7777 
2024-11-11 20:33:18.062587: Pseudo dice [np.float32(0.9476), np.float32(0.6975)] 
2024-11-11 20:33:18.064791: Epoch time: 39.7 s 
2024-11-11 20:33:19.190849:  
2024-11-11 20:33:19.194098: Epoch 283 
2024-11-11 20:33:19.196540: Current learning rate: 0.00741 
2024-11-11 20:33:58.850349: train_loss -0.9086 
2024-11-11 20:33:58.853868: val_loss -0.819 
2024-11-11 20:33:58.856064: Pseudo dice [np.float32(0.9591), np.float32(0.8011)] 
2024-11-11 20:33:58.858262: Epoch time: 39.66 s 
2024-11-11 20:33:59.975456:  
2024-11-11 20:33:59.977829: Epoch 284 
2024-11-11 20:33:59.980093: Current learning rate: 0.0074 
2024-11-11 20:34:39.640393: train_loss -0.9057 
2024-11-11 20:34:39.648028: val_loss -0.8008 
2024-11-11 20:34:39.650349: Pseudo dice [np.float32(0.9498), np.float32(0.7651)] 
2024-11-11 20:34:39.652576: Epoch time: 39.67 s 
2024-11-11 20:34:40.774875:  
2024-11-11 20:34:40.777350: Epoch 285 
2024-11-11 20:34:40.779774: Current learning rate: 0.00739 
2024-11-11 20:35:20.425353: train_loss -0.9183 
2024-11-11 20:35:20.428916: val_loss -0.8097 
2024-11-11 20:35:20.431328: Pseudo dice [np.float32(0.9524), np.float32(0.7573)] 
2024-11-11 20:35:20.433923: Epoch time: 39.65 s 
2024-11-11 20:35:21.553607:  
2024-11-11 20:35:21.555946: Epoch 286 
2024-11-11 20:35:21.558961: Current learning rate: 0.00738 
2024-11-11 20:36:01.229001: train_loss -0.8996 
2024-11-11 20:36:01.234143: val_loss -0.8053 
2024-11-11 20:36:01.236472: Pseudo dice [np.float32(0.947), np.float32(0.7717)] 
2024-11-11 20:36:01.238457: Epoch time: 39.68 s 
2024-11-11 20:36:02.374833:  
2024-11-11 20:36:02.377434: Epoch 287 
2024-11-11 20:36:02.380086: Current learning rate: 0.00738 
2024-11-11 20:36:42.054008: train_loss -0.9051 
2024-11-11 20:36:42.057319: val_loss -0.8363 
2024-11-11 20:36:42.059720: Pseudo dice [np.float32(0.9569), np.float32(0.8321)] 
2024-11-11 20:36:42.061916: Epoch time: 39.68 s 
2024-11-11 20:36:43.196457:  
2024-11-11 20:36:43.199084: Epoch 288 
2024-11-11 20:36:43.201435: Current learning rate: 0.00737 
2024-11-11 20:37:22.870518: train_loss -0.905 
2024-11-11 20:37:22.874875: val_loss -0.8275 
2024-11-11 20:37:22.877092: Pseudo dice [np.float32(0.9548), np.float32(0.8178)] 
2024-11-11 20:37:22.879459: Epoch time: 39.68 s 
2024-11-11 20:37:24.016114:  
2024-11-11 20:37:24.019272: Epoch 289 
2024-11-11 20:37:24.021890: Current learning rate: 0.00736 
2024-11-11 20:38:03.705134: train_loss -0.9074 
2024-11-11 20:38:03.712436: val_loss -0.7934 
2024-11-11 20:38:03.715356: Pseudo dice [np.float32(0.9604), np.float32(0.7948)] 
2024-11-11 20:38:03.717772: Epoch time: 39.69 s 
2024-11-11 20:38:05.194744:  
2024-11-11 20:38:05.197106: Epoch 290 
2024-11-11 20:38:05.199633: Current learning rate: 0.00735 
2024-11-11 20:38:44.885238: train_loss -0.9176 
2024-11-11 20:38:44.890251: val_loss -0.8586 
2024-11-11 20:38:44.892970: Pseudo dice [np.float32(0.9551), np.float32(0.8264)] 
2024-11-11 20:38:44.896458: Epoch time: 39.69 s 
2024-11-11 20:38:46.033997:  
2024-11-11 20:38:46.036546: Epoch 291 
2024-11-11 20:38:46.039103: Current learning rate: 0.00734 
2024-11-11 20:39:25.703298: train_loss -0.9019 
2024-11-11 20:39:25.706673: val_loss -0.7812 
2024-11-11 20:39:25.708922: Pseudo dice [np.float32(0.9557), np.float32(0.7146)] 
2024-11-11 20:39:25.711319: Epoch time: 39.67 s 
2024-11-11 20:39:26.848739:  
2024-11-11 20:39:26.851316: Epoch 292 
2024-11-11 20:39:26.853656: Current learning rate: 0.00733 
2024-11-11 20:40:06.531451: train_loss -0.9223 
2024-11-11 20:40:06.537148: val_loss -0.7702 
2024-11-11 20:40:06.539405: Pseudo dice [np.float32(0.9497), np.float32(0.728)] 
2024-11-11 20:40:06.541645: Epoch time: 39.68 s 
2024-11-11 20:40:07.683824:  
2024-11-11 20:40:07.686455: Epoch 293 
2024-11-11 20:40:07.688813: Current learning rate: 0.00732 
2024-11-11 20:40:47.353072: train_loss -0.9103 
2024-11-11 20:40:47.357624: val_loss -0.8047 
2024-11-11 20:40:47.359982: Pseudo dice [np.float32(0.9579), np.float32(0.7851)] 
2024-11-11 20:40:47.362196: Epoch time: 39.67 s 
2024-11-11 20:40:48.496665:  
2024-11-11 20:40:48.499215: Epoch 294 
2024-11-11 20:40:48.501845: Current learning rate: 0.00731 
2024-11-11 20:41:28.158555: train_loss -0.9036 
2024-11-11 20:41:28.163422: val_loss -0.7658 
2024-11-11 20:41:28.165815: Pseudo dice [np.float32(0.9512), np.float32(0.6542)] 
2024-11-11 20:41:28.168071: Epoch time: 39.66 s 
2024-11-11 20:41:29.298780:  
2024-11-11 20:41:29.301091: Epoch 295 
2024-11-11 20:41:29.303271: Current learning rate: 0.0073 
2024-11-11 20:42:08.967462: train_loss -0.8819 
2024-11-11 20:42:08.975764: val_loss -0.7833 
2024-11-11 20:42:08.978300: Pseudo dice [np.float32(0.9505), np.float32(0.7644)] 
2024-11-11 20:42:08.980864: Epoch time: 39.67 s 
2024-11-11 20:42:10.119801:  
2024-11-11 20:42:10.122201: Epoch 296 
2024-11-11 20:42:10.124564: Current learning rate: 0.00729 
2024-11-11 20:42:49.790607: train_loss -0.8968 
2024-11-11 20:42:49.795046: val_loss -0.8194 
2024-11-11 20:42:49.797442: Pseudo dice [np.float32(0.9468), np.float32(0.7765)] 
2024-11-11 20:42:49.799691: Epoch time: 39.67 s 
2024-11-11 20:42:50.934961:  
2024-11-11 20:42:50.937426: Epoch 297 
2024-11-11 20:42:50.939520: Current learning rate: 0.00728 
2024-11-11 20:43:30.591954: train_loss -0.8884 
2024-11-11 20:43:30.595539: val_loss -0.8276 
2024-11-11 20:43:30.598025: Pseudo dice [np.float32(0.9522), np.float32(0.7913)] 
2024-11-11 20:43:30.600146: Epoch time: 39.66 s 
2024-11-11 20:43:31.732126:  
2024-11-11 20:43:31.734837: Epoch 298 
2024-11-11 20:43:31.737480: Current learning rate: 0.00727 
2024-11-11 20:44:11.385088: train_loss -0.8985 
2024-11-11 20:44:11.403276: val_loss -0.7691 
2024-11-11 20:44:11.406917: Pseudo dice [np.float32(0.9539), np.float32(0.7278)] 
2024-11-11 20:44:11.412331: Epoch time: 39.65 s 
2024-11-11 20:44:12.549563:  
2024-11-11 20:44:12.552230: Epoch 299 
2024-11-11 20:44:12.554590: Current learning rate: 0.00726 
2024-11-11 20:44:52.189723: train_loss -0.9097 
2024-11-11 20:44:52.197545: val_loss -0.7605 
2024-11-11 20:44:52.200591: Pseudo dice [np.float32(0.9541), np.float32(0.6379)] 
2024-11-11 20:44:52.203042: Epoch time: 39.64 s 
2024-11-11 20:44:54.125338:  
2024-11-11 20:44:54.127924: Epoch 300 
2024-11-11 20:44:54.130223: Current learning rate: 0.00725 
2024-11-11 20:45:33.778203: train_loss -0.8723 
2024-11-11 20:45:33.783106: val_loss -0.7071 
2024-11-11 20:45:33.785699: Pseudo dice [np.float32(0.9459), np.float32(0.6127)] 
2024-11-11 20:45:33.788222: Epoch time: 39.65 s 
2024-11-11 20:45:34.922853:  
2024-11-11 20:45:34.925368: Epoch 301 
2024-11-11 20:45:34.927622: Current learning rate: 0.00724 
2024-11-11 20:46:14.582747: train_loss -0.9028 
2024-11-11 20:46:14.586786: val_loss -0.8097 
2024-11-11 20:46:14.589162: Pseudo dice [np.float32(0.953), np.float32(0.7857)] 
2024-11-11 20:46:14.591576: Epoch time: 39.66 s 
2024-11-11 20:46:15.729717:  
2024-11-11 20:46:15.732309: Epoch 302 
2024-11-11 20:46:15.735291: Current learning rate: 0.00724 
2024-11-11 20:46:55.381838: train_loss -0.9092 
2024-11-11 20:46:55.386887: val_loss -0.8547 
2024-11-11 20:46:55.389319: Pseudo dice [np.float32(0.963), np.float32(0.8377)] 
2024-11-11 20:46:55.391732: Epoch time: 39.65 s 
2024-11-11 20:46:56.869202:  
2024-11-11 20:46:56.871774: Epoch 303 
2024-11-11 20:46:56.874110: Current learning rate: 0.00723 
2024-11-11 20:47:36.561056: train_loss -0.925 
2024-11-11 20:47:36.564430: val_loss -0.7466 
2024-11-11 20:47:36.566817: Pseudo dice [np.float32(0.9505), np.float32(0.661)] 
2024-11-11 20:47:36.569073: Epoch time: 39.69 s 
2024-11-11 20:47:37.708867:  
2024-11-11 20:47:37.711352: Epoch 304 
2024-11-11 20:47:37.713750: Current learning rate: 0.00722 
2024-11-11 20:48:17.371762: train_loss -0.9206 
2024-11-11 20:48:17.376692: val_loss -0.8078 
2024-11-11 20:48:17.379005: Pseudo dice [np.float32(0.9599), np.float32(0.8209)] 
2024-11-11 20:48:17.381314: Epoch time: 39.66 s 
2024-11-11 20:48:18.516220:  
2024-11-11 20:48:18.518937: Epoch 305 
2024-11-11 20:48:18.521858: Current learning rate: 0.00721 
2024-11-11 20:48:58.189921: train_loss -0.9297 
2024-11-11 20:48:58.197052: val_loss -0.8475 
2024-11-11 20:48:58.199325: Pseudo dice [np.float32(0.9574), np.float32(0.8194)] 
2024-11-11 20:48:58.201371: Epoch time: 39.67 s 
2024-11-11 20:48:59.332750:  
2024-11-11 20:48:59.335150: Epoch 306 
2024-11-11 20:48:59.337377: Current learning rate: 0.0072 
2024-11-11 20:49:38.996439: train_loss -0.9289 
2024-11-11 20:49:39.000958: val_loss -0.8328 
2024-11-11 20:49:39.003329: Pseudo dice [np.float32(0.9547), np.float32(0.8078)] 
2024-11-11 20:49:39.005664: Epoch time: 39.66 s 
2024-11-11 20:49:40.141113:  
2024-11-11 20:49:40.143615: Epoch 307 
2024-11-11 20:49:40.146086: Current learning rate: 0.00719 
2024-11-11 20:50:19.805387: train_loss -0.9227 
2024-11-11 20:50:19.808704: val_loss -0.8488 
2024-11-11 20:50:19.811098: Pseudo dice [np.float32(0.9605), np.float32(0.8022)] 
2024-11-11 20:50:19.813615: Epoch time: 39.67 s 
2024-11-11 20:50:20.947655:  
2024-11-11 20:50:20.949906: Epoch 308 
2024-11-11 20:50:20.952055: Current learning rate: 0.00718 
2024-11-11 20:51:00.615228: train_loss -0.9121 
2024-11-11 20:51:00.619973: val_loss -0.8457 
2024-11-11 20:51:00.622216: Pseudo dice [np.float32(0.9598), np.float32(0.7941)] 
2024-11-11 20:51:00.624473: Epoch time: 39.67 s 
2024-11-11 20:51:01.760645:  
2024-11-11 20:51:01.763254: Epoch 309 
2024-11-11 20:51:01.765836: Current learning rate: 0.00717 
2024-11-11 20:51:41.433866: train_loss -0.9014 
2024-11-11 20:51:41.439061: val_loss -0.824 
2024-11-11 20:51:41.441625: Pseudo dice [np.float32(0.9584), np.float32(0.8098)] 
2024-11-11 20:51:41.443938: Epoch time: 39.67 s 
2024-11-11 20:51:42.579521:  
2024-11-11 20:51:42.582028: Epoch 310 
2024-11-11 20:51:42.584532: Current learning rate: 0.00716 
2024-11-11 20:52:22.265091: train_loss -0.9136 
2024-11-11 20:52:22.269560: val_loss -0.8259 
2024-11-11 20:52:22.271950: Pseudo dice [np.float32(0.9607), np.float32(0.8134)] 
2024-11-11 20:52:22.274215: Epoch time: 39.69 s 
2024-11-11 20:52:23.412027:  
2024-11-11 20:52:23.414665: Epoch 311 
2024-11-11 20:52:23.417136: Current learning rate: 0.00715 
2024-11-11 20:53:03.101194: train_loss -0.92 
2024-11-11 20:53:03.104798: val_loss -0.8389 
2024-11-11 20:53:03.107454: Pseudo dice [np.float32(0.9583), np.float32(0.8367)] 
2024-11-11 20:53:03.109731: Epoch time: 39.69 s 
2024-11-11 20:53:03.112342: Yayy! New best EMA pseudo Dice: 0.8694999814033508 
2024-11-11 20:53:05.054656:  
2024-11-11 20:53:05.057270: Epoch 312 
2024-11-11 20:53:05.059541: Current learning rate: 0.00714 
2024-11-11 20:53:44.726967: train_loss -0.9071 
2024-11-11 20:53:44.740233: val_loss -0.8358 
2024-11-11 20:53:44.742727: Pseudo dice [np.float32(0.9577), np.float32(0.8353)] 
2024-11-11 20:53:44.745010: Epoch time: 39.67 s 
2024-11-11 20:53:44.747353: Yayy! New best EMA pseudo Dice: 0.8722000122070312 
2024-11-11 20:53:46.714321:  
2024-11-11 20:53:46.716758: Epoch 313 
2024-11-11 20:53:46.719334: Current learning rate: 0.00713 
2024-11-11 20:54:26.393368: train_loss -0.8909 
2024-11-11 20:54:26.396514: val_loss -0.7757 
2024-11-11 20:54:26.398864: Pseudo dice [np.float32(0.9552), np.float32(0.6861)] 
2024-11-11 20:54:26.401379: Epoch time: 39.68 s 
2024-11-11 20:54:27.542748:  
2024-11-11 20:54:27.545690: Epoch 314 
2024-11-11 20:54:27.547931: Current learning rate: 0.00712 
2024-11-11 20:55:07.225095: train_loss -0.9061 
2024-11-11 20:55:07.237162: val_loss -0.7519 
2024-11-11 20:55:07.239303: Pseudo dice [np.float32(0.9482), np.float32(0.6833)] 
2024-11-11 20:55:07.241623: Epoch time: 39.68 s 
2024-11-11 20:55:08.382851:  
2024-11-11 20:55:08.385479: Epoch 315 
2024-11-11 20:55:08.387710: Current learning rate: 0.00711 
2024-11-11 20:55:48.046936: train_loss -0.8867 
2024-11-11 20:55:48.050410: val_loss -0.7611 
2024-11-11 20:55:48.052755: Pseudo dice [np.float32(0.9251), np.float32(0.6799)] 
2024-11-11 20:55:48.055085: Epoch time: 39.67 s 
2024-11-11 20:55:49.541554:  
2024-11-11 20:55:49.544502: Epoch 316 
2024-11-11 20:55:49.547488: Current learning rate: 0.0071 
2024-11-11 20:56:29.227226: train_loss -0.8801 
2024-11-11 20:56:29.231978: val_loss -0.8048 
2024-11-11 20:56:29.234263: Pseudo dice [np.float32(0.9425), np.float32(0.7637)] 
2024-11-11 20:56:29.236394: Epoch time: 39.69 s 
2024-11-11 20:56:30.377407:  
2024-11-11 20:56:30.379834: Epoch 317 
2024-11-11 20:56:30.382324: Current learning rate: 0.0071 
2024-11-11 20:57:10.034657: train_loss -0.8995 
2024-11-11 20:57:10.040777: val_loss -0.7674 
2024-11-11 20:57:10.043127: Pseudo dice [np.float32(0.9484), np.float32(0.6956)] 
2024-11-11 20:57:10.053639: Epoch time: 39.66 s 
2024-11-11 20:57:11.199986:  
2024-11-11 20:57:11.202487: Epoch 318 
2024-11-11 20:57:11.205081: Current learning rate: 0.00709 
2024-11-11 20:57:50.840263: train_loss -0.8997 
2024-11-11 20:57:50.844856: val_loss -0.799 
2024-11-11 20:57:50.847267: Pseudo dice [np.float32(0.95), np.float32(0.7532)] 
2024-11-11 20:57:50.849646: Epoch time: 39.64 s 
2024-11-11 20:57:51.996906:  
2024-11-11 20:57:51.999439: Epoch 319 
2024-11-11 20:57:52.002577: Current learning rate: 0.00708 
2024-11-11 20:58:31.639432: train_loss -0.9131 
2024-11-11 20:58:31.642828: val_loss -0.8072 
2024-11-11 20:58:31.645234: Pseudo dice [np.float32(0.96), np.float32(0.7795)] 
2024-11-11 20:58:31.647634: Epoch time: 39.64 s 
2024-11-11 20:58:32.788165:  
2024-11-11 20:58:32.790730: Epoch 320 
2024-11-11 20:58:32.793409: Current learning rate: 0.00707 
2024-11-11 20:59:12.439140: train_loss -0.8649 
2024-11-11 20:59:12.453428: val_loss -0.7255 
2024-11-11 20:59:12.456149: Pseudo dice [np.float32(0.939), np.float32(0.647)] 
2024-11-11 20:59:12.458732: Epoch time: 39.65 s 
2024-11-11 20:59:13.600110:  
2024-11-11 20:59:13.602767: Epoch 321 
2024-11-11 20:59:13.605093: Current learning rate: 0.00706 
2024-11-11 20:59:53.231571: train_loss -0.8587 
2024-11-11 20:59:53.234946: val_loss -0.8128 
2024-11-11 20:59:53.237331: Pseudo dice [np.float32(0.9462), np.float32(0.7727)] 
2024-11-11 20:59:53.239762: Epoch time: 39.63 s 
2024-11-11 20:59:54.384511:  
2024-11-11 20:59:54.387106: Epoch 322 
2024-11-11 20:59:54.389774: Current learning rate: 0.00705 
2024-11-11 21:00:34.035329: train_loss -0.8835 
2024-11-11 21:00:34.040110: val_loss -0.7939 
2024-11-11 21:00:34.042649: Pseudo dice [np.float32(0.9526), np.float32(0.7817)] 
2024-11-11 21:00:34.044843: Epoch time: 39.65 s 
2024-11-11 21:00:35.184307:  
2024-11-11 21:00:35.186944: Epoch 323 
2024-11-11 21:00:35.189649: Current learning rate: 0.00704 
2024-11-11 21:01:14.817026: train_loss -0.8892 
2024-11-11 21:01:14.826584: val_loss -0.8049 
2024-11-11 21:01:14.829115: Pseudo dice [np.float32(0.952), np.float32(0.78)] 
2024-11-11 21:01:14.831518: Epoch time: 39.63 s 
2024-11-11 21:01:15.971543:  
2024-11-11 21:01:15.974162: Epoch 324 
2024-11-11 21:01:15.976524: Current learning rate: 0.00703 
2024-11-11 21:01:55.583890: train_loss -0.8966 
2024-11-11 21:01:55.588784: val_loss -0.7347 
2024-11-11 21:01:55.591216: Pseudo dice [np.float32(0.946), np.float32(0.6716)] 
2024-11-11 21:01:55.593700: Epoch time: 39.61 s 
2024-11-11 21:01:56.740209:  
2024-11-11 21:01:56.742931: Epoch 325 
2024-11-11 21:01:56.745326: Current learning rate: 0.00702 
2024-11-11 21:02:36.367002: train_loss -0.9032 
2024-11-11 21:02:36.370292: val_loss -0.7696 
2024-11-11 21:02:36.372576: Pseudo dice [np.float32(0.942), np.float32(0.7542)] 
2024-11-11 21:02:36.374813: Epoch time: 39.63 s 
2024-11-11 21:02:37.517917:  
2024-11-11 21:02:37.520202: Epoch 326 
2024-11-11 21:02:37.523576: Current learning rate: 0.00701 
2024-11-11 21:03:17.156166: train_loss -0.8806 
2024-11-11 21:03:17.160876: val_loss -0.6871 
2024-11-11 21:03:17.163156: Pseudo dice [np.float32(0.9271), np.float32(0.6439)] 
2024-11-11 21:03:17.165764: Epoch time: 39.64 s 
2024-11-11 21:03:18.311013:  
2024-11-11 21:03:18.313473: Epoch 327 
2024-11-11 21:03:18.315804: Current learning rate: 0.007 
2024-11-11 21:03:57.958443: train_loss -0.8728 
2024-11-11 21:03:57.964888: val_loss -0.8511 
2024-11-11 21:03:57.967208: Pseudo dice [np.float32(0.9519), np.float32(0.8247)] 
2024-11-11 21:03:57.969585: Epoch time: 39.65 s 
2024-11-11 21:03:59.436795:  
2024-11-11 21:03:59.439317: Epoch 328 
2024-11-11 21:03:59.441626: Current learning rate: 0.00699 
2024-11-11 21:04:39.102721: train_loss -0.8773 
2024-11-11 21:04:39.107287: val_loss -0.8043 
2024-11-11 21:04:39.109590: Pseudo dice [np.float32(0.9522), np.float32(0.7965)] 
2024-11-11 21:04:39.111971: Epoch time: 39.67 s 
2024-11-11 21:04:40.254697:  
2024-11-11 21:04:40.257167: Epoch 329 
2024-11-11 21:04:40.259586: Current learning rate: 0.00698 
2024-11-11 21:05:19.902182: train_loss -0.8921 
2024-11-11 21:05:19.905232: val_loss -0.8328 
2024-11-11 21:05:19.907438: Pseudo dice [np.float32(0.9568), np.float32(0.8158)] 
2024-11-11 21:05:19.909843: Epoch time: 39.65 s 
2024-11-11 21:05:21.058884:  
2024-11-11 21:05:21.062603: Epoch 330 
2024-11-11 21:05:21.064909: Current learning rate: 0.00697 
2024-11-11 21:06:00.717782: train_loss -0.903 
2024-11-11 21:06:00.721761: val_loss -0.8328 
2024-11-11 21:06:00.724103: Pseudo dice [np.float32(0.9543), np.float32(0.7846)] 
2024-11-11 21:06:00.726336: Epoch time: 39.66 s 
2024-11-11 21:06:01.873799:  
2024-11-11 21:06:01.877324: Epoch 331 
2024-11-11 21:06:01.879918: Current learning rate: 0.00696 
2024-11-11 21:06:41.525665: train_loss -0.9139 
2024-11-11 21:06:41.529180: val_loss -0.7842 
2024-11-11 21:06:41.531570: Pseudo dice [np.float32(0.9541), np.float32(0.7247)] 
2024-11-11 21:06:41.533856: Epoch time: 39.65 s 
2024-11-11 21:06:42.670205:  
2024-11-11 21:06:42.672817: Epoch 332 
2024-11-11 21:06:42.675181: Current learning rate: 0.00696 
2024-11-11 21:07:22.309672: train_loss -0.9057 
2024-11-11 21:07:22.315311: val_loss -0.7348 
2024-11-11 21:07:22.317726: Pseudo dice [np.float32(0.9519), np.float32(0.7091)] 
2024-11-11 21:07:22.320215: Epoch time: 39.64 s 
2024-11-11 21:07:23.464119:  
2024-11-11 21:07:23.466711: Epoch 333 
2024-11-11 21:07:23.469079: Current learning rate: 0.00695 
2024-11-11 21:08:03.121952: train_loss -0.9072 
2024-11-11 21:08:03.129396: val_loss -0.7977 
2024-11-11 21:08:03.132017: Pseudo dice [np.float32(0.9601), np.float32(0.7842)] 
2024-11-11 21:08:03.134317: Epoch time: 39.66 s 
2024-11-11 21:08:04.280257:  
2024-11-11 21:08:04.282868: Epoch 334 
2024-11-11 21:08:04.285398: Current learning rate: 0.00694 
2024-11-11 21:08:43.951789: train_loss -0.9113 
2024-11-11 21:08:43.956689: val_loss -0.7494 
2024-11-11 21:08:43.958974: Pseudo dice [np.float32(0.947), np.float32(0.7045)] 
2024-11-11 21:08:43.961240: Epoch time: 39.67 s 
2024-11-11 21:08:45.120754:  
2024-11-11 21:08:45.123214: Epoch 335 
2024-11-11 21:08:45.125813: Current learning rate: 0.00693 
2024-11-11 21:09:24.785561: train_loss -0.9083 
2024-11-11 21:09:24.789304: val_loss -0.7494 
2024-11-11 21:09:24.791870: Pseudo dice [np.float32(0.9459), np.float32(0.707)] 
2024-11-11 21:09:24.794547: Epoch time: 39.67 s 
2024-11-11 21:09:25.952096:  
2024-11-11 21:09:25.954812: Epoch 336 
2024-11-11 21:09:25.957203: Current learning rate: 0.00692 
2024-11-11 21:10:05.635091: train_loss -0.9112 
2024-11-11 21:10:05.644874: val_loss -0.7763 
2024-11-11 21:10:05.647260: Pseudo dice [np.float32(0.9517), np.float32(0.748)] 
2024-11-11 21:10:05.649730: Epoch time: 39.68 s 
2024-11-11 21:10:06.802732:  
2024-11-11 21:10:06.805315: Epoch 337 
2024-11-11 21:10:06.807800: Current learning rate: 0.00691 
2024-11-11 21:10:46.457296: train_loss -0.9204 
2024-11-11 21:10:46.460950: val_loss -0.8641 
2024-11-11 21:10:46.463187: Pseudo dice [np.float32(0.9618), np.float32(0.8373)] 
2024-11-11 21:10:46.465787: Epoch time: 39.66 s 
2024-11-11 21:10:47.621195:  
2024-11-11 21:10:47.623590: Epoch 338 
2024-11-11 21:10:47.626017: Current learning rate: 0.0069 
2024-11-11 21:11:27.289129: train_loss -0.9225 
2024-11-11 21:11:27.293660: val_loss -0.8652 
2024-11-11 21:11:27.295914: Pseudo dice [np.float32(0.9617), np.float32(0.8619)] 
2024-11-11 21:11:27.298358: Epoch time: 39.67 s 
2024-11-11 21:11:28.457192:  
2024-11-11 21:11:28.459685: Epoch 339 
2024-11-11 21:11:28.462156: Current learning rate: 0.00689 
2024-11-11 21:12:08.117921: train_loss -0.9074 
2024-11-11 21:12:08.121315: val_loss -0.796 
2024-11-11 21:12:08.123618: Pseudo dice [np.float32(0.9509), np.float32(0.7822)] 
2024-11-11 21:12:08.125788: Epoch time: 39.66 s 
2024-11-11 21:12:09.283620:  
2024-11-11 21:12:09.285935: Epoch 340 
2024-11-11 21:12:09.288120: Current learning rate: 0.00688 
2024-11-11 21:12:48.947384: train_loss -0.918 
2024-11-11 21:12:48.952214: val_loss -0.8251 
2024-11-11 21:12:48.954610: Pseudo dice [np.float32(0.9515), np.float32(0.8099)] 
2024-11-11 21:12:48.956904: Epoch time: 39.66 s 
2024-11-11 21:12:50.459153:  
2024-11-11 21:12:50.462216: Epoch 341 
2024-11-11 21:12:50.464468: Current learning rate: 0.00687 
2024-11-11 21:13:30.147611: train_loss -0.9103 
2024-11-11 21:13:30.158456: val_loss -0.7878 
2024-11-11 21:13:30.161006: Pseudo dice [np.float32(0.9527), np.float32(0.764)] 
2024-11-11 21:13:30.163881: Epoch time: 39.69 s 
2024-11-11 21:13:31.320740:  
2024-11-11 21:13:31.323239: Epoch 342 
2024-11-11 21:13:31.325560: Current learning rate: 0.00686 
2024-11-11 21:14:11.026149: train_loss -0.8865 
2024-11-11 21:14:11.031350: val_loss -0.7827 
2024-11-11 21:14:11.033674: Pseudo dice [np.float32(0.9473), np.float32(0.7661)] 
2024-11-11 21:14:11.036080: Epoch time: 39.71 s 
2024-11-11 21:14:12.194743:  
2024-11-11 21:14:12.197331: Epoch 343 
2024-11-11 21:14:12.199754: Current learning rate: 0.00685 
2024-11-11 21:14:51.879316: train_loss -0.9016 
2024-11-11 21:14:51.883228: val_loss -0.8006 
2024-11-11 21:14:51.885647: Pseudo dice [np.float32(0.9475), np.float32(0.7564)] 
2024-11-11 21:14:51.887947: Epoch time: 39.69 s 
2024-11-11 21:14:53.041896:  
2024-11-11 21:14:53.044591: Epoch 344 
2024-11-11 21:14:53.047042: Current learning rate: 0.00684 
2024-11-11 21:15:32.724864: train_loss -0.904 
2024-11-11 21:15:32.732474: val_loss -0.7884 
2024-11-11 21:15:32.734745: Pseudo dice [np.float32(0.9506), np.float32(0.7579)] 
2024-11-11 21:15:32.737097: Epoch time: 39.68 s 
2024-11-11 21:15:33.897468:  
2024-11-11 21:15:33.900074: Epoch 345 
2024-11-11 21:15:33.902414: Current learning rate: 0.00683 
2024-11-11 21:16:13.570405: train_loss -0.9058 
2024-11-11 21:16:13.573912: val_loss -0.7464 
2024-11-11 21:16:13.577281: Pseudo dice [np.float32(0.9476), np.float32(0.6679)] 
2024-11-11 21:16:13.579560: Epoch time: 39.67 s 
2024-11-11 21:16:14.735358:  
2024-11-11 21:16:14.738125: Epoch 346 
2024-11-11 21:16:14.740421: Current learning rate: 0.00682 
2024-11-11 21:16:54.402452: train_loss -0.9088 
2024-11-11 21:16:54.406978: val_loss -0.8311 
2024-11-11 21:16:54.409207: Pseudo dice [np.float32(0.9604), np.float32(0.8129)] 
2024-11-11 21:16:54.411891: Epoch time: 39.67 s 
2024-11-11 21:16:55.566397:  
2024-11-11 21:16:55.568811: Epoch 347 
2024-11-11 21:16:55.571077: Current learning rate: 0.00681 
2024-11-11 21:17:35.232625: train_loss -0.9214 
2024-11-11 21:17:35.235979: val_loss -0.8061 
2024-11-11 21:17:35.238405: Pseudo dice [np.float32(0.955), np.float32(0.7746)] 
2024-11-11 21:17:35.240520: Epoch time: 39.67 s 
2024-11-11 21:17:36.402456:  
2024-11-11 21:17:36.405044: Epoch 348 
2024-11-11 21:17:36.407658: Current learning rate: 0.0068 
2024-11-11 21:18:16.078661: train_loss -0.9093 
2024-11-11 21:18:16.083891: val_loss -0.7499 
2024-11-11 21:18:16.086285: Pseudo dice [np.float32(0.9472), np.float32(0.639)] 
2024-11-11 21:18:16.088758: Epoch time: 39.68 s 
2024-11-11 21:18:17.245830:  
2024-11-11 21:18:17.248204: Epoch 349 
2024-11-11 21:18:17.250625: Current learning rate: 0.0068 
2024-11-11 21:18:56.912347: train_loss -0.9055 
2024-11-11 21:18:56.915795: val_loss -0.8056 
2024-11-11 21:18:56.918253: Pseudo dice [np.float32(0.9498), np.float32(0.7831)] 
2024-11-11 21:18:56.920375: Epoch time: 39.67 s 
2024-11-11 21:18:58.859885:  
2024-11-11 21:18:58.862147: Epoch 350 
2024-11-11 21:18:58.864594: Current learning rate: 0.00679 
2024-11-11 21:19:38.513718: train_loss -0.9202 
2024-11-11 21:19:38.518538: val_loss -0.8189 
2024-11-11 21:19:38.520908: Pseudo dice [np.float32(0.9534), np.float32(0.7873)] 
2024-11-11 21:19:38.523173: Epoch time: 39.65 s 
2024-11-11 21:19:39.677594:  
2024-11-11 21:19:39.679988: Epoch 351 
2024-11-11 21:19:39.682135: Current learning rate: 0.00678 
2024-11-11 21:20:19.338847: train_loss -0.9148 
2024-11-11 21:20:19.342254: val_loss -0.8215 
2024-11-11 21:20:19.344531: Pseudo dice [np.float32(0.9524), np.float32(0.7545)] 
2024-11-11 21:20:19.346785: Epoch time: 39.66 s 
2024-11-11 21:20:20.507578:  
2024-11-11 21:20:20.510039: Epoch 352 
2024-11-11 21:20:20.512403: Current learning rate: 0.00677 
2024-11-11 21:21:00.184940: train_loss -0.9058 
2024-11-11 21:21:00.196172: val_loss -0.7938 
2024-11-11 21:21:00.198613: Pseudo dice [np.float32(0.9503), np.float32(0.743)] 
2024-11-11 21:21:00.201028: Epoch time: 39.68 s 
2024-11-11 21:21:01.703293:  
2024-11-11 21:21:01.705963: Epoch 353 
2024-11-11 21:21:01.708232: Current learning rate: 0.00676 
2024-11-11 21:21:41.403570: train_loss -0.9135 
2024-11-11 21:21:41.408662: val_loss -0.7429 
2024-11-11 21:21:41.411397: Pseudo dice [np.float32(0.9525), np.float32(0.6911)] 
2024-11-11 21:21:41.413641: Epoch time: 39.7 s 
2024-11-11 21:21:42.572897:  
2024-11-11 21:21:42.575383: Epoch 354 
2024-11-11 21:21:42.577614: Current learning rate: 0.00675 
2024-11-11 21:22:22.248283: train_loss -0.9077 
2024-11-11 21:22:22.253213: val_loss -0.7685 
2024-11-11 21:22:22.255412: Pseudo dice [np.float32(0.9474), np.float32(0.675)] 
2024-11-11 21:22:22.257699: Epoch time: 39.68 s 
2024-11-11 21:22:23.418948:  
2024-11-11 21:22:23.421805: Epoch 355 
2024-11-11 21:22:23.424021: Current learning rate: 0.00674 
2024-11-11 21:23:03.109075: train_loss -0.9064 
2024-11-11 21:23:03.115500: val_loss -0.7767 
2024-11-11 21:23:03.117849: Pseudo dice [np.float32(0.9533), np.float32(0.758)] 
2024-11-11 21:23:03.120174: Epoch time: 39.69 s 
2024-11-11 21:23:04.280236:  
2024-11-11 21:23:04.282667: Epoch 356 
2024-11-11 21:23:04.284897: Current learning rate: 0.00673 
2024-11-11 21:23:43.967657: train_loss -0.9268 
2024-11-11 21:23:43.972291: val_loss -0.8077 
2024-11-11 21:23:43.974845: Pseudo dice [np.float32(0.9541), np.float32(0.7759)] 
2024-11-11 21:23:43.977562: Epoch time: 39.69 s 
2024-11-11 21:23:45.142021:  
2024-11-11 21:23:45.144720: Epoch 357 
2024-11-11 21:23:45.147442: Current learning rate: 0.00672 
2024-11-11 21:24:24.815450: train_loss -0.9157 
2024-11-11 21:24:24.821621: val_loss -0.8046 
2024-11-11 21:24:24.823861: Pseudo dice [np.float32(0.954), np.float32(0.7695)] 
2024-11-11 21:24:24.826298: Epoch time: 39.67 s 
2024-11-11 21:24:25.986404:  
2024-11-11 21:24:25.988904: Epoch 358 
2024-11-11 21:24:25.991464: Current learning rate: 0.00671 
2024-11-11 21:25:05.649735: train_loss -0.9179 
2024-11-11 21:25:05.654925: val_loss -0.8283 
2024-11-11 21:25:05.657284: Pseudo dice [np.float32(0.9571), np.float32(0.804)] 
2024-11-11 21:25:05.659689: Epoch time: 39.66 s 
2024-11-11 21:25:06.823005:  
2024-11-11 21:25:06.825867: Epoch 359 
2024-11-11 21:25:06.828073: Current learning rate: 0.0067 
2024-11-11 21:25:46.490728: train_loss -0.9178 
2024-11-11 21:25:46.494950: val_loss -0.8271 
2024-11-11 21:25:46.497149: Pseudo dice [np.float32(0.9558), np.float32(0.8067)] 
2024-11-11 21:25:46.499524: Epoch time: 39.67 s 
2024-11-11 21:25:47.656026:  
2024-11-11 21:25:47.659233: Epoch 360 
2024-11-11 21:25:47.661757: Current learning rate: 0.00669 
2024-11-11 21:26:27.320762: train_loss -0.8855 
2024-11-11 21:26:27.325709: val_loss -0.7336 
2024-11-11 21:26:27.328104: Pseudo dice [np.float32(0.9343), np.float32(0.6007)] 
2024-11-11 21:26:27.330390: Epoch time: 39.67 s 
2024-11-11 21:26:28.488573:  
2024-11-11 21:26:28.491587: Epoch 361 
2024-11-11 21:26:28.494576: Current learning rate: 0.00668 
2024-11-11 21:27:08.166423: train_loss -0.8937 
2024-11-11 21:27:08.169842: val_loss -0.7598 
2024-11-11 21:27:08.172179: Pseudo dice [np.float32(0.9498), np.float32(0.7437)] 
2024-11-11 21:27:08.174504: Epoch time: 39.68 s 
2024-11-11 21:27:09.330481:  
2024-11-11 21:27:09.333144: Epoch 362 
2024-11-11 21:27:09.335625: Current learning rate: 0.00667 
2024-11-11 21:27:48.979659: train_loss -0.9009 
2024-11-11 21:27:48.985031: val_loss -0.7847 
2024-11-11 21:27:48.987413: Pseudo dice [np.float32(0.9547), np.float32(0.7568)] 
2024-11-11 21:27:48.989866: Epoch time: 39.65 s 
2024-11-11 21:27:50.147712:  
2024-11-11 21:27:50.150108: Epoch 363 
2024-11-11 21:27:50.152391: Current learning rate: 0.00666 
2024-11-11 21:28:29.796231: train_loss -0.9156 
2024-11-11 21:28:29.799624: val_loss -0.8093 
2024-11-11 21:28:29.802016: Pseudo dice [np.float32(0.9569), np.float32(0.7825)] 
2024-11-11 21:28:29.803978: Epoch time: 39.65 s 
2024-11-11 21:28:30.965341:  
2024-11-11 21:28:30.967698: Epoch 364 
2024-11-11 21:28:30.970002: Current learning rate: 0.00665 
2024-11-11 21:29:10.592734: train_loss -0.9 
2024-11-11 21:29:10.599373: val_loss -0.8119 
2024-11-11 21:29:10.601499: Pseudo dice [np.float32(0.9546), np.float32(0.7779)] 
2024-11-11 21:29:10.603778: Epoch time: 39.63 s 
2024-11-11 21:29:11.768224:  
2024-11-11 21:29:11.770812: Epoch 365 
2024-11-11 21:29:11.773411: Current learning rate: 0.00665 
2024-11-11 21:29:51.407191: train_loss -0.9096 
2024-11-11 21:29:51.410497: val_loss -0.7923 
2024-11-11 21:29:51.412729: Pseudo dice [np.float32(0.9445), np.float32(0.7536)] 
2024-11-11 21:29:51.414995: Epoch time: 39.64 s 
2024-11-11 21:29:52.920847:  
2024-11-11 21:29:52.923428: Epoch 366 
2024-11-11 21:29:52.926054: Current learning rate: 0.00664 
2024-11-11 21:30:32.566941: train_loss -0.9048 
2024-11-11 21:30:32.571324: val_loss -0.8391 
2024-11-11 21:30:32.573632: Pseudo dice [np.float32(0.9624), np.float32(0.8131)] 
2024-11-11 21:30:32.576930: Epoch time: 39.65 s 
2024-11-11 21:30:33.740025:  
2024-11-11 21:30:33.742535: Epoch 367 
2024-11-11 21:30:33.744760: Current learning rate: 0.00663 
2024-11-11 21:31:13.393886: train_loss -0.9142 
2024-11-11 21:31:13.397060: val_loss -0.8102 
2024-11-11 21:31:13.399448: Pseudo dice [np.float32(0.9563), np.float32(0.811)] 
2024-11-11 21:31:13.402001: Epoch time: 39.66 s 
2024-11-11 21:31:14.564445:  
2024-11-11 21:31:14.567143: Epoch 368 
2024-11-11 21:31:14.569670: Current learning rate: 0.00662 
2024-11-11 21:31:54.210320: train_loss -0.9171 
2024-11-11 21:31:54.214844: val_loss -0.7786 
2024-11-11 21:31:54.217116: Pseudo dice [np.float32(0.9551), np.float32(0.7814)] 
2024-11-11 21:31:54.219425: Epoch time: 39.65 s 
2024-11-11 21:31:55.389533:  
2024-11-11 21:31:55.392364: Epoch 369 
2024-11-11 21:31:55.394799: Current learning rate: 0.00661 
2024-11-11 21:32:35.042792: train_loss -0.9249 
2024-11-11 21:32:35.046317: val_loss -0.7887 
2024-11-11 21:32:35.048776: Pseudo dice [np.float32(0.9554), np.float32(0.7867)] 
2024-11-11 21:32:35.050985: Epoch time: 39.65 s 
2024-11-11 21:32:36.212966:  
2024-11-11 21:32:36.215423: Epoch 370 
2024-11-11 21:32:36.217935: Current learning rate: 0.0066 
2024-11-11 21:33:15.855992: train_loss -0.9168 
2024-11-11 21:33:15.865291: val_loss -0.8317 
2024-11-11 21:33:15.867817: Pseudo dice [np.float32(0.9584), np.float32(0.8218)] 
2024-11-11 21:33:15.870014: Epoch time: 39.64 s 
2024-11-11 21:33:17.033272:  
2024-11-11 21:33:17.035866: Epoch 371 
2024-11-11 21:33:17.038392: Current learning rate: 0.00659 
2024-11-11 21:33:56.673126: train_loss -0.9105 
2024-11-11 21:33:56.676280: val_loss -0.7517 
2024-11-11 21:33:56.678805: Pseudo dice [np.float32(0.9313), np.float32(0.6875)] 
2024-11-11 21:33:56.681097: Epoch time: 39.64 s 
2024-11-11 21:33:57.846713:  
2024-11-11 21:33:57.849015: Epoch 372 
2024-11-11 21:33:57.851250: Current learning rate: 0.00658 
2024-11-11 21:34:37.509565: train_loss -0.8819 
2024-11-11 21:34:37.514035: val_loss -0.7726 
2024-11-11 21:34:37.516427: Pseudo dice [np.float32(0.9531), np.float32(0.7246)] 
2024-11-11 21:34:37.518559: Epoch time: 39.66 s 
2024-11-11 21:34:38.685671:  
2024-11-11 21:34:38.688058: Epoch 373 
2024-11-11 21:34:38.690770: Current learning rate: 0.00657 
2024-11-11 21:35:18.334216: train_loss -0.8758 
2024-11-11 21:35:18.337709: val_loss -0.7808 
2024-11-11 21:35:18.340192: Pseudo dice [np.float32(0.955), np.float32(0.7638)] 
2024-11-11 21:35:18.342303: Epoch time: 39.65 s 
2024-11-11 21:35:19.509170:  
2024-11-11 21:35:19.511631: Epoch 374 
2024-11-11 21:35:19.513779: Current learning rate: 0.00656 
2024-11-11 21:35:59.159228: train_loss -0.901 
2024-11-11 21:35:59.167960: val_loss -0.7983 
2024-11-11 21:35:59.170167: Pseudo dice [np.float32(0.9473), np.float32(0.7699)] 
2024-11-11 21:35:59.172571: Epoch time: 39.65 s 
2024-11-11 21:36:00.333179:  
2024-11-11 21:36:00.335645: Epoch 375 
2024-11-11 21:36:00.338044: Current learning rate: 0.00655 
2024-11-11 21:36:39.972476: train_loss -0.9159 
2024-11-11 21:36:39.975928: val_loss -0.8066 
2024-11-11 21:36:39.978266: Pseudo dice [np.float32(0.9589), np.float32(0.801)] 
2024-11-11 21:36:39.980653: Epoch time: 39.64 s 
2024-11-11 21:36:41.141867:  
2024-11-11 21:36:41.144824: Epoch 376 
2024-11-11 21:36:41.147262: Current learning rate: 0.00654 
2024-11-11 21:37:20.792894: train_loss -0.9047 
2024-11-11 21:37:20.797848: val_loss -0.8093 
2024-11-11 21:37:20.800623: Pseudo dice [np.float32(0.957), np.float32(0.7814)] 
2024-11-11 21:37:20.802894: Epoch time: 39.65 s 
2024-11-11 21:37:21.962242:  
2024-11-11 21:37:21.964882: Epoch 377 
2024-11-11 21:37:21.967412: Current learning rate: 0.00653 
2024-11-11 21:38:01.607394: train_loss -0.8806 
2024-11-11 21:38:01.617463: val_loss -0.8208 
2024-11-11 21:38:01.619714: Pseudo dice [np.float32(0.9483), np.float32(0.7694)] 
2024-11-11 21:38:01.622174: Epoch time: 39.65 s 
2024-11-11 21:38:03.120006:  
2024-11-11 21:38:03.122622: Epoch 378 
2024-11-11 21:38:03.125173: Current learning rate: 0.00652 
2024-11-11 21:38:42.782069: train_loss -0.8957 
2024-11-11 21:38:42.786669: val_loss -0.7705 
2024-11-11 21:38:42.788832: Pseudo dice [np.float32(0.9498), np.float32(0.7487)] 
2024-11-11 21:38:42.791022: Epoch time: 39.66 s 
2024-11-11 21:38:43.955119:  
2024-11-11 21:38:43.957795: Epoch 379 
2024-11-11 21:38:43.960091: Current learning rate: 0.00651 
2024-11-11 21:39:23.624421: train_loss -0.9014 
2024-11-11 21:39:23.627650: val_loss -0.8193 
2024-11-11 21:39:23.630072: Pseudo dice [np.float32(0.9545), np.float32(0.7662)] 
2024-11-11 21:39:23.632352: Epoch time: 39.67 s 
2024-11-11 21:39:24.793364:  
2024-11-11 21:39:24.796034: Epoch 380 
2024-11-11 21:39:24.798560: Current learning rate: 0.0065 
2024-11-11 21:40:04.455201: train_loss -0.9064 
2024-11-11 21:40:04.463388: val_loss -0.8276 
2024-11-11 21:40:04.465853: Pseudo dice [np.float32(0.956), np.float32(0.7976)] 
2024-11-11 21:40:04.468223: Epoch time: 39.66 s 
2024-11-11 21:40:05.632849:  
2024-11-11 21:40:05.635507: Epoch 381 
2024-11-11 21:40:05.637887: Current learning rate: 0.00649 
2024-11-11 21:40:45.301507: train_loss -0.9168 
2024-11-11 21:40:45.305172: val_loss -0.829 
2024-11-11 21:40:45.307539: Pseudo dice [np.float32(0.9575), np.float32(0.793)] 
2024-11-11 21:40:45.309821: Epoch time: 39.67 s 
2024-11-11 21:40:46.491549:  
2024-11-11 21:40:46.493970: Epoch 382 
2024-11-11 21:40:46.496445: Current learning rate: 0.00648 
2024-11-11 21:41:26.145749: train_loss -0.9249 
2024-11-11 21:41:26.150330: val_loss -0.7542 
2024-11-11 21:41:26.152769: Pseudo dice [np.float32(0.9447), np.float32(0.7045)] 
2024-11-11 21:41:26.155094: Epoch time: 39.66 s 
2024-11-11 21:41:27.336877:  
2024-11-11 21:41:27.339555: Epoch 383 
2024-11-11 21:41:27.341895: Current learning rate: 0.00648 
2024-11-11 21:42:07.012368: train_loss -0.907 
2024-11-11 21:42:07.020485: val_loss -0.7778 
2024-11-11 21:42:07.023064: Pseudo dice [np.float32(0.9515), np.float32(0.6826)] 
2024-11-11 21:42:07.025561: Epoch time: 39.68 s 
2024-11-11 21:42:08.202361:  
2024-11-11 21:42:08.204801: Epoch 384 
2024-11-11 21:42:08.207190: Current learning rate: 0.00647 
2024-11-11 21:42:47.890562: train_loss -0.8984 
2024-11-11 21:42:47.895402: val_loss -0.7814 
2024-11-11 21:42:47.897753: Pseudo dice [np.float32(0.9474), np.float32(0.736)] 
2024-11-11 21:42:47.900288: Epoch time: 39.69 s 
2024-11-11 21:42:49.077616:  
2024-11-11 21:42:49.080009: Epoch 385 
2024-11-11 21:42:49.082419: Current learning rate: 0.00646 
2024-11-11 21:43:28.747062: train_loss -0.9075 
2024-11-11 21:43:28.750478: val_loss -0.8017 
2024-11-11 21:43:28.752715: Pseudo dice [np.float32(0.9491), np.float32(0.7656)] 
2024-11-11 21:43:28.754966: Epoch time: 39.67 s 
2024-11-11 21:43:29.928569:  
2024-11-11 21:43:29.930892: Epoch 386 
2024-11-11 21:43:29.933229: Current learning rate: 0.00645 
2024-11-11 21:44:09.584169: train_loss -0.9227 
2024-11-11 21:44:09.593214: val_loss -0.8271 
2024-11-11 21:44:09.595435: Pseudo dice [np.float32(0.9609), np.float32(0.8118)] 
2024-11-11 21:44:09.597513: Epoch time: 39.66 s 
2024-11-11 21:44:10.781521:  
2024-11-11 21:44:10.783996: Epoch 387 
2024-11-11 21:44:10.786375: Current learning rate: 0.00644 
2024-11-11 21:44:50.464072: train_loss -0.9104 
2024-11-11 21:44:50.467541: val_loss -0.7266 
2024-11-11 21:44:50.469919: Pseudo dice [np.float32(0.9525), np.float32(0.6935)] 
2024-11-11 21:44:50.472259: Epoch time: 39.68 s 
2024-11-11 21:44:51.648341:  
2024-11-11 21:44:51.650702: Epoch 388 
2024-11-11 21:44:51.652896: Current learning rate: 0.00643 
2024-11-11 21:45:31.316553: train_loss -0.9182 
2024-11-11 21:45:31.321000: val_loss -0.7716 
2024-11-11 21:45:31.323346: Pseudo dice [np.float32(0.9531), np.float32(0.7295)] 
2024-11-11 21:45:31.325707: Epoch time: 39.67 s 
2024-11-11 21:45:32.504852:  
2024-11-11 21:45:32.507540: Epoch 389 
2024-11-11 21:45:32.510107: Current learning rate: 0.00642 
2024-11-11 21:46:12.150467: train_loss -0.9218 
2024-11-11 21:46:12.166763: val_loss -0.8508 
2024-11-11 21:46:12.169517: Pseudo dice [np.float32(0.9608), np.float32(0.8076)] 
2024-11-11 21:46:12.171868: Epoch time: 39.65 s 
2024-11-11 21:46:13.695390:  
2024-11-11 21:46:13.697779: Epoch 390 
2024-11-11 21:46:13.700033: Current learning rate: 0.00641 
2024-11-11 21:46:53.355616: train_loss -0.9164 
2024-11-11 21:46:53.361156: val_loss -0.814 
2024-11-11 21:46:53.363838: Pseudo dice [np.float32(0.9566), np.float32(0.792)] 
2024-11-11 21:46:53.366398: Epoch time: 39.66 s 
2024-11-11 21:46:54.547212:  
2024-11-11 21:46:54.550087: Epoch 391 
2024-11-11 21:46:54.552792: Current learning rate: 0.0064 
2024-11-11 21:47:34.218094: train_loss -0.9285 
2024-11-11 21:47:34.221633: val_loss -0.8126 
2024-11-11 21:47:34.224084: Pseudo dice [np.float32(0.9592), np.float32(0.8016)] 
2024-11-11 21:47:34.226491: Epoch time: 39.67 s 
2024-11-11 21:47:35.441848:  
2024-11-11 21:47:35.444430: Epoch 392 
2024-11-11 21:47:35.446760: Current learning rate: 0.00639 
2024-11-11 21:48:15.095199: train_loss -0.928 
2024-11-11 21:48:15.099900: val_loss -0.8193 
2024-11-11 21:48:15.102411: Pseudo dice [np.float32(0.9549), np.float32(0.7644)] 
2024-11-11 21:48:15.104853: Epoch time: 39.65 s 
2024-11-11 21:48:16.282946:  
2024-11-11 21:48:16.285397: Epoch 393 
2024-11-11 21:48:16.287825: Current learning rate: 0.00638 
2024-11-11 21:48:55.952543: train_loss -0.9066 
2024-11-11 21:48:55.955847: val_loss -0.7813 
2024-11-11 21:48:55.958351: Pseudo dice [np.float32(0.9499), np.float32(0.7589)] 
2024-11-11 21:48:55.960727: Epoch time: 39.67 s 
2024-11-11 21:48:57.146186:  
2024-11-11 21:48:57.148700: Epoch 394 
2024-11-11 21:48:57.151080: Current learning rate: 0.00637 
2024-11-11 21:49:36.815561: train_loss -0.8904 
2024-11-11 21:49:36.820426: val_loss -0.8316 
2024-11-11 21:49:36.822918: Pseudo dice [np.float32(0.9482), np.float32(0.8216)] 
2024-11-11 21:49:36.825222: Epoch time: 39.67 s 
2024-11-11 21:49:38.006196:  
2024-11-11 21:49:38.008747: Epoch 395 
2024-11-11 21:49:38.011415: Current learning rate: 0.00636 
2024-11-11 21:50:17.675386: train_loss -0.8847 
2024-11-11 21:50:17.682536: val_loss -0.7783 
2024-11-11 21:50:17.685049: Pseudo dice [np.float32(0.9511), np.float32(0.7285)] 
2024-11-11 21:50:17.687692: Epoch time: 39.67 s 
2024-11-11 21:50:18.869617:  
2024-11-11 21:50:18.872072: Epoch 396 
2024-11-11 21:50:18.874403: Current learning rate: 0.00635 
2024-11-11 21:50:58.545592: train_loss -0.9052 
2024-11-11 21:50:58.550221: val_loss -0.8002 
2024-11-11 21:50:58.552538: Pseudo dice [np.float32(0.9565), np.float32(0.787)] 
2024-11-11 21:50:58.555164: Epoch time: 39.68 s 
2024-11-11 21:50:59.739619:  
2024-11-11 21:50:59.742166: Epoch 397 
2024-11-11 21:50:59.744392: Current learning rate: 0.00634 
2024-11-11 21:51:39.402483: train_loss -0.9176 
2024-11-11 21:51:39.405935: val_loss -0.8562 
2024-11-11 21:51:39.408416: Pseudo dice [np.float32(0.9578), np.float32(0.8341)] 
2024-11-11 21:51:39.410620: Epoch time: 39.66 s 
2024-11-11 21:51:40.589392:  
2024-11-11 21:51:40.591900: Epoch 398 
2024-11-11 21:51:40.594426: Current learning rate: 0.00633 
2024-11-11 21:52:20.248478: train_loss -0.8972 
2024-11-11 21:52:20.253183: val_loss -0.7975 
2024-11-11 21:52:20.255688: Pseudo dice [np.float32(0.9515), np.float32(0.7455)] 
2024-11-11 21:52:20.257913: Epoch time: 39.66 s 
2024-11-11 21:52:21.442011:  
2024-11-11 21:52:21.444493: Epoch 399 
2024-11-11 21:52:21.446980: Current learning rate: 0.00632 
2024-11-11 21:53:01.096264: train_loss -0.9152 
2024-11-11 21:53:01.100377: val_loss -0.7422 
2024-11-11 21:53:01.102720: Pseudo dice [np.float32(0.9472), np.float32(0.69)] 
2024-11-11 21:53:01.105133: Epoch time: 39.66 s 
2024-11-11 21:53:03.066806:  
2024-11-11 21:53:03.069429: Epoch 400 
2024-11-11 21:53:03.072007: Current learning rate: 0.00631 
2024-11-11 21:53:42.712739: train_loss -0.9081 
2024-11-11 21:53:42.717078: val_loss -0.7821 
2024-11-11 21:53:42.719318: Pseudo dice [np.float32(0.9528), np.float32(0.776)] 
2024-11-11 21:53:42.721555: Epoch time: 39.65 s 
2024-11-11 21:53:43.900537:  
2024-11-11 21:53:43.902945: Epoch 401 
2024-11-11 21:53:43.905339: Current learning rate: 0.0063 
2024-11-11 21:54:23.553244: train_loss -0.8857 
2024-11-11 21:54:23.556808: val_loss -0.8469 
2024-11-11 21:54:23.559053: Pseudo dice [np.float32(0.9574), np.float32(0.8344)] 
2024-11-11 21:54:23.561516: Epoch time: 39.65 s 
2024-11-11 21:54:25.075886:  
2024-11-11 21:54:25.078652: Epoch 402 
2024-11-11 21:54:25.081533: Current learning rate: 0.0063 
2024-11-11 21:55:04.752893: train_loss -0.898 
2024-11-11 21:55:04.757541: val_loss -0.8398 
2024-11-11 21:55:04.759707: Pseudo dice [np.float32(0.9601), np.float32(0.8394)] 
2024-11-11 21:55:04.761947: Epoch time: 39.68 s 
2024-11-11 21:55:05.938406:  
2024-11-11 21:55:05.940908: Epoch 403 
2024-11-11 21:55:05.943291: Current learning rate: 0.00629 
2024-11-11 21:55:45.606910: train_loss -0.8945 
2024-11-11 21:55:45.614912: val_loss -0.7754 
2024-11-11 21:55:45.617342: Pseudo dice [np.float32(0.9544), np.float32(0.7493)] 
2024-11-11 21:55:45.619696: Epoch time: 39.67 s 
2024-11-11 21:55:46.799497:  
2024-11-11 21:55:46.802190: Epoch 404 
2024-11-11 21:55:46.804635: Current learning rate: 0.00628 
2024-11-11 21:56:26.457587: train_loss -0.909 
2024-11-11 21:56:26.462292: val_loss -0.8436 
2024-11-11 21:56:26.464746: Pseudo dice [np.float32(0.9603), np.float32(0.8307)] 
2024-11-11 21:56:26.467090: Epoch time: 39.66 s 
2024-11-11 21:56:27.645755:  
2024-11-11 21:56:27.648581: Epoch 405 
2024-11-11 21:56:27.651048: Current learning rate: 0.00627 
2024-11-11 21:57:07.294516: train_loss -0.9169 
2024-11-11 21:57:07.298169: val_loss -0.8135 
2024-11-11 21:57:07.300966: Pseudo dice [np.float32(0.9549), np.float32(0.7941)] 
2024-11-11 21:57:07.303429: Epoch time: 39.65 s 
2024-11-11 21:57:08.486168:  
2024-11-11 21:57:08.489012: Epoch 406 
2024-11-11 21:57:08.491472: Current learning rate: 0.00626 
2024-11-11 21:57:48.129024: train_loss -0.9174 
2024-11-11 21:57:48.137782: val_loss -0.8089 
2024-11-11 21:57:48.140049: Pseudo dice [np.float32(0.9552), np.float32(0.7838)] 
2024-11-11 21:57:48.142771: Epoch time: 39.64 s 
2024-11-11 21:57:49.319628:  
2024-11-11 21:57:49.322322: Epoch 407 
2024-11-11 21:57:49.324832: Current learning rate: 0.00625 
2024-11-11 21:58:28.972461: train_loss -0.9156 
2024-11-11 21:58:28.975806: val_loss -0.7356 
2024-11-11 21:58:28.977998: Pseudo dice [np.float32(0.9517), np.float32(0.6393)] 
2024-11-11 21:58:28.980662: Epoch time: 39.65 s 
2024-11-11 21:58:30.165471:  
2024-11-11 21:58:30.168157: Epoch 408 
2024-11-11 21:58:30.170614: Current learning rate: 0.00624 
2024-11-11 21:59:09.828085: train_loss -0.8905 
2024-11-11 21:59:09.832664: val_loss -0.7776 
2024-11-11 21:59:09.834837: Pseudo dice [np.float32(0.9524), np.float32(0.7102)] 
2024-11-11 21:59:09.837044: Epoch time: 39.66 s 
2024-11-11 21:59:11.012649:  
2024-11-11 21:59:11.015134: Epoch 409 
2024-11-11 21:59:11.017295: Current learning rate: 0.00623 
2024-11-11 21:59:50.665886: train_loss -0.9015 
2024-11-11 21:59:50.673117: val_loss -0.8174 
2024-11-11 21:59:50.675547: Pseudo dice [np.float32(0.9556), np.float32(0.7993)] 
2024-11-11 21:59:50.678455: Epoch time: 39.65 s 
2024-11-11 21:59:51.858964:  
2024-11-11 21:59:51.861498: Epoch 410 
2024-11-11 21:59:51.864004: Current learning rate: 0.00622 
2024-11-11 22:00:31.526390: train_loss -0.8997 
2024-11-11 22:00:31.531139: val_loss -0.7625 
2024-11-11 22:00:31.533637: Pseudo dice [np.float32(0.9444), np.float32(0.6844)] 
2024-11-11 22:00:31.536083: Epoch time: 39.67 s 
2024-11-11 22:00:32.656130:  
2024-11-11 22:00:32.658786: Epoch 411 
2024-11-11 22:00:32.661625: Current learning rate: 0.00621 
2024-11-11 22:01:12.346750: train_loss -0.9151 
2024-11-11 22:01:12.350310: val_loss -0.8184 
2024-11-11 22:01:12.352759: Pseudo dice [np.float32(0.9565), np.float32(0.8056)] 
2024-11-11 22:01:12.355134: Epoch time: 39.69 s 
2024-11-11 22:01:13.467965:  
2024-11-11 22:01:13.470477: Epoch 412 
2024-11-11 22:01:13.472918: Current learning rate: 0.0062 
2024-11-11 22:01:53.122235: train_loss -0.9148 
2024-11-11 22:01:53.130008: val_loss -0.7604 
2024-11-11 22:01:53.132271: Pseudo dice [np.float32(0.9531), np.float32(0.7254)] 
2024-11-11 22:01:53.134650: Epoch time: 39.66 s 
2024-11-11 22:01:54.254742:  
2024-11-11 22:01:54.257361: Epoch 413 
2024-11-11 22:01:54.259578: Current learning rate: 0.00619 
2024-11-11 22:02:33.908965: train_loss -0.9182 
2024-11-11 22:02:33.912686: val_loss -0.7971 
2024-11-11 22:02:33.914952: Pseudo dice [np.float32(0.954), np.float32(0.775)] 
2024-11-11 22:02:33.917216: Epoch time: 39.66 s 
2024-11-11 22:02:35.038574:  
2024-11-11 22:02:35.041376: Epoch 414 
2024-11-11 22:02:35.044009: Current learning rate: 0.00618 
2024-11-11 22:03:14.687531: train_loss -0.9267 
2024-11-11 22:03:14.692012: val_loss -0.8289 
2024-11-11 22:03:14.694274: Pseudo dice [np.float32(0.963), np.float32(0.8265)] 
2024-11-11 22:03:14.696432: Epoch time: 39.65 s 
2024-11-11 22:03:16.169295:  
2024-11-11 22:03:16.171844: Epoch 415 
2024-11-11 22:03:16.174457: Current learning rate: 0.00617 
2024-11-11 22:03:55.820568: train_loss -0.9271 
2024-11-11 22:03:55.823926: val_loss -0.8246 
2024-11-11 22:03:55.826213: Pseudo dice [np.float32(0.9607), np.float32(0.8025)] 
2024-11-11 22:03:55.828449: Epoch time: 39.65 s 
2024-11-11 22:03:56.955058:  
2024-11-11 22:03:56.957502: Epoch 416 
2024-11-11 22:03:56.959685: Current learning rate: 0.00616 
2024-11-11 22:04:36.610866: train_loss -0.9216 
2024-11-11 22:04:36.615611: val_loss -0.8295 
2024-11-11 22:04:36.617939: Pseudo dice [np.float32(0.956), np.float32(0.8191)] 
2024-11-11 22:04:36.620454: Epoch time: 39.66 s 
2024-11-11 22:04:37.747826:  
2024-11-11 22:04:37.750248: Epoch 417 
2024-11-11 22:04:37.752479: Current learning rate: 0.00615 
2024-11-11 22:05:17.428425: train_loss -0.9284 
2024-11-11 22:05:17.432691: val_loss -0.8033 
2024-11-11 22:05:17.435668: Pseudo dice [np.float32(0.9581), np.float32(0.7604)] 
2024-11-11 22:05:17.438600: Epoch time: 39.68 s 
2024-11-11 22:05:18.564183:  
2024-11-11 22:05:18.566767: Epoch 418 
2024-11-11 22:05:18.569280: Current learning rate: 0.00614 
2024-11-11 22:05:58.254806: train_loss -0.9319 
2024-11-11 22:05:58.259320: val_loss -0.8258 
2024-11-11 22:05:58.261903: Pseudo dice [np.float32(0.9565), np.float32(0.7954)] 
2024-11-11 22:05:58.264002: Epoch time: 39.69 s 
2024-11-11 22:05:59.388458:  
2024-11-11 22:05:59.390792: Epoch 419 
2024-11-11 22:05:59.393136: Current learning rate: 0.00613 
2024-11-11 22:06:39.057989: train_loss -0.9243 
2024-11-11 22:06:39.061697: val_loss -0.8674 
2024-11-11 22:06:39.064034: Pseudo dice [np.float32(0.9538), np.float32(0.838)] 
2024-11-11 22:06:39.066195: Epoch time: 39.67 s 
2024-11-11 22:06:40.198343:  
2024-11-11 22:06:40.201563: Epoch 420 
2024-11-11 22:06:40.204084: Current learning rate: 0.00612 
2024-11-11 22:07:19.870367: train_loss -0.9171 
2024-11-11 22:07:19.883202: val_loss -0.8165 
2024-11-11 22:07:19.885463: Pseudo dice [np.float32(0.9498), np.float32(0.7842)] 
2024-11-11 22:07:19.887666: Epoch time: 39.67 s 
2024-11-11 22:07:21.015821:  
2024-11-11 22:07:21.018239: Epoch 421 
2024-11-11 22:07:21.020499: Current learning rate: 0.00612 
2024-11-11 22:08:00.685552: train_loss -0.9232 
2024-11-11 22:08:00.688931: val_loss -0.861 
2024-11-11 22:08:00.691200: Pseudo dice [np.float32(0.9551), np.float32(0.8518)] 
2024-11-11 22:08:00.693322: Epoch time: 39.67 s 
2024-11-11 22:08:01.815257:  
2024-11-11 22:08:01.817865: Epoch 422 
2024-11-11 22:08:01.820200: Current learning rate: 0.00611 
2024-11-11 22:08:41.482543: train_loss -0.9139 
2024-11-11 22:08:41.487254: val_loss -0.8467 
2024-11-11 22:08:41.489461: Pseudo dice [np.float32(0.9541), np.float32(0.8038)] 
2024-11-11 22:08:41.491589: Epoch time: 39.67 s 
2024-11-11 22:08:41.493700: Yayy! New best EMA pseudo Dice: 0.8729000091552734 
2024-11-11 22:08:43.401607:  
2024-11-11 22:08:43.404286: Epoch 423 
2024-11-11 22:08:43.406796: Current learning rate: 0.0061 
2024-11-11 22:09:23.035810: train_loss -0.9113 
2024-11-11 22:09:23.039003: val_loss -0.8145 
2024-11-11 22:09:23.041468: Pseudo dice [np.float32(0.9521), np.float32(0.811)] 
2024-11-11 22:09:23.043739: Epoch time: 39.64 s 
2024-11-11 22:09:23.045919: Yayy! New best EMA pseudo Dice: 0.8737000226974487 
2024-11-11 22:09:24.973866:  
2024-11-11 22:09:24.976466: Epoch 424 
2024-11-11 22:09:24.978852: Current learning rate: 0.00609 
2024-11-11 22:10:04.605255: train_loss -0.8882 
2024-11-11 22:10:04.610358: val_loss -0.7914 
2024-11-11 22:10:04.613382: Pseudo dice [np.float32(0.9472), np.float32(0.7516)] 
2024-11-11 22:10:04.615918: Epoch time: 39.63 s 
2024-11-11 22:10:05.742591:  
2024-11-11 22:10:05.745001: Epoch 425 
2024-11-11 22:10:05.747397: Current learning rate: 0.00608 
2024-11-11 22:10:45.383922: train_loss -0.8987 
2024-11-11 22:10:45.387235: val_loss -0.8689 
2024-11-11 22:10:45.389832: Pseudo dice [np.float32(0.9561), np.float32(0.8497)] 
2024-11-11 22:10:45.392543: Epoch time: 39.64 s 
2024-11-11 22:10:45.395037: Yayy! New best EMA pseudo Dice: 0.8744999766349792 
2024-11-11 22:10:47.305216:  
2024-11-11 22:10:47.307782: Epoch 426 
2024-11-11 22:10:47.310143: Current learning rate: 0.00607 
2024-11-11 22:11:26.934753: train_loss -0.8875 
2024-11-11 22:11:26.939431: val_loss -0.7557 
2024-11-11 22:11:26.941770: Pseudo dice [np.float32(0.9471), np.float32(0.7484)] 
2024-11-11 22:11:26.944278: Epoch time: 39.63 s 
2024-11-11 22:11:28.065832:  
2024-11-11 22:11:28.070491: Epoch 427 
2024-11-11 22:11:28.072805: Current learning rate: 0.00606 
2024-11-11 22:12:07.709784: train_loss -0.8983 
2024-11-11 22:12:07.718498: val_loss -0.7635 
2024-11-11 22:12:07.720922: Pseudo dice [np.float32(0.9459), np.float32(0.7262)] 
2024-11-11 22:12:07.723166: Epoch time: 39.65 s 
2024-11-11 22:12:09.179955:  
2024-11-11 22:12:09.182666: Epoch 428 
2024-11-11 22:12:09.185226: Current learning rate: 0.00605 
2024-11-11 22:12:48.841221: train_loss -0.9057 
2024-11-11 22:12:48.845921: val_loss -0.828 
2024-11-11 22:12:48.848305: Pseudo dice [np.float32(0.9547), np.float32(0.8062)] 
2024-11-11 22:12:48.850475: Epoch time: 39.66 s 
2024-11-11 22:12:49.976317:  
2024-11-11 22:12:49.978659: Epoch 429 
2024-11-11 22:12:49.980910: Current learning rate: 0.00604 
2024-11-11 22:13:29.642106: train_loss -0.9003 
2024-11-11 22:13:29.649832: val_loss -0.7242 
2024-11-11 22:13:29.652134: Pseudo dice [np.float32(0.9406), np.float32(0.6979)] 
2024-11-11 22:13:29.654462: Epoch time: 39.67 s 
2024-11-11 22:13:30.780294:  
2024-11-11 22:13:30.782757: Epoch 430 
2024-11-11 22:13:30.785251: Current learning rate: 0.00603 
2024-11-11 22:14:10.441829: train_loss -0.8967 
2024-11-11 22:14:10.446424: val_loss -0.8008 
2024-11-11 22:14:10.448705: Pseudo dice [np.float32(0.9574), np.float32(0.7663)] 
2024-11-11 22:14:10.450978: Epoch time: 39.66 s 
2024-11-11 22:14:11.574077:  
2024-11-11 22:14:11.576506: Epoch 431 
2024-11-11 22:14:11.578967: Current learning rate: 0.00602 
2024-11-11 22:14:51.246905: train_loss -0.9005 
2024-11-11 22:14:51.250357: val_loss -0.7865 
2024-11-11 22:14:51.252918: Pseudo dice [np.float32(0.9574), np.float32(0.7578)] 
2024-11-11 22:14:51.255476: Epoch time: 39.67 s 
2024-11-11 22:14:52.383234:  
2024-11-11 22:14:52.385945: Epoch 432 
2024-11-11 22:14:52.388354: Current learning rate: 0.00601 
2024-11-11 22:15:32.060082: train_loss -0.9033 
2024-11-11 22:15:32.064764: val_loss -0.7193 
2024-11-11 22:15:32.067055: Pseudo dice [np.float32(0.9479), np.float32(0.5801)] 
2024-11-11 22:15:32.070258: Epoch time: 39.68 s 
2024-11-11 22:15:33.198029:  
2024-11-11 22:15:33.200731: Epoch 433 
2024-11-11 22:15:33.203082: Current learning rate: 0.006 
2024-11-11 22:16:12.872032: train_loss -0.881 
2024-11-11 22:16:12.875390: val_loss -0.7973 
2024-11-11 22:16:12.877709: Pseudo dice [np.float32(0.9539), np.float32(0.757)] 
2024-11-11 22:16:12.880123: Epoch time: 39.68 s 
2024-11-11 22:16:14.008112:  
2024-11-11 22:16:14.010483: Epoch 434 
2024-11-11 22:16:14.012917: Current learning rate: 0.00599 
2024-11-11 22:16:53.680382: train_loss -0.8976 
2024-11-11 22:16:53.685204: val_loss -0.8155 
2024-11-11 22:16:53.687610: Pseudo dice [np.float32(0.9601), np.float32(0.8216)] 
2024-11-11 22:16:53.689902: Epoch time: 39.67 s 
2024-11-11 22:16:54.816654:  
2024-11-11 22:16:54.819172: Epoch 435 
2024-11-11 22:16:54.821819: Current learning rate: 0.00598 
2024-11-11 22:17:34.486742: train_loss -0.9108 
2024-11-11 22:17:34.490165: val_loss -0.8076 
2024-11-11 22:17:34.492666: Pseudo dice [np.float32(0.9583), np.float32(0.8028)] 
2024-11-11 22:17:34.494958: Epoch time: 39.67 s 
2024-11-11 22:17:35.620265:  
2024-11-11 22:17:35.622582: Epoch 436 
2024-11-11 22:17:35.625144: Current learning rate: 0.00597 
2024-11-11 22:18:15.300648: train_loss -0.8988 
2024-11-11 22:18:15.305262: val_loss -0.8308 
2024-11-11 22:18:15.307386: Pseudo dice [np.float32(0.9567), np.float32(0.8207)] 
2024-11-11 22:18:15.309835: Epoch time: 39.68 s 
2024-11-11 22:18:16.435408:  
2024-11-11 22:18:16.438054: Epoch 437 
2024-11-11 22:18:16.440534: Current learning rate: 0.00596 
2024-11-11 22:18:56.085814: train_loss -0.8969 
2024-11-11 22:18:56.089279: val_loss -0.7627 
2024-11-11 22:18:56.091580: Pseudo dice [np.float32(0.9557), np.float32(0.7416)] 
2024-11-11 22:18:56.093816: Epoch time: 39.65 s 
2024-11-11 22:18:57.221119:  
2024-11-11 22:18:57.223775: Epoch 438 
2024-11-11 22:18:57.226190: Current learning rate: 0.00595 
2024-11-11 22:19:36.866952: train_loss -0.9172 
2024-11-11 22:19:36.872092: val_loss -0.7897 
2024-11-11 22:19:36.874775: Pseudo dice [np.float32(0.961), np.float32(0.7728)] 
2024-11-11 22:19:36.877322: Epoch time: 39.65 s 
2024-11-11 22:19:38.003101:  
2024-11-11 22:19:38.005826: Epoch 439 
2024-11-11 22:19:38.008358: Current learning rate: 0.00594 
2024-11-11 22:20:17.642512: train_loss -0.9301 
2024-11-11 22:20:17.646224: val_loss -0.8547 
2024-11-11 22:20:17.648648: Pseudo dice [np.float32(0.9633), np.float32(0.8382)] 
2024-11-11 22:20:17.651027: Epoch time: 39.64 s 
2024-11-11 22:20:18.777287:  
2024-11-11 22:20:18.779834: Epoch 440 
2024-11-11 22:20:18.782313: Current learning rate: 0.00593 
2024-11-11 22:20:58.410315: train_loss -0.9 
2024-11-11 22:20:58.414968: val_loss -0.7489 
2024-11-11 22:20:58.417329: Pseudo dice [np.float32(0.9422), np.float32(0.696)] 
2024-11-11 22:20:58.419684: Epoch time: 39.63 s 
2024-11-11 22:20:59.878414:  
2024-11-11 22:20:59.881403: Epoch 441 
2024-11-11 22:20:59.884309: Current learning rate: 0.00592 
2024-11-11 22:21:39.534579: train_loss -0.9126 
2024-11-11 22:21:39.537934: val_loss -0.8444 
2024-11-11 22:21:39.540231: Pseudo dice [np.float32(0.9571), np.float32(0.8046)] 
2024-11-11 22:21:39.542368: Epoch time: 39.66 s 
2024-11-11 22:21:40.667302:  
2024-11-11 22:21:40.669740: Epoch 442 
2024-11-11 22:21:40.672096: Current learning rate: 0.00592 
2024-11-11 22:22:20.312133: train_loss -0.9156 
2024-11-11 22:22:20.316682: val_loss -0.7518 
2024-11-11 22:22:20.319190: Pseudo dice [np.float32(0.9432), np.float32(0.6292)] 
2024-11-11 22:22:20.321623: Epoch time: 39.65 s 
2024-11-11 22:22:21.449432:  
2024-11-11 22:22:21.453712: Epoch 443 
2024-11-11 22:22:21.456453: Current learning rate: 0.00591 
2024-11-11 22:23:01.098860: train_loss -0.9131 
2024-11-11 22:23:01.102150: val_loss -0.7774 
2024-11-11 22:23:01.104311: Pseudo dice [np.float32(0.9512), np.float32(0.7502)] 
2024-11-11 22:23:01.107019: Epoch time: 39.65 s 
2024-11-11 22:23:02.223708:  
2024-11-11 22:23:02.226664: Epoch 444 
2024-11-11 22:23:02.229440: Current learning rate: 0.0059 
2024-11-11 22:23:41.875324: train_loss -0.9019 
2024-11-11 22:23:41.880237: val_loss -0.8385 
2024-11-11 22:23:41.882757: Pseudo dice [np.float32(0.9571), np.float32(0.8081)] 
2024-11-11 22:23:41.885070: Epoch time: 39.65 s 
2024-11-11 22:23:43.001609:  
2024-11-11 22:23:43.004280: Epoch 445 
2024-11-11 22:23:43.006857: Current learning rate: 0.00589 
2024-11-11 22:24:22.661621: train_loss -0.9015 
2024-11-11 22:24:22.664747: val_loss -0.8381 
2024-11-11 22:24:22.667205: Pseudo dice [np.float32(0.9569), np.float32(0.8035)] 
2024-11-11 22:24:22.669708: Epoch time: 39.66 s 
2024-11-11 22:24:23.788438:  
2024-11-11 22:24:23.791054: Epoch 446 
2024-11-11 22:24:23.793330: Current learning rate: 0.00588 
2024-11-11 22:25:03.454356: train_loss -0.9067 
2024-11-11 22:25:03.458848: val_loss -0.8356 
2024-11-11 22:25:03.461051: Pseudo dice [np.float32(0.9549), np.float32(0.7907)] 
2024-11-11 22:25:03.463285: Epoch time: 39.67 s 
2024-11-11 22:25:04.579009:  
2024-11-11 22:25:04.581892: Epoch 447 
2024-11-11 22:25:04.584313: Current learning rate: 0.00587 
2024-11-11 22:25:44.239955: train_loss -0.9111 
2024-11-11 22:25:44.243359: val_loss -0.837 
2024-11-11 22:25:44.245659: Pseudo dice [np.float32(0.9542), np.float32(0.8065)] 
2024-11-11 22:25:44.247897: Epoch time: 39.66 s 
2024-11-11 22:25:45.362542:  
2024-11-11 22:25:45.365174: Epoch 448 
2024-11-11 22:25:45.367802: Current learning rate: 0.00586 
2024-11-11 22:26:25.057393: train_loss -0.8965 
2024-11-11 22:26:25.066434: val_loss -0.733 
2024-11-11 22:26:25.069045: Pseudo dice [np.float32(0.9353), np.float32(0.6621)] 
2024-11-11 22:26:25.071342: Epoch time: 39.7 s 
2024-11-11 22:26:26.184118:  
2024-11-11 22:26:26.186444: Epoch 449 
2024-11-11 22:26:26.188906: Current learning rate: 0.00585 
2024-11-11 22:27:05.855636: train_loss -0.9078 
2024-11-11 22:27:05.859200: val_loss -0.7735 
2024-11-11 22:27:05.861673: Pseudo dice [np.float32(0.954), np.float32(0.743)] 
2024-11-11 22:27:05.863952: Epoch time: 39.67 s 
2024-11-11 22:27:07.759144:  
2024-11-11 22:27:07.761811: Epoch 450 
2024-11-11 22:27:07.764216: Current learning rate: 0.00584 
2024-11-11 22:27:47.412687: train_loss -0.9245 
2024-11-11 22:27:47.417420: val_loss -0.8259 
2024-11-11 22:27:47.419971: Pseudo dice [np.float32(0.9525), np.float32(0.7927)] 
2024-11-11 22:27:47.422418: Epoch time: 39.65 s 
2024-11-11 22:27:48.554753:  
2024-11-11 22:27:48.557163: Epoch 451 
2024-11-11 22:27:48.559431: Current learning rate: 0.00583 
2024-11-11 22:28:28.233392: train_loss -0.9157 
2024-11-11 22:28:28.237640: val_loss -0.817 
2024-11-11 22:28:28.240054: Pseudo dice [np.float32(0.9527), np.float32(0.7826)] 
2024-11-11 22:28:28.242454: Epoch time: 39.68 s 
2024-11-11 22:28:29.354232:  
2024-11-11 22:28:29.356880: Epoch 452 
2024-11-11 22:28:29.359164: Current learning rate: 0.00582 
2024-11-11 22:29:08.986597: train_loss -0.9162 
2024-11-11 22:29:08.991055: val_loss -0.7775 
2024-11-11 22:29:08.993239: Pseudo dice [np.float32(0.9515), np.float32(0.7075)] 
2024-11-11 22:29:08.995500: Epoch time: 39.63 s 
2024-11-11 22:29:10.109401:  
2024-11-11 22:29:10.111795: Epoch 453 
2024-11-11 22:29:10.114184: Current learning rate: 0.00581 
2024-11-11 22:29:49.747472: train_loss -0.9259 
2024-11-11 22:29:49.750765: val_loss -0.8362 
2024-11-11 22:29:49.753170: Pseudo dice [np.float32(0.9643), np.float32(0.8021)] 
2024-11-11 22:29:49.755522: Epoch time: 39.64 s 
2024-11-11 22:29:50.870262:  
2024-11-11 22:29:50.872963: Epoch 454 
2024-11-11 22:29:50.875502: Current learning rate: 0.0058 
2024-11-11 22:30:30.528224: train_loss -0.9155 
2024-11-11 22:30:30.533642: val_loss -0.8316 
2024-11-11 22:30:30.535876: Pseudo dice [np.float32(0.9538), np.float32(0.8126)] 
2024-11-11 22:30:30.538417: Epoch time: 39.66 s 
2024-11-11 22:30:32.006107:  
2024-11-11 22:30:32.008579: Epoch 455 
2024-11-11 22:30:32.010991: Current learning rate: 0.00579 
2024-11-11 22:31:11.677325: train_loss -0.8983 
2024-11-11 22:31:11.681164: val_loss -0.8098 
2024-11-11 22:31:11.683588: Pseudo dice [np.float32(0.9464), np.float32(0.7929)] 
2024-11-11 22:31:11.685879: Epoch time: 39.67 s 
2024-11-11 22:31:12.805107:  
2024-11-11 22:31:12.807623: Epoch 456 
2024-11-11 22:31:12.810031: Current learning rate: 0.00578 
2024-11-11 22:31:52.467592: train_loss -0.8955 
2024-11-11 22:31:52.472249: val_loss -0.8519 
2024-11-11 22:31:52.474436: Pseudo dice [np.float32(0.957), np.float32(0.8269)] 
2024-11-11 22:31:52.476610: Epoch time: 39.66 s 
2024-11-11 22:31:53.595951:  
2024-11-11 22:31:53.598357: Epoch 457 
2024-11-11 22:31:53.600740: Current learning rate: 0.00577 
2024-11-11 22:32:33.239892: train_loss -0.9044 
2024-11-11 22:32:33.248849: val_loss -0.8323 
2024-11-11 22:32:33.251179: Pseudo dice [np.float32(0.9555), np.float32(0.8093)] 
2024-11-11 22:32:33.253399: Epoch time: 39.65 s 
2024-11-11 22:32:34.373178:  
2024-11-11 22:32:34.375533: Epoch 458 
2024-11-11 22:32:34.377836: Current learning rate: 0.00576 
2024-11-11 22:33:14.028652: train_loss -0.911 
2024-11-11 22:33:14.033246: val_loss -0.8144 
2024-11-11 22:33:14.035682: Pseudo dice [np.float32(0.9586), np.float32(0.7713)] 
2024-11-11 22:33:14.038100: Epoch time: 39.66 s 
2024-11-11 22:33:15.157943:  
2024-11-11 22:33:15.160398: Epoch 459 
2024-11-11 22:33:15.162850: Current learning rate: 0.00575 
2024-11-11 22:33:54.819075: train_loss -0.9149 
2024-11-11 22:33:54.822342: val_loss -0.7177 
2024-11-11 22:33:54.824765: Pseudo dice [np.float32(0.9356), np.float32(0.6374)] 
2024-11-11 22:33:54.827003: Epoch time: 39.66 s 
2024-11-11 22:33:55.946234:  
2024-11-11 22:33:55.948567: Epoch 460 
2024-11-11 22:33:55.950938: Current learning rate: 0.00574 
2024-11-11 22:34:35.608410: train_loss -0.9237 
2024-11-11 22:34:35.613276: val_loss -0.8422 
2024-11-11 22:34:35.615669: Pseudo dice [np.float32(0.9543), np.float32(0.8069)] 
2024-11-11 22:34:35.618239: Epoch time: 39.66 s 
2024-11-11 22:34:36.737508:  
2024-11-11 22:34:36.739727: Epoch 461 
2024-11-11 22:34:36.742204: Current learning rate: 0.00573 
2024-11-11 22:35:16.393758: train_loss -0.9162 
2024-11-11 22:35:16.397102: val_loss -0.8376 
2024-11-11 22:35:16.399464: Pseudo dice [np.float32(0.9585), np.float32(0.8023)] 
2024-11-11 22:35:16.401833: Epoch time: 39.66 s 
2024-11-11 22:35:17.517186:  
2024-11-11 22:35:17.519772: Epoch 462 
2024-11-11 22:35:17.522174: Current learning rate: 0.00572 
2024-11-11 22:35:57.177511: train_loss -0.9246 
2024-11-11 22:35:57.182081: val_loss -0.8664 
2024-11-11 22:35:57.184391: Pseudo dice [np.float32(0.9616), np.float32(0.8562)] 
2024-11-11 22:35:57.186632: Epoch time: 39.66 s 
2024-11-11 22:35:58.306613:  
2024-11-11 22:35:58.309339: Epoch 463 
2024-11-11 22:35:58.311793: Current learning rate: 0.00571 
2024-11-11 22:36:37.972139: train_loss -0.9203 
2024-11-11 22:36:37.980620: val_loss -0.8023 
2024-11-11 22:36:37.982889: Pseudo dice [np.float32(0.9546), np.float32(0.7769)] 
2024-11-11 22:36:37.985229: Epoch time: 39.67 s 
2024-11-11 22:36:39.099354:  
2024-11-11 22:36:39.101841: Epoch 464 
2024-11-11 22:36:39.104008: Current learning rate: 0.0057 
2024-11-11 22:37:18.775793: train_loss -0.9145 
2024-11-11 22:37:18.780757: val_loss -0.7352 
2024-11-11 22:37:18.783039: Pseudo dice [np.float32(0.9462), np.float32(0.6548)] 
2024-11-11 22:37:18.785555: Epoch time: 39.68 s 
2024-11-11 22:37:19.909774:  
2024-11-11 22:37:19.912406: Epoch 465 
2024-11-11 22:37:19.914729: Current learning rate: 0.0057 
2024-11-11 22:37:59.569684: train_loss -0.908 
2024-11-11 22:37:59.573243: val_loss -0.8425 
2024-11-11 22:37:59.575627: Pseudo dice [np.float32(0.9531), np.float32(0.8081)] 
2024-11-11 22:37:59.577846: Epoch time: 39.66 s 
2024-11-11 22:38:00.697854:  
2024-11-11 22:38:00.700350: Epoch 466 
2024-11-11 22:38:00.702781: Current learning rate: 0.00569 
2024-11-11 22:38:40.358612: train_loss -0.9223 
2024-11-11 22:38:40.363328: val_loss -0.8081 
2024-11-11 22:38:40.365770: Pseudo dice [np.float32(0.9604), np.float32(0.7791)] 
2024-11-11 22:38:40.367990: Epoch time: 39.66 s 
2024-11-11 22:38:41.483177:  
2024-11-11 22:38:41.485829: Epoch 467 
2024-11-11 22:38:41.488440: Current learning rate: 0.00568 
2024-11-11 22:39:21.172995: train_loss -0.9191 
2024-11-11 22:39:21.176299: val_loss -0.8536 
2024-11-11 22:39:21.178559: Pseudo dice [np.float32(0.9618), np.float32(0.8689)] 
2024-11-11 22:39:21.180793: Epoch time: 39.69 s 
2024-11-11 22:39:22.636690:  
2024-11-11 22:39:22.640859: Epoch 468 
2024-11-11 22:39:22.643388: Current learning rate: 0.00567 
2024-11-11 22:40:02.321725: train_loss -0.9275 
2024-11-11 22:40:02.326249: val_loss -0.7562 
2024-11-11 22:40:02.328613: Pseudo dice [np.float32(0.9473), np.float32(0.6926)] 
2024-11-11 22:40:02.330889: Epoch time: 39.69 s 
2024-11-11 22:40:03.449177:  
2024-11-11 22:40:03.451596: Epoch 469 
2024-11-11 22:40:03.454165: Current learning rate: 0.00566 
2024-11-11 22:40:43.125455: train_loss -0.9098 
2024-11-11 22:40:43.134571: val_loss -0.8248 
2024-11-11 22:40:43.137127: Pseudo dice [np.float32(0.9556), np.float32(0.8049)] 
2024-11-11 22:40:43.139429: Epoch time: 39.68 s 
2024-11-11 22:40:44.256112:  
2024-11-11 22:40:44.258435: Epoch 470 
2024-11-11 22:40:44.260824: Current learning rate: 0.00565 
2024-11-11 22:41:23.907279: train_loss -0.9185 
2024-11-11 22:41:23.912336: val_loss -0.782 
2024-11-11 22:41:23.915103: Pseudo dice [np.float32(0.9427), np.float32(0.6802)] 
2024-11-11 22:41:23.917799: Epoch time: 39.65 s 
2024-11-11 22:41:25.038802:  
2024-11-11 22:41:25.041028: Epoch 471 
2024-11-11 22:41:25.043554: Current learning rate: 0.00564 
2024-11-11 22:42:04.684456: train_loss -0.921 
2024-11-11 22:42:04.688556: val_loss -0.7839 
2024-11-11 22:42:04.691105: Pseudo dice [np.float32(0.9459), np.float32(0.7197)] 
2024-11-11 22:42:04.693770: Epoch time: 39.65 s 
2024-11-11 22:42:05.814522:  
2024-11-11 22:42:05.817068: Epoch 472 
2024-11-11 22:42:05.819437: Current learning rate: 0.00563 
2024-11-11 22:42:45.466160: train_loss -0.9105 
2024-11-11 22:42:45.471255: val_loss -0.7752 
2024-11-11 22:42:45.473747: Pseudo dice [np.float32(0.9505), np.float32(0.7319)] 
2024-11-11 22:42:45.476105: Epoch time: 39.65 s 
2024-11-11 22:42:46.600853:  
2024-11-11 22:42:46.603598: Epoch 473 
2024-11-11 22:42:46.606188: Current learning rate: 0.00562 
2024-11-11 22:43:26.244903: train_loss -0.9252 
2024-11-11 22:43:26.248325: val_loss -0.8296 
2024-11-11 22:43:26.250815: Pseudo dice [np.float32(0.9605), np.float32(0.808)] 
2024-11-11 22:43:26.253118: Epoch time: 39.65 s 
2024-11-11 22:43:27.377052:  
2024-11-11 22:43:27.379547: Epoch 474 
2024-11-11 22:43:27.382095: Current learning rate: 0.00561 
2024-11-11 22:44:07.009218: train_loss -0.9282 
2024-11-11 22:44:07.014270: val_loss -0.8099 
2024-11-11 22:44:07.016657: Pseudo dice [np.float32(0.9585), np.float32(0.7863)] 
2024-11-11 22:44:07.018901: Epoch time: 39.63 s 
2024-11-11 22:44:08.138227:  
2024-11-11 22:44:08.140740: Epoch 475 
2024-11-11 22:44:08.143228: Current learning rate: 0.0056 
2024-11-11 22:44:47.782819: train_loss -0.9339 
2024-11-11 22:44:47.796809: val_loss -0.8364 
2024-11-11 22:44:47.799216: Pseudo dice [np.float32(0.9594), np.float32(0.8257)] 
2024-11-11 22:44:47.801642: Epoch time: 39.65 s 
2024-11-11 22:44:48.919156:  
2024-11-11 22:44:48.921584: Epoch 476 
2024-11-11 22:44:48.924018: Current learning rate: 0.00559 
2024-11-11 22:45:28.581980: train_loss -0.9326 
2024-11-11 22:45:28.586495: val_loss -0.8218 
2024-11-11 22:45:28.588809: Pseudo dice [np.float32(0.954), np.float32(0.7938)] 
2024-11-11 22:45:28.591217: Epoch time: 39.66 s 
2024-11-11 22:45:29.714122:  
2024-11-11 22:45:29.716711: Epoch 477 
2024-11-11 22:45:29.719353: Current learning rate: 0.00558 
2024-11-11 22:46:09.352230: train_loss -0.9383 
2024-11-11 22:46:09.355757: val_loss -0.8512 
2024-11-11 22:46:09.358110: Pseudo dice [np.float32(0.9642), np.float32(0.8289)] 
2024-11-11 22:46:09.360324: Epoch time: 39.64 s 
2024-11-11 22:46:10.506235:  
2024-11-11 22:46:10.509430: Epoch 478 
2024-11-11 22:46:10.511828: Current learning rate: 0.00557 
2024-11-11 22:46:50.133025: train_loss -0.9202 
2024-11-11 22:46:50.137569: val_loss -0.8663 
2024-11-11 22:46:50.139971: Pseudo dice [np.float32(0.9601), np.float32(0.8425)] 
2024-11-11 22:46:50.142495: Epoch time: 39.63 s 
2024-11-11 22:46:51.286988:  
2024-11-11 22:46:51.289593: Epoch 479 
2024-11-11 22:46:51.291943: Current learning rate: 0.00556 
2024-11-11 22:47:30.945966: train_loss -0.9277 
2024-11-11 22:47:30.949514: val_loss -0.7999 
2024-11-11 22:47:30.952127: Pseudo dice [np.float32(0.9575), np.float32(0.7971)] 
2024-11-11 22:47:30.954567: Epoch time: 39.66 s 
2024-11-11 22:47:32.087627:  
2024-11-11 22:47:32.090054: Epoch 480 
2024-11-11 22:47:32.092475: Current learning rate: 0.00555 
2024-11-11 22:48:11.720660: train_loss -0.9301 
2024-11-11 22:48:11.725046: val_loss -0.8023 
2024-11-11 22:48:11.727399: Pseudo dice [np.float32(0.9573), np.float32(0.8091)] 
2024-11-11 22:48:11.729557: Epoch time: 39.63 s 
2024-11-11 22:48:13.201421:  
2024-11-11 22:48:13.203819: Epoch 481 
2024-11-11 22:48:13.206102: Current learning rate: 0.00554 
2024-11-11 22:48:52.898074: train_loss -0.9293 
2024-11-11 22:48:52.901369: val_loss -0.8238 
2024-11-11 22:48:52.903725: Pseudo dice [np.float32(0.9574), np.float32(0.797)] 
2024-11-11 22:48:52.906201: Epoch time: 39.7 s 
2024-11-11 22:48:54.036706:  
2024-11-11 22:48:54.039221: Epoch 482 
2024-11-11 22:48:54.041793: Current learning rate: 0.00553 
2024-11-11 22:49:33.697294: train_loss -0.9331 
2024-11-11 22:49:33.701948: val_loss -0.8009 
2024-11-11 22:49:33.704173: Pseudo dice [np.float32(0.9562), np.float32(0.7724)] 
2024-11-11 22:49:33.706490: Epoch time: 39.66 s 
2024-11-11 22:49:34.844217:  
2024-11-11 22:49:34.846691: Epoch 483 
2024-11-11 22:49:34.849496: Current learning rate: 0.00552 
2024-11-11 22:50:14.531459: train_loss -0.9041 
2024-11-11 22:50:14.534775: val_loss -0.8436 
2024-11-11 22:50:14.537158: Pseudo dice [np.float32(0.9498), np.float32(0.8052)] 
2024-11-11 22:50:14.539164: Epoch time: 39.69 s 
2024-11-11 22:50:15.672067:  
2024-11-11 22:50:15.674653: Epoch 484 
2024-11-11 22:50:15.676936: Current learning rate: 0.00551 
2024-11-11 22:50:55.370666: train_loss -0.8751 
2024-11-11 22:50:55.375681: val_loss -0.7965 
2024-11-11 22:50:55.378183: Pseudo dice [np.float32(0.9451), np.float32(0.7413)] 
2024-11-11 22:50:55.380577: Epoch time: 39.7 s 
2024-11-11 22:50:56.515368:  
2024-11-11 22:50:56.517936: Epoch 485 
2024-11-11 22:50:56.520363: Current learning rate: 0.0055 
2024-11-11 22:51:36.194372: train_loss -0.9039 
2024-11-11 22:51:36.197934: val_loss -0.8141 
2024-11-11 22:51:36.200302: Pseudo dice [np.float32(0.9581), np.float32(0.7955)] 
2024-11-11 22:51:36.202680: Epoch time: 39.68 s 
2024-11-11 22:51:37.334969:  
2024-11-11 22:51:37.337538: Epoch 486 
2024-11-11 22:51:37.339844: Current learning rate: 0.00549 
2024-11-11 22:52:17.012306: train_loss -0.9115 
2024-11-11 22:52:17.017301: val_loss -0.7899 
2024-11-11 22:52:17.019590: Pseudo dice [np.float32(0.9562), np.float32(0.7428)] 
2024-11-11 22:52:17.022068: Epoch time: 39.68 s 
2024-11-11 22:52:18.160728:  
2024-11-11 22:52:18.163290: Epoch 487 
2024-11-11 22:52:18.165627: Current learning rate: 0.00548 
2024-11-11 22:52:57.846174: train_loss -0.9089 
2024-11-11 22:52:57.849742: val_loss -0.8127 
2024-11-11 22:52:57.852036: Pseudo dice [np.float32(0.9572), np.float32(0.7863)] 
2024-11-11 22:52:57.854461: Epoch time: 39.69 s 
2024-11-11 22:52:58.993212:  
2024-11-11 22:52:58.995702: Epoch 488 
2024-11-11 22:52:58.999534: Current learning rate: 0.00547 
2024-11-11 22:53:38.673545: train_loss -0.9142 
2024-11-11 22:53:38.677912: val_loss -0.7984 
2024-11-11 22:53:38.680195: Pseudo dice [np.float32(0.95), np.float32(0.7473)] 
2024-11-11 22:53:38.682388: Epoch time: 39.68 s 
2024-11-11 22:53:39.821822:  
2024-11-11 22:53:39.824316: Epoch 489 
2024-11-11 22:53:39.826702: Current learning rate: 0.00546 
2024-11-11 22:54:19.378713: train_loss -0.9082 
2024-11-11 22:54:19.382297: val_loss -0.7713 
2024-11-11 22:54:19.384550: Pseudo dice [np.float32(0.9542), np.float32(0.7254)] 
2024-11-11 22:54:19.386781: Epoch time: 39.56 s 
2024-11-11 22:54:20.523488:  
2024-11-11 22:54:20.526114: Epoch 490 
2024-11-11 22:54:20.528754: Current learning rate: 0.00546 
2024-11-11 22:55:00.090891: train_loss -0.9306 
2024-11-11 22:55:00.095157: val_loss -0.7757 
2024-11-11 22:55:00.097668: Pseudo dice [np.float32(0.9456), np.float32(0.7252)] 
2024-11-11 22:55:00.100261: Epoch time: 39.57 s 
2024-11-11 22:55:01.240334:  
2024-11-11 22:55:01.242920: Epoch 491 
2024-11-11 22:55:01.245465: Current learning rate: 0.00545 
2024-11-11 22:55:40.804336: train_loss -0.9283 
2024-11-11 22:55:40.807734: val_loss -0.8003 
2024-11-11 22:55:40.810213: Pseudo dice [np.float32(0.9569), np.float32(0.7892)] 
2024-11-11 22:55:40.812702: Epoch time: 39.57 s 
2024-11-11 22:55:41.951330:  
2024-11-11 22:55:41.954157: Epoch 492 
2024-11-11 22:55:41.956732: Current learning rate: 0.00544 
2024-11-11 22:56:21.517477: train_loss -0.9278 
2024-11-11 22:56:21.521845: val_loss -0.8409 
2024-11-11 22:56:21.523997: Pseudo dice [np.float32(0.9642), np.float32(0.8296)] 
2024-11-11 22:56:21.526178: Epoch time: 39.57 s 
2024-11-11 22:56:22.659730:  
2024-11-11 22:56:22.662095: Epoch 493 
2024-11-11 22:56:22.664505: Current learning rate: 0.00543 
2024-11-11 22:57:02.240931: train_loss -0.9211 
2024-11-11 22:57:02.244182: val_loss -0.8147 
2024-11-11 22:57:02.246714: Pseudo dice [np.float32(0.9585), np.float32(0.807)] 
2024-11-11 22:57:02.249169: Epoch time: 39.58 s 
2024-11-11 22:57:03.721990:  
2024-11-11 22:57:03.724372: Epoch 494 
2024-11-11 22:57:03.727159: Current learning rate: 0.00542 
2024-11-11 22:57:43.289396: train_loss -0.9062 
2024-11-11 22:57:43.294075: val_loss -0.8206 
2024-11-11 22:57:43.296554: Pseudo dice [np.float32(0.9586), np.float32(0.7738)] 
2024-11-11 22:57:43.298950: Epoch time: 39.57 s 
2024-11-11 22:57:44.433878:  
2024-11-11 22:57:44.436630: Epoch 495 
2024-11-11 22:57:44.439398: Current learning rate: 0.00541 
2024-11-11 22:58:23.975661: train_loss -0.9128 
2024-11-11 22:58:23.979124: val_loss -0.8206 
2024-11-11 22:58:23.981850: Pseudo dice [np.float32(0.955), np.float32(0.7756)] 
2024-11-11 22:58:23.984253: Epoch time: 39.54 s 
2024-11-11 22:58:25.132818:  
2024-11-11 22:58:25.135401: Epoch 496 
2024-11-11 22:58:25.137957: Current learning rate: 0.0054 
2024-11-11 22:59:04.689043: train_loss -0.9033 
2024-11-11 22:59:04.693542: val_loss -0.8024 
2024-11-11 22:59:04.695853: Pseudo dice [np.float32(0.9491), np.float32(0.7792)] 
2024-11-11 22:59:04.698024: Epoch time: 39.56 s 
2024-11-11 22:59:05.844110:  
2024-11-11 22:59:05.846603: Epoch 497 
2024-11-11 22:59:05.849568: Current learning rate: 0.00539 
2024-11-11 22:59:45.422414: train_loss -0.891 
2024-11-11 22:59:45.425456: val_loss -0.7708 
2024-11-11 22:59:45.427789: Pseudo dice [np.float32(0.954), np.float32(0.7077)] 
2024-11-11 22:59:45.430110: Epoch time: 39.58 s 
2024-11-11 22:59:46.572798:  
2024-11-11 22:59:46.575507: Epoch 498 
2024-11-11 22:59:46.577832: Current learning rate: 0.00538 
2024-11-11 23:00:26.141988: train_loss -0.9119 
2024-11-11 23:00:26.146699: val_loss -0.863 
2024-11-11 23:00:26.149197: Pseudo dice [np.float32(0.9505), np.float32(0.8262)] 
2024-11-11 23:00:26.151418: Epoch time: 39.57 s 
2024-11-11 23:00:27.288695:  
2024-11-11 23:00:27.291101: Epoch 499 
2024-11-11 23:00:27.293523: Current learning rate: 0.00537 
2024-11-11 23:01:06.893339: train_loss -0.8986 
2024-11-11 23:01:06.896758: val_loss -0.7558 
2024-11-11 23:01:06.898848: Pseudo dice [np.float32(0.9446), np.float32(0.6843)] 
2024-11-11 23:01:06.901061: Epoch time: 39.61 s 
2024-11-11 23:01:08.838274:  
2024-11-11 23:01:08.840707: Epoch 500 
2024-11-11 23:01:08.843424: Current learning rate: 0.00536 
2024-11-11 23:01:48.419780: train_loss -0.9033 
2024-11-11 23:01:48.425931: val_loss -0.8159 
2024-11-11 23:01:48.428308: Pseudo dice [np.float32(0.9544), np.float32(0.7773)] 
2024-11-11 23:01:48.430599: Epoch time: 39.58 s 
2024-11-11 23:01:49.569067:  
2024-11-11 23:01:49.571660: Epoch 501 
2024-11-11 23:01:49.579631: Current learning rate: 0.00535 
2024-11-11 23:02:29.146393: train_loss -0.9071 
2024-11-11 23:02:29.149425: val_loss -0.7437 
2024-11-11 23:02:29.151830: Pseudo dice [np.float32(0.9411), np.float32(0.6528)] 
2024-11-11 23:02:29.154114: Epoch time: 39.58 s 
2024-11-11 23:02:30.292662:  
2024-11-11 23:02:30.297434: Epoch 502 
2024-11-11 23:02:30.299893: Current learning rate: 0.00534 
2024-11-11 23:03:09.870900: train_loss -0.9124 
2024-11-11 23:03:09.875340: val_loss -0.8383 
2024-11-11 23:03:09.877730: Pseudo dice [np.float32(0.9572), np.float32(0.8326)] 
2024-11-11 23:03:09.879877: Epoch time: 39.58 s 
2024-11-11 23:03:11.015107:  
2024-11-11 23:03:11.018007: Epoch 503 
2024-11-11 23:03:11.026369: Current learning rate: 0.00533 
2024-11-11 23:03:50.612806: train_loss -0.9144 
2024-11-11 23:03:50.616088: val_loss -0.8076 
2024-11-11 23:03:50.618667: Pseudo dice [np.float32(0.9613), np.float32(0.7995)] 
2024-11-11 23:03:50.620844: Epoch time: 39.6 s 
2024-11-11 23:03:51.751662:  
2024-11-11 23:03:51.754326: Epoch 504 
2024-11-11 23:03:51.763253: Current learning rate: 0.00532 
2024-11-11 23:04:31.337468: train_loss -0.9211 
2024-11-11 23:04:31.342178: val_loss -0.8179 
2024-11-11 23:04:31.344626: Pseudo dice [np.float32(0.958), np.float32(0.7815)] 
2024-11-11 23:04:31.347004: Epoch time: 39.59 s 
2024-11-11 23:04:32.485626:  
2024-11-11 23:04:32.490684: Epoch 505 
2024-11-11 23:04:32.493159: Current learning rate: 0.00531 
2024-11-11 23:05:12.078906: train_loss -0.9181 
2024-11-11 23:05:12.082283: val_loss -0.7825 
2024-11-11 23:05:12.084934: Pseudo dice [np.float32(0.9566), np.float32(0.7767)] 
2024-11-11 23:05:12.087123: Epoch time: 39.59 s 
2024-11-11 23:05:13.225478:  
2024-11-11 23:05:13.228052: Epoch 506 
2024-11-11 23:05:13.237204: Current learning rate: 0.0053 
2024-11-11 23:05:52.825638: train_loss -0.9237 
2024-11-11 23:05:52.829962: val_loss -0.7972 
2024-11-11 23:05:52.832121: Pseudo dice [np.float32(0.9518), np.float32(0.7431)] 
2024-11-11 23:05:52.834289: Epoch time: 39.6 s 
2024-11-11 23:05:54.310035:  
2024-11-11 23:05:54.312640: Epoch 507 
2024-11-11 23:05:54.320714: Current learning rate: 0.00529 
2024-11-11 23:06:33.944679: train_loss -0.9286 
2024-11-11 23:06:33.948029: val_loss -0.8254 
2024-11-11 23:06:33.950290: Pseudo dice [np.float32(0.9529), np.float32(0.8223)] 
2024-11-11 23:06:33.952512: Epoch time: 39.64 s 
2024-11-11 23:06:35.090176:  
2024-11-11 23:06:35.095522: Epoch 508 
2024-11-11 23:06:35.098007: Current learning rate: 0.00528 
2024-11-11 23:07:14.727782: train_loss -0.9276 
2024-11-11 23:07:14.732659: val_loss -0.831 
2024-11-11 23:07:14.735140: Pseudo dice [np.float32(0.9611), np.float32(0.8263)] 
2024-11-11 23:07:14.737466: Epoch time: 39.64 s 
2024-11-11 23:07:15.876948:  
2024-11-11 23:07:15.879632: Epoch 509 
2024-11-11 23:07:15.889580: Current learning rate: 0.00527 
2024-11-11 23:07:55.486485: train_loss -0.9214 
2024-11-11 23:07:55.489729: val_loss -0.7472 
2024-11-11 23:07:55.492047: Pseudo dice [np.float32(0.9456), np.float32(0.6669)] 
2024-11-11 23:07:55.494544: Epoch time: 39.61 s 
2024-11-11 23:07:56.634105:  
2024-11-11 23:07:56.636468: Epoch 510 
2024-11-11 23:07:56.638732: Current learning rate: 0.00526 
2024-11-11 23:08:36.266021: train_loss -0.917 
2024-11-11 23:08:36.271710: val_loss -0.8043 
2024-11-11 23:08:36.274817: Pseudo dice [np.float32(0.9554), np.float32(0.7591)] 
2024-11-11 23:08:36.277275: Epoch time: 39.63 s 
2024-11-11 23:08:37.416147:  
2024-11-11 23:08:37.418482: Epoch 511 
2024-11-11 23:08:37.420805: Current learning rate: 0.00525 
2024-11-11 23:09:17.042721: train_loss -0.9091 
2024-11-11 23:09:17.049585: val_loss -0.8073 
2024-11-11 23:09:17.051821: Pseudo dice [np.float32(0.9577), np.float32(0.8143)] 
2024-11-11 23:09:17.053931: Epoch time: 39.63 s 
2024-11-11 23:09:18.190906:  
2024-11-11 23:09:18.193401: Epoch 512 
2024-11-11 23:09:18.203551: Current learning rate: 0.00524 
2024-11-11 23:09:57.811589: train_loss -0.9268 
2024-11-11 23:09:57.816254: val_loss -0.8168 
2024-11-11 23:09:57.818417: Pseudo dice [np.float32(0.9613), np.float32(0.8093)] 
2024-11-11 23:09:57.820779: Epoch time: 39.62 s 
2024-11-11 23:09:58.958481:  
2024-11-11 23:09:58.960941: Epoch 513 
2024-11-11 23:09:58.963347: Current learning rate: 0.00523 
2024-11-11 23:10:38.558923: train_loss -0.9278 
2024-11-11 23:10:38.562572: val_loss -0.8434 
2024-11-11 23:10:38.564947: Pseudo dice [np.float32(0.9583), np.float32(0.8103)] 
2024-11-11 23:10:38.567200: Epoch time: 39.6 s 
2024-11-11 23:10:39.705112:  
2024-11-11 23:10:39.707693: Epoch 514 
2024-11-11 23:10:39.715755: Current learning rate: 0.00522 
2024-11-11 23:11:19.317497: train_loss -0.9287 
2024-11-11 23:11:19.323285: val_loss -0.8651 
2024-11-11 23:11:19.326217: Pseudo dice [np.float32(0.9611), np.float32(0.8544)] 
2024-11-11 23:11:19.328414: Epoch time: 39.61 s 
2024-11-11 23:11:20.462890:  
2024-11-11 23:11:20.468437: Epoch 515 
2024-11-11 23:11:20.471020: Current learning rate: 0.00521 
2024-11-11 23:12:00.045984: train_loss -0.929 
2024-11-11 23:12:00.049413: val_loss -0.8471 
2024-11-11 23:12:00.051959: Pseudo dice [np.float32(0.9558), np.float32(0.8159)] 
2024-11-11 23:12:00.054711: Epoch time: 39.58 s 
2024-11-11 23:12:01.187524:  
2024-11-11 23:12:01.192654: Epoch 516 
2024-11-11 23:12:01.194955: Current learning rate: 0.0052 
2024-11-11 23:12:40.775675: train_loss -0.9295 
2024-11-11 23:12:40.780804: val_loss -0.8487 
2024-11-11 23:12:40.783291: Pseudo dice [np.float32(0.9592), np.float32(0.8146)] 
2024-11-11 23:12:40.785891: Epoch time: 39.59 s 
2024-11-11 23:12:41.930045:  
2024-11-11 23:12:41.932585: Epoch 517 
2024-11-11 23:12:41.940250: Current learning rate: 0.00519 
2024-11-11 23:13:21.528843: train_loss -0.9277 
2024-11-11 23:13:21.532617: val_loss -0.7922 
2024-11-11 23:13:21.535093: Pseudo dice [np.float32(0.9544), np.float32(0.753)] 
2024-11-11 23:13:21.537509: Epoch time: 39.6 s 
2024-11-11 23:13:22.678486:  
2024-11-11 23:13:22.680889: Epoch 518 
2024-11-11 23:13:22.688485: Current learning rate: 0.00518 
2024-11-11 23:14:02.248518: train_loss -0.9339 
2024-11-11 23:14:02.253218: val_loss -0.8387 
2024-11-11 23:14:02.255613: Pseudo dice [np.float32(0.9606), np.float32(0.842)] 
2024-11-11 23:14:02.258486: Epoch time: 39.57 s 
2024-11-11 23:14:02.260771: Yayy! New best EMA pseudo Dice: 0.8748000264167786 
2024-11-11 23:14:04.203213:  
2024-11-11 23:14:04.209196: Epoch 519 
2024-11-11 23:14:04.211499: Current learning rate: 0.00518 
2024-11-11 23:14:43.765912: train_loss -0.9365 
2024-11-11 23:14:43.769801: val_loss -0.8618 
2024-11-11 23:14:43.772395: Pseudo dice [np.float32(0.964), np.float32(0.847)] 
2024-11-11 23:14:43.774776: Epoch time: 39.56 s 
2024-11-11 23:14:43.777036: Yayy! New best EMA pseudo Dice: 0.8779000043869019 
2024-11-11 23:14:46.169396:  
2024-11-11 23:14:46.172225: Epoch 520 
2024-11-11 23:14:46.174727: Current learning rate: 0.00517 
2024-11-11 23:15:25.759214: train_loss -0.9374 
2024-11-11 23:15:25.763736: val_loss -0.8765 
2024-11-11 23:15:25.766242: Pseudo dice [np.float32(0.9634), np.float32(0.8573)] 
2024-11-11 23:15:25.768720: Epoch time: 39.59 s 
2024-11-11 23:15:25.771420: Yayy! New best EMA pseudo Dice: 0.8810999989509583 
2024-11-11 23:15:27.672142:  
2024-11-11 23:15:27.674674: Epoch 521 
2024-11-11 23:15:27.677166: Current learning rate: 0.00516 
2024-11-11 23:16:07.268575: train_loss -0.9256 
2024-11-11 23:16:07.271764: val_loss -0.8653 
2024-11-11 23:16:07.274230: Pseudo dice [np.float32(0.9573), np.float32(0.8616)] 
2024-11-11 23:16:07.278758: Epoch time: 39.6 s 
2024-11-11 23:16:07.281091: Yayy! New best EMA pseudo Dice: 0.8840000033378601 
2024-11-11 23:16:09.296498:  
2024-11-11 23:16:09.298963: Epoch 522 
2024-11-11 23:16:09.301570: Current learning rate: 0.00515 
2024-11-11 23:16:48.888252: train_loss -0.9299 
2024-11-11 23:16:48.892950: val_loss -0.7936 
2024-11-11 23:16:48.895310: Pseudo dice [np.float32(0.9583), np.float32(0.7704)] 
2024-11-11 23:16:48.897470: Epoch time: 39.59 s 
2024-11-11 23:16:50.033168:  
2024-11-11 23:16:50.035823: Epoch 523 
2024-11-11 23:16:50.043960: Current learning rate: 0.00514 
2024-11-11 23:17:29.643697: train_loss -0.9223 
2024-11-11 23:17:29.647066: val_loss -0.856 
2024-11-11 23:17:29.650032: Pseudo dice [np.float32(0.9545), np.float32(0.836)] 
2024-11-11 23:17:29.652271: Epoch time: 39.61 s 
2024-11-11 23:17:30.794843:  
2024-11-11 23:17:30.797607: Epoch 524 
2024-11-11 23:17:30.800038: Current learning rate: 0.00513 
2024-11-11 23:18:10.436765: train_loss -0.9213 
2024-11-11 23:18:10.441424: val_loss -0.8435 
2024-11-11 23:18:10.443769: Pseudo dice [np.float32(0.9667), np.float32(0.8321)] 
2024-11-11 23:18:10.446479: Epoch time: 39.64 s 
2024-11-11 23:18:10.448956: Yayy! New best EMA pseudo Dice: 0.8848999738693237 
2024-11-11 23:18:12.395666:  
2024-11-11 23:18:12.401074: Epoch 525 
2024-11-11 23:18:12.403665: Current learning rate: 0.00512 
2024-11-11 23:18:52.025003: train_loss -0.9253 
2024-11-11 23:18:52.028481: val_loss -0.8179 
2024-11-11 23:18:52.030659: Pseudo dice [np.float32(0.9587), np.float32(0.8066)] 
2024-11-11 23:18:52.033057: Epoch time: 39.63 s 
2024-11-11 23:18:53.166409:  
2024-11-11 23:18:53.173054: Epoch 526 
2024-11-11 23:18:53.175556: Current learning rate: 0.00511 
2024-11-11 23:19:32.804040: train_loss -0.9216 
2024-11-11 23:19:32.808745: val_loss -0.8194 
2024-11-11 23:19:32.811196: Pseudo dice [np.float32(0.9543), np.float32(0.7757)] 
2024-11-11 23:19:32.813685: Epoch time: 39.64 s 
2024-11-11 23:19:33.948426:  
2024-11-11 23:19:33.954430: Epoch 527 
2024-11-11 23:19:33.957234: Current learning rate: 0.0051 
2024-11-11 23:20:13.556601: train_loss -0.921 
2024-11-11 23:20:13.560968: val_loss -0.7628 
2024-11-11 23:20:13.563432: Pseudo dice [np.float32(0.9516), np.float32(0.7142)] 
2024-11-11 23:20:13.566083: Epoch time: 39.61 s 
2024-11-11 23:20:14.699006:  
2024-11-11 23:20:14.701622: Epoch 528 
2024-11-11 23:20:14.710916: Current learning rate: 0.00509 
2024-11-11 23:20:54.306922: train_loss -0.9243 
2024-11-11 23:20:54.311285: val_loss -0.8064 
2024-11-11 23:20:54.313895: Pseudo dice [np.float32(0.9535), np.float32(0.7467)] 
2024-11-11 23:20:54.316261: Epoch time: 39.61 s 
2024-11-11 23:20:55.452055:  
2024-11-11 23:20:55.454859: Epoch 529 
2024-11-11 23:20:55.462664: Current learning rate: 0.00508 
2024-11-11 23:21:35.074573: train_loss -0.8925 
2024-11-11 23:21:35.078188: val_loss -0.8095 
2024-11-11 23:21:35.080824: Pseudo dice [np.float32(0.938), np.float32(0.7875)] 
2024-11-11 23:21:35.083075: Epoch time: 39.62 s 
2024-11-11 23:21:36.220120:  
2024-11-11 23:21:36.228839: Epoch 530 
2024-11-11 23:21:36.231444: Current learning rate: 0.00507 
2024-11-11 23:22:15.847604: train_loss -0.8931 
2024-11-11 23:22:15.852439: val_loss -0.8513 
2024-11-11 23:22:15.855798: Pseudo dice [np.float32(0.9499), np.float32(0.8248)] 
2024-11-11 23:22:15.858269: Epoch time: 39.63 s 
2024-11-11 23:22:16.998946:  
2024-11-11 23:22:17.004765: Epoch 531 
2024-11-11 23:22:17.007391: Current learning rate: 0.00506 
2024-11-11 23:22:56.609707: train_loss -0.8913 
2024-11-11 23:22:56.613110: val_loss -0.7987 
2024-11-11 23:22:56.615356: Pseudo dice [np.float32(0.9457), np.float32(0.8085)] 
2024-11-11 23:22:56.617841: Epoch time: 39.61 s 
2024-11-11 23:22:57.751784:  
2024-11-11 23:22:57.754670: Epoch 532 
2024-11-11 23:22:57.757221: Current learning rate: 0.00505 
2024-11-11 23:23:37.363482: train_loss -0.9111 
2024-11-11 23:23:37.368725: val_loss -0.8616 
2024-11-11 23:23:37.370751: Pseudo dice [np.float32(0.9563), np.float32(0.8202)] 
2024-11-11 23:23:37.372922: Epoch time: 39.61 s 
2024-11-11 23:23:38.822970:  
2024-11-11 23:23:38.825698: Epoch 533 
2024-11-11 23:23:38.833591: Current learning rate: 0.00504 
2024-11-11 23:24:18.449398: train_loss -0.9238 
2024-11-11 23:24:18.452650: val_loss -0.8358 
2024-11-11 23:24:18.455108: Pseudo dice [np.float32(0.9616), np.float32(0.8382)] 
2024-11-11 23:24:18.457497: Epoch time: 39.63 s 
2024-11-11 23:24:19.593216:  
2024-11-11 23:24:19.595706: Epoch 534 
2024-11-11 23:24:19.604288: Current learning rate: 0.00503 
2024-11-11 23:24:59.215795: train_loss -0.9169 
2024-11-11 23:24:59.220839: val_loss -0.813 
2024-11-11 23:24:59.223439: Pseudo dice [np.float32(0.9634), np.float32(0.822)] 
2024-11-11 23:24:59.225881: Epoch time: 39.62 s 
2024-11-11 23:25:00.360346:  
2024-11-11 23:25:00.366359: Epoch 535 
2024-11-11 23:25:00.368777: Current learning rate: 0.00502 
2024-11-11 23:25:40.000299: train_loss -0.914 
2024-11-11 23:25:40.003814: val_loss -0.8283 
2024-11-11 23:25:40.006201: Pseudo dice [np.float32(0.9596), np.float32(0.8049)] 
2024-11-11 23:25:40.008547: Epoch time: 39.64 s 
2024-11-11 23:25:41.145838:  
2024-11-11 23:25:41.148701: Epoch 536 
2024-11-11 23:25:41.156854: Current learning rate: 0.00501 
2024-11-11 23:26:20.790262: train_loss -0.9052 
2024-11-11 23:26:20.828457: val_loss -0.7963 
2024-11-11 23:26:20.830794: Pseudo dice [np.float32(0.9428), np.float32(0.7663)] 
2024-11-11 23:26:20.833068: Epoch time: 39.65 s 
2024-11-11 23:26:21.974267:  
2024-11-11 23:26:21.976938: Epoch 537 
2024-11-11 23:26:21.985003: Current learning rate: 0.005 
2024-11-11 23:27:01.625765: train_loss -0.9067 
2024-11-11 23:27:01.629209: val_loss -0.7967 
2024-11-11 23:27:01.631614: Pseudo dice [np.float32(0.9488), np.float32(0.7515)] 
2024-11-11 23:27:01.633979: Epoch time: 39.65 s 
2024-11-11 23:27:02.769178:  
2024-11-11 23:27:02.775570: Epoch 538 
2024-11-11 23:27:02.778058: Current learning rate: 0.00499 
2024-11-11 23:27:42.404076: train_loss -0.9204 
2024-11-11 23:27:42.411783: val_loss -0.8071 
2024-11-11 23:27:42.414072: Pseudo dice [np.float32(0.9605), np.float32(0.7647)] 
2024-11-11 23:27:42.416665: Epoch time: 39.64 s 
2024-11-11 23:27:43.551424:  
2024-11-11 23:27:43.554038: Epoch 539 
2024-11-11 23:27:43.562322: Current learning rate: 0.00498 
2024-11-11 23:28:23.191550: train_loss -0.9098 
2024-11-11 23:28:23.194714: val_loss -0.8117 
2024-11-11 23:28:23.197107: Pseudo dice [np.float32(0.952), np.float32(0.7926)] 
2024-11-11 23:28:23.199431: Epoch time: 39.64 s 
2024-11-11 23:28:24.335074:  
2024-11-11 23:28:24.337590: Epoch 540 
2024-11-11 23:28:24.346152: Current learning rate: 0.00497 
2024-11-11 23:29:03.962600: train_loss -0.9306 
2024-11-11 23:29:03.967603: val_loss -0.8059 
2024-11-11 23:29:03.970079: Pseudo dice [np.float32(0.9594), np.float32(0.8128)] 
2024-11-11 23:29:03.972668: Epoch time: 39.63 s 
2024-11-11 23:29:05.109176:  
2024-11-11 23:29:05.114671: Epoch 541 
2024-11-11 23:29:05.117180: Current learning rate: 0.00496 
2024-11-11 23:29:44.732248: train_loss -0.9227 
2024-11-11 23:29:44.736171: val_loss -0.707 
2024-11-11 23:29:44.741099: Pseudo dice [np.float32(0.9451), np.float32(0.6664)] 
2024-11-11 23:29:44.744597: Epoch time: 39.62 s 
2024-11-11 23:29:45.877257:  
2024-11-11 23:29:45.880018: Epoch 542 
2024-11-11 23:29:45.888355: Current learning rate: 0.00495 
2024-11-11 23:30:25.529138: train_loss -0.9204 
2024-11-11 23:30:25.533685: val_loss -0.7636 
2024-11-11 23:30:25.536122: Pseudo dice [np.float32(0.9473), np.float32(0.7237)] 
2024-11-11 23:30:25.538369: Epoch time: 39.65 s 
2024-11-11 23:30:26.671183:  
2024-11-11 23:30:26.673786: Epoch 543 
2024-11-11 23:30:26.681925: Current learning rate: 0.00494 
2024-11-11 23:31:06.323395: train_loss -0.9241 
2024-11-11 23:31:06.331560: val_loss -0.8421 
2024-11-11 23:31:06.333888: Pseudo dice [np.float32(0.9615), np.float32(0.8309)] 
2024-11-11 23:31:06.336128: Epoch time: 39.65 s 
2024-11-11 23:31:07.467695:  
2024-11-11 23:31:07.472640: Epoch 544 
2024-11-11 23:31:07.475057: Current learning rate: 0.00493 
2024-11-11 23:31:47.097032: train_loss -0.9214 
2024-11-11 23:31:47.101294: val_loss -0.8383 
2024-11-11 23:31:47.103460: Pseudo dice [np.float32(0.9593), np.float32(0.7951)] 
2024-11-11 23:31:47.105960: Epoch time: 39.63 s 
2024-11-11 23:31:48.243218:  
2024-11-11 23:31:48.246026: Epoch 545 
2024-11-11 23:31:48.253317: Current learning rate: 0.00492 
2024-11-11 23:32:27.885415: train_loss -0.898 
2024-11-11 23:32:27.889083: val_loss -0.8332 
2024-11-11 23:32:27.891489: Pseudo dice [np.float32(0.9574), np.float32(0.8147)] 
2024-11-11 23:32:27.893784: Epoch time: 39.64 s 
2024-11-11 23:32:29.341187:  
2024-11-11 23:32:29.346505: Epoch 546 
2024-11-11 23:32:29.348891: Current learning rate: 0.00491 
2024-11-11 23:33:08.994077: train_loss -0.9137 
2024-11-11 23:33:08.998799: val_loss -0.7208 
2024-11-11 23:33:09.001153: Pseudo dice [np.float32(0.9481), np.float32(0.6696)] 
2024-11-11 23:33:09.003421: Epoch time: 39.65 s 
2024-11-11 23:33:10.141011:  
2024-11-11 23:33:10.143383: Epoch 547 
2024-11-11 23:33:10.151224: Current learning rate: 0.0049 
2024-11-11 23:33:49.780777: train_loss -0.9082 
2024-11-11 23:33:49.784197: val_loss -0.7507 
2024-11-11 23:33:49.786532: Pseudo dice [np.float32(0.9485), np.float32(0.72)] 
2024-11-11 23:33:49.788928: Epoch time: 39.64 s 
2024-11-11 23:33:50.919372:  
2024-11-11 23:33:50.921754: Epoch 548 
2024-11-11 23:33:50.929276: Current learning rate: 0.00489 
2024-11-11 23:34:30.566131: train_loss -0.9252 
2024-11-11 23:34:30.570665: val_loss -0.817 
2024-11-11 23:34:30.573191: Pseudo dice [np.float32(0.9606), np.float32(0.82)] 
2024-11-11 23:34:30.575314: Epoch time: 39.65 s 
2024-11-11 23:34:31.707811:  
2024-11-11 23:34:31.712661: Epoch 549 
2024-11-11 23:34:31.714998: Current learning rate: 0.00488 
2024-11-11 23:35:11.337731: train_loss -0.9282 
2024-11-11 23:35:11.344935: val_loss -0.8203 
2024-11-11 23:35:11.347251: Pseudo dice [np.float32(0.9545), np.float32(0.7642)] 
2024-11-11 23:35:11.349509: Epoch time: 39.63 s 
2024-11-11 23:35:13.259467:  
2024-11-11 23:35:13.266319: Epoch 550 
2024-11-11 23:35:13.268717: Current learning rate: 0.00487 
2024-11-11 23:35:52.896979: train_loss -0.9185 
2024-11-11 23:35:52.901910: val_loss -0.839 
2024-11-11 23:35:52.904699: Pseudo dice [np.float32(0.9546), np.float32(0.8057)] 
2024-11-11 23:35:52.907052: Epoch time: 39.64 s 
2024-11-11 23:35:54.043014:  
2024-11-11 23:35:54.046554: Epoch 551 
2024-11-11 23:35:54.055115: Current learning rate: 0.00486 
2024-11-11 23:36:33.677079: train_loss -0.9182 
2024-11-11 23:36:33.680671: val_loss -0.8121 
2024-11-11 23:36:33.682960: Pseudo dice [np.float32(0.9545), np.float32(0.7891)] 
2024-11-11 23:36:33.685443: Epoch time: 39.64 s 
2024-11-11 23:36:34.817293:  
2024-11-11 23:36:34.824436: Epoch 552 
2024-11-11 23:36:34.827005: Current learning rate: 0.00485 
2024-11-11 23:37:14.458941: train_loss -0.9337 
2024-11-11 23:37:14.463411: val_loss -0.8267 
2024-11-11 23:37:14.465936: Pseudo dice [np.float32(0.9563), np.float32(0.7894)] 
2024-11-11 23:37:14.468137: Epoch time: 39.64 s 
2024-11-11 23:37:15.600500:  
2024-11-11 23:37:15.603380: Epoch 553 
2024-11-11 23:37:15.611079: Current learning rate: 0.00484 
2024-11-11 23:37:55.234094: train_loss -0.9293 
2024-11-11 23:37:55.237479: val_loss -0.868 
2024-11-11 23:37:55.240261: Pseudo dice [np.float32(0.9612), np.float32(0.8439)] 
2024-11-11 23:37:55.243803: Epoch time: 39.63 s 
2024-11-11 23:37:56.378024:  
2024-11-11 23:37:56.384086: Epoch 554 
2024-11-11 23:37:56.386531: Current learning rate: 0.00484 
2024-11-11 23:38:36.009841: train_loss -0.93 
2024-11-11 23:38:36.017714: val_loss -0.8336 
2024-11-11 23:38:36.020380: Pseudo dice [np.float32(0.9556), np.float32(0.7919)] 
2024-11-11 23:38:36.022933: Epoch time: 39.63 s 
2024-11-11 23:38:37.158788:  
2024-11-11 23:38:37.163587: Epoch 555 
2024-11-11 23:38:37.166036: Current learning rate: 0.00483 
2024-11-11 23:39:16.793323: train_loss -0.9258 
2024-11-11 23:39:16.796558: val_loss -0.7794 
2024-11-11 23:39:16.798791: Pseudo dice [np.float32(0.9531), np.float32(0.7575)] 
2024-11-11 23:39:16.800978: Epoch time: 39.64 s 
2024-11-11 23:39:17.932203:  
2024-11-11 23:39:17.934816: Epoch 556 
2024-11-11 23:39:17.942797: Current learning rate: 0.00482 
2024-11-11 23:39:57.568189: train_loss -0.9218 
2024-11-11 23:39:57.572983: val_loss -0.8088 
2024-11-11 23:39:57.575310: Pseudo dice [np.float32(0.953), np.float32(0.7788)] 
2024-11-11 23:39:57.577866: Epoch time: 39.64 s 
2024-11-11 23:39:58.712740:  
2024-11-11 23:39:58.718221: Epoch 557 
2024-11-11 23:39:58.720633: Current learning rate: 0.00481 
2024-11-11 23:40:38.340684: train_loss -0.9273 
2024-11-11 23:40:38.344388: val_loss -0.7868 
2024-11-11 23:40:38.347100: Pseudo dice [np.float32(0.9524), np.float32(0.7343)] 
2024-11-11 23:40:38.349478: Epoch time: 39.63 s 
2024-11-11 23:40:39.484910:  
2024-11-11 23:40:39.487230: Epoch 558 
2024-11-11 23:40:39.494754: Current learning rate: 0.0048 
2024-11-11 23:41:19.116414: train_loss -0.9293 
2024-11-11 23:41:19.121007: val_loss -0.8108 
2024-11-11 23:41:19.123432: Pseudo dice [np.float32(0.9568), np.float32(0.7612)] 
2024-11-11 23:41:19.125613: Epoch time: 39.63 s 
2024-11-11 23:41:20.573605:  
2024-11-11 23:41:20.576296: Epoch 559 
2024-11-11 23:41:20.583800: Current learning rate: 0.00479 
2024-11-11 23:42:00.203708: train_loss -0.9316 
2024-11-11 23:42:00.213825: val_loss -0.8503 
2024-11-11 23:42:00.216187: Pseudo dice [np.float32(0.9618), np.float32(0.8386)] 
2024-11-11 23:42:00.218760: Epoch time: 39.63 s 
2024-11-11 23:42:01.347927:  
2024-11-11 23:42:01.354028: Epoch 560 
2024-11-11 23:42:01.356263: Current learning rate: 0.00478 
2024-11-11 23:42:40.992214: train_loss -0.9141 
2024-11-11 23:42:40.996889: val_loss -0.8189 
2024-11-11 23:42:40.999221: Pseudo dice [np.float32(0.96), np.float32(0.8056)] 
2024-11-11 23:42:41.001663: Epoch time: 39.65 s 
2024-11-11 23:42:42.143800:  
2024-11-11 23:42:42.147467: Epoch 561 
2024-11-11 23:42:42.155116: Current learning rate: 0.00477 
2024-11-11 23:43:21.803420: train_loss -0.9262 
2024-11-11 23:43:21.807058: val_loss -0.8543 
2024-11-11 23:43:21.809604: Pseudo dice [np.float32(0.9636), np.float32(0.8388)] 
2024-11-11 23:43:21.811842: Epoch time: 39.66 s 
2024-11-11 23:43:22.946156:  
2024-11-11 23:43:22.952610: Epoch 562 
2024-11-11 23:43:22.955057: Current learning rate: 0.00476 
2024-11-11 23:44:02.605810: train_loss -0.9166 
2024-11-11 23:44:02.610620: val_loss -0.8309 
2024-11-11 23:44:02.613170: Pseudo dice [np.float32(0.9568), np.float32(0.8084)] 
2024-11-11 23:44:02.615270: Epoch time: 39.66 s 
2024-11-11 23:44:03.752150:  
2024-11-11 23:44:03.757654: Epoch 563 
2024-11-11 23:44:03.760329: Current learning rate: 0.00475 
2024-11-11 23:44:43.386367: train_loss -0.9264 
2024-11-11 23:44:43.389801: val_loss -0.8471 
2024-11-11 23:44:43.392706: Pseudo dice [np.float32(0.9607), np.float32(0.8221)] 
2024-11-11 23:44:43.395327: Epoch time: 39.64 s 
2024-11-11 23:44:44.528919:  
2024-11-11 23:44:44.531670: Epoch 564 
2024-11-11 23:44:44.539226: Current learning rate: 0.00474 
2024-11-11 23:45:24.171676: train_loss -0.9319 
2024-11-11 23:45:24.176355: val_loss -0.7594 
2024-11-11 23:45:24.178632: Pseudo dice [np.float32(0.9537), np.float32(0.7257)] 
2024-11-11 23:45:24.181164: Epoch time: 39.64 s 
2024-11-11 23:45:25.318489:  
2024-11-11 23:45:25.321115: Epoch 565 
2024-11-11 23:45:25.328826: Current learning rate: 0.00473 
2024-11-11 23:46:04.959671: train_loss -0.9226 
2024-11-11 23:46:04.963166: val_loss -0.8286 
2024-11-11 23:46:04.966077: Pseudo dice [np.float32(0.9556), np.float32(0.7999)] 
2024-11-11 23:46:04.969004: Epoch time: 39.64 s 
2024-11-11 23:46:06.102522:  
2024-11-11 23:46:06.109014: Epoch 566 
2024-11-11 23:46:06.111412: Current learning rate: 0.00472 
2024-11-11 23:46:45.739070: train_loss -0.9222 
2024-11-11 23:46:45.743593: val_loss -0.7928 
2024-11-11 23:46:45.746656: Pseudo dice [np.float32(0.9574), np.float32(0.77)] 
2024-11-11 23:46:45.749815: Epoch time: 39.64 s 
2024-11-11 23:46:46.882705:  
2024-11-11 23:46:46.888685: Epoch 567 
2024-11-11 23:46:46.890821: Current learning rate: 0.00471 
2024-11-11 23:47:26.536341: train_loss -0.9084 
2024-11-11 23:47:26.539545: val_loss -0.809 
2024-11-11 23:47:26.541755: Pseudo dice [np.float32(0.9466), np.float32(0.7473)] 
2024-11-11 23:47:26.543850: Epoch time: 39.65 s 
2024-11-11 23:47:27.679665:  
2024-11-11 23:47:27.685048: Epoch 568 
2024-11-11 23:47:27.687523: Current learning rate: 0.0047 
2024-11-11 23:48:07.313532: train_loss -0.916 
2024-11-11 23:48:07.318228: val_loss -0.8165 
2024-11-11 23:48:07.320392: Pseudo dice [np.float32(0.9579), np.float32(0.8041)] 
2024-11-11 23:48:07.322712: Epoch time: 39.63 s 
2024-11-11 23:48:08.457637:  
2024-11-11 23:48:08.460175: Epoch 569 
2024-11-11 23:48:08.468422: Current learning rate: 0.00469 
2024-11-11 23:48:48.090580: train_loss -0.9287 
2024-11-11 23:48:48.094121: val_loss -0.8105 
2024-11-11 23:48:48.096440: Pseudo dice [np.float32(0.9592), np.float32(0.8071)] 
2024-11-11 23:48:48.098623: Epoch time: 39.63 s 
2024-11-11 23:48:49.236358:  
2024-11-11 23:48:49.238879: Epoch 570 
2024-11-11 23:48:49.246609: Current learning rate: 0.00468 
2024-11-11 23:49:28.884745: train_loss -0.9097 
2024-11-11 23:49:28.889444: val_loss -0.8 
2024-11-11 23:49:28.891732: Pseudo dice [np.float32(0.9508), np.float32(0.7695)] 
2024-11-11 23:49:28.894157: Epoch time: 39.65 s 
2024-11-11 23:49:30.031441:  
2024-11-11 23:49:30.036945: Epoch 571 
2024-11-11 23:49:30.039341: Current learning rate: 0.00467 
2024-11-11 23:50:09.668520: train_loss -0.9226 
2024-11-11 23:50:09.671764: val_loss -0.8496 
2024-11-11 23:50:09.674337: Pseudo dice [np.float32(0.9606), np.float32(0.8085)] 
2024-11-11 23:50:09.676795: Epoch time: 39.64 s 
2024-11-11 23:50:11.120770:  
2024-11-11 23:50:11.123618: Epoch 572 
2024-11-11 23:50:11.132388: Current learning rate: 0.00466 
2024-11-11 23:50:50.784431: train_loss -0.935 
2024-11-11 23:50:50.789771: val_loss -0.7786 
2024-11-11 23:50:50.792021: Pseudo dice [np.float32(0.9578), np.float32(0.7916)] 
2024-11-11 23:50:50.794340: Epoch time: 39.66 s 
2024-11-11 23:50:51.947124:  
2024-11-11 23:50:51.953296: Epoch 573 
2024-11-11 23:50:51.955745: Current learning rate: 0.00465 
2024-11-11 23:51:31.590556: train_loss -0.9334 
2024-11-11 23:51:31.598545: val_loss -0.85 
2024-11-11 23:51:31.600763: Pseudo dice [np.float32(0.9587), np.float32(0.8151)] 
2024-11-11 23:51:31.603065: Epoch time: 39.64 s 
2024-11-11 23:51:32.753472:  
2024-11-11 23:51:32.758497: Epoch 574 
2024-11-11 23:51:32.761050: Current learning rate: 0.00464 
2024-11-11 23:52:12.395852: train_loss -0.9313 
2024-11-11 23:52:12.400401: val_loss -0.8471 
2024-11-11 23:52:12.404076: Pseudo dice [np.float32(0.9603), np.float32(0.8109)] 
2024-11-11 23:52:12.407660: Epoch time: 39.64 s 
2024-11-11 23:52:13.563725:  
2024-11-11 23:52:13.567047: Epoch 575 
2024-11-11 23:52:13.576929: Current learning rate: 0.00463 
2024-11-11 23:52:53.192161: train_loss -0.9299 
2024-11-11 23:52:53.196048: val_loss -0.7592 
2024-11-11 23:52:53.198614: Pseudo dice [np.float32(0.9507), np.float32(0.716)] 
2024-11-11 23:52:53.201102: Epoch time: 39.63 s 
2024-11-11 23:52:54.358603:  
2024-11-11 23:52:54.361878: Epoch 576 
2024-11-11 23:52:54.368586: Current learning rate: 0.00462 
2024-11-11 23:53:33.994067: train_loss -0.9218 
2024-11-11 23:53:33.998708: val_loss -0.8453 
2024-11-11 23:53:34.001155: Pseudo dice [np.float32(0.9592), np.float32(0.8339)] 
2024-11-11 23:53:34.003661: Epoch time: 39.64 s 
2024-11-11 23:53:35.158183:  
2024-11-11 23:53:35.163608: Epoch 577 
2024-11-11 23:53:35.165993: Current learning rate: 0.00461 
2024-11-11 23:54:14.781292: train_loss -0.9322 
2024-11-11 23:54:14.790790: val_loss -0.8519 
2024-11-11 23:54:14.793268: Pseudo dice [np.float32(0.9647), np.float32(0.8443)] 
2024-11-11 23:54:14.795560: Epoch time: 39.62 s 
2024-11-11 23:54:15.950032:  
2024-11-11 23:54:15.952540: Epoch 578 
2024-11-11 23:54:15.959771: Current learning rate: 0.0046 
2024-11-11 23:54:55.564457: train_loss -0.933 
2024-11-11 23:54:55.569163: val_loss -0.7894 
2024-11-11 23:54:55.571718: Pseudo dice [np.float32(0.9603), np.float32(0.7715)] 
2024-11-11 23:54:55.574202: Epoch time: 39.62 s 
2024-11-11 23:54:56.724441:  
2024-11-11 23:54:56.727010: Epoch 579 
2024-11-11 23:54:56.729358: Current learning rate: 0.00459 
2024-11-11 23:55:36.357541: train_loss -0.9162 
2024-11-11 23:55:36.361203: val_loss -0.8604 
2024-11-11 23:55:36.363426: Pseudo dice [np.float32(0.9616), np.float32(0.8472)] 
2024-11-11 23:55:36.365772: Epoch time: 39.63 s 
2024-11-11 23:55:37.517521:  
2024-11-11 23:55:37.520010: Epoch 580 
2024-11-11 23:55:37.527595: Current learning rate: 0.00458 
2024-11-11 23:56:17.133409: train_loss -0.9216 
2024-11-11 23:56:17.138158: val_loss -0.7797 
2024-11-11 23:56:17.140390: Pseudo dice [np.float32(0.9536), np.float32(0.7283)] 
2024-11-11 23:56:17.142607: Epoch time: 39.62 s 
2024-11-11 23:56:18.292175:  
2024-11-11 23:56:18.294524: Epoch 581 
2024-11-11 23:56:18.302697: Current learning rate: 0.00457 
2024-11-11 23:56:57.925162: train_loss -0.9128 
2024-11-11 23:56:57.933058: val_loss -0.8326 
2024-11-11 23:56:57.935376: Pseudo dice [np.float32(0.9572), np.float32(0.8053)] 
2024-11-11 23:56:57.938456: Epoch time: 39.63 s 
2024-11-11 23:56:59.090324:  
2024-11-11 23:56:59.096048: Epoch 582 
2024-11-11 23:56:59.098663: Current learning rate: 0.00456 
2024-11-11 23:57:38.723747: train_loss -0.9095 
2024-11-11 23:57:38.728085: val_loss -0.7532 
2024-11-11 23:57:38.730374: Pseudo dice [np.float32(0.9478), np.float32(0.701)] 
2024-11-11 23:57:38.732710: Epoch time: 39.63 s 
2024-11-11 23:57:39.884531:  
2024-11-11 23:57:39.887045: Epoch 583 
2024-11-11 23:57:39.894694: Current learning rate: 0.00455 
2024-11-11 23:58:19.523049: train_loss -0.917 
2024-11-11 23:58:19.526524: val_loss -0.8189 
2024-11-11 23:58:19.528960: Pseudo dice [np.float32(0.9565), np.float32(0.8044)] 
2024-11-11 23:58:19.531248: Epoch time: 39.64 s 
2024-11-11 23:58:20.683338:  
2024-11-11 23:58:20.686009: Epoch 584 
2024-11-11 23:58:20.693588: Current learning rate: 0.00454 
2024-11-11 23:59:00.324779: train_loss -0.9232 
2024-11-11 23:59:00.329219: val_loss -0.8169 
2024-11-11 23:59:00.331439: Pseudo dice [np.float32(0.9603), np.float32(0.8278)] 
2024-11-11 23:59:00.333605: Epoch time: 39.64 s 
2024-11-11 23:59:01.798202:  
2024-11-11 23:59:01.806296: Epoch 585 
2024-11-11 23:59:01.808751: Current learning rate: 0.00453 
2024-11-11 23:59:41.451773: train_loss -0.9338 
2024-11-11 23:59:41.459013: val_loss -0.8262 
2024-11-11 23:59:41.461443: Pseudo dice [np.float32(0.9637), np.float32(0.8283)] 
2024-11-11 23:59:41.463784: Epoch time: 39.65 s 
2024-11-11 23:59:42.614865:  
2024-11-11 23:59:42.620928: Epoch 586 
2024-11-11 23:59:42.623286: Current learning rate: 0.00452 
2024-11-12 00:00:22.274227: train_loss -0.9111 
2024-11-12 00:00:22.278805: val_loss -0.8473 
2024-11-12 00:00:22.281141: Pseudo dice [np.float32(0.9622), np.float32(0.8304)] 
2024-11-12 00:00:22.283474: Epoch time: 39.66 s 
2024-11-12 00:00:23.437387:  
2024-11-12 00:00:23.440206: Epoch 587 
2024-11-12 00:00:23.448390: Current learning rate: 0.00451 
2024-11-12 00:01:03.096502: train_loss -0.9279 
2024-11-12 00:01:03.103610: val_loss -0.7876 
2024-11-12 00:01:03.105987: Pseudo dice [np.float32(0.9588), np.float32(0.7702)] 
2024-11-12 00:01:03.108545: Epoch time: 39.66 s 
2024-11-12 00:01:04.264907:  
2024-11-12 00:01:04.267467: Epoch 588 
2024-11-12 00:01:04.269873: Current learning rate: 0.0045 
2024-11-12 00:01:43.893386: train_loss -0.9262 
2024-11-12 00:01:43.897960: val_loss -0.817 
2024-11-12 00:01:43.900401: Pseudo dice [np.float32(0.9571), np.float32(0.7983)] 
2024-11-12 00:01:43.902841: Epoch time: 39.63 s 
2024-11-12 00:01:45.059393:  
2024-11-12 00:01:45.061918: Epoch 589 
2024-11-12 00:01:45.069776: Current learning rate: 0.00449 
2024-11-12 00:02:24.705345: train_loss -0.924 
2024-11-12 00:02:24.709035: val_loss -0.8375 
2024-11-12 00:02:24.711262: Pseudo dice [np.float32(0.9533), np.float32(0.7938)] 
2024-11-12 00:02:24.713582: Epoch time: 39.65 s 
2024-11-12 00:02:25.868219:  
2024-11-12 00:02:25.870533: Epoch 590 
2024-11-12 00:02:25.878281: Current learning rate: 0.00448 
2024-11-12 00:03:05.488642: train_loss -0.9152 
2024-11-12 00:03:05.493564: val_loss -0.743 
2024-11-12 00:03:05.495815: Pseudo dice [np.float32(0.9407), np.float32(0.6347)] 
2024-11-12 00:03:05.498398: Epoch time: 39.62 s 
2024-11-12 00:03:06.655150:  
2024-11-12 00:03:06.659324: Epoch 591 
2024-11-12 00:03:06.661955: Current learning rate: 0.00447 
2024-11-12 00:03:46.268095: train_loss -0.9123 
2024-11-12 00:03:46.271734: val_loss -0.8149 
2024-11-12 00:03:46.274295: Pseudo dice [np.float32(0.9622), np.float32(0.8196)] 
2024-11-12 00:03:46.276789: Epoch time: 39.61 s 
2024-11-12 00:03:47.432989:  
2024-11-12 00:03:47.435535: Epoch 592 
2024-11-12 00:03:47.444474: Current learning rate: 0.00446 
2024-11-12 00:04:27.040248: train_loss -0.925 
2024-11-12 00:04:27.046906: val_loss -0.8257 
2024-11-12 00:04:27.049239: Pseudo dice [np.float32(0.9595), np.float32(0.8189)] 
2024-11-12 00:04:27.051618: Epoch time: 39.61 s 
2024-11-12 00:04:28.209345:  
2024-11-12 00:04:28.214297: Epoch 593 
2024-11-12 00:04:28.216840: Current learning rate: 0.00445 
2024-11-12 00:05:07.806665: train_loss -0.9261 
2024-11-12 00:05:07.814005: val_loss -0.8223 
2024-11-12 00:05:07.816198: Pseudo dice [np.float32(0.9604), np.float32(0.8203)] 
2024-11-12 00:05:07.818515: Epoch time: 39.6 s 
2024-11-12 00:05:08.971842:  
2024-11-12 00:05:08.974372: Epoch 594 
2024-11-12 00:05:08.982686: Current learning rate: 0.00444 
2024-11-12 00:05:48.573698: train_loss -0.9369 
2024-11-12 00:05:48.578635: val_loss -0.8172 
2024-11-12 00:05:48.580796: Pseudo dice [np.float32(0.9601), np.float32(0.827)] 
2024-11-12 00:05:48.583075: Epoch time: 39.6 s 
2024-11-12 00:05:49.736443:  
2024-11-12 00:05:49.739399: Epoch 595 
2024-11-12 00:05:49.748122: Current learning rate: 0.00443 
2024-11-12 00:06:29.326510: train_loss -0.9403 
2024-11-12 00:06:29.329952: val_loss -0.861 
2024-11-12 00:06:29.332315: Pseudo dice [np.float32(0.9606), np.float32(0.8431)] 
2024-11-12 00:06:29.334583: Epoch time: 39.59 s 
2024-11-12 00:06:30.488344:  
2024-11-12 00:06:30.493695: Epoch 596 
2024-11-12 00:06:30.496115: Current learning rate: 0.00442 
2024-11-12 00:07:10.066705: train_loss -0.9294 
2024-11-12 00:07:10.071621: val_loss -0.7925 
2024-11-12 00:07:10.074077: Pseudo dice [np.float32(0.9584), np.float32(0.7661)] 
2024-11-12 00:07:10.076433: Epoch time: 39.58 s 
2024-11-12 00:07:11.225875:  
2024-11-12 00:07:11.228457: Epoch 597 
2024-11-12 00:07:11.236182: Current learning rate: 0.00441 
2024-11-12 00:07:50.827871: train_loss -0.9264 
2024-11-12 00:07:50.832180: val_loss -0.842 
2024-11-12 00:07:50.834602: Pseudo dice [np.float32(0.9543), np.float32(0.8094)] 
2024-11-12 00:07:50.836968: Epoch time: 39.6 s 
2024-11-12 00:07:52.304178:  
2024-11-12 00:07:52.306660: Epoch 598 
2024-11-12 00:07:52.314405: Current learning rate: 0.0044 
2024-11-12 00:08:31.904466: train_loss -0.9313 
2024-11-12 00:08:31.909069: val_loss -0.8284 
2024-11-12 00:08:31.911474: Pseudo dice [np.float32(0.9598), np.float32(0.8254)] 
2024-11-12 00:08:31.913881: Epoch time: 39.6 s 
2024-11-12 00:08:33.067797:  
2024-11-12 00:08:33.073271: Epoch 599 
2024-11-12 00:08:33.075652: Current learning rate: 0.00439 
2024-11-12 00:09:12.692518: train_loss -0.9299 
2024-11-12 00:09:12.695800: val_loss -0.7656 
2024-11-12 00:09:12.698185: Pseudo dice [np.float32(0.9541), np.float32(0.7188)] 
2024-11-12 00:09:12.700330: Epoch time: 39.63 s 
2024-11-12 00:09:14.611165:  
2024-11-12 00:09:14.613577: Epoch 600 
2024-11-12 00:09:14.615951: Current learning rate: 0.00438 
2024-11-12 00:09:54.237047: train_loss -0.9384 
2024-11-12 00:09:54.242107: val_loss -0.8354 
2024-11-12 00:09:54.244558: Pseudo dice [np.float32(0.956), np.float32(0.8262)] 
2024-11-12 00:09:54.247106: Epoch time: 39.63 s 
2024-11-12 00:09:55.403845:  
2024-11-12 00:09:55.406336: Epoch 601 
2024-11-12 00:09:55.414399: Current learning rate: 0.00437 
2024-11-12 00:10:35.038042: train_loss -0.9397 
2024-11-12 00:10:35.041554: val_loss -0.8237 
2024-11-12 00:10:35.043864: Pseudo dice [np.float32(0.963), np.float32(0.8383)] 
2024-11-12 00:10:35.046219: Epoch time: 39.64 s 
2024-11-12 00:10:36.197725:  
2024-11-12 00:10:36.203882: Epoch 602 
2024-11-12 00:10:36.206444: Current learning rate: 0.00436 
2024-11-12 00:11:15.825934: train_loss -0.9343 
2024-11-12 00:11:15.830635: val_loss -0.8544 
2024-11-12 00:11:15.832916: Pseudo dice [np.float32(0.9584), np.float32(0.8268)] 
2024-11-12 00:11:15.835141: Epoch time: 39.63 s 
2024-11-12 00:11:16.986105:  
2024-11-12 00:11:16.988764: Epoch 603 
2024-11-12 00:11:16.996196: Current learning rate: 0.00435 
2024-11-12 00:11:56.615801: train_loss -0.9347 
2024-11-12 00:11:56.619121: val_loss -0.8497 
2024-11-12 00:11:56.621523: Pseudo dice [np.float32(0.9601), np.float32(0.8208)] 
2024-11-12 00:11:56.623808: Epoch time: 39.63 s 
2024-11-12 00:11:57.773531:  
2024-11-12 00:11:57.775836: Epoch 604 
2024-11-12 00:11:57.777968: Current learning rate: 0.00434 
2024-11-12 00:12:37.374222: train_loss -0.9403 
2024-11-12 00:12:37.379045: val_loss -0.8382 
2024-11-12 00:12:37.381226: Pseudo dice [np.float32(0.9605), np.float32(0.8502)] 
2024-11-12 00:12:37.383918: Epoch time: 39.6 s 
2024-11-12 00:12:38.538227:  
2024-11-12 00:12:38.540714: Epoch 605 
2024-11-12 00:12:38.548944: Current learning rate: 0.00433 
2024-11-12 00:13:18.161612: train_loss -0.9461 
2024-11-12 00:13:18.164703: val_loss -0.7976 
2024-11-12 00:13:18.166933: Pseudo dice [np.float32(0.9593), np.float32(0.7811)] 
2024-11-12 00:13:18.169075: Epoch time: 39.62 s 
2024-11-12 00:13:19.322499:  
2024-11-12 00:13:19.324904: Epoch 606 
2024-11-12 00:13:19.332222: Current learning rate: 0.00432 
2024-11-12 00:13:58.942852: train_loss -0.9432 
2024-11-12 00:13:58.950537: val_loss -0.8316 
2024-11-12 00:13:58.952867: Pseudo dice [np.float32(0.9622), np.float32(0.8204)] 
2024-11-12 00:13:58.954966: Epoch time: 39.62 s 
2024-11-12 00:14:00.105976:  
2024-11-12 00:14:00.111497: Epoch 607 
2024-11-12 00:14:00.113703: Current learning rate: 0.00431 
2024-11-12 00:14:39.747880: train_loss -0.9271 
2024-11-12 00:14:39.751271: val_loss -0.8266 
2024-11-12 00:14:39.753585: Pseudo dice [np.float32(0.9632), np.float32(0.8066)] 
2024-11-12 00:14:39.755851: Epoch time: 39.64 s 
2024-11-12 00:14:40.907047:  
2024-11-12 00:14:40.909879: Epoch 608 
2024-11-12 00:14:40.917682: Current learning rate: 0.0043 
2024-11-12 00:15:20.535153: train_loss -0.9365 
2024-11-12 00:15:20.539794: val_loss -0.7918 
2024-11-12 00:15:20.542133: Pseudo dice [np.float32(0.9598), np.float32(0.7682)] 
2024-11-12 00:15:20.544255: Epoch time: 39.63 s 
2024-11-12 00:15:21.699419:  
2024-11-12 00:15:21.701971: Epoch 609 
2024-11-12 00:15:21.709949: Current learning rate: 0.00429 
2024-11-12 00:16:01.352380: train_loss -0.9123 
2024-11-12 00:16:01.355952: val_loss -0.7607 
2024-11-12 00:16:01.358019: Pseudo dice [np.float32(0.9456), np.float32(0.7052)] 
2024-11-12 00:16:01.360349: Epoch time: 39.65 s 
2024-11-12 00:16:02.511790:  
2024-11-12 00:16:02.516910: Epoch 610 
2024-11-12 00:16:02.519918: Current learning rate: 0.00429 
2024-11-12 00:16:42.135411: train_loss -0.9198 
2024-11-12 00:16:42.140130: val_loss -0.8325 
2024-11-12 00:16:42.142464: Pseudo dice [np.float32(0.9565), np.float32(0.8088)] 
2024-11-12 00:16:42.144777: Epoch time: 39.62 s 
2024-11-12 00:16:43.611195:  
2024-11-12 00:16:43.613517: Epoch 611 
2024-11-12 00:16:43.621154: Current learning rate: 0.00428 
2024-11-12 00:17:23.246071: train_loss -0.9283 
2024-11-12 00:17:23.249482: val_loss -0.8469 
2024-11-12 00:17:23.251755: Pseudo dice [np.float32(0.9601), np.float32(0.8251)] 
2024-11-12 00:17:23.254109: Epoch time: 39.64 s 
2024-11-12 00:17:24.413550:  
2024-11-12 00:17:24.418050: Epoch 612 
2024-11-12 00:17:24.425817: Current learning rate: 0.00427 
2024-11-12 00:18:04.040719: train_loss -0.9335 
2024-11-12 00:18:04.045328: val_loss -0.8516 
2024-11-12 00:18:04.047687: Pseudo dice [np.float32(0.9614), np.float32(0.8305)] 
2024-11-12 00:18:04.049961: Epoch time: 39.63 s 
2024-11-12 00:18:05.206487:  
2024-11-12 00:18:05.211617: Epoch 613 
2024-11-12 00:18:05.214234: Current learning rate: 0.00426 
2024-11-12 00:18:44.843387: train_loss -0.9328 
2024-11-12 00:18:44.852461: val_loss -0.8567 
2024-11-12 00:18:44.854760: Pseudo dice [np.float32(0.9632), np.float32(0.8353)] 
2024-11-12 00:18:44.856871: Epoch time: 39.64 s 
2024-11-12 00:18:46.018503:  
2024-11-12 00:18:46.021149: Epoch 614 
2024-11-12 00:18:46.029301: Current learning rate: 0.00425 
2024-11-12 00:19:25.647143: train_loss -0.9297 
2024-11-12 00:19:25.652111: val_loss -0.8528 
2024-11-12 00:19:25.654669: Pseudo dice [np.float32(0.9619), np.float32(0.8303)] 
2024-11-12 00:19:25.656991: Epoch time: 39.63 s 
2024-11-12 00:19:26.819318:  
2024-11-12 00:19:26.825075: Epoch 615 
2024-11-12 00:19:26.827468: Current learning rate: 0.00424 
2024-11-12 00:20:06.464834: train_loss -0.9204 
2024-11-12 00:20:06.468255: val_loss -0.8082 
2024-11-12 00:20:06.470619: Pseudo dice [np.float32(0.9502), np.float32(0.762)] 
2024-11-12 00:20:06.472820: Epoch time: 39.65 s 
2024-11-12 00:20:07.634619:  
2024-11-12 00:20:07.641547: Epoch 616 
2024-11-12 00:20:07.648978: Current learning rate: 0.00423 
2024-11-12 00:20:47.261666: train_loss -0.9301 
2024-11-12 00:20:47.266504: val_loss -0.8762 
2024-11-12 00:20:47.268832: Pseudo dice [np.float32(0.962), np.float32(0.8758)] 
2024-11-12 00:20:47.271064: Epoch time: 39.63 s 
2024-11-12 00:20:48.431238:  
2024-11-12 00:20:48.434004: Epoch 617 
2024-11-12 00:20:48.441776: Current learning rate: 0.00422 
2024-11-12 00:21:28.055008: train_loss -0.9344 
2024-11-12 00:21:28.058386: val_loss -0.861 
2024-11-12 00:21:28.060868: Pseudo dice [np.float32(0.9604), np.float32(0.8306)] 
2024-11-12 00:21:28.062975: Epoch time: 39.62 s 
2024-11-12 00:21:28.065233: Yayy! New best EMA pseudo Dice: 0.8853999972343445 
2024-11-12 00:21:29.997450:  
2024-11-12 00:21:30.003681: Epoch 618 
2024-11-12 00:21:30.006591: Current learning rate: 0.00421 
2024-11-12 00:22:09.633162: train_loss -0.9324 
2024-11-12 00:22:09.642102: val_loss -0.8028 
2024-11-12 00:22:09.644594: Pseudo dice [np.float32(0.9504), np.float32(0.776)] 
2024-11-12 00:22:09.647146: Epoch time: 39.64 s 
2024-11-12 00:22:10.806200:  
2024-11-12 00:22:10.808908: Epoch 619 
2024-11-12 00:22:10.817378: Current learning rate: 0.0042 
2024-11-12 00:22:50.439916: train_loss -0.9359 
2024-11-12 00:22:50.443267: val_loss -0.7903 
2024-11-12 00:22:50.445648: Pseudo dice [np.float32(0.9606), np.float32(0.8077)] 
2024-11-12 00:22:50.447783: Epoch time: 39.63 s 
2024-11-12 00:22:51.608588:  
2024-11-12 00:22:51.611170: Epoch 620 
2024-11-12 00:22:51.620109: Current learning rate: 0.00419 
2024-11-12 00:23:31.254545: train_loss -0.9249 
2024-11-12 00:23:31.259070: val_loss -0.8523 
2024-11-12 00:23:31.261341: Pseudo dice [np.float32(0.9635), np.float32(0.8382)] 
2024-11-12 00:23:31.263866: Epoch time: 39.65 s 
2024-11-12 00:23:32.427890:  
2024-11-12 00:23:32.433213: Epoch 621 
2024-11-12 00:23:32.435881: Current learning rate: 0.00418 
2024-11-12 00:24:12.062971: train_loss -0.9315 
2024-11-12 00:24:12.066597: val_loss -0.8613 
2024-11-12 00:24:12.069300: Pseudo dice [np.float32(0.9646), np.float32(0.8493)] 
2024-11-12 00:24:12.071586: Epoch time: 39.64 s 
2024-11-12 00:24:12.074017: Yayy! New best EMA pseudo Dice: 0.8873000144958496 
2024-11-12 00:24:14.024122:  
2024-11-12 00:24:14.029874: Epoch 622 
2024-11-12 00:24:14.032189: Current learning rate: 0.00417 
2024-11-12 00:24:53.639834: train_loss -0.9228 
2024-11-12 00:24:53.644453: val_loss -0.7878 
2024-11-12 00:24:53.647076: Pseudo dice [np.float32(0.9528), np.float32(0.7364)] 
2024-11-12 00:24:53.649487: Epoch time: 39.62 s 
2024-11-12 00:24:55.124451:  
2024-11-12 00:24:55.127424: Epoch 623 
2024-11-12 00:24:55.134638: Current learning rate: 0.00416 
2024-11-12 00:25:34.788746: train_loss -0.912 
2024-11-12 00:25:34.791898: val_loss -0.7851 
2024-11-12 00:25:34.794123: Pseudo dice [np.float32(0.9563), np.float32(0.8054)] 
2024-11-12 00:25:34.796472: Epoch time: 39.67 s 
2024-11-12 00:25:35.955482:  
2024-11-12 00:25:35.961493: Epoch 624 
2024-11-12 00:25:35.964247: Current learning rate: 0.00415 
2024-11-12 00:26:15.594723: train_loss -0.931 
2024-11-12 00:26:15.599829: val_loss -0.7896 
2024-11-12 00:26:15.602274: Pseudo dice [np.float32(0.9494), np.float32(0.7609)] 
2024-11-12 00:26:15.604513: Epoch time: 39.64 s 
2024-11-12 00:26:16.765697:  
2024-11-12 00:26:16.768317: Epoch 625 
2024-11-12 00:26:16.776304: Current learning rate: 0.00414 
2024-11-12 00:26:56.402151: train_loss -0.9277 
2024-11-12 00:26:56.405458: val_loss -0.7803 
2024-11-12 00:26:56.407722: Pseudo dice [np.float32(0.9498), np.float32(0.7512)] 
2024-11-12 00:26:56.410009: Epoch time: 39.64 s 
2024-11-12 00:26:57.567390:  
2024-11-12 00:26:57.569920: Epoch 626 
2024-11-12 00:26:57.578305: Current learning rate: 0.00413 
2024-11-12 00:27:37.241010: train_loss -0.9334 
2024-11-12 00:27:37.245155: val_loss -0.809 
2024-11-12 00:27:37.247499: Pseudo dice [np.float32(0.9562), np.float32(0.8121)] 
2024-11-12 00:27:37.249872: Epoch time: 39.67 s 
2024-11-12 00:27:38.412888:  
2024-11-12 00:27:38.418760: Epoch 627 
2024-11-12 00:27:38.421232: Current learning rate: 0.00412 
2024-11-12 00:28:18.070578: train_loss -0.9378 
2024-11-12 00:28:18.073618: val_loss -0.8148 
2024-11-12 00:28:18.075857: Pseudo dice [np.float32(0.9574), np.float32(0.7884)] 
2024-11-12 00:28:18.078191: Epoch time: 39.66 s 
2024-11-12 00:28:19.241289:  
2024-11-12 00:28:19.244014: Epoch 628 
2024-11-12 00:28:19.251812: Current learning rate: 0.00411 
2024-11-12 00:28:58.901078: train_loss -0.9319 
2024-11-12 00:28:58.905523: val_loss -0.7587 
2024-11-12 00:28:58.907751: Pseudo dice [np.float32(0.9589), np.float32(0.7503)] 
2024-11-12 00:28:58.910069: Epoch time: 39.66 s 
2024-11-12 00:29:00.071261:  
2024-11-12 00:29:00.073881: Epoch 629 
2024-11-12 00:29:00.082681: Current learning rate: 0.0041 
2024-11-12 00:29:39.741080: train_loss -0.9352 
2024-11-12 00:29:39.744074: val_loss -0.8457 
2024-11-12 00:29:39.746333: Pseudo dice [np.float32(0.9634), np.float32(0.8111)] 
2024-11-12 00:29:39.748425: Epoch time: 39.67 s 
2024-11-12 00:29:40.907792:  
2024-11-12 00:29:40.910561: Epoch 630 
2024-11-12 00:29:40.918800: Current learning rate: 0.00409 
2024-11-12 00:30:20.539152: train_loss -0.943 
2024-11-12 00:30:20.543893: val_loss -0.8527 
2024-11-12 00:30:20.546201: Pseudo dice [np.float32(0.9589), np.float32(0.8276)] 
2024-11-12 00:30:20.548730: Epoch time: 39.63 s 
2024-11-12 00:30:21.714589:  
2024-11-12 00:30:21.717418: Epoch 631 
2024-11-12 00:30:21.725112: Current learning rate: 0.00408 
2024-11-12 00:31:01.356040: train_loss -0.9309 
2024-11-12 00:31:01.359356: val_loss -0.8217 
2024-11-12 00:31:01.362102: Pseudo dice [np.float32(0.9623), np.float32(0.8039)] 
2024-11-12 00:31:01.364790: Epoch time: 39.64 s 
2024-11-12 00:31:02.525412:  
2024-11-12 00:31:02.531579: Epoch 632 
2024-11-12 00:31:02.534207: Current learning rate: 0.00407 
2024-11-12 00:31:42.127398: train_loss -0.939 
2024-11-12 00:31:42.132138: val_loss -0.8267 
2024-11-12 00:31:42.134811: Pseudo dice [np.float32(0.9622), np.float32(0.7968)] 
2024-11-12 00:31:42.137048: Epoch time: 39.6 s 
2024-11-12 00:31:43.291085:  
2024-11-12 00:31:43.293588: Epoch 633 
2024-11-12 00:31:43.300994: Current learning rate: 0.00406 
2024-11-12 00:32:22.890979: train_loss -0.938 
2024-11-12 00:32:22.894488: val_loss -0.821 
2024-11-12 00:32:22.897178: Pseudo dice [np.float32(0.9569), np.float32(0.7824)] 
2024-11-12 00:32:22.899472: Epoch time: 39.6 s 
2024-11-12 00:32:24.055214:  
2024-11-12 00:32:24.058003: Epoch 634 
2024-11-12 00:32:24.066195: Current learning rate: 0.00405 
2024-11-12 00:33:03.640796: train_loss -0.9349 
2024-11-12 00:33:03.645626: val_loss -0.8256 
2024-11-12 00:33:03.648272: Pseudo dice [np.float32(0.9586), np.float32(0.7878)] 
2024-11-12 00:33:03.650589: Epoch time: 39.59 s 
2024-11-12 00:33:05.112790:  
2024-11-12 00:33:05.118400: Epoch 635 
2024-11-12 00:33:05.122138: Current learning rate: 0.00404 
2024-11-12 00:33:44.728149: train_loss -0.9245 
2024-11-12 00:33:44.731530: val_loss -0.8111 
2024-11-12 00:33:44.733975: Pseudo dice [np.float32(0.9467), np.float32(0.752)] 
2024-11-12 00:33:44.736137: Epoch time: 39.62 s 
2024-11-12 00:33:45.896200:  
2024-11-12 00:33:45.898999: Epoch 636 
2024-11-12 00:33:45.906892: Current learning rate: 0.00403 
2024-11-12 00:34:25.503552: train_loss -0.932 
2024-11-12 00:34:25.508220: val_loss -0.8164 
2024-11-12 00:34:25.510577: Pseudo dice [np.float32(0.9596), np.float32(0.7978)] 
2024-11-12 00:34:25.512873: Epoch time: 39.61 s 
2024-11-12 00:34:26.672422:  
2024-11-12 00:34:26.674715: Epoch 637 
2024-11-12 00:34:26.683628: Current learning rate: 0.00402 
2024-11-12 00:35:06.271301: train_loss -0.9273 
2024-11-12 00:35:06.274559: val_loss -0.7939 
2024-11-12 00:35:06.277033: Pseudo dice [np.float32(0.9577), np.float32(0.776)] 
2024-11-12 00:35:06.279394: Epoch time: 39.6 s 
2024-11-12 00:35:07.438265:  
2024-11-12 00:35:07.444088: Epoch 638 
2024-11-12 00:35:07.446482: Current learning rate: 0.00401 
2024-11-12 00:35:47.032209: train_loss -0.9297 
2024-11-12 00:35:47.036721: val_loss -0.7824 
2024-11-12 00:35:47.039083: Pseudo dice [np.float32(0.9545), np.float32(0.7231)] 
2024-11-12 00:35:47.041240: Epoch time: 39.6 s 
2024-11-12 00:35:48.202923:  
2024-11-12 00:35:48.205461: Epoch 639 
2024-11-12 00:35:48.214581: Current learning rate: 0.004 
2024-11-12 00:36:27.803868: train_loss -0.9166 
2024-11-12 00:36:27.807182: val_loss -0.8345 
2024-11-12 00:36:27.809723: Pseudo dice [np.float32(0.9578), np.float32(0.8048)] 
2024-11-12 00:36:27.811843: Epoch time: 39.6 s 
2024-11-12 00:36:28.967687:  
2024-11-12 00:36:28.974453: Epoch 640 
2024-11-12 00:36:28.976964: Current learning rate: 0.00399 
2024-11-12 00:37:08.595588: train_loss -0.9077 
2024-11-12 00:37:08.600369: val_loss -0.7645 
2024-11-12 00:37:08.602788: Pseudo dice [np.float32(0.948), np.float32(0.7516)] 
2024-11-12 00:37:08.605610: Epoch time: 39.63 s 
2024-11-12 00:37:09.768156:  
2024-11-12 00:37:09.770846: Epoch 641 
2024-11-12 00:37:09.779739: Current learning rate: 0.00398 
2024-11-12 00:37:49.391744: train_loss -0.8934 
2024-11-12 00:37:49.394883: val_loss -0.7538 
2024-11-12 00:37:49.397199: Pseudo dice [np.float32(0.9497), np.float32(0.6762)] 
2024-11-12 00:37:49.399410: Epoch time: 39.62 s 
2024-11-12 00:37:50.553054:  
2024-11-12 00:37:50.555483: Epoch 642 
2024-11-12 00:37:50.563149: Current learning rate: 0.00397 
2024-11-12 00:38:30.162901: train_loss -0.9203 
2024-11-12 00:38:30.167677: val_loss -0.7879 
2024-11-12 00:38:30.170261: Pseudo dice [np.float32(0.9537), np.float32(0.7458)] 
2024-11-12 00:38:30.172668: Epoch time: 39.61 s 
2024-11-12 00:38:31.329425:  
2024-11-12 00:38:31.335934: Epoch 643 
2024-11-12 00:38:31.338517: Current learning rate: 0.00396 
2024-11-12 00:39:10.947884: train_loss -0.9347 
2024-11-12 00:39:10.951266: val_loss -0.8203 
2024-11-12 00:39:10.953691: Pseudo dice [np.float32(0.9567), np.float32(0.7825)] 
2024-11-12 00:39:10.956323: Epoch time: 39.62 s 
2024-11-12 00:39:12.111611:  
2024-11-12 00:39:12.114463: Epoch 644 
2024-11-12 00:39:12.124358: Current learning rate: 0.00395 
2024-11-12 00:39:51.692767: train_loss -0.9186 
2024-11-12 00:39:51.697177: val_loss -0.7928 
2024-11-12 00:39:51.699633: Pseudo dice [np.float32(0.9579), np.float32(0.7356)] 
2024-11-12 00:39:51.701799: Epoch time: 39.58 s 
2024-11-12 00:39:52.862483:  
2024-11-12 00:39:52.865203: Epoch 645 
2024-11-12 00:39:52.873900: Current learning rate: 0.00394 
2024-11-12 00:40:32.452217: train_loss -0.9269 
2024-11-12 00:40:32.455545: val_loss -0.7264 
2024-11-12 00:40:32.458015: Pseudo dice [np.float32(0.948), np.float32(0.6535)] 
2024-11-12 00:40:32.460597: Epoch time: 39.59 s 
2024-11-12 00:40:33.617079:  
2024-11-12 00:40:33.622186: Epoch 646 
2024-11-12 00:40:33.641647: Current learning rate: 0.00393 
2024-11-12 00:41:13.238769: train_loss -0.9232 
2024-11-12 00:41:13.243524: val_loss -0.7901 
2024-11-12 00:41:13.246130: Pseudo dice [np.float32(0.9545), np.float32(0.7672)] 
2024-11-12 00:41:13.248517: Epoch time: 39.62 s 
2024-11-12 00:41:14.407796:  
2024-11-12 00:41:14.410306: Epoch 647 
2024-11-12 00:41:14.417412: Current learning rate: 0.00392 
2024-11-12 00:41:53.981881: train_loss -0.9275 
2024-11-12 00:41:53.985062: val_loss -0.8166 
2024-11-12 00:41:53.987435: Pseudo dice [np.float32(0.9547), np.float32(0.8168)] 
2024-11-12 00:41:53.989845: Epoch time: 39.58 s 
2024-11-12 00:41:55.454564:  
2024-11-12 00:41:55.458067: Epoch 648 
2024-11-12 00:41:55.466786: Current learning rate: 0.00391 
2024-11-12 00:42:35.052244: train_loss -0.9261 
2024-11-12 00:42:35.057214: val_loss -0.7979 
2024-11-12 00:42:35.059546: Pseudo dice [np.float32(0.9553), np.float32(0.8082)] 
2024-11-12 00:42:35.061933: Epoch time: 39.6 s 
2024-11-12 00:42:36.222292:  
2024-11-12 00:42:36.229365: Epoch 649 
2024-11-12 00:42:36.231936: Current learning rate: 0.0039 
2024-11-12 00:43:15.825092: train_loss -0.9331 
2024-11-12 00:43:15.828288: val_loss -0.7836 
2024-11-12 00:43:15.830487: Pseudo dice [np.float32(0.9487), np.float32(0.7194)] 
2024-11-12 00:43:15.833022: Epoch time: 39.6 s 
2024-11-12 00:43:17.800131:  
2024-11-12 00:43:17.806629: Epoch 650 
2024-11-12 00:43:17.809229: Current learning rate: 0.00389 
2024-11-12 00:43:57.411334: train_loss -0.9342 
2024-11-12 00:43:57.416296: val_loss -0.8265 
2024-11-12 00:43:57.418588: Pseudo dice [np.float32(0.961), np.float32(0.8027)] 
2024-11-12 00:43:57.420856: Epoch time: 39.61 s 
2024-11-12 00:43:58.578367:  
2024-11-12 00:43:58.580926: Epoch 651 
2024-11-12 00:43:58.583276: Current learning rate: 0.00388 
2024-11-12 00:44:38.175195: train_loss -0.9277 
2024-11-12 00:44:38.178631: val_loss -0.8084 
2024-11-12 00:44:38.180791: Pseudo dice [np.float32(0.9598), np.float32(0.7874)] 
2024-11-12 00:44:38.183173: Epoch time: 39.6 s 
2024-11-12 00:44:39.345231:  
2024-11-12 00:44:39.347833: Epoch 652 
2024-11-12 00:44:39.358863: Current learning rate: 0.00387 
2024-11-12 00:45:18.970496: train_loss -0.9389 
2024-11-12 00:45:18.975109: val_loss -0.7957 
2024-11-12 00:45:18.978660: Pseudo dice [np.float32(0.9598), np.float32(0.7806)] 
2024-11-12 00:45:18.981083: Epoch time: 39.63 s 
2024-11-12 00:45:20.140627:  
2024-11-12 00:45:20.143073: Epoch 653 
2024-11-12 00:45:20.151298: Current learning rate: 0.00386 
2024-11-12 00:45:59.766984: train_loss -0.9313 
2024-11-12 00:45:59.771496: val_loss -0.8419 
2024-11-12 00:45:59.773779: Pseudo dice [np.float32(0.9596), np.float32(0.8372)] 
2024-11-12 00:45:59.776505: Epoch time: 39.63 s 
2024-11-12 00:46:00.930738:  
2024-11-12 00:46:00.937033: Epoch 654 
2024-11-12 00:46:00.954502: Current learning rate: 0.00385 
2024-11-12 00:46:40.579360: train_loss -0.9222 
2024-11-12 00:46:40.583566: val_loss -0.8297 
2024-11-12 00:46:40.585774: Pseudo dice [np.float32(0.9596), np.float32(0.8098)] 
2024-11-12 00:46:40.587944: Epoch time: 39.65 s 
2024-11-12 00:46:41.744390:  
2024-11-12 00:46:41.751300: Epoch 655 
2024-11-12 00:46:41.754844: Current learning rate: 0.00384 
2024-11-12 00:47:21.363929: train_loss -0.9382 
2024-11-12 00:47:21.367664: val_loss -0.8133 
2024-11-12 00:47:21.370302: Pseudo dice [np.float32(0.9598), np.float32(0.8051)] 
2024-11-12 00:47:21.374974: Epoch time: 39.62 s 
2024-11-12 00:47:22.539861:  
2024-11-12 00:47:22.542477: Epoch 656 
2024-11-12 00:47:22.551699: Current learning rate: 0.00383 
2024-11-12 00:48:02.174836: train_loss -0.9386 
2024-11-12 00:48:02.179628: val_loss -0.835 
2024-11-12 00:48:02.182348: Pseudo dice [np.float32(0.9568), np.float32(0.8152)] 
2024-11-12 00:48:02.185096: Epoch time: 39.64 s 
2024-11-12 00:48:03.341233:  
2024-11-12 00:48:03.347606: Epoch 657 
2024-11-12 00:48:03.350377: Current learning rate: 0.00382 
2024-11-12 00:48:42.961535: train_loss -0.9387 
2024-11-12 00:48:42.964905: val_loss -0.7949 
2024-11-12 00:48:42.967400: Pseudo dice [np.float32(0.9555), np.float32(0.7933)] 
2024-11-12 00:48:42.969773: Epoch time: 39.62 s 
2024-11-12 00:48:44.126877:  
2024-11-12 00:48:44.129478: Epoch 658 
2024-11-12 00:48:44.140983: Current learning rate: 0.00381 
2024-11-12 00:49:23.749323: train_loss -0.9304 
2024-11-12 00:49:23.753845: val_loss -0.8632 
2024-11-12 00:49:23.756349: Pseudo dice [np.float32(0.9585), np.float32(0.8569)] 
2024-11-12 00:49:23.759118: Epoch time: 39.62 s 
2024-11-12 00:49:24.920112:  
2024-11-12 00:49:24.922730: Epoch 659 
2024-11-12 00:49:24.931111: Current learning rate: 0.0038 
2024-11-12 00:50:04.572021: train_loss -0.938 
2024-11-12 00:50:04.576376: val_loss -0.8532 
2024-11-12 00:50:04.578780: Pseudo dice [np.float32(0.9628), np.float32(0.8289)] 
2024-11-12 00:50:04.581031: Epoch time: 39.65 s 
2024-11-12 00:50:06.051748:  
2024-11-12 00:50:06.057945: Epoch 660 
2024-11-12 00:50:06.060365: Current learning rate: 0.00379 
2024-11-12 00:50:45.693757: train_loss -0.9339 
2024-11-12 00:50:45.698200: val_loss -0.8426 
2024-11-12 00:50:45.700413: Pseudo dice [np.float32(0.9578), np.float32(0.8203)] 
2024-11-12 00:50:45.702885: Epoch time: 39.64 s 
2024-11-12 00:50:46.871550:  
2024-11-12 00:50:46.877825: Epoch 661 
2024-11-12 00:50:46.880638: Current learning rate: 0.00378 
2024-11-12 00:51:26.505574: train_loss -0.9351 
2024-11-12 00:51:26.508962: val_loss -0.8369 
2024-11-12 00:51:26.511270: Pseudo dice [np.float32(0.9602), np.float32(0.8205)] 
2024-11-12 00:51:26.513571: Epoch time: 39.64 s 
2024-11-12 00:51:27.674064:  
2024-11-12 00:51:27.680557: Epoch 662 
2024-11-12 00:51:27.685441: Current learning rate: 0.00377 
2024-11-12 00:52:07.319162: train_loss -0.9326 
2024-11-12 00:52:07.323862: val_loss -0.8388 
2024-11-12 00:52:07.326147: Pseudo dice [np.float32(0.9627), np.float32(0.8343)] 
2024-11-12 00:52:07.328382: Epoch time: 39.65 s 
2024-11-12 00:52:08.499943:  
2024-11-12 00:52:08.502624: Epoch 663 
2024-11-12 00:52:08.512274: Current learning rate: 0.00376 
2024-11-12 00:52:48.137813: train_loss -0.9301 
2024-11-12 00:52:48.141175: val_loss -0.8448 
2024-11-12 00:52:48.143667: Pseudo dice [np.float32(0.9631), np.float32(0.8557)] 
2024-11-12 00:52:48.145811: Epoch time: 39.64 s 
2024-11-12 00:52:49.301560:  
2024-11-12 00:52:49.304352: Epoch 664 
2024-11-12 00:52:49.312557: Current learning rate: 0.00375 
2024-11-12 00:53:28.920521: train_loss -0.9333 
2024-11-12 00:53:28.925225: val_loss -0.8227 
2024-11-12 00:53:28.927379: Pseudo dice [np.float32(0.9597), np.float32(0.8436)] 
2024-11-12 00:53:28.929673: Epoch time: 39.62 s 
2024-11-12 00:53:30.088460:  
2024-11-12 00:53:30.095868: Epoch 665 
2024-11-12 00:53:30.098228: Current learning rate: 0.00374 
2024-11-12 00:54:09.709215: train_loss -0.9371 
2024-11-12 00:54:09.712515: val_loss -0.8347 
2024-11-12 00:54:09.715136: Pseudo dice [np.float32(0.9652), np.float32(0.8388)] 
2024-11-12 00:54:09.717484: Epoch time: 39.62 s 
2024-11-12 00:54:09.719882: Yayy! New best EMA pseudo Dice: 0.8877000212669373 
2024-11-12 00:54:11.637733:  
2024-11-12 00:54:11.640255: Epoch 666 
2024-11-12 00:54:11.648387: Current learning rate: 0.00373 
2024-11-12 00:54:51.262865: train_loss -0.9344 
2024-11-12 00:54:51.267774: val_loss -0.8394 
2024-11-12 00:54:51.270381: Pseudo dice [np.float32(0.9612), np.float32(0.8313)] 
2024-11-12 00:54:51.272758: Epoch time: 39.63 s 
2024-11-12 00:54:51.275020: Yayy! New best EMA pseudo Dice: 0.8884999752044678 
2024-11-12 00:54:53.252188:  
2024-11-12 00:54:53.254761: Epoch 667 
2024-11-12 00:54:53.257411: Current learning rate: 0.00372 
2024-11-12 00:55:32.842115: train_loss -0.9401 
2024-11-12 00:55:32.845675: val_loss -0.8427 
2024-11-12 00:55:32.848277: Pseudo dice [np.float32(0.9575), np.float32(0.825)] 
2024-11-12 00:55:32.850730: Epoch time: 39.59 s 
2024-11-12 00:55:32.853084: Yayy! New best EMA pseudo Dice: 0.8888000249862671 
2024-11-12 00:55:34.891788:  
2024-11-12 00:55:34.894723: Epoch 668 
2024-11-12 00:55:34.897285: Current learning rate: 0.00371 
2024-11-12 00:56:14.493877: train_loss -0.9408 
2024-11-12 00:56:14.498357: val_loss -0.8449 
2024-11-12 00:56:14.500766: Pseudo dice [np.float32(0.9611), np.float32(0.8318)] 
2024-11-12 00:56:14.503053: Epoch time: 39.6 s 
2024-11-12 00:56:14.505305: Yayy! New best EMA pseudo Dice: 0.8895999789237976 
2024-11-12 00:56:16.512686:  
2024-11-12 00:56:16.515098: Epoch 669 
2024-11-12 00:56:16.517683: Current learning rate: 0.0037 
2024-11-12 00:56:56.128687: train_loss -0.9449 
2024-11-12 00:56:56.132153: val_loss -0.8359 
2024-11-12 00:56:56.134543: Pseudo dice [np.float32(0.9614), np.float32(0.8098)] 
2024-11-12 00:56:56.136845: Epoch time: 39.62 s 
2024-11-12 00:56:57.312027:  
2024-11-12 00:56:57.314545: Epoch 670 
2024-11-12 00:56:57.322991: Current learning rate: 0.00369 
2024-11-12 00:57:36.944304: train_loss -0.9299 
2024-11-12 00:57:36.948512: val_loss -0.8295 
2024-11-12 00:57:36.950875: Pseudo dice [np.float32(0.9525), np.float32(0.8062)] 
2024-11-12 00:57:36.953117: Epoch time: 39.63 s 
2024-11-12 00:57:38.122384:  
2024-11-12 00:57:38.124956: Epoch 671 
2024-11-12 00:57:38.127241: Current learning rate: 0.00368 
2024-11-12 00:58:17.764132: train_loss -0.9317 
2024-11-12 00:58:17.767203: val_loss -0.8691 
2024-11-12 00:58:17.774478: Pseudo dice [np.float32(0.9608), np.float32(0.8423)] 
2024-11-12 00:58:17.776826: Epoch time: 39.64 s 
2024-11-12 00:58:19.264791:  
2024-11-12 00:58:19.267572: Epoch 672 
2024-11-12 00:58:19.274595: Current learning rate: 0.00367 
2024-11-12 00:58:58.937341: train_loss -0.9318 
2024-11-12 00:58:58.941759: val_loss -0.8072 
2024-11-12 00:58:58.944247: Pseudo dice [np.float32(0.9586), np.float32(0.7644)] 
2024-11-12 00:58:58.946408: Epoch time: 39.67 s 
2024-11-12 00:59:00.117807:  
2024-11-12 00:59:00.120923: Epoch 673 
2024-11-12 00:59:00.129979: Current learning rate: 0.00366 
2024-11-12 00:59:39.772986: train_loss -0.9241 
2024-11-12 00:59:39.776463: val_loss -0.8261 
2024-11-12 00:59:39.778902: Pseudo dice [np.float32(0.958), np.float32(0.8164)] 
2024-11-12 00:59:39.781239: Epoch time: 39.66 s 
2024-11-12 00:59:40.957207:  
2024-11-12 00:59:40.962639: Epoch 674 
2024-11-12 00:59:40.965112: Current learning rate: 0.00365 
2024-11-12 01:00:20.610146: train_loss -0.9288 
2024-11-12 01:00:20.614895: val_loss -0.8355 
2024-11-12 01:00:20.617440: Pseudo dice [np.float32(0.9532), np.float32(0.796)] 
2024-11-12 01:00:20.619859: Epoch time: 39.65 s 
2024-11-12 01:00:21.801054:  
2024-11-12 01:00:21.803840: Epoch 675 
2024-11-12 01:00:21.812208: Current learning rate: 0.00364 
2024-11-12 01:01:01.451122: train_loss -0.9305 
2024-11-12 01:01:01.454577: val_loss -0.8398 
2024-11-12 01:01:01.457106: Pseudo dice [np.float32(0.9562), np.float32(0.8463)] 
2024-11-12 01:01:01.459592: Epoch time: 39.65 s 
2024-11-12 01:01:02.634804:  
2024-11-12 01:01:02.640628: Epoch 676 
2024-11-12 01:01:02.643157: Current learning rate: 0.00363 
2024-11-12 01:01:42.290103: train_loss -0.9185 
2024-11-12 01:01:42.294641: val_loss -0.8315 
2024-11-12 01:01:42.297560: Pseudo dice [np.float32(0.9551), np.float32(0.7923)] 
2024-11-12 01:01:42.299860: Epoch time: 39.66 s 
2024-11-12 01:01:43.473024:  
2024-11-12 01:01:43.475682: Epoch 677 
2024-11-12 01:01:43.483877: Current learning rate: 0.00362 
2024-11-12 01:02:23.124957: train_loss -0.9221 
2024-11-12 01:02:23.128556: val_loss -0.7409 
2024-11-12 01:02:23.133089: Pseudo dice [np.float32(0.9482), np.float32(0.6894)] 
2024-11-12 01:02:23.135446: Epoch time: 39.65 s 
2024-11-12 01:02:24.311673:  
2024-11-12 01:02:24.314338: Epoch 678 
2024-11-12 01:02:24.322440: Current learning rate: 0.00361 
2024-11-12 01:03:03.957488: train_loss -0.932 
2024-11-12 01:03:03.962166: val_loss -0.7746 
2024-11-12 01:03:03.965078: Pseudo dice [np.float32(0.9578), np.float32(0.7719)] 
2024-11-12 01:03:03.967351: Epoch time: 39.65 s 
2024-11-12 01:03:05.140789:  
2024-11-12 01:03:05.146819: Epoch 679 
2024-11-12 01:03:05.149210: Current learning rate: 0.0036 
2024-11-12 01:03:44.761069: train_loss -0.9126 
2024-11-12 01:03:44.764410: val_loss -0.8406 
2024-11-12 01:03:44.766777: Pseudo dice [np.float32(0.955), np.float32(0.8105)] 
2024-11-12 01:03:44.769332: Epoch time: 39.62 s 
2024-11-12 01:03:45.947541:  
2024-11-12 01:03:45.950891: Epoch 680 
2024-11-12 01:03:45.959813: Current learning rate: 0.00359 
2024-11-12 01:04:25.581423: train_loss -0.9219 
2024-11-12 01:04:25.585998: val_loss -0.8087 
2024-11-12 01:04:25.588320: Pseudo dice [np.float32(0.9555), np.float32(0.7882)] 
2024-11-12 01:04:25.590449: Epoch time: 39.63 s 
2024-11-12 01:04:26.766208:  
2024-11-12 01:04:26.769075: Epoch 681 
2024-11-12 01:04:26.777603: Current learning rate: 0.00358 
2024-11-12 01:05:06.379358: train_loss -0.9282 
2024-11-12 01:05:06.383030: val_loss -0.8515 
2024-11-12 01:05:06.385742: Pseudo dice [np.float32(0.961), np.float32(0.8472)] 
2024-11-12 01:05:06.388051: Epoch time: 39.61 s 
2024-11-12 01:05:07.563453:  
2024-11-12 01:05:07.569212: Epoch 682 
2024-11-12 01:05:07.571990: Current learning rate: 0.00357 
2024-11-12 01:05:47.179013: train_loss -0.9329 
2024-11-12 01:05:47.186422: val_loss -0.8656 
2024-11-12 01:05:47.188844: Pseudo dice [np.float32(0.9616), np.float32(0.8479)] 
2024-11-12 01:05:47.190972: Epoch time: 39.62 s 
2024-11-12 01:05:48.363960:  
2024-11-12 01:05:48.366413: Epoch 683 
2024-11-12 01:05:48.374079: Current learning rate: 0.00356 
2024-11-12 01:06:27.965260: train_loss -0.9364 
2024-11-12 01:06:27.968624: val_loss -0.8232 
2024-11-12 01:06:27.970967: Pseudo dice [np.float32(0.962), np.float32(0.8325)] 
2024-11-12 01:06:27.973270: Epoch time: 39.6 s 
2024-11-12 01:06:29.145855:  
2024-11-12 01:06:29.148537: Epoch 684 
2024-11-12 01:06:29.157755: Current learning rate: 0.00355 
2024-11-12 01:07:08.757205: train_loss -0.9264 
2024-11-12 01:07:08.761638: val_loss -0.8405 
2024-11-12 01:07:08.764144: Pseudo dice [np.float32(0.9589), np.float32(0.8517)] 
2024-11-12 01:07:08.766497: Epoch time: 39.61 s 
2024-11-12 01:07:10.252581:  
2024-11-12 01:07:10.258769: Epoch 685 
2024-11-12 01:07:10.261096: Current learning rate: 0.00354 
2024-11-12 01:07:49.870776: train_loss -0.9364 
2024-11-12 01:07:49.874387: val_loss -0.8467 
2024-11-12 01:07:49.876759: Pseudo dice [np.float32(0.9602), np.float32(0.8186)] 
2024-11-12 01:07:49.878894: Epoch time: 39.62 s 
2024-11-12 01:07:51.052700:  
2024-11-12 01:07:51.055202: Epoch 686 
2024-11-12 01:07:51.063686: Current learning rate: 0.00353 
2024-11-12 01:08:30.665679: train_loss -0.9382 
2024-11-12 01:08:30.678264: val_loss -0.7814 
2024-11-12 01:08:30.680805: Pseudo dice [np.float32(0.9644), np.float32(0.7844)] 
2024-11-12 01:08:30.683196: Epoch time: 39.61 s 
2024-11-12 01:08:31.861678:  
2024-11-12 01:08:31.867250: Epoch 687 
2024-11-12 01:08:31.869669: Current learning rate: 0.00352 
2024-11-12 01:09:11.490058: train_loss -0.9406 
2024-11-12 01:09:11.493622: val_loss -0.8455 
2024-11-12 01:09:11.496044: Pseudo dice [np.float32(0.96), np.float32(0.8185)] 
2024-11-12 01:09:11.498569: Epoch time: 39.63 s 
2024-11-12 01:09:12.678839:  
2024-11-12 01:09:12.681390: Epoch 688 
2024-11-12 01:09:12.689869: Current learning rate: 0.00351 
2024-11-12 01:09:52.287856: train_loss -0.9392 
2024-11-12 01:09:52.292968: val_loss -0.8428 
2024-11-12 01:09:52.295775: Pseudo dice [np.float32(0.9533), np.float32(0.7976)] 
2024-11-12 01:09:52.298124: Epoch time: 39.61 s 
2024-11-12 01:09:53.476707:  
2024-11-12 01:09:53.479237: Epoch 689 
2024-11-12 01:09:53.486789: Current learning rate: 0.0035 
2024-11-12 01:10:33.091283: train_loss -0.9322 
2024-11-12 01:10:33.098061: val_loss -0.8091 
2024-11-12 01:10:33.100623: Pseudo dice [np.float32(0.9619), np.float32(0.7847)] 
2024-11-12 01:10:33.102984: Epoch time: 39.62 s 
2024-11-12 01:10:34.278361:  
2024-11-12 01:10:34.280724: Epoch 690 
2024-11-12 01:10:34.282992: Current learning rate: 0.00349 
2024-11-12 01:11:13.901470: train_loss -0.9325 
2024-11-12 01:11:13.906385: val_loss -0.8299 
2024-11-12 01:11:13.908914: Pseudo dice [np.float32(0.9605), np.float32(0.8101)] 
2024-11-12 01:11:13.911750: Epoch time: 39.62 s 
2024-11-12 01:11:15.091488:  
2024-11-12 01:11:15.094171: Epoch 691 
2024-11-12 01:11:15.101733: Current learning rate: 0.00348 
2024-11-12 01:11:54.699169: train_loss -0.9325 
2024-11-12 01:11:54.702466: val_loss -0.7692 
2024-11-12 01:11:54.704781: Pseudo dice [np.float32(0.953), np.float32(0.694)] 
2024-11-12 01:11:54.707193: Epoch time: 39.61 s 
2024-11-12 01:11:55.885265:  
2024-11-12 01:11:55.888031: Epoch 692 
2024-11-12 01:11:55.896199: Current learning rate: 0.00346 
2024-11-12 01:12:35.493369: train_loss -0.9403 
2024-11-12 01:12:35.502383: val_loss -0.8538 
2024-11-12 01:12:35.504756: Pseudo dice [np.float32(0.9592), np.float32(0.8265)] 
2024-11-12 01:12:35.507142: Epoch time: 39.61 s 
2024-11-12 01:12:36.688274:  
2024-11-12 01:12:36.695099: Epoch 693 
2024-11-12 01:12:36.697782: Current learning rate: 0.00345 
2024-11-12 01:13:16.309101: train_loss -0.9426 
2024-11-12 01:13:16.312680: val_loss -0.8316 
2024-11-12 01:13:16.315267: Pseudo dice [np.float32(0.9625), np.float32(0.8585)] 
2024-11-12 01:13:16.317677: Epoch time: 39.62 s 
2024-11-12 01:13:17.496566:  
2024-11-12 01:13:17.499018: Epoch 694 
2024-11-12 01:13:17.507544: Current learning rate: 0.00344 
2024-11-12 01:13:57.112534: train_loss -0.9465 
2024-11-12 01:13:57.117320: val_loss -0.8447 
2024-11-12 01:13:57.119871: Pseudo dice [np.float32(0.9596), np.float32(0.8225)] 
2024-11-12 01:13:57.122391: Epoch time: 39.62 s 
2024-11-12 01:13:58.302113:  
2024-11-12 01:13:58.304456: Epoch 695 
2024-11-12 01:13:58.312663: Current learning rate: 0.00343 
2024-11-12 01:14:37.906667: train_loss -0.9349 
2024-11-12 01:14:37.913440: val_loss -0.784 
2024-11-12 01:14:37.915727: Pseudo dice [np.float32(0.9577), np.float32(0.7443)] 
2024-11-12 01:14:37.918589: Epoch time: 39.61 s 
2024-11-12 01:14:39.090101:  
2024-11-12 01:14:39.092603: Epoch 696 
2024-11-12 01:14:39.101428: Current learning rate: 0.00342 
2024-11-12 01:15:18.703434: train_loss -0.9386 
2024-11-12 01:15:18.708143: val_loss -0.8393 
2024-11-12 01:15:18.710497: Pseudo dice [np.float32(0.9631), np.float32(0.8386)] 
2024-11-12 01:15:18.712845: Epoch time: 39.61 s 
2024-11-12 01:15:20.204520:  
2024-11-12 01:15:20.207056: Epoch 697 
2024-11-12 01:15:20.215686: Current learning rate: 0.00341 
2024-11-12 01:15:59.832250: train_loss -0.9454 
2024-11-12 01:15:59.835640: val_loss -0.7996 
2024-11-12 01:15:59.838255: Pseudo dice [np.float32(0.9604), np.float32(0.7985)] 
2024-11-12 01:15:59.840439: Epoch time: 39.63 s 
2024-11-12 01:16:01.018052:  
2024-11-12 01:16:01.020869: Epoch 698 
2024-11-12 01:16:01.033258: Current learning rate: 0.0034 
2024-11-12 01:16:40.665203: train_loss -0.9441 
2024-11-12 01:16:40.677962: val_loss -0.8859 
2024-11-12 01:16:40.680377: Pseudo dice [np.float32(0.968), np.float32(0.8721)] 
2024-11-12 01:16:40.682735: Epoch time: 39.65 s 
2024-11-12 01:16:41.860382:  
2024-11-12 01:16:41.862784: Epoch 699 
2024-11-12 01:16:41.865105: Current learning rate: 0.00339 
2024-11-12 01:17:21.493481: train_loss -0.9458 
2024-11-12 01:17:21.496938: val_loss -0.8377 
2024-11-12 01:17:21.499131: Pseudo dice [np.float32(0.9638), np.float32(0.8226)] 
2024-11-12 01:17:21.501463: Epoch time: 39.63 s 
2024-11-12 01:17:23.488333:  
2024-11-12 01:17:23.493832: Epoch 700 
2024-11-12 01:17:23.496275: Current learning rate: 0.00338 
2024-11-12 01:18:03.127838: train_loss -0.9463 
2024-11-12 01:18:03.132774: val_loss -0.8307 
2024-11-12 01:18:03.135157: Pseudo dice [np.float32(0.967), np.float32(0.8067)] 
2024-11-12 01:18:03.137607: Epoch time: 39.64 s 
2024-11-12 01:18:04.318353:  
2024-11-12 01:18:04.324841: Epoch 701 
2024-11-12 01:18:04.327210: Current learning rate: 0.00337 
2024-11-12 01:18:43.964872: train_loss -0.9449 
2024-11-12 01:18:43.968101: val_loss -0.8602 
2024-11-12 01:18:43.970325: Pseudo dice [np.float32(0.9621), np.float32(0.8572)] 
2024-11-12 01:18:43.972699: Epoch time: 39.65 s 
2024-11-12 01:18:45.148067:  
2024-11-12 01:18:45.150800: Epoch 702 
2024-11-12 01:18:45.160006: Current learning rate: 0.00336 
2024-11-12 01:19:24.794122: train_loss -0.9478 
2024-11-12 01:19:24.799376: val_loss -0.8595 
2024-11-12 01:19:24.801979: Pseudo dice [np.float32(0.9643), np.float32(0.8657)] 
2024-11-12 01:19:24.804209: Epoch time: 39.65 s 
2024-11-12 01:19:24.806702: Yayy! New best EMA pseudo Dice: 0.8913999795913696 
2024-11-12 01:19:26.803487:  
2024-11-12 01:19:26.810468: Epoch 703 
2024-11-12 01:19:26.813855: Current learning rate: 0.00335 
2024-11-12 01:20:06.423718: train_loss -0.9472 
2024-11-12 01:20:06.427047: val_loss -0.8778 
2024-11-12 01:20:06.429363: Pseudo dice [np.float32(0.9619), np.float32(0.8591)] 
2024-11-12 01:20:06.431575: Epoch time: 39.62 s 
2024-11-12 01:20:06.433866: Yayy! New best EMA pseudo Dice: 0.8932999968528748 
2024-11-12 01:20:08.398467:  
2024-11-12 01:20:08.404843: Epoch 704 
2024-11-12 01:20:08.407597: Current learning rate: 0.00334 
2024-11-12 01:20:48.023426: train_loss -0.9442 
2024-11-12 01:20:48.033477: val_loss -0.8094 
2024-11-12 01:20:48.035823: Pseudo dice [np.float32(0.9568), np.float32(0.7901)] 
2024-11-12 01:20:48.038215: Epoch time: 39.63 s 
2024-11-12 01:20:49.217078:  
2024-11-12 01:20:49.219557: Epoch 705 
2024-11-12 01:20:49.227957: Current learning rate: 0.00333 
2024-11-12 01:21:28.850342: train_loss -0.9456 
2024-11-12 01:21:28.853620: val_loss -0.8512 
2024-11-12 01:21:28.855958: Pseudo dice [np.float32(0.966), np.float32(0.854)] 
2024-11-12 01:21:28.858045: Epoch time: 39.63 s 
2024-11-12 01:21:30.039721:  
2024-11-12 01:21:30.042042: Epoch 706 
2024-11-12 01:21:30.050067: Current learning rate: 0.00332 
2024-11-12 01:22:09.670780: train_loss -0.9428 
2024-11-12 01:22:09.675514: val_loss -0.8603 
2024-11-12 01:22:09.677733: Pseudo dice [np.float32(0.9654), np.float32(0.8499)] 
2024-11-12 01:22:09.679848: Epoch time: 39.63 s 
2024-11-12 01:22:09.681966: Yayy! New best EMA pseudo Dice: 0.894599974155426 
2024-11-12 01:22:11.640401:  
2024-11-12 01:22:11.646722: Epoch 707 
2024-11-12 01:22:11.649117: Current learning rate: 0.00331 
2024-11-12 01:22:51.255866: train_loss -0.9462 
2024-11-12 01:22:51.259579: val_loss -0.8049 
2024-11-12 01:22:51.261941: Pseudo dice [np.float32(0.9612), np.float32(0.8065)] 
2024-11-12 01:22:51.264115: Epoch time: 39.62 s 
2024-11-12 01:22:52.439974:  
2024-11-12 01:22:52.445791: Epoch 708 
2024-11-12 01:22:52.447983: Current learning rate: 0.0033 
2024-11-12 01:23:32.054811: train_loss -0.9424 
2024-11-12 01:23:32.059681: val_loss -0.8428 
2024-11-12 01:23:32.062034: Pseudo dice [np.float32(0.9632), np.float32(0.8073)] 
2024-11-12 01:23:32.064406: Epoch time: 39.62 s 
2024-11-12 01:23:33.562089:  
2024-11-12 01:23:33.564482: Epoch 709 
2024-11-12 01:23:33.571441: Current learning rate: 0.00329 
2024-11-12 01:24:13.214536: train_loss -0.9475 
2024-11-12 01:24:13.217824: val_loss -0.8482 
2024-11-12 01:24:13.220268: Pseudo dice [np.float32(0.9617), np.float32(0.8341)] 
2024-11-12 01:24:13.222468: Epoch time: 39.65 s 
2024-11-12 01:24:14.405553:  
2024-11-12 01:24:14.411433: Epoch 710 
2024-11-12 01:24:14.413689: Current learning rate: 0.00328 
2024-11-12 01:24:54.057867: train_loss -0.9455 
2024-11-12 01:24:54.065997: val_loss -0.8577 
2024-11-12 01:24:54.068341: Pseudo dice [np.float32(0.962), np.float32(0.8345)] 
2024-11-12 01:24:54.070618: Epoch time: 39.65 s 
2024-11-12 01:24:55.249584:  
2024-11-12 01:24:55.252335: Epoch 711 
2024-11-12 01:24:55.260333: Current learning rate: 0.00327 
2024-11-12 01:25:34.881357: train_loss -0.9403 
2024-11-12 01:25:34.884775: val_loss -0.8647 
2024-11-12 01:25:34.887038: Pseudo dice [np.float32(0.9633), np.float32(0.8431)] 
2024-11-12 01:25:34.889375: Epoch time: 39.63 s 
2024-11-12 01:25:34.891518: Yayy! New best EMA pseudo Dice: 0.8946999907493591 
2024-11-12 01:25:36.878630:  
2024-11-12 01:25:36.883804: Epoch 712 
2024-11-12 01:25:36.886282: Current learning rate: 0.00326 
2024-11-12 01:26:16.493846: train_loss -0.9422 
2024-11-12 01:26:16.498381: val_loss -0.8357 
2024-11-12 01:26:16.500826: Pseudo dice [np.float32(0.9629), np.float32(0.8413)] 
2024-11-12 01:26:16.503163: Epoch time: 39.62 s 
2024-11-12 01:26:16.505417: Yayy! New best EMA pseudo Dice: 0.8953999876976013 
2024-11-12 01:26:18.493855:  
2024-11-12 01:26:18.496314: Epoch 713 
2024-11-12 01:26:18.498760: Current learning rate: 0.00325 
2024-11-12 01:26:58.117806: train_loss -0.9326 
2024-11-12 01:26:58.121978: val_loss -0.8298 
2024-11-12 01:26:58.124618: Pseudo dice [np.float32(0.9595), np.float32(0.8224)] 
2024-11-12 01:26:58.129164: Epoch time: 39.63 s 
2024-11-12 01:26:59.309062:  
2024-11-12 01:26:59.311512: Epoch 714 
2024-11-12 01:26:59.319834: Current learning rate: 0.00324 
2024-11-12 01:27:38.926726: train_loss -0.9447 
2024-11-12 01:27:38.931514: val_loss -0.8246 
2024-11-12 01:27:38.934090: Pseudo dice [np.float32(0.9576), np.float32(0.8)] 
2024-11-12 01:27:38.936365: Epoch time: 39.62 s 
2024-11-12 01:27:40.123466:  
2024-11-12 01:27:40.126362: Epoch 715 
2024-11-12 01:27:40.129065: Current learning rate: 0.00323 
2024-11-12 01:28:19.744420: train_loss -0.9376 
2024-11-12 01:28:19.748287: val_loss -0.8528 
2024-11-12 01:28:19.750648: Pseudo dice [np.float32(0.9648), np.float32(0.8368)] 
2024-11-12 01:28:19.752834: Epoch time: 39.62 s 
2024-11-12 01:28:20.930063:  
2024-11-12 01:28:20.932646: Epoch 716 
2024-11-12 01:28:20.935174: Current learning rate: 0.00322 
2024-11-12 01:29:00.544693: train_loss -0.9372 
2024-11-12 01:29:00.549138: val_loss -0.8497 
2024-11-12 01:29:00.551452: Pseudo dice [np.float32(0.9623), np.float32(0.8304)] 
2024-11-12 01:29:00.555748: Epoch time: 39.62 s 
2024-11-12 01:29:01.734705:  
2024-11-12 01:29:01.737449: Epoch 717 
2024-11-12 01:29:01.749285: Current learning rate: 0.00321 
2024-11-12 01:29:41.360027: train_loss -0.9449 
2024-11-12 01:29:41.363352: val_loss -0.816 
2024-11-12 01:29:41.365705: Pseudo dice [np.float32(0.9607), np.float32(0.7876)] 
2024-11-12 01:29:41.367789: Epoch time: 39.63 s 
2024-11-12 01:29:42.550471:  
2024-11-12 01:29:42.553090: Epoch 718 
2024-11-12 01:29:42.555487: Current learning rate: 0.0032 
2024-11-12 01:30:22.155683: train_loss -0.9473 
2024-11-12 01:30:22.160139: val_loss -0.8077 
2024-11-12 01:30:22.162446: Pseudo dice [np.float32(0.9595), np.float32(0.8218)] 
2024-11-12 01:30:22.166932: Epoch time: 39.61 s 
2024-11-12 01:30:23.351686:  
2024-11-12 01:30:23.354608: Epoch 719 
2024-11-12 01:30:23.364341: Current learning rate: 0.00319 
2024-11-12 01:31:02.985421: train_loss -0.9434 
2024-11-12 01:31:02.988711: val_loss -0.839 
2024-11-12 01:31:02.990655: Pseudo dice [np.float32(0.9628), np.float32(0.7878)] 
2024-11-12 01:31:02.992963: Epoch time: 39.63 s 
2024-11-12 01:31:04.173806:  
2024-11-12 01:31:04.176297: Epoch 720 
2024-11-12 01:31:04.183623: Current learning rate: 0.00318 
2024-11-12 01:31:43.802743: train_loss -0.9405 
2024-11-12 01:31:43.812249: val_loss -0.8311 
2024-11-12 01:31:43.814480: Pseudo dice [np.float32(0.9564), np.float32(0.8191)] 
2024-11-12 01:31:43.816988: Epoch time: 39.63 s 
2024-11-12 01:31:45.315559:  
2024-11-12 01:31:45.321449: Epoch 721 
2024-11-12 01:31:45.323687: Current learning rate: 0.00317 
2024-11-12 01:32:24.971050: train_loss -0.9421 
2024-11-12 01:32:24.974530: val_loss -0.8471 
2024-11-12 01:32:24.976967: Pseudo dice [np.float32(0.9591), np.float32(0.8197)] 
2024-11-12 01:32:24.979535: Epoch time: 39.66 s 
2024-11-12 01:32:26.165585:  
2024-11-12 01:32:26.168161: Epoch 722 
2024-11-12 01:32:26.175776: Current learning rate: 0.00316 
2024-11-12 01:33:05.821729: train_loss -0.9445 
2024-11-12 01:33:05.826618: val_loss -0.8593 
2024-11-12 01:33:05.829170: Pseudo dice [np.float32(0.9634), np.float32(0.8495)] 
2024-11-12 01:33:05.831558: Epoch time: 39.66 s 
2024-11-12 01:33:07.013452:  
2024-11-12 01:33:07.016196: Epoch 723 
2024-11-12 01:33:07.025337: Current learning rate: 0.00315 
2024-11-12 01:33:46.653556: train_loss -0.9464 
2024-11-12 01:33:46.657203: val_loss -0.8603 
2024-11-12 01:33:46.659893: Pseudo dice [np.float32(0.959), np.float32(0.8274)] 
2024-11-12 01:33:46.662176: Epoch time: 39.64 s 
2024-11-12 01:33:47.846574:  
2024-11-12 01:33:47.852631: Epoch 724 
2024-11-12 01:33:47.855056: Current learning rate: 0.00314 
2024-11-12 01:34:27.483458: train_loss -0.9488 
2024-11-12 01:34:27.488518: val_loss -0.7901 
2024-11-12 01:34:27.491140: Pseudo dice [np.float32(0.9591), np.float32(0.7922)] 
2024-11-12 01:34:27.493565: Epoch time: 39.64 s 
2024-11-12 01:34:28.682199:  
2024-11-12 01:34:28.684815: Epoch 725 
2024-11-12 01:34:28.692662: Current learning rate: 0.00313 
2024-11-12 01:35:08.324839: train_loss -0.9486 
2024-11-12 01:35:08.328547: val_loss -0.8824 
2024-11-12 01:35:08.330921: Pseudo dice [np.float32(0.9665), np.float32(0.8592)] 
2024-11-12 01:35:08.333365: Epoch time: 39.64 s 
2024-11-12 01:35:09.516035:  
2024-11-12 01:35:09.521294: Epoch 726 
2024-11-12 01:35:09.523665: Current learning rate: 0.00312 
2024-11-12 01:35:49.147434: train_loss -0.9514 
2024-11-12 01:35:49.152363: val_loss -0.8537 
2024-11-12 01:35:49.154725: Pseudo dice [np.float32(0.9675), np.float32(0.8628)] 
2024-11-12 01:35:49.156997: Epoch time: 39.63 s 
2024-11-12 01:35:50.335927:  
2024-11-12 01:35:50.338468: Epoch 727 
2024-11-12 01:35:50.347095: Current learning rate: 0.00311 
2024-11-12 01:36:29.985236: train_loss -0.9511 
2024-11-12 01:36:29.992300: val_loss -0.8259 
2024-11-12 01:36:29.994644: Pseudo dice [np.float32(0.9633), np.float32(0.8144)] 
2024-11-12 01:36:29.996889: Epoch time: 39.65 s 
2024-11-12 01:36:31.179366:  
2024-11-12 01:36:31.181699: Epoch 728 
2024-11-12 01:36:31.189558: Current learning rate: 0.0031 
2024-11-12 01:37:10.816349: train_loss -0.9413 
2024-11-12 01:37:10.821491: val_loss -0.8646 
2024-11-12 01:37:10.823986: Pseudo dice [np.float32(0.962), np.float32(0.8495)] 
2024-11-12 01:37:10.826377: Epoch time: 39.64 s 
2024-11-12 01:37:12.007423:  
2024-11-12 01:37:12.009943: Epoch 729 
2024-11-12 01:37:12.012307: Current learning rate: 0.00309 
2024-11-12 01:37:51.624462: train_loss -0.9492 
2024-11-12 01:37:51.628037: val_loss -0.8021 
2024-11-12 01:37:51.630637: Pseudo dice [np.float32(0.9611), np.float32(0.803)] 
2024-11-12 01:37:51.632929: Epoch time: 39.62 s 
2024-11-12 01:37:52.814618:  
2024-11-12 01:37:52.817083: Epoch 730 
2024-11-12 01:37:52.824888: Current learning rate: 0.00308 
2024-11-12 01:38:32.439910: train_loss -0.9481 
2024-11-12 01:38:32.444569: val_loss -0.8338 
2024-11-12 01:38:32.446912: Pseudo dice [np.float32(0.9668), np.float32(0.8499)] 
2024-11-12 01:38:32.449350: Epoch time: 39.63 s 
2024-11-12 01:38:32.451454: Yayy! New best EMA pseudo Dice: 0.8955000042915344 
2024-11-12 01:38:34.469700:  
2024-11-12 01:38:34.472367: Epoch 731 
2024-11-12 01:38:34.479775: Current learning rate: 0.00307 
2024-11-12 01:39:14.082784: train_loss -0.941 
2024-11-12 01:39:14.087452: val_loss -0.8306 
2024-11-12 01:39:14.089858: Pseudo dice [np.float32(0.9622), np.float32(0.7971)] 
2024-11-12 01:39:14.092150: Epoch time: 39.61 s 
2024-11-12 01:39:15.269811:  
2024-11-12 01:39:15.275283: Epoch 732 
2024-11-12 01:39:15.277624: Current learning rate: 0.00306 
2024-11-12 01:39:54.901509: train_loss -0.95 
2024-11-12 01:39:54.906299: val_loss -0.7861 
2024-11-12 01:39:54.908713: Pseudo dice [np.float32(0.9567), np.float32(0.7445)] 
2024-11-12 01:39:54.910938: Epoch time: 39.63 s 
2024-11-12 01:39:56.410063:  
2024-11-12 01:39:56.414161: Epoch 733 
2024-11-12 01:39:56.423547: Current learning rate: 0.00305 
2024-11-12 01:40:36.058036: train_loss -0.9342 
2024-11-12 01:40:36.062017: val_loss -0.8095 
2024-11-12 01:40:36.064735: Pseudo dice [np.float32(0.9575), np.float32(0.7971)] 
2024-11-12 01:40:36.067079: Epoch time: 39.65 s 
2024-11-12 01:40:37.247874:  
2024-11-12 01:40:37.250314: Epoch 734 
2024-11-12 01:40:37.258742: Current learning rate: 0.00304 
2024-11-12 01:41:16.904554: train_loss -0.9453 
2024-11-12 01:41:16.909470: val_loss -0.8322 
2024-11-12 01:41:16.911841: Pseudo dice [np.float32(0.9647), np.float32(0.8413)] 
2024-11-12 01:41:16.914228: Epoch time: 39.66 s 
2024-11-12 01:41:18.097354:  
2024-11-12 01:41:18.103179: Epoch 735 
2024-11-12 01:41:18.105736: Current learning rate: 0.00303 
2024-11-12 01:41:57.744881: train_loss -0.9472 
2024-11-12 01:41:57.748312: val_loss -0.7668 
2024-11-12 01:41:57.750948: Pseudo dice [np.float32(0.9583), np.float32(0.7861)] 
2024-11-12 01:41:57.753099: Epoch time: 39.65 s 
2024-11-12 01:41:58.931407:  
2024-11-12 01:41:58.933872: Epoch 736 
2024-11-12 01:41:58.941638: Current learning rate: 0.00302 
2024-11-12 01:42:38.582406: train_loss -0.9429 
2024-11-12 01:42:38.587166: val_loss -0.8486 
2024-11-12 01:42:38.589557: Pseudo dice [np.float32(0.9602), np.float32(0.8234)] 
2024-11-12 01:42:38.591853: Epoch time: 39.65 s 
2024-11-12 01:42:39.776181:  
2024-11-12 01:42:39.781689: Epoch 737 
2024-11-12 01:42:39.784370: Current learning rate: 0.00301 
2024-11-12 01:43:19.427868: train_loss -0.941 
2024-11-12 01:43:19.431013: val_loss -0.8403 
2024-11-12 01:43:19.433237: Pseudo dice [np.float32(0.9585), np.float32(0.8251)] 
2024-11-12 01:43:19.435410: Epoch time: 39.65 s 
2024-11-12 01:43:20.617590:  
2024-11-12 01:43:20.620171: Epoch 738 
2024-11-12 01:43:20.628608: Current learning rate: 0.003 
2024-11-12 01:44:00.264023: train_loss -0.9467 
2024-11-12 01:44:00.268688: val_loss -0.8432 
2024-11-12 01:44:00.270986: Pseudo dice [np.float32(0.9589), np.float32(0.822)] 
2024-11-12 01:44:00.273192: Epoch time: 39.65 s 
2024-11-12 01:44:01.451838:  
2024-11-12 01:44:01.468370: Epoch 739 
2024-11-12 01:44:01.476939: Current learning rate: 0.00299 
2024-11-12 01:44:41.103151: train_loss -0.9486 
2024-11-12 01:44:41.107025: val_loss -0.8331 
2024-11-12 01:44:41.109539: Pseudo dice [np.float32(0.9595), np.float32(0.8032)] 
2024-11-12 01:44:41.111963: Epoch time: 39.65 s 
2024-11-12 01:44:42.290995:  
2024-11-12 01:44:42.296872: Epoch 740 
2024-11-12 01:44:42.299487: Current learning rate: 0.00297 
2024-11-12 01:45:21.923553: train_loss -0.9529 
2024-11-12 01:45:21.928266: val_loss -0.8629 
2024-11-12 01:45:21.930486: Pseudo dice [np.float32(0.9635), np.float32(0.8475)] 
2024-11-12 01:45:21.932721: Epoch time: 39.63 s 
2024-11-12 01:45:23.132657:  
2024-11-12 01:45:23.135375: Epoch 741 
2024-11-12 01:45:23.143959: Current learning rate: 0.00296 
2024-11-12 01:46:02.771461: train_loss -0.9503 
2024-11-12 01:46:02.775010: val_loss -0.857 
2024-11-12 01:46:02.777880: Pseudo dice [np.float32(0.9627), np.float32(0.8267)] 
2024-11-12 01:46:02.780245: Epoch time: 39.64 s 
2024-11-12 01:46:03.962319:  
2024-11-12 01:46:03.964984: Epoch 742 
2024-11-12 01:46:03.973695: Current learning rate: 0.00295 
2024-11-12 01:46:43.608506: train_loss -0.9517 
2024-11-12 01:46:43.613364: val_loss -0.8302 
2024-11-12 01:46:43.615734: Pseudo dice [np.float32(0.9602), np.float32(0.795)] 
2024-11-12 01:46:43.618375: Epoch time: 39.65 s 
2024-11-12 01:46:44.796653:  
2024-11-12 01:46:44.804159: Epoch 743 
2024-11-12 01:46:44.806728: Current learning rate: 0.00294 
2024-11-12 01:47:24.438072: train_loss -0.9518 
2024-11-12 01:47:24.441574: val_loss -0.8092 
2024-11-12 01:47:24.444054: Pseudo dice [np.float32(0.9596), np.float32(0.7832)] 
2024-11-12 01:47:24.446265: Epoch time: 39.64 s 
2024-11-12 01:47:25.626122:  
2024-11-12 01:47:25.628751: Epoch 744 
2024-11-12 01:47:25.635512: Current learning rate: 0.00293 
2024-11-12 01:48:05.267041: train_loss -0.9442 
2024-11-12 01:48:05.271508: val_loss -0.8138 
2024-11-12 01:48:05.273793: Pseudo dice [np.float32(0.9553), np.float32(0.7747)] 
2024-11-12 01:48:05.276105: Epoch time: 39.64 s 
2024-11-12 01:48:06.777980:  
2024-11-12 01:48:06.780692: Epoch 745 
2024-11-12 01:48:06.788815: Current learning rate: 0.00292 
2024-11-12 01:48:46.433183: train_loss -0.9495 
2024-11-12 01:48:46.436575: val_loss -0.8231 
2024-11-12 01:48:46.439075: Pseudo dice [np.float32(0.9625), np.float32(0.8107)] 
2024-11-12 01:48:46.441922: Epoch time: 39.66 s 
2024-11-12 01:48:47.622734:  
2024-11-12 01:48:47.628129: Epoch 746 
2024-11-12 01:48:47.630497: Current learning rate: 0.00291 
2024-11-12 01:49:27.261048: train_loss -0.9516 
2024-11-12 01:49:27.265599: val_loss -0.7901 
2024-11-12 01:49:27.268110: Pseudo dice [np.float32(0.956), np.float32(0.7757)] 
2024-11-12 01:49:27.270561: Epoch time: 39.64 s 
2024-11-12 01:49:28.457001:  
2024-11-12 01:49:28.459420: Epoch 747 
2024-11-12 01:49:28.467719: Current learning rate: 0.0029 
2024-11-12 01:50:08.129211: train_loss -0.943 
2024-11-12 01:50:08.132686: val_loss -0.7793 
2024-11-12 01:50:08.134969: Pseudo dice [np.float32(0.9591), np.float32(0.7099)] 
2024-11-12 01:50:08.137621: Epoch time: 39.67 s 
2024-11-12 01:50:09.316124:  
2024-11-12 01:50:09.321907: Epoch 748 
2024-11-12 01:50:09.324459: Current learning rate: 0.00289 
2024-11-12 01:50:48.958912: train_loss -0.9471 
2024-11-12 01:50:48.963550: val_loss -0.8511 
2024-11-12 01:50:48.966218: Pseudo dice [np.float32(0.964), np.float32(0.852)] 
2024-11-12 01:50:48.968729: Epoch time: 39.64 s 
2024-11-12 01:50:50.147826:  
2024-11-12 01:50:50.150339: Epoch 749 
2024-11-12 01:50:50.152869: Current learning rate: 0.00288 
2024-11-12 01:51:29.786733: train_loss -0.9442 
2024-11-12 01:51:29.790115: val_loss -0.8439 
2024-11-12 01:51:29.792503: Pseudo dice [np.float32(0.9601), np.float32(0.819)] 
2024-11-12 01:51:29.794755: Epoch time: 39.64 s 
2024-11-12 01:51:31.741882:  
2024-11-12 01:51:31.744308: Epoch 750 
2024-11-12 01:51:31.746698: Current learning rate: 0.00287 
2024-11-12 01:52:11.375839: train_loss -0.9294 
2024-11-12 01:52:11.380455: val_loss -0.7702 
2024-11-12 01:52:11.382759: Pseudo dice [np.float32(0.9599), np.float32(0.6787)] 
2024-11-12 01:52:11.385092: Epoch time: 39.64 s 
2024-11-12 01:52:12.563388:  
2024-11-12 01:52:12.565893: Epoch 751 
2024-11-12 01:52:12.568562: Current learning rate: 0.00286 
2024-11-12 01:52:52.209374: train_loss -0.9222 
2024-11-12 01:52:52.212955: val_loss -0.8295 
2024-11-12 01:52:52.215261: Pseudo dice [np.float32(0.9658), np.float32(0.8115)] 
2024-11-12 01:52:52.218003: Epoch time: 39.65 s 
2024-11-12 01:52:53.398913:  
2024-11-12 01:52:53.401358: Epoch 752 
2024-11-12 01:52:53.408672: Current learning rate: 0.00285 
2024-11-12 01:53:33.038325: train_loss -0.9437 
2024-11-12 01:53:33.043000: val_loss -0.8069 
2024-11-12 01:53:33.045328: Pseudo dice [np.float32(0.955), np.float32(0.7792)] 
2024-11-12 01:53:33.047483: Epoch time: 39.64 s 
2024-11-12 01:53:34.230966:  
2024-11-12 01:53:34.233337: Epoch 753 
2024-11-12 01:53:34.242552: Current learning rate: 0.00284 
